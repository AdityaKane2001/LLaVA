
================================================= Sat Apr  6 06:59:05 PM UTC 2024 =========================================================

[2024-04-06 14:59:07,745] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 14:59:09,288] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6: setting --include=localhost:6
[2024-04-06 14:59:09,288] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --pretrain_mm_mlp_adapter /data/data1/akane/mve-clip-dino-pretrain/checkpoints/mm_projector.bin --pretrain_resampler /data/data1/akane/mve-clip-dino-pretrain/checkpoints/resampler.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/mve-clip-dino-finetune/checkpoints --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50000 --max_steps 1 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-finetune-7b
[2024-04-06 14:59:11,364] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 14:59:13,102] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6]}
[2024-04-06 14:59:13,102] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-06 14:59:13,102] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-06 14:59:13,102] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-06 14:59:13,102] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6
[2024-04-06 14:59:16,280] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 14:59:17,434] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 14:59:17,434] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-06 14:59:19,104] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]
[2024-04-06 14:59:25,818] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-06 14:59:28,373] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1182, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.41 GiB is free. Including non-PyTorch memory, this process has 75.73 GiB memory in use. Of the allocated memory 72.90 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-04-06 15:00:05,160] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1064436
[2024-04-06 15:00:05,161] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--resampler_grid_size', '24', '--pretrain_mm_mlp_adapter', '/data/data1/akane/mve-clip-dino-pretrain/checkpoints/mm_projector.bin', '--pretrain_resampler', '/data/data1/akane/mve-clip-dino-pretrain/checkpoints/resampler.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/mve-clip-dino-finetune/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--max_steps', '1', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'mve-clip-dino-finetune-7b'] exits with return code = 1
