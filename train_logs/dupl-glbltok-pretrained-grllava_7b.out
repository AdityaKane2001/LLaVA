
[2024-02-25 12:26:29,145] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1968380
[2024-02-25 12:26:29,145] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1968381
[2024-02-25 12:26:29,346] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1968382
[2024-02-25 12:26:29,481] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1968383
[2024-02-25 12:26:29,517] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--mm_vision_use_additional_adapter', 'True', '--mm_vision_use_pretrained_additional_adapter', 'True', '--mm_vision_use_global_tokens', 'True', '--mm_vision_use_granular_tokens', 'False', '--mm_vision_granular_select_layers', '6 12 18', '--mm_vision_granular_tokens_strategy', 'pool', '--mm_vision_granular_tokens_per_layer', '192', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/dupl-glbltok-pretrained-grllava-v1.5-7b/checkpoints/', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '25', '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'dupl-glbltok-pretrained-grllava-7b-it'] exits with return code = 1
[2024-02-25 12:27:50,592] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:27:52,566] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-25 12:27:52,566] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_additional_adapter True --mm_vision_use_pretrained_additional_adapter True --mm_vision_use_global_tokens True --mm_vision_use_granular_tokens False --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/dupl-glbltok-pretrained-grllava-v1.5-7b/checkpoints/ --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 25 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name dupl-glbltok-pretrained-grllava-7b-it
[2024-02-25 12:27:54,531] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:27:57,426] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-25 12:27:57,427] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-25 12:27:57,427] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-25 12:27:57,427] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-25 12:27:57,427] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-25 12:28:02,851] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:28:02,864] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:28:02,892] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:28:02,956] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 12:28:04,027] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 12:28:04,028] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-25 12:28:04,028] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 12:28:04,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 12:28:04,334] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-25 12:28:15,998] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.54s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.54s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.55s/it]
Merged granular config!
Granular tokens config loaded!
self.use_additional_adapter=True
self.use_pretrained_additional_adapter=True
self.use_global_tokens=True
self.use_granular_tokens=False
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.63s/it]self.granular_layers='6 12 18'

self.granular_tokens_per_layer=192
self.granular_tokens_strategy='pool'
Merged granular config!
Granular tokens config loaded!
self.use_additional_adapter=True
self.use_pretrained_additional_adapter=True
self.use_global_tokens=True
self.use_granular_tokens=False
self.granular_layers='6 12 18'
self.granular_tokens_per_layer=192
self.granular_tokens_strategy='pool'
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.71s/it]
Merged granular config!
Granular tokens config loaded!
self.use_additional_adapter=True
self.use_pretrained_additional_adapter=True
self.use_global_tokens=True
self.use_granular_tokens=False
self.granular_layers='6 12 18'
self.granular_tokens_per_layer=192
self.granular_tokens_strategy='pool'
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.80s/it]
Merged granular config!
Granular tokens config loaded!
self.use_additional_adapter=True
self.use_pretrained_additional_adapter=True
self.use_global_tokens=True
self.use_granular_tokens=False
self.granular_layers='6 12 18'
self.granular_tokens_per_layer=192
self.granular_tokens_strategy='pool'
[2024-02-25 12:28:28,232] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
`pretrain_mm_mlp_adapter` is not None, loading weights into adapter
`pretrain_mm_mlp_adapter` is not None, loading weights into adapter
`pretrain_mm_mlp_adapter` is not None, loading weights into adapter
Additional adapter is set to be pretrained, loading pretrained weights to granular adapter
`pretrain_mm_mlp_adapter` is not None, loading weights into adapter
Additional adapter is set to be pretrained, loading pretrained weights to granular adapter
Additional adapter is set to be pretrained, loading pretrained weights to granular adapter
Additional adapter is set to be pretrained, loading pretrained weights to granular adapter
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240225_122857-764y9rsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dupl-glbltok-pretrained-grllava-7b-it
wandb: â­ï¸ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: ðŸš€ View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/764y9rsm
  0%|          | 0/5198 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/5198 [00:33<47:38:47, 33.01s/it]                                                   {'loss': 1.3899, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
  0%|          | 1/5198 [00:33<47:38:47, 33.01s/it]  0%|          | 2/5198 [00:51<35:14:57, 24.42s/it]                                                   {'loss': 1.4141, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
  0%|          | 2/5198 [00:51<35:14:57, 24.42s/it]  0%|          | 3/5198 [01:09<31:03:06, 21.52s/it]                                                   {'loss': 1.3982, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
  0%|          | 3/5198 [01:09<31:03:06, 21.52s/it]  0%|          | 4/5198 [01:27<29:12:18, 20.24s/it]                                                   {'loss': 1.4447, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
  0%|          | 4/5198 [01:27<29:12:18, 20.24s/it]  0%|          | 5/5198 [01:45<27:41:02, 19.19s/it]                                                   {'loss': 1.3982, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
  0%|          | 5/5198 [01:45<27:41:02, 19.19s/it]  0%|          | 6/5198 [02:02<26:53:44, 18.65s/it]                                                   {'loss': 1.4463, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
  0%|          | 6/5198 [02:02<26:53:44, 18.65s/it]  0%|          | 7/5198 [02:19<26:03:12, 18.07s/it]                                                   {'loss': 1.4234, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
  0%|          | 7/5198 [02:19<26:03:12, 18.07s/it]  0%|          | 8/5198 [02:36<25:41:36, 17.82s/it]                                                   {'loss': 1.3734, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
  0%|          | 8/5198 [02:36<25:41:36, 17.82s/it]  0%|          | 9/5198 [02:54<25:27:54, 17.67s/it]                                                   {'loss': 1.4206, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
  0%|          | 9/5198 [02:54<25:27:54, 17.67s/it]  0%|          | 10/5198 [03:11<25:17:09, 17.55s/it]                                                    {'loss': 1.3487, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
  0%|          | 10/5198 [03:11<25:17:09, 17.55s/it]  0%|          | 11/5198 [03:29<25:17:22, 17.55s/it]                                                    {'loss': 1.3754, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
  0%|          | 11/5198 [03:29<25:17:22, 17.55s/it]  0%|          | 12/5198 [03:45<24:51:12, 17.25s/it]                                                    {'loss': 1.3613, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
  0%|          | 12/5198 [03:45<24:51:12, 17.25s/it]  0%|          | 13/5198 [04:03<25:10:17, 17.48s/it]                                                    {'loss': 1.2818, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
  0%|          | 13/5198 [04:03<25:10:17, 17.48s/it]  0%|          | 14/5198 [04:21<25:23:19, 17.63s/it]                                                    {'loss': 1.2221, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
  0%|          | 14/5198 [04:21<25:23:19, 17.63s/it]  0%|          | 15/5198 [04:39<25:32:29, 17.74s/it]                                                    {'loss': 1.2331, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
  0%|          | 15/5198 [04:39<25:32:29, 17.74s/it]  0%|          | 16/5198 [04:56<25:03:49, 17.41s/it]                                                    {'loss': 1.2795, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
  0%|          | 16/5198 [04:56<25:03:49, 17.41s/it]  0%|          | 17/5198 [05:14<25:25:20, 17.66s/it]                                                    {'loss': 1.1887, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
  0%|          | 17/5198 [05:14<25:25:20, 17.66s/it]  0%|          | 18/5198 [05:31<25:08:04, 17.47s/it]                                                    {'loss': 1.1425, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
  0%|          | 18/5198 [05:31<25:08:04, 17.47s/it]  0%|          | 19/5198 [05:49<25:14:23, 17.54s/it]                                                    {'loss': 1.2069, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
  0%|          | 19/5198 [05:49<25:14:23, 17.54s/it]  0%|          | 20/5198 [06:08<25:48:24, 17.94s/it]                                                    {'loss': 1.0967, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
  0%|          | 20/5198 [06:08<25:48:24, 17.94s/it]  0%|          | 21/5198 [06:24<25:08:50, 17.49s/it]                                                    {'loss': 1.1857, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
  0%|          | 21/5198 [06:24<25:08:50, 17.49s/it]  0%|          | 22/5198 [06:43<25:42:50, 17.88s/it]                                                    {'loss': 1.143, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
  0%|          | 22/5198 [06:43<25:42:50, 17.88s/it]  0%|          | 23/5198 [07:00<25:19:52, 17.62s/it]                                                    {'loss': 1.0597, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
  0%|          | 23/5198 [07:00<25:19:52, 17.62s/it]  0%|          | 24/5198 [07:17<25:13:24, 17.55s/it]                                                    {'loss': 1.1605, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
  0%|          | 24/5198 [07:17<25:13:24, 17.55s/it]  0%|          | 25/5198 [07:35<25:28:18, 17.73s/it]                                                    {'loss': 1.113, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.0}
  0%|          | 25/5198 [07:35<25:28:18, 17.73s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  1%|          | 26/5198 [08:51<50:15:31, 34.98s/it]                                                    {'loss': 1.104, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
  1%|          | 26/5198 [08:51<50:15:31, 34.98s/it]  1%|          | 27/5198 [09:08<42:47:48, 29.79s/it]                                                    {'loss': 1.1013, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
  1%|          | 27/5198 [09:08<42:47:48, 29.79s/it]  1%|          | 28/5198 [09:25<37:01:40, 25.78s/it]                                                    {'loss': 1.1004, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
  1%|          | 28/5198 [09:25<37:01:40, 25.78s/it]  1%|          | 29/5198 [09:43<33:50:38, 23.57s/it]                                                    {'loss': 1.0649, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
  1%|          | 29/5198 [09:43<33:50:38, 23.57s/it]  1%|          | 30/5198 [10:01<31:11:43, 21.73s/it]                                                    {'loss': 1.0549, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
  1%|          | 30/5198 [10:01<31:11:43, 21.73s/it]  1%|          | 31/5198 [10:17<28:58:14, 20.18s/it]                                                    {'loss': 1.1011, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
  1%|          | 31/5198 [10:17<28:58:14, 20.18s/it]  1%|          | 32/5198 [10:35<27:48:59, 19.38s/it]                                                    {'loss': 1.0263, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
  1%|          | 32/5198 [10:35<27:48:59, 19.38s/it]  1%|          | 33/5198 [10:53<27:25:36, 19.12s/it]                                                    {'loss': 1.0514, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
  1%|          | 33/5198 [10:53<27:25:36, 19.12s/it]  1%|          | 34/5198 [11:10<26:35:01, 18.53s/it]                                                    {'loss': 1.034, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
  1%|          | 34/5198 [11:10<26:35:01, 18.53s/it]  1%|          | 35/5198 [11:29<26:36:38, 18.55s/it]                                                    {'loss': 1.009, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
  1%|          | 35/5198 [11:29<26:36:38, 18.55s/it]  1%|          | 36/5198 [11:46<26:05:20, 18.19s/it]                                                    {'loss': 1.0343, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
  1%|          | 36/5198 [11:46<26:05:20, 18.19s/it]  1%|          | 37/5198 [12:04<25:45:22, 17.97s/it]                                                    {'loss': 0.2996, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
  1%|          | 37/5198 [12:04<25:45:22, 17.97s/it]  1%|          | 38/5198 [12:23<26:10:37, 18.26s/it]                                                    {'loss': 1.0647, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
  1%|          | 38/5198 [12:23<26:10:37, 18.26s/it]  1%|          | 39/5198 [12:40<25:39:13, 17.90s/it]                                                    {'loss': 1.0567, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 39/5198 [12:40<25:39:13, 17.90s/it]  1%|          | 40/5198 [12:57<25:20:18, 17.68s/it]                                                    {'loss': 1.0271, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
  1%|          | 40/5198 [12:57<25:20:18, 17.68s/it]  1%|          | 41/5198 [13:14<25:05:33, 17.52s/it]                                                    {'loss': 1.0206, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
  1%|          | 41/5198 [13:14<25:05:33, 17.52s/it]  1%|          | 42/5198 [13:31<24:55:12, 17.40s/it]                                                    {'loss': 1.071, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
  1%|          | 42/5198 [13:31<24:55:12, 17.40s/it]  1%|          | 43/5198 [13:48<24:52:48, 17.38s/it]                                                    {'loss': 0.9716, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
  1%|          | 43/5198 [13:48<24:52:48, 17.38s/it]  1%|          | 44/5198 [14:06<25:07:46, 17.55s/it]                                                    {'loss': 1.043, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
  1%|          | 44/5198 [14:06<25:07:46, 17.55s/it]  1%|          | 45/5198 [14:24<25:03:20, 17.50s/it]                                                    {'loss': 1.0439, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
  1%|          | 45/5198 [14:24<25:03:20, 17.50s/it]  1%|          | 46/5198 [14:42<25:26:12, 17.77s/it]                                                    {'loss': 0.997, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
  1%|          | 46/5198 [14:42<25:26:12, 17.77s/it]  1%|          | 47/5198 [15:02<26:12:26, 18.32s/it]                                                    {'loss': 0.9821, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
  1%|          | 47/5198 [15:02<26:12:26, 18.32s/it]  1%|          | 48/5198 [15:19<25:37:20, 17.91s/it]                                                    {'loss': 0.9808, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
  1%|          | 48/5198 [15:19<25:37:20, 17.91s/it]  1%|          | 49/5198 [15:36<25:28:13, 17.81s/it]                                                    {'loss': 0.9821, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
  1%|          | 49/5198 [15:36<25:28:13, 17.81s/it]  1%|          | 50/5198 [15:55<25:39:15, 17.94s/it]                                                    {'loss': 0.9749, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
  1%|          | 50/5198 [15:55<25:39:15, 17.94s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  1%|          | 51/5198 [17:09<49:56:23, 34.93s/it]                                                    {'loss': 1.014, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
  1%|          | 51/5198 [17:09<49:56:23, 34.93s/it]  1%|          | 52/5198 [17:27<42:37:58, 29.82s/it]                                                    {'loss': 1.0147, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
  1%|          | 52/5198 [17:27<42:37:58, 29.82s/it]  1%|          | 53/5198 [17:45<37:28:24, 26.22s/it]                                                    {'loss': 0.9915, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
  1%|          | 53/5198 [17:45<37:28:24, 26.22s/it]  1%|          | 54/5198 [18:03<34:09:35, 23.91s/it]                                                    {'loss': 0.9761, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
  1%|          | 54/5198 [18:03<34:09:35, 23.91s/it]  1%|          | 55/5198 [18:21<31:31:47, 22.07s/it]                                                    {'loss': 1.0093, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
  1%|          | 55/5198 [18:21<31:31:47, 22.07s/it]  1%|          | 56/5198 [18:40<30:04:55, 21.06s/it]                                                    {'loss': 0.3135, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
  1%|          | 56/5198 [18:40<30:04:55, 21.06s/it]  1%|          | 57/5198 [18:57<28:24:24, 19.89s/it]                                                    {'loss': 0.9793, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
  1%|          | 57/5198 [18:57<28:24:24, 19.89s/it]  1%|          | 58/5198 [19:14<27:12:21, 19.05s/it]                                                    {'loss': 1.0139, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
  1%|          | 58/5198 [19:14<27:12:21, 19.05s/it]  1%|          | 59/5198 [19:31<26:27:31, 18.53s/it]                                                    {'loss': 1.0343, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
  1%|          | 59/5198 [19:31<26:27:31, 18.53s/it]  1%|          | 60/5198 [19:50<26:21:14, 18.47s/it]                                                    {'loss': 0.9983, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
  1%|          | 60/5198 [19:50<26:21:14, 18.47s/it]  1%|          | 61/5198 [20:08<26:21:22, 18.47s/it]                                                    {'loss': 0.96, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
  1%|          | 61/5198 [20:08<26:21:22, 18.47s/it]  1%|          | 62/5198 [20:26<25:54:17, 18.16s/it]                                                    {'loss': 0.9752, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
  1%|          | 62/5198 [20:26<25:54:17, 18.16s/it]  1%|          | 63/5198 [20:44<26:02:46, 18.26s/it]                                                    {'loss': 0.9219, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
  1%|          | 63/5198 [20:44<26:02:46, 18.26s/it]  1%|          | 64/5198 [21:03<26:09:26, 18.34s/it]                                                    {'loss': 0.9961, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
  1%|          | 64/5198 [21:03<26:09:26, 18.34s/it]  1%|â–         | 65/5198 [21:20<25:53:54, 18.16s/it]                                                    {'loss': 0.9867, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
  1%|â–         | 65/5198 [21:20<25:53:54, 18.16s/it]  1%|â–         | 66/5198 [21:39<25:55:50, 18.19s/it]                                                    {'loss': 0.9633, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
  1%|â–         | 66/5198 [21:39<25:55:50, 18.19s/it]  1%|â–         | 67/5198 [21:56<25:38:32, 17.99s/it]                                                    {'loss': 0.8896, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
  1%|â–         | 67/5198 [21:56<25:38:32, 17.99s/it]  1%|â–         | 68/5198 [22:14<25:31:33, 17.91s/it]                                                    {'loss': 0.979, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
  1%|â–         | 68/5198 [22:14<25:31:33, 17.91s/it]  1%|â–         | 69/5198 [22:32<25:34:12, 17.95s/it]                                                    {'loss': 0.9692, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
  1%|â–         | 69/5198 [22:32<25:34:12, 17.95s/it]  1%|â–         | 70/5198 [22:50<25:47:31, 18.11s/it]                                                    {'loss': 0.9659, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
  1%|â–         | 70/5198 [22:50<25:47:31, 18.11s/it]  1%|â–         | 71/5198 [23:08<25:44:26, 18.07s/it]                                                    {'loss': 0.9452, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
  1%|â–         | 71/5198 [23:08<25:44:26, 18.07s/it]  1%|â–         | 72/5198 [23:26<25:41:59, 18.05s/it]                                                    {'loss': 0.912, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
  1%|â–         | 72/5198 [23:26<25:41:59, 18.05s/it]  1%|â–         | 73/5198 [23:44<25:25:54, 17.86s/it]                                                    {'loss': 0.9893, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
  1%|â–         | 73/5198 [23:44<25:25:54, 17.86s/it]  1%|â–         | 74/5198 [24:00<24:52:12, 17.47s/it]                                                    {'loss': 0.9424, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
  1%|â–         | 74/5198 [24:00<24:52:12, 17.47s/it]  1%|â–         | 75/5198 [24:18<24:54:45, 17.51s/it]                                                    {'loss': 0.9304, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
  1%|â–         | 75/5198 [24:18<24:54:45, 17.51s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  1%|â–         | 76/5198 [25:35<50:07:38, 35.23s/it]                                                    {'loss': 0.9376, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
  1%|â–         | 76/5198 [25:35<50:07:38, 35.23s/it]  1%|â–         | 77/5198 [25:53<42:42:38, 30.03s/it]                                                    {'loss': 0.9045, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.01}
  1%|â–         | 77/5198 [25:53<42:42:38, 30.03s/it]  2%|â–         | 78/5198 [26:10<37:32:00, 26.39s/it]                                                    {'loss': 0.9742, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 78/5198 [26:10<37:32:00, 26.39s/it]  2%|â–         | 79/5198 [26:28<33:42:44, 23.71s/it]                                                    {'loss': 1.0187, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
  2%|â–         | 79/5198 [26:28<33:42:44, 23.71s/it]  2%|â–         | 80/5198 [26:46<31:16:20, 22.00s/it]                                                    {'loss': 0.9578, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
  2%|â–         | 80/5198 [26:46<31:16:20, 22.00s/it]  2%|â–         | 81/5198 [27:03<29:20:33, 20.64s/it]                                                    {'loss': 1.0046, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
  2%|â–         | 81/5198 [27:03<29:20:33, 20.64s/it]  2%|â–         | 82/5198 [27:21<28:01:32, 19.72s/it]                                                    {'loss': 1.0309, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
  2%|â–         | 82/5198 [27:21<28:01:32, 19.72s/it]  2%|â–         | 83/5198 [27:38<26:48:25, 18.87s/it]                                                    {'loss': 1.0215, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
  2%|â–         | 83/5198 [27:38<26:48:25, 18.87s/it]  2%|â–         | 84/5198 [27:56<26:39:47, 18.77s/it]                                                    {'loss': 0.9676, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
  2%|â–         | 84/5198 [27:56<26:39:47, 18.77s/it]  2%|â–         | 85/5198 [28:14<26:03:35, 18.35s/it]                                                    {'loss': 0.9443, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
  2%|â–         | 85/5198 [28:14<26:03:35, 18.35s/it]  2%|â–         | 86/5198 [28:30<25:16:19, 17.80s/it]                                                    {'loss': 0.9434, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
  2%|â–         | 86/5198 [28:30<25:16:19, 17.80s/it]  2%|â–         | 87/5198 [28:48<25:11:22, 17.74s/it]                                                    {'loss': 0.9978, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
  2%|â–         | 87/5198 [28:48<25:11:22, 17.74s/it]  2%|â–         | 88/5198 [29:06<25:11:25, 17.75s/it]                                                    {'loss': 0.9676, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
  2%|â–         | 88/5198 [29:06<25:11:25, 17.75s/it]  2%|â–         | 89/5198 [29:23<24:52:49, 17.53s/it]                                                    {'loss': 0.2904, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
  2%|â–         | 89/5198 [29:23<24:52:49, 17.53s/it]  2%|â–         | 90/5198 [29:39<24:35:05, 17.33s/it]                                                    {'loss': 0.9098, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
  2%|â–         | 90/5198 [29:39<24:35:05, 17.33s/it]  2%|â–         | 91/5198 [29:58<24:53:13, 17.54s/it]                                                    {'loss': 0.9742, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
  2%|â–         | 91/5198 [29:58<24:53:13, 17.54s/it]  2%|â–         | 92/5198 [30:15<24:54:29, 17.56s/it]                                                    {'loss': 0.9844, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
  2%|â–         | 92/5198 [30:15<24:54:29, 17.56s/it]  2%|â–         | 93/5198 [30:33<24:57:33, 17.60s/it]                                                    {'loss': 0.983, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
  2%|â–         | 93/5198 [30:33<24:57:33, 17.60s/it]  2%|â–         | 94/5198 [30:51<25:19:40, 17.86s/it]                                                    {'loss': 0.9405, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
  2%|â–         | 94/5198 [30:51<25:19:40, 17.86s/it]  2%|â–         | 95/5198 [31:10<25:30:37, 18.00s/it]                                                    {'loss': 0.306, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
  2%|â–         | 95/5198 [31:10<25:30:37, 18.00s/it]  2%|â–         | 96/5198 [31:28<25:33:27, 18.03s/it]                                                    {'loss': 0.9524, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
  2%|â–         | 96/5198 [31:28<25:33:27, 18.03s/it]  2%|â–         | 97/5198 [31:46<25:34:24, 18.05s/it]                                                    {'loss': 0.9552, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
  2%|â–         | 97/5198 [31:46<25:34:24, 18.05s/it]  2%|â–         | 98/5198 [32:04<25:38:44, 18.10s/it]                                                    {'loss': 0.9808, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
  2%|â–         | 98/5198 [32:04<25:38:44, 18.10s/it]  2%|â–         | 99/5198 [32:22<25:40:57, 18.13s/it]                                                    {'loss': 0.8918, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
  2%|â–         | 99/5198 [32:22<25:40:57, 18.13s/it]  2%|â–         | 100/5198 [32:40<25:28:36, 17.99s/it]                                                     {'loss': 0.2922, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
  2%|â–         | 100/5198 [32:40<25:28:36, 17.99s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  2%|â–         | 101/5198 [34:13<57:26:41, 40.57s/it]                                                     {'loss': 0.939, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
  2%|â–         | 101/5198 [34:13<57:26:41, 40.57s/it]  2%|â–         | 102/5198 [34:30<47:11:00, 33.33s/it]                                                     {'loss': 1.0415, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
  2%|â–         | 102/5198 [34:30<47:11:00, 33.33s/it]  2%|â–         | 103/5198 [34:47<40:36:59, 28.70s/it]                                                     {'loss': 0.9837, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
  2%|â–         | 103/5198 [34:47<40:36:59, 28.70s/it]  2%|â–         | 104/5198 [35:05<36:00:31, 25.45s/it]                                                     {'loss': 0.9667, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
  2%|â–         | 104/5198 [35:05<36:00:31, 25.45s/it]  2%|â–         | 105/5198 [35:23<32:40:31, 23.10s/it]                                                     {'loss': 0.2645, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
  2%|â–         | 105/5198 [35:23<32:40:31, 23.10s/it]  2%|â–         | 106/5198 [35:41<30:24:08, 21.49s/it]                                                     {'loss': 0.944, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
  2%|â–         | 106/5198 [35:41<30:24:08, 21.49s/it]  2%|â–         | 107/5198 [35:58<28:44:15, 20.32s/it]                                                     {'loss': 1.0123, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
  2%|â–         | 107/5198 [35:58<28:44:15, 20.32s/it]  2%|â–         | 108/5198 [36:16<27:41:53, 19.59s/it]                                                     {'loss': 0.9411, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
  2%|â–         | 108/5198 [36:16<27:41:53, 19.59s/it]  2%|â–         | 109/5198 [36:33<26:40:31, 18.87s/it]                                                     {'loss': 0.9733, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
  2%|â–         | 109/5198 [36:33<26:40:31, 18.87s/it]  2%|â–         | 110/5198 [36:51<26:04:50, 18.45s/it]                                                     {'loss': 0.961, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
  2%|â–         | 110/5198 [36:51<26:04:50, 18.45s/it]  2%|â–         | 111/5198 [37:08<25:36:20, 18.12s/it]                                                     {'loss': 0.9437, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
  2%|â–         | 111/5198 [37:08<25:36:20, 18.12s/it]  2%|â–         | 112/5198 [37:26<25:24:58, 17.99s/it]                                                     {'loss': 1.0264, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
  2%|â–         | 112/5198 [37:26<25:24:58, 17.99s/it]  2%|â–         | 113/5198 [37:44<25:15:50, 17.89s/it]                                                     {'loss': 0.9332, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
  2%|â–         | 113/5198 [37:44<25:15:50, 17.89s/it]  2%|â–         | 114/5198 [38:01<25:06:42, 17.78s/it]                                                     {'loss': 0.9621, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
  2%|â–         | 114/5198 [38:01<25:06:42, 17.78s/it]  2%|â–         | 115/5198 [38:19<25:05:24, 17.77s/it]                                                     {'loss': 0.956, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
  2%|â–         | 115/5198 [38:19<25:05:24, 17.77s/it]  2%|â–         | 116/5198 [38:36<24:55:25, 17.66s/it]                                                     {'loss': 0.2947, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
  2%|â–         | 116/5198 [38:36<24:55:25, 17.66s/it]  2%|â–         | 117/5198 [38:54<24:57:41, 17.69s/it]                                                     {'loss': 0.9838, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
  2%|â–         | 117/5198 [38:54<24:57:41, 17.69s/it]  2%|â–         | 118/5198 [39:10<24:25:58, 17.31s/it]                                                     {'loss': 0.9506, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
  2%|â–         | 118/5198 [39:10<24:25:58, 17.31s/it]  2%|â–         | 119/5198 [39:27<24:16:18, 17.20s/it]                                                     {'loss': 1.0531, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
  2%|â–         | 119/5198 [39:27<24:16:18, 17.20s/it]  2%|â–         | 120/5198 [39:46<24:45:28, 17.55s/it]                                                     {'loss': 0.9397, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
  2%|â–         | 120/5198 [39:46<24:45:28, 17.55s/it]  2%|â–         | 121/5198 [40:04<25:12:18, 17.87s/it]                                                     {'loss': 0.9902, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
  2%|â–         | 121/5198 [40:04<25:12:18, 17.87s/it]  2%|â–         | 122/5198 [40:25<26:17:05, 18.64s/it]                                                     {'loss': 1.0111, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
  2%|â–         | 122/5198 [40:25<26:17:05, 18.64s/it]  2%|â–         | 123/5198 [40:43<26:05:45, 18.51s/it]                                                     {'loss': 0.9056, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
  2%|â–         | 123/5198 [40:43<26:05:45, 18.51s/it]  2%|â–         | 124/5198 [41:01<25:42:23, 18.24s/it]                                                     {'loss': 0.9428, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
  2%|â–         | 124/5198 [41:01<25:42:23, 18.24s/it]  2%|â–         | 125/5198 [41:19<25:48:01, 18.31s/it]                                                     {'loss': 0.9641, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
  2%|â–         | 125/5198 [41:19<25:48:01, 18.31s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  2%|â–         | 126/5198 [42:44<53:46:52, 38.17s/it]                                                     {'loss': 0.9483, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
  2%|â–         | 126/5198 [42:44<53:46:52, 38.17s/it]  2%|â–         | 127/5198 [43:00<44:37:51, 31.68s/it]                                                     {'loss': 0.9928, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
  2%|â–         | 127/5198 [43:00<44:37:51, 31.68s/it]  2%|â–         | 128/5198 [43:18<38:48:37, 27.56s/it]                                                     {'loss': 0.8547, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
  2%|â–         | 128/5198 [43:18<38:48:37, 27.56s/it]  2%|â–         | 129/5198 [43:36<34:55:56, 24.81s/it]                                                     {'loss': 0.9295, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.02}
  2%|â–         | 129/5198 [43:36<34:55:56, 24.81s/it]  3%|â–Ž         | 130/5198 [43:54<31:42:36, 22.52s/it]                                                     {'loss': 0.9507, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
  3%|â–Ž         | 130/5198 [43:54<31:42:36, 22.52s/it]  3%|â–Ž         | 131/5198 [44:11<29:39:01, 21.07s/it]                                                     {'loss': 0.9682, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
  3%|â–Ž         | 131/5198 [44:11<29:39:01, 21.07s/it]  3%|â–Ž         | 132/5198 [44:30<28:50:02, 20.49s/it]                                                     {'loss': 0.9122, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
  3%|â–Ž         | 132/5198 [44:30<28:50:02, 20.49s/it]  3%|â–Ž         | 133/5198 [44:49<28:05:31, 19.97s/it]                                                     {'loss': 0.912, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
  3%|â–Ž         | 133/5198 [44:49<28:05:31, 19.97s/it]  3%|â–Ž         | 134/5198 [45:07<27:19:47, 19.43s/it]                                                     {'loss': 0.9423, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
  3%|â–Ž         | 134/5198 [45:07<27:19:47, 19.43s/it]  3%|â–Ž         | 135/5198 [45:24<26:16:14, 18.68s/it]                                                     {'loss': 0.9257, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
  3%|â–Ž         | 135/5198 [45:24<26:16:14, 18.68s/it]  3%|â–Ž         | 136/5198 [45:43<26:15:51, 18.68s/it]                                                     {'loss': 0.9046, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
  3%|â–Ž         | 136/5198 [45:43<26:15:51, 18.68s/it]  3%|â–Ž         | 137/5198 [46:01<25:55:36, 18.44s/it]                                                     {'loss': 0.9854, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
  3%|â–Ž         | 137/5198 [46:01<25:55:36, 18.44s/it]  3%|â–Ž         | 138/5198 [46:19<25:38:25, 18.24s/it]                                                     {'loss': 0.9994, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
  3%|â–Ž         | 138/5198 [46:19<25:38:25, 18.24s/it]  3%|â–Ž         | 139/5198 [46:37<25:37:58, 18.24s/it]                                                     {'loss': 0.9267, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
  3%|â–Ž         | 139/5198 [46:37<25:37:58, 18.24s/it]  3%|â–Ž         | 140/5198 [46:54<25:05:59, 17.86s/it]                                                     {'loss': 0.9969, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
  3%|â–Ž         | 140/5198 [46:54<25:05:59, 17.86s/it]  3%|â–Ž         | 141/5198 [47:13<25:34:14, 18.20s/it]                                                     {'loss': 0.9632, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
  3%|â–Ž         | 141/5198 [47:13<25:34:14, 18.20s/it]  3%|â–Ž         | 142/5198 [47:32<25:47:33, 18.36s/it]                                                     {'loss': 0.9665, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
  3%|â–Ž         | 142/5198 [47:32<25:47:33, 18.36s/it]  3%|â–Ž         | 143/5198 [47:49<25:31:56, 18.18s/it]                                                     {'loss': 0.9922, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
  3%|â–Ž         | 143/5198 [47:49<25:31:56, 18.18s/it]  3%|â–Ž         | 144/5198 [48:08<25:33:38, 18.21s/it]                                                     {'loss': 0.9716, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
  3%|â–Ž         | 144/5198 [48:08<25:33:38, 18.21s/it]  3%|â–Ž         | 145/5198 [48:24<24:59:53, 17.81s/it]                                                     {'loss': 0.9555, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
  3%|â–Ž         | 145/5198 [48:24<24:59:53, 17.81s/it]  3%|â–Ž         | 146/5198 [48:43<25:16:24, 18.01s/it]                                                     {'loss': 0.9597, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
  3%|â–Ž         | 146/5198 [48:43<25:16:24, 18.01s/it]  3%|â–Ž         | 147/5198 [49:01<25:04:48, 17.88s/it]                                                     {'loss': 0.9217, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
  3%|â–Ž         | 147/5198 [49:01<25:04:48, 17.88s/it]  3%|â–Ž         | 148/5198 [49:18<24:52:31, 17.73s/it]                                                     {'loss': 0.9297, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
  3%|â–Ž         | 148/5198 [49:18<24:52:31, 17.73s/it]  3%|â–Ž         | 149/5198 [49:35<24:37:48, 17.56s/it]                                                     {'loss': 0.9423, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
  3%|â–Ž         | 149/5198 [49:35<24:37:48, 17.56s/it]  3%|â–Ž         | 150/5198 [49:53<24:37:02, 17.56s/it]                                                     {'loss': 0.9318, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
  3%|â–Ž         | 150/5198 [49:53<24:37:02, 17.56s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  3%|â–Ž         | 151/5198 [51:17<52:39:33, 37.56s/it]                                                     {'loss': 0.9919, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/5198 [51:17<52:39:33, 37.56s/it]  3%|â–Ž         | 152/5198 [51:34<44:08:08, 31.49s/it]                                                     {'loss': 0.2919, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/5198 [51:34<44:08:08, 31.49s/it]  3%|â–Ž         | 153/5198 [51:52<38:18:22, 27.33s/it]                                                     {'loss': 0.9843, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/5198 [51:52<38:18:22, 27.33s/it]  3%|â–Ž         | 154/5198 [52:09<33:59:01, 24.25s/it]                                                     {'loss': 0.8991, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/5198 [52:09<33:59:01, 24.25s/it]  3%|â–Ž         | 155/5198 [52:27<31:22:59, 22.40s/it]                                                     {'loss': 0.9005, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/5198 [52:27<31:22:59, 22.40s/it]  3%|â–Ž         | 156/5198 [52:45<29:20:15, 20.95s/it]                                                     {'loss': 1.0164, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/5198 [52:45<29:20:15, 20.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2090 > 2048). Running this sequence through the model will result in indexing errors
  3%|â–Ž         | 157/5198 [53:02<27:41:30, 19.78s/it]                                                     {'loss': 0.9632, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/5198 [53:02<27:41:30, 19.78s/it]  3%|â–Ž         | 158/5198 [53:19<26:53:47, 19.21s/it]                                                     {'loss': 0.9803, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/5198 [53:19<26:53:47, 19.21s/it]  3%|â–Ž         | 159/5198 [53:38<26:28:45, 18.92s/it]                                                     {'loss': 0.9327, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/5198 [53:38<26:28:45, 18.92s/it]  3%|â–Ž         | 160/5198 [53:56<26:09:29, 18.69s/it]                                                     {'loss': 0.9439, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/5198 [53:56<26:09:29, 18.69s/it]  3%|â–Ž         | 161/5198 [54:14<25:56:50, 18.54s/it]                                                     {'loss': 0.964, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/5198 [54:14<25:56:50, 18.54s/it]  3%|â–Ž         | 162/5198 [54:32<25:33:27, 18.27s/it]                                                     {'loss': 0.9901, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/5198 [54:32<25:33:27, 18.27s/it]  3%|â–Ž         | 163/5198 [54:49<25:16:08, 18.07s/it]                                                     {'loss': 0.9598, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/5198 [54:49<25:16:08, 18.07s/it]  3%|â–Ž         | 164/5198 [55:07<25:06:28, 17.96s/it]                                                     {'loss': 0.3397, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/5198 [55:07<25:06:28, 17.96s/it]  3%|â–Ž         | 165/5198 [55:24<24:48:07, 17.74s/it]                                                     {'loss': 0.9542, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/5198 [55:24<24:48:07, 17.74s/it]  3%|â–Ž         | 166/5198 [55:43<25:09:23, 18.00s/it]                                                     {'loss': 0.9624, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/5198 [55:43<25:09:23, 18.00s/it]  3%|â–Ž         | 167/5198 [56:02<25:26:35, 18.21s/it]                                                     {'loss': 0.9756, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/5198 [56:02<25:26:35, 18.21s/it]  3%|â–Ž         | 168/5198 [56:21<25:50:17, 18.49s/it]                                                     {'loss': 0.9368, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/5198 [56:21<25:50:17, 18.49s/it]  3%|â–Ž         | 169/5198 [56:38<25:27:08, 18.22s/it]                                                     {'loss': 0.9497, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/5198 [56:38<25:27:08, 18.22s/it]  3%|â–Ž         | 170/5198 [56:56<25:13:24, 18.06s/it]                                                     {'loss': 0.9241, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/5198 [56:56<25:13:24, 18.06s/it]  3%|â–Ž         | 171/5198 [57:14<25:15:28, 18.09s/it]                                                     {'loss': 0.8336, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/5198 [57:14<25:15:28, 18.09s/it]  3%|â–Ž         | 172/5198 [57:32<25:11:50, 18.05s/it]                                                     {'loss': 0.9791, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/5198 [57:32<25:11:50, 18.05s/it]  3%|â–Ž         | 173/5198 [57:49<24:48:12, 17.77s/it]                                                     {'loss': 0.9337, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/5198 [57:49<24:48:12, 17.77s/it]  3%|â–Ž         | 174/5198 [58:07<24:48:56, 17.78s/it]                                                     {'loss': 0.9658, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/5198 [58:07<24:48:56, 17.78s/it]  3%|â–Ž         | 175/5198 [58:24<24:22:50, 17.47s/it]                                                     {'loss': 0.9628, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/5198 [58:24<24:22:50, 17.47s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  3%|â–Ž         | 176/5198 [59:48<52:14:56, 37.45s/it]                                                     {'loss': 0.9312, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/5198 [59:48<52:14:56, 37.45s/it]  3%|â–Ž         | 177/5198 [1:00:06<43:57:52, 31.52s/it]                                                       {'loss': 0.9376, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/5198 [1:00:06<43:57:52, 31.52s/it]  3%|â–Ž         | 178/5198 [1:00:23<38:10:59, 27.38s/it]                                                       {'loss': 0.9007, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/5198 [1:00:23<38:10:59, 27.38s/it]  3%|â–Ž         | 179/5198 [1:00:40<33:51:04, 24.28s/it]                                                       {'loss': 0.9693, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/5198 [1:00:40<33:51:04, 24.28s/it]  3%|â–Ž         | 180/5198 [1:00:58<31:02:07, 22.27s/it]                                                       {'loss': 0.3131, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/5198 [1:00:58<31:02:07, 22.27s/it]  3%|â–Ž         | 181/5198 [1:01:16<29:18:48, 21.03s/it]                                                       {'loss': 0.9329, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/5198 [1:01:16<29:18:48, 21.03s/it]  4%|â–Ž         | 182/5198 [1:01:34<28:10:35, 20.22s/it]                                                       {'loss': 1.0079, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
  4%|â–Ž         | 182/5198 [1:01:34<28:10:35, 20.22s/it]  4%|â–Ž         | 183/5198 [1:01:52<27:10:53, 19.51s/it]                                                       {'loss': 0.9283, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
  4%|â–Ž         | 183/5198 [1:01:52<27:10:53, 19.51s/it]  4%|â–Ž         | 184/5198 [1:02:10<26:19:03, 18.90s/it]                                                       {'loss': 0.931, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
  4%|â–Ž         | 184/5198 [1:02:10<26:19:03, 18.90s/it]  4%|â–Ž         | 185/5198 [1:02:28<26:10:55, 18.80s/it]                                                       {'loss': 0.9911, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
  4%|â–Ž         | 185/5198 [1:02:28<26:10:55, 18.80s/it]  4%|â–Ž         | 186/5198 [1:02:46<25:49:08, 18.55s/it]                                                       {'loss': 0.9717, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
  4%|â–Ž         | 186/5198 [1:02:46<25:49:08, 18.55s/it]  4%|â–Ž         | 187/5198 [1:03:04<25:36:14, 18.39s/it]                                                       {'loss': 0.3407, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
  4%|â–Ž         | 187/5198 [1:03:04<25:36:14, 18.39s/it]  4%|â–Ž         | 188/5198 [1:03:21<24:44:00, 17.77s/it]                                                       {'loss': 0.918, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
  4%|â–Ž         | 188/5198 [1:03:21<24:44:00, 17.77s/it]  4%|â–Ž         | 189/5198 [1:03:39<24:57:28, 17.94s/it]                                                       {'loss': 0.8653, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
  4%|â–Ž         | 189/5198 [1:03:39<24:57:28, 17.94s/it]  4%|â–Ž         | 190/5198 [1:03:55<24:24:28, 17.55s/it]                                                       {'loss': 0.3007, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
  4%|â–Ž         | 190/5198 [1:03:55<24:24:28, 17.55s/it]  4%|â–Ž         | 191/5198 [1:04:13<24:35:04, 17.68s/it]                                                       {'loss': 0.9305, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
  4%|â–Ž         | 191/5198 [1:04:13<24:35:04, 17.68s/it]  4%|â–Ž         | 192/5198 [1:04:31<24:22:29, 17.53s/it]                                                       {'loss': 0.9357, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
  4%|â–Ž         | 192/5198 [1:04:31<24:22:29, 17.53s/it]  4%|â–Ž         | 193/5198 [1:04:48<24:14:34, 17.44s/it]                                                       {'loss': 1.029, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
  4%|â–Ž         | 193/5198 [1:04:48<24:14:34, 17.44s/it]  4%|â–Ž         | 194/5198 [1:05:06<24:33:18, 17.67s/it]                                                       {'loss': 0.969, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
  4%|â–Ž         | 194/5198 [1:05:06<24:33:18, 17.67s/it]  4%|â–         | 195/5198 [1:05:24<24:29:38, 17.63s/it]                                                       {'loss': 0.9096, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
  4%|â–         | 195/5198 [1:05:24<24:29:38, 17.63s/it]  4%|â–         | 196/5198 [1:05:42<24:46:15, 17.83s/it]                                                       {'loss': 0.9809, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
  4%|â–         | 196/5198 [1:05:42<24:46:15, 17.83s/it]  4%|â–         | 197/5198 [1:06:01<25:07:55, 18.09s/it]                                                       {'loss': 0.9888, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
  4%|â–         | 197/5198 [1:06:01<25:07:55, 18.09s/it]  4%|â–         | 198/5198 [1:06:19<25:22:14, 18.27s/it]                                                       {'loss': 0.9654, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
  4%|â–         | 198/5198 [1:06:19<25:22:14, 18.27s/it]  4%|â–         | 199/5198 [1:06:37<25:00:55, 18.01s/it]                                                       {'loss': 0.9117, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
  4%|â–         | 199/5198 [1:06:37<25:00:55, 18.01s/it]  4%|â–         | 200/5198 [1:06:55<24:59:45, 18.00s/it]                                                       {'loss': 0.9434, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
  4%|â–         | 200/5198 [1:06:55<24:59:45, 18.00s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  4%|â–         | 201/5198 [1:08:31<57:24:09, 41.35s/it]                                                       {'loss': 0.9915, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
  4%|â–         | 201/5198 [1:08:31<57:24:09, 41.35s/it]  4%|â–         | 202/5198 [1:08:48<47:24:28, 34.16s/it]                                                       {'loss': 0.9928, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
  4%|â–         | 202/5198 [1:08:48<47:24:28, 34.16s/it]  4%|â–         | 203/5198 [1:09:05<40:24:08, 29.12s/it]                                                       {'loss': 0.9721, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
  4%|â–         | 203/5198 [1:09:05<40:24:08, 29.12s/it]  4%|â–         | 204/5198 [1:09:23<35:37:20, 25.68s/it]                                                       {'loss': 0.907, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
  4%|â–         | 204/5198 [1:09:23<35:37:20, 25.68s/it]  4%|â–         | 205/5198 [1:09:40<32:08:06, 23.17s/it]                                                       {'loss': 0.3409, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
  4%|â–         | 205/5198 [1:09:40<32:08:06, 23.17s/it]  4%|â–         | 206/5198 [1:09:57<29:22:09, 21.18s/it]                                                       {'loss': 0.9292, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
  4%|â–         | 206/5198 [1:09:57<29:22:09, 21.18s/it]  4%|â–         | 207/5198 [1:10:15<27:57:25, 20.17s/it]                                                       {'loss': 0.9397, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
  4%|â–         | 207/5198 [1:10:15<27:57:25, 20.17s/it]  4%|â–         | 208/5198 [1:10:33<27:07:38, 19.57s/it]                                                       {'loss': 0.9462, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
  4%|â–         | 208/5198 [1:10:33<27:07:38, 19.57s/it]  4%|â–         | 209/5198 [1:10:51<26:22:02, 19.03s/it]                                                       {'loss': 0.986, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
  4%|â–         | 209/5198 [1:10:51<26:22:02, 19.03s/it]  4%|â–         | 210/5198 [1:11:10<26:22:13, 19.03s/it]                                                       {'loss': 0.9768, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
  4%|â–         | 210/5198 [1:11:10<26:22:13, 19.03s/it]  4%|â–         | 211/5198 [1:11:27<25:54:19, 18.70s/it]                                                       {'loss': 0.9531, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
  4%|â–         | 211/5198 [1:11:27<25:54:19, 18.70s/it]  4%|â–         | 212/5198 [1:11:45<25:24:34, 18.35s/it]                                                       {'loss': 0.8773, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
  4%|â–         | 212/5198 [1:11:45<25:24:34, 18.35s/it]  4%|â–         | 213/5198 [1:12:03<25:15:47, 18.24s/it]                                                       {'loss': 0.9215, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
  4%|â–         | 213/5198 [1:12:03<25:15:47, 18.24s/it]  4%|â–         | 214/5198 [1:12:21<25:09:03, 18.17s/it]                                                       {'loss': 0.9408, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
  4%|â–         | 214/5198 [1:12:21<25:09:03, 18.17s/it]  4%|â–         | 215/5198 [1:12:39<25:16:31, 18.26s/it]                                                       {'loss': 0.9518, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
  4%|â–         | 215/5198 [1:12:39<25:16:31, 18.26s/it]  4%|â–         | 216/5198 [1:13:00<26:07:02, 18.87s/it]                                                       {'loss': 0.967, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
  4%|â–         | 216/5198 [1:13:00<26:07:02, 18.87s/it]  4%|â–         | 217/5198 [1:13:17<25:19:31, 18.30s/it]                                                       {'loss': 0.3074, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
  4%|â–         | 217/5198 [1:13:17<25:19:31, 18.30s/it]  4%|â–         | 218/5198 [1:13:34<24:59:35, 18.07s/it]                                                       {'loss': 0.3351, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
  4%|â–         | 218/5198 [1:13:34<24:59:35, 18.07s/it]  4%|â–         | 219/5198 [1:13:52<24:48:47, 17.94s/it]                                                       {'loss': 0.9979, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
  4%|â–         | 219/5198 [1:13:52<24:48:47, 17.94s/it]  4%|â–         | 220/5198 [1:14:09<24:37:49, 17.81s/it]                                                       {'loss': 0.2995, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
  4%|â–         | 220/5198 [1:14:09<24:37:49, 17.81s/it]  4%|â–         | 221/5198 [1:14:28<24:44:53, 17.90s/it]                                                       {'loss': 0.9477, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
  4%|â–         | 221/5198 [1:14:28<24:44:53, 17.90s/it]  4%|â–         | 222/5198 [1:14:45<24:40:10, 17.85s/it]                                                       {'loss': 0.9394, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
  4%|â–         | 222/5198 [1:14:45<24:40:10, 17.85s/it]  4%|â–         | 223/5198 [1:15:03<24:37:26, 17.82s/it]                                                       {'loss': 1.0234, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
  4%|â–         | 223/5198 [1:15:03<24:37:26, 17.82s/it]  4%|â–         | 224/5198 [1:15:20<24:07:25, 17.46s/it]                                                       {'loss': 0.9724, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
  4%|â–         | 224/5198 [1:15:20<24:07:25, 17.46s/it]  4%|â–         | 225/5198 [1:15:37<24:06:04, 17.45s/it]                                                       {'loss': 0.9362, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
  4%|â–         | 225/5198 [1:15:37<24:06:04, 17.45s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  4%|â–         | 226/5198 [1:17:02<52:12:59, 37.81s/it]                                                       {'loss': 0.2817, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
  4%|â–         | 226/5198 [1:17:02<52:12:59, 37.81s/it]  4%|â–         | 227/5198 [1:17:19<43:34:50, 31.56s/it]                                                       {'loss': 0.9016, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
  4%|â–         | 227/5198 [1:17:19<43:34:50, 31.56s/it]  4%|â–         | 228/5198 [1:17:36<37:34:44, 27.22s/it]                                                       {'loss': 0.9771, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
  4%|â–         | 228/5198 [1:17:36<37:34:44, 27.22s/it]  4%|â–         | 229/5198 [1:17:54<33:29:57, 24.27s/it]                                                       {'loss': 0.921, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
  4%|â–         | 229/5198 [1:17:54<33:29:57, 24.27s/it]  4%|â–         | 230/5198 [1:18:11<30:26:36, 22.06s/it]                                                       {'loss': 0.9132, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
  4%|â–         | 230/5198 [1:18:11<30:26:36, 22.06s/it]  4%|â–         | 231/5198 [1:18:28<28:18:03, 20.51s/it]                                                       {'loss': 0.9189, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
  4%|â–         | 231/5198 [1:18:28<28:18:03, 20.51s/it]  4%|â–         | 232/5198 [1:18:46<27:23:56, 19.86s/it]                                                       {'loss': 0.9242, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
  4%|â–         | 232/5198 [1:18:46<27:23:56, 19.86s/it]  4%|â–         | 233/5198 [1:19:03<26:12:11, 19.00s/it]                                                       {'loss': 0.9406, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.04}
  4%|â–         | 233/5198 [1:19:03<26:12:11, 19.00s/it]  5%|â–         | 234/5198 [1:19:21<25:46:51, 18.70s/it]                                                       {'loss': 0.9258, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
  5%|â–         | 234/5198 [1:19:21<25:46:51, 18.70s/it]  5%|â–         | 235/5198 [1:19:38<24:55:17, 18.08s/it]                                                       {'loss': 0.9336, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
  5%|â–         | 235/5198 [1:19:38<24:55:17, 18.08s/it]  5%|â–         | 236/5198 [1:19:56<24:56:33, 18.10s/it]                                                       {'loss': 0.9912, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
  5%|â–         | 236/5198 [1:19:56<24:56:33, 18.10s/it]  5%|â–         | 237/5198 [1:20:14<24:54:09, 18.07s/it]                                                       {'loss': 0.9696, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
  5%|â–         | 237/5198 [1:20:14<24:54:09, 18.07s/it]  5%|â–         | 238/5198 [1:20:31<24:38:41, 17.89s/it]                                                       {'loss': 0.9747, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
  5%|â–         | 238/5198 [1:20:31<24:38:41, 17.89s/it]  5%|â–         | 239/5198 [1:20:49<24:48:24, 18.01s/it]                                                       {'loss': 0.9469, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
  5%|â–         | 239/5198 [1:20:49<24:48:24, 18.01s/it]  5%|â–         | 240/5198 [1:21:06<24:15:24, 17.61s/it]                                                       {'loss': 0.9865, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
  5%|â–         | 240/5198 [1:21:06<24:15:24, 17.61s/it]  5%|â–         | 241/5198 [1:21:24<24:20:17, 17.68s/it]                                                       {'loss': 0.9576, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
  5%|â–         | 241/5198 [1:21:24<24:20:17, 17.68s/it]  5%|â–         | 242/5198 [1:21:42<24:28:24, 17.78s/it]                                                       {'loss': 0.8638, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
  5%|â–         | 242/5198 [1:21:42<24:28:24, 17.78s/it]  5%|â–         | 243/5198 [1:22:00<24:30:10, 17.80s/it]                                                       {'loss': 0.9055, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
  5%|â–         | 243/5198 [1:22:00<24:30:10, 17.80s/it]  5%|â–         | 244/5198 [1:22:17<24:23:35, 17.73s/it]                                                       {'loss': 0.9287, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
  5%|â–         | 244/5198 [1:22:17<24:23:35, 17.73s/it]  5%|â–         | 245/5198 [1:22:35<24:19:32, 17.68s/it]                                                       {'loss': 0.946, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
  5%|â–         | 245/5198 [1:22:35<24:19:32, 17.68s/it]  5%|â–         | 246/5198 [1:22:52<24:12:22, 17.60s/it]                                                       {'loss': 0.9993, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
  5%|â–         | 246/5198 [1:22:52<24:12:22, 17.60s/it]  5%|â–         | 247/5198 [1:23:10<24:07:52, 17.55s/it]                                                       {'loss': 0.9321, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
  5%|â–         | 247/5198 [1:23:10<24:07:52, 17.55s/it]  5%|â–         | 248/5198 [1:23:28<24:11:42, 17.60s/it]                                                       {'loss': 0.9402, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
  5%|â–         | 248/5198 [1:23:28<24:11:42, 17.60s/it]  5%|â–         | 249/5198 [1:23:46<24:40:46, 17.95s/it]                                                       {'loss': 0.9062, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
  5%|â–         | 249/5198 [1:23:46<24:40:46, 17.95s/it]  5%|â–         | 250/5198 [1:24:04<24:32:12, 17.85s/it]                                                       {'loss': 0.8956, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
  5%|â–         | 250/5198 [1:24:04<24:32:12, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  5%|â–         | 251/5198 [1:25:29<52:07:46, 37.94s/it]                                                       {'loss': 0.3212, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
  5%|â–         | 251/5198 [1:25:29<52:07:46, 37.94s/it]  5%|â–         | 252/5198 [1:25:45<43:19:29, 31.53s/it]                                                       {'loss': 0.9341, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
  5%|â–         | 252/5198 [1:25:45<43:19:29, 31.53s/it]  5%|â–         | 253/5198 [1:26:04<37:52:38, 27.57s/it]                                                       {'loss': 0.9424, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
  5%|â–         | 253/5198 [1:26:04<37:52:38, 27.57s/it]  5%|â–         | 254/5198 [1:26:22<34:03:27, 24.80s/it]                                                       {'loss': 0.9273, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
  5%|â–         | 254/5198 [1:26:22<34:03:27, 24.80s/it]  5%|â–         | 255/5198 [1:26:40<31:16:42, 22.78s/it]                                                       {'loss': 0.9637, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
  5%|â–         | 255/5198 [1:26:40<31:16:42, 22.78s/it]  5%|â–         | 256/5198 [1:26:57<28:48:24, 20.98s/it]                                                       {'loss': 0.905, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
  5%|â–         | 256/5198 [1:26:57<28:48:24, 20.98s/it]  5%|â–         | 257/5198 [1:27:15<27:49:56, 20.28s/it]                                                       {'loss': 0.8623, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
  5%|â–         | 257/5198 [1:27:15<27:49:56, 20.28s/it]  5%|â–         | 258/5198 [1:27:33<26:53:16, 19.59s/it]                                                       {'loss': 0.9451, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
  5%|â–         | 258/5198 [1:27:33<26:53:16, 19.59s/it]  5%|â–         | 259/5198 [1:27:52<26:24:39, 19.25s/it]                                                       {'loss': 0.9841, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
  5%|â–         | 259/5198 [1:27:52<26:24:39, 19.25s/it]  5%|â–Œ         | 260/5198 [1:28:10<25:48:09, 18.81s/it]                                                       {'loss': 0.9404, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
  5%|â–Œ         | 260/5198 [1:28:10<25:48:09, 18.81s/it]  5%|â–Œ         | 261/5198 [1:28:27<25:08:01, 18.33s/it]                                                       {'loss': 0.952, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
  5%|â–Œ         | 261/5198 [1:28:27<25:08:01, 18.33s/it]  5%|â–Œ         | 262/5198 [1:28:45<25:03:58, 18.28s/it]                                                       {'loss': 0.8116, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
  5%|â–Œ         | 262/5198 [1:28:45<25:03:58, 18.28s/it]  5%|â–Œ         | 263/5198 [1:29:02<24:40:43, 18.00s/it]                                                       {'loss': 0.9595, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
  5%|â–Œ         | 263/5198 [1:29:02<24:40:43, 18.00s/it]  5%|â–Œ         | 264/5198 [1:29:20<24:37:00, 17.96s/it]                                                       {'loss': 0.939, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
  5%|â–Œ         | 264/5198 [1:29:20<24:37:00, 17.96s/it]  5%|â–Œ         | 265/5198 [1:29:38<24:18:22, 17.74s/it]                                                       {'loss': 0.9396, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
  5%|â–Œ         | 265/5198 [1:29:38<24:18:22, 17.74s/it]  5%|â–Œ         | 266/5198 [1:29:56<24:24:32, 17.82s/it]                                                       {'loss': 0.9355, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
  5%|â–Œ         | 266/5198 [1:29:56<24:24:32, 17.82s/it]  5%|â–Œ         | 267/5198 [1:30:13<24:26:06, 17.84s/it]                                                       {'loss': 0.9416, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
  5%|â–Œ         | 267/5198 [1:30:13<24:26:06, 17.84s/it]  5%|â–Œ         | 268/5198 [1:30:30<23:58:06, 17.50s/it]                                                       {'loss': 1.0124, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
  5%|â–Œ         | 268/5198 [1:30:30<23:58:06, 17.50s/it]  5%|â–Œ         | 269/5198 [1:30:48<24:04:22, 17.58s/it]                                                       {'loss': 0.8637, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
  5%|â–Œ         | 269/5198 [1:30:48<24:04:22, 17.58s/it]  5%|â–Œ         | 270/5198 [1:31:05<23:46:42, 17.37s/it]                                                       {'loss': 0.9309, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
  5%|â–Œ         | 270/5198 [1:31:05<23:46:42, 17.37s/it]  5%|â–Œ         | 271/5198 [1:31:23<23:58:01, 17.51s/it]                                                       {'loss': 0.9486, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
  5%|â–Œ         | 271/5198 [1:31:23<23:58:01, 17.51s/it]  5%|â–Œ         | 272/5198 [1:31:40<23:44:39, 17.35s/it]                                                       {'loss': 0.9645, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
  5%|â–Œ         | 272/5198 [1:31:40<23:44:39, 17.35s/it]  5%|â–Œ         | 273/5198 [1:31:57<23:36:57, 17.26s/it]                                                       {'loss': 0.9501, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
  5%|â–Œ         | 273/5198 [1:31:57<23:36:57, 17.26s/it]  5%|â–Œ         | 274/5198 [1:32:15<24:03:37, 17.59s/it]                                                       {'loss': 0.9468, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
  5%|â–Œ         | 274/5198 [1:32:15<24:03:37, 17.59s/it]  5%|â–Œ         | 275/5198 [1:32:32<23:59:17, 17.54s/it]                                                       {'loss': 0.9607, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
  5%|â–Œ         | 275/5198 [1:32:32<23:59:17, 17.54s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  5%|â–Œ         | 276/5198 [1:33:57<51:19:02, 37.53s/it]                                                       {'loss': 0.9304, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
  5%|â–Œ         | 276/5198 [1:33:57<51:19:02, 37.53s/it]  5%|â–Œ         | 277/5198 [1:34:14<43:10:47, 31.59s/it]                                                       {'loss': 0.968, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
  5%|â–Œ         | 277/5198 [1:34:14<43:10:47, 31.59s/it]  5%|â–Œ         | 278/5198 [1:34:32<37:28:25, 27.42s/it]                                                       {'loss': 0.9566, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
  5%|â–Œ         | 278/5198 [1:34:32<37:28:25, 27.42s/it]  5%|â–Œ         | 279/5198 [1:34:50<33:34:02, 24.57s/it]                                                       {'loss': 0.9594, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
  5%|â–Œ         | 279/5198 [1:34:50<33:34:02, 24.57s/it]  5%|â–Œ         | 280/5198 [1:35:08<30:53:03, 22.61s/it]                                                       {'loss': 0.9605, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
  5%|â–Œ         | 280/5198 [1:35:08<30:53:03, 22.61s/it]  5%|â–Œ         | 281/5198 [1:35:26<29:11:13, 21.37s/it]                                                       {'loss': 0.9318, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
  5%|â–Œ         | 281/5198 [1:35:26<29:11:13, 21.37s/it]  5%|â–Œ         | 282/5198 [1:35:45<28:03:32, 20.55s/it]                                                       {'loss': 0.9339, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
  5%|â–Œ         | 282/5198 [1:35:45<28:03:32, 20.55s/it]  5%|â–Œ         | 283/5198 [1:36:03<27:08:25, 19.88s/it]                                                       {'loss': 0.9177, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
  5%|â–Œ         | 283/5198 [1:36:03<27:08:25, 19.88s/it]  5%|â–Œ         | 284/5198 [1:36:22<26:26:48, 19.37s/it]                                                       {'loss': 0.9737, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
  5%|â–Œ         | 284/5198 [1:36:22<26:26:48, 19.37s/it]  5%|â–Œ         | 285/5198 [1:36:39<25:37:36, 18.78s/it]                                                       {'loss': 0.8624, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.05}
  5%|â–Œ         | 285/5198 [1:36:39<25:37:36, 18.78s/it]  6%|â–Œ         | 286/5198 [1:36:57<25:09:37, 18.44s/it]                                                       {'loss': 0.9231, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
  6%|â–Œ         | 286/5198 [1:36:57<25:09:37, 18.44s/it]  6%|â–Œ         | 287/5198 [1:37:14<24:37:10, 18.05s/it]                                                       {'loss': 0.8444, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
  6%|â–Œ         | 287/5198 [1:37:14<24:37:10, 18.05s/it]  6%|â–Œ         | 288/5198 [1:37:32<24:44:23, 18.14s/it]                                                       {'loss': 0.8889, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
  6%|â–Œ         | 288/5198 [1:37:32<24:44:23, 18.14s/it]  6%|â–Œ         | 289/5198 [1:37:50<24:30:59, 17.98s/it]                                                       {'loss': 0.9482, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
  6%|â–Œ         | 289/5198 [1:37:50<24:30:59, 17.98s/it]  6%|â–Œ         | 290/5198 [1:38:08<24:26:00, 17.92s/it]                                                       {'loss': 0.9306, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
  6%|â–Œ         | 290/5198 [1:38:08<24:26:00, 17.92s/it]  6%|â–Œ         | 291/5198 [1:38:25<24:16:13, 17.81s/it]                                                       {'loss': 0.9629, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
  6%|â–Œ         | 291/5198 [1:38:25<24:16:13, 17.81s/it]  6%|â–Œ         | 292/5198 [1:38:42<24:01:41, 17.63s/it]                                                       {'loss': 0.9125, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
  6%|â–Œ         | 292/5198 [1:38:42<24:01:41, 17.63s/it]  6%|â–Œ         | 293/5198 [1:39:00<24:11:51, 17.76s/it]                                                       {'loss': 0.9069, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
  6%|â–Œ         | 293/5198 [1:39:00<24:11:51, 17.76s/it]  6%|â–Œ         | 294/5198 [1:39:17<23:56:01, 17.57s/it]                                                       {'loss': 0.9658, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
  6%|â–Œ         | 294/5198 [1:39:17<23:56:01, 17.57s/it]  6%|â–Œ         | 295/5198 [1:39:36<24:22:54, 17.90s/it]                                                       {'loss': 0.9112, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
  6%|â–Œ         | 295/5198 [1:39:36<24:22:54, 17.90s/it]  6%|â–Œ         | 296/5198 [1:39:54<24:11:47, 17.77s/it]                                                       {'loss': 0.9645, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
  6%|â–Œ         | 296/5198 [1:39:54<24:11:47, 17.77s/it]  6%|â–Œ         | 297/5198 [1:40:11<24:05:56, 17.70s/it]                                                       {'loss': 0.3358, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
  6%|â–Œ         | 297/5198 [1:40:11<24:05:56, 17.70s/it]  6%|â–Œ         | 298/5198 [1:40:29<24:06:22, 17.71s/it]                                                       {'loss': 0.9276, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
  6%|â–Œ         | 298/5198 [1:40:29<24:06:22, 17.71s/it]  6%|â–Œ         | 299/5198 [1:40:46<23:45:03, 17.45s/it]                                                       {'loss': 0.9407, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
  6%|â–Œ         | 299/5198 [1:40:46<23:45:03, 17.45s/it]  6%|â–Œ         | 300/5198 [1:41:03<23:52:30, 17.55s/it]                                                       {'loss': 0.8973, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
  6%|â–Œ         | 300/5198 [1:41:03<23:52:30, 17.55s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  6%|â–Œ         | 301/5198 [1:42:40<55:55:01, 41.11s/it]                                                       {'loss': 0.9707, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
  6%|â–Œ         | 301/5198 [1:42:40<55:55:01, 41.11s/it]  6%|â–Œ         | 302/5198 [1:42:58<46:31:28, 34.21s/it]                                                       {'loss': 0.9903, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
  6%|â–Œ         | 302/5198 [1:42:58<46:31:28, 34.21s/it]  6%|â–Œ         | 303/5198 [1:43:15<39:46:21, 29.25s/it]                                                       {'loss': 0.9091, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
  6%|â–Œ         | 303/5198 [1:43:15<39:46:21, 29.25s/it]  6%|â–Œ         | 304/5198 [1:43:34<35:18:06, 25.97s/it]                                                       {'loss': 0.8867, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
  6%|â–Œ         | 304/5198 [1:43:34<35:18:06, 25.97s/it]  6%|â–Œ         | 305/5198 [1:43:52<32:01:22, 23.56s/it]                                                       {'loss': 0.9455, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
  6%|â–Œ         | 305/5198 [1:43:52<32:01:22, 23.56s/it]  6%|â–Œ         | 306/5198 [1:44:09<29:37:22, 21.80s/it]                                                       {'loss': 0.8745, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
  6%|â–Œ         | 306/5198 [1:44:09<29:37:22, 21.80s/it]  6%|â–Œ         | 307/5198 [1:44:26<27:35:03, 20.30s/it]                                                       {'loss': 0.9712, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
  6%|â–Œ         | 307/5198 [1:44:26<27:35:03, 20.30s/it]  6%|â–Œ         | 308/5198 [1:44:44<26:35:13, 19.57s/it]                                                       {'loss': 0.8866, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
  6%|â–Œ         | 308/5198 [1:44:44<26:35:13, 19.57s/it]  6%|â–Œ         | 309/5198 [1:45:01<25:28:40, 18.76s/it]                                                       {'loss': 0.9217, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
  6%|â–Œ         | 309/5198 [1:45:01<25:28:40, 18.76s/it]  6%|â–Œ         | 310/5198 [1:45:18<24:46:40, 18.25s/it]                                                       {'loss': 0.9335, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
  6%|â–Œ         | 310/5198 [1:45:18<24:46:40, 18.25s/it]  6%|â–Œ         | 311/5198 [1:45:36<24:53:08, 18.33s/it]                                                       {'loss': 0.9149, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
  6%|â–Œ         | 311/5198 [1:45:36<24:53:08, 18.33s/it]  6%|â–Œ         | 312/5198 [1:45:54<24:35:43, 18.12s/it]                                                       {'loss': 0.8655, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
  6%|â–Œ         | 312/5198 [1:45:54<24:35:43, 18.12s/it]  6%|â–Œ         | 313/5198 [1:46:11<24:11:40, 17.83s/it]                                                       {'loss': 0.962, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
  6%|â–Œ         | 313/5198 [1:46:11<24:11:40, 17.83s/it]  6%|â–Œ         | 314/5198 [1:46:28<23:53:23, 17.61s/it]                                                       {'loss': 0.9927, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
  6%|â–Œ         | 314/5198 [1:46:28<23:53:23, 17.61s/it]  6%|â–Œ         | 315/5198 [1:46:46<24:02:21, 17.72s/it]                                                       {'loss': 0.8729, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
  6%|â–Œ         | 315/5198 [1:46:46<24:02:21, 17.72s/it]  6%|â–Œ         | 316/5198 [1:47:04<24:11:58, 17.84s/it]                                                       {'loss': 0.934, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
  6%|â–Œ         | 316/5198 [1:47:04<24:11:58, 17.84s/it]  6%|â–Œ         | 317/5198 [1:47:23<24:25:55, 18.02s/it]                                                       {'loss': 0.9522, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
  6%|â–Œ         | 317/5198 [1:47:23<24:25:55, 18.02s/it]  6%|â–Œ         | 318/5198 [1:47:41<24:19:24, 17.94s/it]                                                       {'loss': 0.939, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
  6%|â–Œ         | 318/5198 [1:47:41<24:19:24, 17.94s/it]  6%|â–Œ         | 319/5198 [1:47:58<24:12:03, 17.86s/it]                                                       {'loss': 0.9263, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
  6%|â–Œ         | 319/5198 [1:47:58<24:12:03, 17.86s/it]  6%|â–Œ         | 320/5198 [1:48:16<24:16:59, 17.92s/it]                                                       {'loss': 0.9142, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
  6%|â–Œ         | 320/5198 [1:48:16<24:16:59, 17.92s/it]  6%|â–Œ         | 321/5198 [1:48:34<24:04:49, 17.78s/it]                                                       {'loss': 0.8992, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
  6%|â–Œ         | 321/5198 [1:48:34<24:04:49, 17.78s/it]  6%|â–Œ         | 322/5198 [1:48:51<23:57:02, 17.68s/it]                                                       {'loss': 0.2809, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
  6%|â–Œ         | 322/5198 [1:48:51<23:57:02, 17.68s/it]  6%|â–Œ         | 323/5198 [1:49:09<24:05:30, 17.79s/it]                                                       {'loss': 1.0135, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
  6%|â–Œ         | 323/5198 [1:49:09<24:05:30, 17.79s/it]  6%|â–Œ         | 324/5198 [1:49:27<23:57:37, 17.70s/it]                                                       {'loss': 0.9443, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
  6%|â–Œ         | 324/5198 [1:49:27<23:57:37, 17.70s/it]  6%|â–‹         | 325/5198 [1:49:44<23:55:44, 17.68s/it]                                                       {'loss': 0.9919, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
  6%|â–‹         | 325/5198 [1:49:44<23:55:44, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  6%|â–‹         | 326/5198 [1:51:11<52:01:57, 38.45s/it]                                                       {'loss': 0.8688, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
  6%|â–‹         | 326/5198 [1:51:11<52:01:57, 38.45s/it]  6%|â–‹         | 327/5198 [1:51:28<43:08:10, 31.88s/it]                                                       {'loss': 0.9824, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
  6%|â–‹         | 327/5198 [1:51:28<43:08:10, 31.88s/it]  6%|â–‹         | 328/5198 [1:51:46<37:41:24, 27.86s/it]                                                       {'loss': 0.9222, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
  6%|â–‹         | 328/5198 [1:51:46<37:41:24, 27.86s/it]  6%|â–‹         | 329/5198 [1:52:03<33:15:39, 24.59s/it]                                                       {'loss': 0.9211, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
  6%|â–‹         | 329/5198 [1:52:03<33:15:39, 24.59s/it]  6%|â–‹         | 330/5198 [1:52:21<30:34:00, 22.60s/it]                                                       {'loss': 0.8864, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
  6%|â–‹         | 330/5198 [1:52:21<30:34:00, 22.60s/it]  6%|â–‹         | 331/5198 [1:52:40<28:47:23, 21.30s/it]                                                       {'loss': 0.9507, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
  6%|â–‹         | 331/5198 [1:52:40<28:47:23, 21.30s/it]  6%|â–‹         | 332/5198 [1:52:57<27:25:23, 20.29s/it]                                                       {'loss': 0.9301, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
  6%|â–‹         | 332/5198 [1:52:57<27:25:23, 20.29s/it]  6%|â–‹         | 333/5198 [1:53:15<26:07:32, 19.33s/it]                                                       {'loss': 0.92, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
  6%|â–‹         | 333/5198 [1:53:15<26:07:32, 19.33s/it]  6%|â–‹         | 334/5198 [1:53:32<25:09:42, 18.62s/it]                                                       {'loss': 0.863, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
  6%|â–‹         | 334/5198 [1:53:32<25:09:42, 18.62s/it]  6%|â–‹         | 335/5198 [1:53:48<24:22:15, 18.04s/it]                                                       {'loss': 0.9344, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
  6%|â–‹         | 335/5198 [1:53:48<24:22:15, 18.04s/it]  6%|â–‹         | 336/5198 [1:54:07<24:40:50, 18.27s/it]                                                       {'loss': 0.9404, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
  6%|â–‹         | 336/5198 [1:54:07<24:40:50, 18.27s/it]  6%|â–‹         | 337/5198 [1:54:24<24:12:22, 17.93s/it]                                                       {'loss': 0.9149, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.06}
  6%|â–‹         | 337/5198 [1:54:24<24:12:22, 17.93s/it]  7%|â–‹         | 338/5198 [1:54:41<23:34:43, 17.47s/it]                                                       {'loss': 0.9389, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
  7%|â–‹         | 338/5198 [1:54:41<23:34:43, 17.47s/it]  7%|â–‹         | 339/5198 [1:54:58<23:22:38, 17.32s/it]                                                       {'loss': 0.9589, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
  7%|â–‹         | 339/5198 [1:54:58<23:22:38, 17.32s/it]  7%|â–‹         | 340/5198 [1:55:16<23:42:37, 17.57s/it]                                                       {'loss': 0.9226, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
  7%|â–‹         | 340/5198 [1:55:16<23:42:37, 17.57s/it]  7%|â–‹         | 341/5198 [1:55:33<23:34:19, 17.47s/it]                                                       {'loss': 0.9553, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
  7%|â–‹         | 341/5198 [1:55:33<23:34:19, 17.47s/it]  7%|â–‹         | 342/5198 [1:55:51<23:53:55, 17.72s/it]                                                       {'loss': 0.8732, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
  7%|â–‹         | 342/5198 [1:55:51<23:53:55, 17.72s/it]  7%|â–‹         | 343/5198 [1:56:09<23:43:17, 17.59s/it]                                                       {'loss': 0.9744, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
  7%|â–‹         | 343/5198 [1:56:09<23:43:17, 17.59s/it]  7%|â–‹         | 344/5198 [1:56:26<23:51:41, 17.70s/it]                                                       {'loss': 0.908, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
  7%|â–‹         | 344/5198 [1:56:26<23:51:41, 17.70s/it]  7%|â–‹         | 345/5198 [1:56:44<23:43:29, 17.60s/it]                                                       {'loss': 0.9136, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
  7%|â–‹         | 345/5198 [1:56:44<23:43:29, 17.60s/it]  7%|â–‹         | 346/5198 [1:57:02<23:46:27, 17.64s/it]                                                       {'loss': 0.9275, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
  7%|â–‹         | 346/5198 [1:57:02<23:46:27, 17.64s/it]  7%|â–‹         | 347/5198 [1:57:19<23:38:25, 17.54s/it]                                                       {'loss': 0.9602, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
  7%|â–‹         | 347/5198 [1:57:19<23:38:25, 17.54s/it]  7%|â–‹         | 348/5198 [1:57:36<23:35:48, 17.52s/it]                                                       {'loss': 0.9195, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
  7%|â–‹         | 348/5198 [1:57:36<23:35:48, 17.52s/it]  7%|â–‹         | 349/5198 [1:57:55<23:58:39, 17.80s/it]                                                       {'loss': 0.9461, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
  7%|â–‹         | 349/5198 [1:57:55<23:58:39, 17.80s/it]  7%|â–‹         | 350/5198 [1:58:13<23:59:33, 17.82s/it]                                                       {'loss': 0.8753, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
  7%|â–‹         | 350/5198 [1:58:13<23:59:33, 17.82s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  7%|â–‹         | 351/5198 [1:59:38<51:10:25, 38.01s/it]                                                       {'loss': 0.9173, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
  7%|â–‹         | 351/5198 [1:59:38<51:10:25, 38.01s/it]  7%|â–‹         | 352/5198 [1:59:55<42:54:08, 31.87s/it]                                                       {'loss': 0.3242, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
  7%|â–‹         | 352/5198 [1:59:55<42:54:08, 31.87s/it]  7%|â–‹         | 353/5198 [2:00:13<36:59:29, 27.49s/it]                                                       {'loss': 0.8961, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
  7%|â–‹         | 353/5198 [2:00:13<36:59:29, 27.49s/it]  7%|â–‹         | 354/5198 [2:00:31<33:22:53, 24.81s/it]                                                       {'loss': 0.9761, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
  7%|â–‹         | 354/5198 [2:00:31<33:22:53, 24.81s/it]  7%|â–‹         | 355/5198 [2:00:49<30:22:49, 22.58s/it]                                                       {'loss': 0.9073, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
  7%|â–‹         | 355/5198 [2:00:49<30:22:49, 22.58s/it]  7%|â–‹         | 356/5198 [2:01:07<28:42:23, 21.34s/it]                                                       {'loss': 0.916, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
  7%|â–‹         | 356/5198 [2:01:07<28:42:23, 21.34s/it]  7%|â–‹         | 357/5198 [2:01:26<27:34:39, 20.51s/it]                                                       {'loss': 0.9104, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
  7%|â–‹         | 357/5198 [2:01:26<27:34:39, 20.51s/it]  7%|â–‹         | 358/5198 [2:01:44<26:33:27, 19.75s/it]                                                       {'loss': 0.9833, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
  7%|â–‹         | 358/5198 [2:01:44<26:33:27, 19.75s/it]  7%|â–‹         | 359/5198 [2:02:01<25:45:16, 19.16s/it]                                                       {'loss': 0.9873, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
  7%|â–‹         | 359/5198 [2:02:01<25:45:16, 19.16s/it]  7%|â–‹         | 360/5198 [2:02:18<24:47:38, 18.45s/it]                                                       {'loss': 0.9263, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
  7%|â–‹         | 360/5198 [2:02:18<24:47:38, 18.45s/it]  7%|â–‹         | 361/5198 [2:02:36<24:34:43, 18.29s/it]                                                       {'loss': 0.9306, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
  7%|â–‹         | 361/5198 [2:02:36<24:34:43, 18.29s/it]  7%|â–‹         | 362/5198 [2:02:54<24:14:56, 18.05s/it]                                                       {'loss': 0.8968, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
  7%|â–‹         | 362/5198 [2:02:54<24:14:56, 18.05s/it]  7%|â–‹         | 363/5198 [2:03:11<23:53:13, 17.79s/it]                                                       {'loss': 0.9237, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
  7%|â–‹         | 363/5198 [2:03:11<23:53:13, 17.79s/it]  7%|â–‹         | 364/5198 [2:03:28<23:52:27, 17.78s/it]                                                       {'loss': 0.9082, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
  7%|â–‹         | 364/5198 [2:03:28<23:52:27, 17.78s/it]  7%|â–‹         | 365/5198 [2:03:46<23:51:52, 17.78s/it]                                                       {'loss': 0.9052, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
  7%|â–‹         | 365/5198 [2:03:46<23:51:52, 17.78s/it]  7%|â–‹         | 366/5198 [2:04:04<23:48:04, 17.73s/it]                                                       {'loss': 0.9365, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
  7%|â–‹         | 366/5198 [2:04:04<23:48:04, 17.73s/it]  7%|â–‹         | 367/5198 [2:04:22<23:50:07, 17.76s/it]                                                       {'loss': 0.9621, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
  7%|â–‹         | 367/5198 [2:04:22<23:50:07, 17.76s/it]  7%|â–‹         | 368/5198 [2:04:40<24:03:43, 17.93s/it]                                                       {'loss': 0.9097, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
  7%|â–‹         | 368/5198 [2:04:40<24:03:43, 17.93s/it]  7%|â–‹         | 369/5198 [2:04:58<24:00:44, 17.90s/it]                                                       {'loss': 0.9277, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
  7%|â–‹         | 369/5198 [2:04:58<24:00:44, 17.90s/it]  7%|â–‹         | 370/5198 [2:05:16<24:11:03, 18.03s/it]                                                       {'loss': 0.9617, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
  7%|â–‹         | 370/5198 [2:05:16<24:11:03, 18.03s/it]  7%|â–‹         | 371/5198 [2:05:34<23:59:43, 17.90s/it]                                                       {'loss': 0.9304, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
  7%|â–‹         | 371/5198 [2:05:34<23:59:43, 17.90s/it]  7%|â–‹         | 372/5198 [2:05:51<23:47:00, 17.74s/it]                                                       {'loss': 0.9272, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
  7%|â–‹         | 372/5198 [2:05:51<23:47:00, 17.74s/it]  7%|â–‹         | 373/5198 [2:06:09<23:44:52, 17.72s/it]                                                       {'loss': 0.9045, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
  7%|â–‹         | 373/5198 [2:06:09<23:44:52, 17.72s/it]  7%|â–‹         | 374/5198 [2:06:27<24:00:12, 17.91s/it]                                                       {'loss': 0.9466, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
  7%|â–‹         | 374/5198 [2:06:27<24:00:12, 17.91s/it]  7%|â–‹         | 375/5198 [2:06:45<24:02:28, 17.94s/it]                                                       {'loss': 0.9029, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
  7%|â–‹         | 375/5198 [2:06:45<24:02:28, 17.94s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  7%|â–‹         | 376/5198 [2:08:09<50:26:56, 37.66s/it]                                                       {'loss': 0.893, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
  7%|â–‹         | 376/5198 [2:08:09<50:26:56, 37.66s/it]  7%|â–‹         | 377/5198 [2:08:26<42:08:10, 31.46s/it]                                                       {'loss': 0.9716, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
  7%|â–‹         | 377/5198 [2:08:26<42:08:10, 31.46s/it]  7%|â–‹         | 378/5198 [2:08:45<36:59:45, 27.63s/it]                                                       {'loss': 0.885, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
  7%|â–‹         | 378/5198 [2:08:45<36:59:45, 27.63s/it]  7%|â–‹         | 379/5198 [2:09:02<32:58:46, 24.64s/it]                                                       {'loss': 0.9174, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
  7%|â–‹         | 379/5198 [2:09:02<32:58:46, 24.64s/it]  7%|â–‹         | 380/5198 [2:09:19<30:00:23, 22.42s/it]                                                       {'loss': 0.9061, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
  7%|â–‹         | 380/5198 [2:09:19<30:00:23, 22.42s/it]  7%|â–‹         | 381/5198 [2:09:36<27:47:54, 20.78s/it]                                                       {'loss': 0.9298, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
  7%|â–‹         | 381/5198 [2:09:36<27:47:54, 20.78s/it]  7%|â–‹         | 382/5198 [2:09:53<26:11:01, 19.57s/it]                                                       {'loss': 0.9107, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
  7%|â–‹         | 382/5198 [2:09:53<26:11:01, 19.57s/it]  7%|â–‹         | 383/5198 [2:10:12<25:47:08, 19.28s/it]                                                       {'loss': 0.9123, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
  7%|â–‹         | 383/5198 [2:10:12<25:47:08, 19.28s/it]  7%|â–‹         | 384/5198 [2:10:29<25:03:48, 18.74s/it]                                                       {'loss': 0.8978, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
  7%|â–‹         | 384/5198 [2:10:29<25:03:48, 18.74s/it]  7%|â–‹         | 385/5198 [2:10:47<24:36:21, 18.40s/it]                                                       {'loss': 0.9102, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
  7%|â–‹         | 385/5198 [2:10:47<24:36:21, 18.40s/it]  7%|â–‹         | 386/5198 [2:11:05<24:32:15, 18.36s/it]                                                       {'loss': 0.9367, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
  7%|â–‹         | 386/5198 [2:11:05<24:32:15, 18.36s/it]  7%|â–‹         | 387/5198 [2:11:22<23:59:19, 17.95s/it]                                                       {'loss': 0.9392, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
  7%|â–‹         | 387/5198 [2:11:22<23:59:19, 17.95s/it]  7%|â–‹         | 388/5198 [2:11:39<23:38:24, 17.69s/it]                                                       {'loss': 0.8973, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
  7%|â–‹         | 388/5198 [2:11:39<23:38:24, 17.69s/it]  7%|â–‹         | 389/5198 [2:11:56<23:17:42, 17.44s/it]                                                       {'loss': 0.9366, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.07}
  7%|â–‹         | 389/5198 [2:11:56<23:17:42, 17.44s/it]  8%|â–Š         | 390/5198 [2:12:13<23:17:34, 17.44s/it]                                                       {'loss': 0.9366, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
  8%|â–Š         | 390/5198 [2:12:13<23:17:34, 17.44s/it]  8%|â–Š         | 391/5198 [2:12:31<23:28:51, 17.59s/it]                                                       {'loss': 0.9449, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
  8%|â–Š         | 391/5198 [2:12:31<23:28:51, 17.59s/it]  8%|â–Š         | 392/5198 [2:12:49<23:35:14, 17.67s/it]                                                       {'loss': 0.9681, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
  8%|â–Š         | 392/5198 [2:12:49<23:35:14, 17.67s/it]  8%|â–Š         | 393/5198 [2:13:07<23:37:04, 17.69s/it]                                                       {'loss': 0.9263, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
  8%|â–Š         | 393/5198 [2:13:07<23:37:04, 17.69s/it]  8%|â–Š         | 394/5198 [2:13:24<23:18:13, 17.46s/it]                                                       {'loss': 0.96, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
  8%|â–Š         | 394/5198 [2:13:24<23:18:13, 17.46s/it]  8%|â–Š         | 395/5198 [2:13:42<23:39:44, 17.74s/it]                                                       {'loss': 0.929, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
  8%|â–Š         | 395/5198 [2:13:42<23:39:44, 17.74s/it]  8%|â–Š         | 396/5198 [2:13:59<23:23:59, 17.54s/it]                                                       {'loss': 0.956, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
  8%|â–Š         | 396/5198 [2:13:59<23:23:59, 17.54s/it]  8%|â–Š         | 397/5198 [2:14:17<23:27:56, 17.60s/it]                                                       {'loss': 0.8922, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
  8%|â–Š         | 397/5198 [2:14:17<23:27:56, 17.60s/it]  8%|â–Š         | 398/5198 [2:14:34<23:19:09, 17.49s/it]                                                       {'loss': 0.964, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
  8%|â–Š         | 398/5198 [2:14:34<23:19:09, 17.49s/it]  8%|â–Š         | 399/5198 [2:14:53<23:39:05, 17.74s/it]                                                       {'loss': 0.943, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
  8%|â–Š         | 399/5198 [2:14:53<23:39:05, 17.74s/it]  8%|â–Š         | 400/5198 [2:15:11<23:46:05, 17.83s/it]                                                       {'loss': 0.8812, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
  8%|â–Š         | 400/5198 [2:15:11<23:46:05, 17.83s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  8%|â–Š         | 401/5198 [2:16:45<54:10:57, 40.66s/it]                                                       {'loss': 0.9001, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
  8%|â–Š         | 401/5198 [2:16:45<54:10:57, 40.66s/it]  8%|â–Š         | 402/5198 [2:17:02<45:00:30, 33.78s/it]                                                       {'loss': 0.9197, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
  8%|â–Š         | 402/5198 [2:17:02<45:00:30, 33.78s/it]  8%|â–Š         | 403/5198 [2:17:20<38:34:48, 28.97s/it]                                                       {'loss': 0.9507, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
  8%|â–Š         | 403/5198 [2:17:20<38:34:48, 28.97s/it]  8%|â–Š         | 404/5198 [2:17:37<33:50:22, 25.41s/it]                                                       {'loss': 0.939, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
  8%|â–Š         | 404/5198 [2:17:37<33:50:22, 25.41s/it]  8%|â–Š         | 405/5198 [2:17:55<30:50:23, 23.16s/it]                                                       {'loss': 0.954, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
  8%|â–Š         | 405/5198 [2:17:55<30:50:23, 23.16s/it]  8%|â–Š         | 406/5198 [2:18:13<28:33:42, 21.46s/it]                                                       {'loss': 0.9608, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
  8%|â–Š         | 406/5198 [2:18:13<28:33:42, 21.46s/it]  8%|â–Š         | 407/5198 [2:18:30<26:55:23, 20.23s/it]                                                       {'loss': 0.9027, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
  8%|â–Š         | 407/5198 [2:18:30<26:55:23, 20.23s/it]  8%|â–Š         | 408/5198 [2:18:48<26:01:20, 19.56s/it]                                                       {'loss': 0.8959, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
  8%|â–Š         | 408/5198 [2:18:48<26:01:20, 19.56s/it]  8%|â–Š         | 409/5198 [2:19:06<25:13:19, 18.96s/it]                                                       {'loss': 0.2937, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
  8%|â–Š         | 409/5198 [2:19:06<25:13:19, 18.96s/it]  8%|â–Š         | 410/5198 [2:19:23<24:36:06, 18.50s/it]                                                       {'loss': 0.3068, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
  8%|â–Š         | 410/5198 [2:19:23<24:36:06, 18.50s/it]  8%|â–Š         | 411/5198 [2:19:41<24:14:05, 18.23s/it]                                                       {'loss': 0.8987, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
  8%|â–Š         | 411/5198 [2:19:41<24:14:05, 18.23s/it]  8%|â–Š         | 412/5198 [2:19:58<23:56:18, 18.01s/it]                                                       {'loss': 0.9325, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
  8%|â–Š         | 412/5198 [2:19:58<23:56:18, 18.01s/it]  8%|â–Š         | 413/5198 [2:20:15<23:33:20, 17.72s/it]                                                       {'loss': 0.9186, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
  8%|â–Š         | 413/5198 [2:20:15<23:33:20, 17.72s/it]  8%|â–Š         | 414/5198 [2:20:33<23:38:07, 17.79s/it]                                                       {'loss': 0.9286, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
  8%|â–Š         | 414/5198 [2:20:33<23:38:07, 17.79s/it]  8%|â–Š         | 415/5198 [2:20:51<23:42:08, 17.84s/it]                                                       {'loss': 0.9022, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
  8%|â–Š         | 415/5198 [2:20:51<23:42:08, 17.84s/it]  8%|â–Š         | 416/5198 [2:21:08<23:22:30, 17.60s/it]                                                       {'loss': 0.9485, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
  8%|â–Š         | 416/5198 [2:21:08<23:22:30, 17.60s/it]  8%|â–Š         | 417/5198 [2:21:26<23:39:51, 17.82s/it]                                                       {'loss': 0.9688, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
  8%|â–Š         | 417/5198 [2:21:26<23:39:51, 17.82s/it]  8%|â–Š         | 418/5198 [2:21:45<23:56:10, 18.03s/it]                                                       {'loss': 0.905, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
  8%|â–Š         | 418/5198 [2:21:45<23:56:10, 18.03s/it]  8%|â–Š         | 419/5198 [2:22:03<23:56:40, 18.04s/it]                                                       {'loss': 0.9473, 'learning_rate': 1.9866031259177463e-05, 'epoch': 0.08}
  8%|â–Š         | 419/5198 [2:22:03<23:56:40, 18.04s/it]  8%|â–Š         | 420/5198 [2:22:19<23:13:07, 17.49s/it]                                                       {'loss': 0.942, 'learning_rate': 1.9865012851231022e-05, 'epoch': 0.08}
  8%|â–Š         | 420/5198 [2:22:19<23:13:07, 17.49s/it]  8%|â–Š         | 421/5198 [2:22:37<23:29:04, 17.70s/it]                                                       {'loss': 0.9682, 'learning_rate': 1.9863990613346936e-05, 'epoch': 0.08}
  8%|â–Š         | 421/5198 [2:22:37<23:29:04, 17.70s/it]  8%|â–Š         | 422/5198 [2:22:55<23:28:27, 17.69s/it]                                                       {'loss': 0.9263, 'learning_rate': 1.986296454592206e-05, 'epoch': 0.08}
  8%|â–Š         | 422/5198 [2:22:55<23:28:27, 17.69s/it]  8%|â–Š         | 423/5198 [2:23:12<23:11:12, 17.48s/it]                                                       {'loss': 0.9587, 'learning_rate': 1.9861934649354763e-05, 'epoch': 0.08}
  8%|â–Š         | 423/5198 [2:23:12<23:11:12, 17.48s/it]  8%|â–Š         | 424/5198 [2:23:30<23:14:25, 17.53s/it]                                                       {'loss': 0.9192, 'learning_rate': 1.9860900924044873e-05, 'epoch': 0.08}
  8%|â–Š         | 424/5198 [2:23:30<23:14:25, 17.53s/it]  8%|â–Š         | 425/5198 [2:23:47<23:20:05, 17.60s/it]                                                       {'loss': 0.8348, 'learning_rate': 1.9859863370393726e-05, 'epoch': 0.08}
  8%|â–Š         | 425/5198 [2:23:47<23:20:05, 17.60s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  8%|â–Š         | 426/5198 [2:25:14<50:56:42, 38.43s/it]                                                       {'loss': 0.9451, 'learning_rate': 1.9858821988804132e-05, 'epoch': 0.08}
  8%|â–Š         | 426/5198 [2:25:14<50:56:42, 38.43s/it]  8%|â–Š         | 427/5198 [2:25:33<42:52:40, 32.35s/it]                                                       {'loss': 0.8953, 'learning_rate': 1.9857776779680393e-05, 'epoch': 0.08}
  8%|â–Š         | 427/5198 [2:25:33<42:52:40, 32.35s/it]  8%|â–Š         | 428/5198 [2:25:50<37:01:53, 27.95s/it]                                                       {'loss': 0.9367, 'learning_rate': 1.98567277434283e-05, 'epoch': 0.08}
  8%|â–Š         | 428/5198 [2:25:50<37:01:53, 27.95s/it]  8%|â–Š         | 429/5198 [2:26:08<33:03:08, 24.95s/it]                                                       {'loss': 0.9197, 'learning_rate': 1.9855674880455115e-05, 'epoch': 0.08}
  8%|â–Š         | 429/5198 [2:26:08<33:03:08, 24.95s/it]  8%|â–Š         | 430/5198 [2:26:26<30:03:08, 22.69s/it]                                                       {'loss': 0.9749, 'learning_rate': 1.98546181911696e-05, 'epoch': 0.08}
  8%|â–Š         | 430/5198 [2:26:26<30:03:08, 22.69s/it]  8%|â–Š         | 431/5198 [2:26:43<28:05:31, 21.21s/it]                                                       {'loss': 0.9536, 'learning_rate': 1.9853557675982e-05, 'epoch': 0.08}
  8%|â–Š         | 431/5198 [2:26:43<28:05:31, 21.21s/it]  8%|â–Š         | 432/5198 [2:27:01<26:42:23, 20.17s/it]                                                       {'loss': 0.8791, 'learning_rate': 1.985249333530404e-05, 'epoch': 0.08}
  8%|â–Š         | 432/5198 [2:27:01<26:42:23, 20.17s/it]  8%|â–Š         | 433/5198 [2:27:20<26:07:12, 19.73s/it]                                                       {'loss': 0.8815, 'learning_rate': 1.9851425169548938e-05, 'epoch': 0.08}
  8%|â–Š         | 433/5198 [2:27:20<26:07:12, 19.73s/it]  8%|â–Š         | 434/5198 [2:27:37<25:14:23, 19.07s/it]                                                       {'loss': 0.3169, 'learning_rate': 1.9850353179131392e-05, 'epoch': 0.08}
  8%|â–Š         | 434/5198 [2:27:37<25:14:23, 19.07s/it]  8%|â–Š         | 435/5198 [2:27:55<24:28:23, 18.50s/it]                                                       {'loss': 0.3255, 'learning_rate': 1.9849277364467585e-05, 'epoch': 0.08}
  8%|â–Š         | 435/5198 [2:27:55<24:28:23, 18.50s/it]  8%|â–Š         | 436/5198 [2:28:12<23:50:52, 18.03s/it]                                                       {'loss': 0.8968, 'learning_rate': 1.984819772597518e-05, 'epoch': 0.08}
  8%|â–Š         | 436/5198 [2:28:12<23:50:52, 18.03s/it]  8%|â–Š         | 437/5198 [2:28:29<23:26:31, 17.73s/it]                                                       {'loss': 0.9226, 'learning_rate': 1.9847114264073336e-05, 'epoch': 0.08}
  8%|â–Š         | 437/5198 [2:28:29<23:26:31, 17.73s/it]  8%|â–Š         | 438/5198 [2:28:47<23:48:29, 18.01s/it]                                                       {'loss': 0.9371, 'learning_rate': 1.984602697918269e-05, 'epoch': 0.08}
  8%|â–Š         | 438/5198 [2:28:47<23:48:29, 18.01s/it]  8%|â–Š         | 439/5198 [2:29:04<23:27:31, 17.75s/it]                                                       {'loss': 0.912, 'learning_rate': 1.9844935871725363e-05, 'epoch': 0.08}
  8%|â–Š         | 439/5198 [2:29:04<23:27:31, 17.75s/it]  8%|â–Š         | 440/5198 [2:29:22<23:22:49, 17.69s/it]                                                       {'loss': 0.8899, 'learning_rate': 1.9843840942124956e-05, 'epoch': 0.08}
  8%|â–Š         | 440/5198 [2:29:22<23:22:49, 17.69s/it]  8%|â–Š         | 441/5198 [2:29:39<23:02:16, 17.43s/it]                                                       {'loss': 0.9748, 'learning_rate': 1.9842742190806566e-05, 'epoch': 0.08}
  8%|â–Š         | 441/5198 [2:29:39<23:02:16, 17.43s/it]  9%|â–Š         | 442/5198 [2:29:56<22:55:57, 17.36s/it]                                                       {'loss': 0.9048, 'learning_rate': 1.984163961819676e-05, 'epoch': 0.09}
  9%|â–Š         | 442/5198 [2:29:56<22:55:57, 17.36s/it]  9%|â–Š         | 443/5198 [2:30:14<23:17:29, 17.63s/it]                                                       {'loss': 0.925, 'learning_rate': 1.9840533224723595e-05, 'epoch': 0.09}
  9%|â–Š         | 443/5198 [2:30:14<23:17:29, 17.63s/it]  9%|â–Š         | 444/5198 [2:30:32<23:11:25, 17.56s/it]                                                       {'loss': 0.9218, 'learning_rate': 1.9839423010816616e-05, 'epoch': 0.09}
  9%|â–Š         | 444/5198 [2:30:32<23:11:25, 17.56s/it]  9%|â–Š         | 445/5198 [2:30:49<23:14:56, 17.61s/it]                                                       {'loss': 0.9299, 'learning_rate': 1.983830897690684e-05, 'epoch': 0.09}
  9%|â–Š         | 445/5198 [2:30:49<23:14:56, 17.61s/it]  9%|â–Š         | 446/5198 [2:31:07<23:17:45, 17.65s/it]                                                       {'loss': 0.9345, 'learning_rate': 1.9837191123426777e-05, 'epoch': 0.09}
  9%|â–Š         | 446/5198 [2:31:07<23:17:45, 17.65s/it]  9%|â–Š         | 447/5198 [2:31:26<23:42:01, 17.96s/it]                                                       {'loss': 0.9415, 'learning_rate': 1.983606945081042e-05, 'epoch': 0.09}
  9%|â–Š         | 447/5198 [2:31:26<23:42:01, 17.96s/it]  9%|â–Š         | 448/5198 [2:31:44<23:45:00, 18.00s/it]                                                       {'loss': 0.9271, 'learning_rate': 1.983494395949323e-05, 'epoch': 0.09}
  9%|â–Š         | 448/5198 [2:31:44<23:45:00, 18.00s/it]  9%|â–Š         | 449/5198 [2:32:03<24:02:49, 18.23s/it]                                                       {'loss': 0.9347, 'learning_rate': 1.983381464991217e-05, 'epoch': 0.09}
  9%|â–Š         | 449/5198 [2:32:03<24:02:49, 18.23s/it]  9%|â–Š         | 450/5198 [2:32:20<23:49:18, 18.06s/it]                                                       {'loss': 0.8647, 'learning_rate': 1.9832681522505676e-05, 'epoch': 0.09}
  9%|â–Š         | 450/5198 [2:32:20<23:49:18, 18.06s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  9%|â–Š         | 451/5198 [2:33:48<51:25:11, 39.00s/it]                                                       {'loss': 0.9792, 'learning_rate': 1.9831544577713663e-05, 'epoch': 0.09}
  9%|â–Š         | 451/5198 [2:33:48<51:25:11, 39.00s/it]  9%|â–Š         | 452/5198 [2:34:06<42:57:25, 32.58s/it]                                                       {'loss': 0.9434, 'learning_rate': 1.983040381597754e-05, 'epoch': 0.09}
  9%|â–Š         | 452/5198 [2:34:06<42:57:25, 32.58s/it]  9%|â–Š         | 453/5198 [2:34:23<36:45:35, 27.89s/it]                                                       {'loss': 0.863, 'learning_rate': 1.982925923774018e-05, 'epoch': 0.09}
  9%|â–Š         | 453/5198 [2:34:23<36:45:35, 27.89s/it]  9%|â–Š         | 454/5198 [2:34:40<32:36:24, 24.74s/it]                                                       {'loss': 0.8869, 'learning_rate': 1.9828110843445954e-05, 'epoch': 0.09}
  9%|â–Š         | 454/5198 [2:34:40<32:36:24, 24.74s/it]  9%|â–‰         | 455/5198 [2:34:58<29:53:52, 22.69s/it]                                                       {'loss': 0.9036, 'learning_rate': 1.982695863354071e-05, 'epoch': 0.09}
  9%|â–‰         | 455/5198 [2:34:58<29:53:52, 22.69s/it]  9%|â–‰         | 456/5198 [2:35:16<27:54:39, 21.19s/it]                                                       {'loss': 0.8846, 'learning_rate': 1.9825802608471767e-05, 'epoch': 0.09}
  9%|â–‰         | 456/5198 [2:35:16<27:54:39, 21.19s/it]  9%|â–‰         | 457/5198 [2:35:33<26:23:22, 20.04s/it]                                                       {'loss': 0.9557, 'learning_rate': 1.982464276868794e-05, 'epoch': 0.09}
  9%|â–‰         | 457/5198 [2:35:33<26:23:22, 20.04s/it]  9%|â–‰         | 458/5198 [2:35:51<25:34:08, 19.42s/it]                                                       {'loss': 0.9427, 'learning_rate': 1.982347911463952e-05, 'epoch': 0.09}
  9%|â–‰         | 458/5198 [2:35:51<25:34:08, 19.42s/it]  9%|â–‰         | 459/5198 [2:36:09<25:00:39, 19.00s/it]                                                       {'loss': 0.9103, 'learning_rate': 1.9822311646778277e-05, 'epoch': 0.09}
  9%|â–‰         | 459/5198 [2:36:09<25:00:39, 19.00s/it]  9%|â–‰         | 460/5198 [2:36:27<24:44:34, 18.80s/it]                                                       {'loss': 0.9123, 'learning_rate': 1.982114036555746e-05, 'epoch': 0.09}
  9%|â–‰         | 460/5198 [2:36:27<24:44:34, 18.80s/it]  9%|â–‰         | 461/5198 [2:36:45<24:09:20, 18.36s/it]                                                       {'loss': 0.8873, 'learning_rate': 1.9819965271431797e-05, 'epoch': 0.09}
  9%|â–‰         | 461/5198 [2:36:45<24:09:20, 18.36s/it]  9%|â–‰         | 462/5198 [2:37:02<23:37:29, 17.96s/it]                                                       {'loss': 0.9177, 'learning_rate': 1.9818786364857506e-05, 'epoch': 0.09}
  9%|â–‰         | 462/5198 [2:37:02<23:37:29, 17.96s/it]  9%|â–‰         | 463/5198 [2:37:20<23:37:56, 17.97s/it]                                                       {'loss': 0.9315, 'learning_rate': 1.9817603646292278e-05, 'epoch': 0.09}
  9%|â–‰         | 463/5198 [2:37:20<23:37:56, 17.97s/it]  9%|â–‰         | 464/5198 [2:37:37<23:32:05, 17.90s/it]                                                       {'loss': 0.8999, 'learning_rate': 1.9816417116195287e-05, 'epoch': 0.09}
  9%|â–‰         | 464/5198 [2:37:37<23:32:05, 17.90s/it]  9%|â–‰         | 465/5198 [2:37:55<23:25:21, 17.82s/it]                                                       {'loss': 0.9275, 'learning_rate': 1.9815226775027182e-05, 'epoch': 0.09}
  9%|â–‰         | 465/5198 [2:37:55<23:25:21, 17.82s/it]  9%|â–‰         | 466/5198 [2:38:13<23:20:53, 17.76s/it]                                                       {'loss': 0.9865, 'learning_rate': 1.9814032623250093e-05, 'epoch': 0.09}
  9%|â–‰         | 466/5198 [2:38:13<23:20:53, 17.76s/it]WARNING: tokenization mismatch: 1 vs. 70. (ignored)
  9%|â–‰         | 467/5198 [2:38:31<23:36:41, 17.97s/it]                                                       {'loss': 0.9035, 'learning_rate': 1.9812834661327632e-05, 'epoch': 0.09}
  9%|â–‰         | 467/5198 [2:38:31<23:36:41, 17.97s/it]  9%|â–‰         | 468/5198 [2:38:49<23:26:28, 17.84s/it]                                                       {'loss': 0.8645, 'learning_rate': 1.9811632889724888e-05, 'epoch': 0.09}
  9%|â–‰         | 468/5198 [2:38:49<23:26:28, 17.84s/it]  9%|â–‰         | 469/5198 [2:39:07<23:31:11, 17.90s/it]                                                       {'loss': 0.8464, 'learning_rate': 1.9810427308908437e-05, 'epoch': 0.09}
  9%|â–‰         | 469/5198 [2:39:07<23:31:11, 17.90s/it]  9%|â–‰         | 470/5198 [2:39:24<23:12:25, 17.67s/it]                                                       {'loss': 0.8433, 'learning_rate': 1.9809217919346318e-05, 'epoch': 0.09}
  9%|â–‰         | 470/5198 [2:39:24<23:12:25, 17.67s/it]  9%|â–‰         | 471/5198 [2:39:41<23:06:48, 17.60s/it]                                                       {'loss': 0.9794, 'learning_rate': 1.980800472150806e-05, 'epoch': 0.09}
  9%|â–‰         | 471/5198 [2:39:41<23:06:48, 17.60s/it]  9%|â–‰         | 472/5198 [2:39:58<22:49:49, 17.39s/it]                                                       {'loss': 0.9204, 'learning_rate': 1.9806787715864674e-05, 'epoch': 0.09}
  9%|â–‰         | 472/5198 [2:39:58<22:49:49, 17.39s/it]  9%|â–‰         | 473/5198 [2:40:15<22:40:49, 17.28s/it]                                                       {'loss': 0.955, 'learning_rate': 1.9805566902888637e-05, 'epoch': 0.09}
  9%|â–‰         | 473/5198 [2:40:15<22:40:49, 17.28s/it]  9%|â–‰         | 474/5198 [2:40:33<22:46:11, 17.35s/it]                                                       {'loss': 0.9364, 'learning_rate': 1.9804342283053916e-05, 'epoch': 0.09}
  9%|â–‰         | 474/5198 [2:40:33<22:46:11, 17.35s/it]  9%|â–‰         | 475/5198 [2:40:50<22:35:30, 17.22s/it]                                                       {'loss': 0.9789, 'learning_rate': 1.980311385683594e-05, 'epoch': 0.09}
  9%|â–‰         | 475/5198 [2:40:50<22:35:30, 17.22s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  9%|â–‰         | 476/5198 [2:42:16<49:45:27, 37.93s/it]                                                       {'loss': 0.9372, 'learning_rate': 1.980188162471164e-05, 'epoch': 0.09}
  9%|â–‰         | 476/5198 [2:42:16<49:45:27, 37.93s/it]  9%|â–‰         | 477/5198 [2:42:32<41:13:26, 31.44s/it]                                                       {'loss': 0.9239, 'learning_rate': 1.98006455871594e-05, 'epoch': 0.09}
  9%|â–‰         | 477/5198 [2:42:32<41:13:26, 31.44s/it]  9%|â–‰         | 478/5198 [2:42:50<35:46:35, 27.29s/it]                                                       {'loss': 0.8693, 'learning_rate': 1.97994057446591e-05, 'epoch': 0.09}
  9%|â–‰         | 478/5198 [2:42:50<35:46:35, 27.29s/it]  9%|â–‰         | 479/5198 [2:43:07<31:58:35, 24.39s/it]                                                       {'loss': 0.9106, 'learning_rate': 1.979816209769209e-05, 'epoch': 0.09}
  9%|â–‰         | 479/5198 [2:43:07<31:58:35, 24.39s/it]  9%|â–‰         | 480/5198 [2:43:25<29:19:55, 22.38s/it]                                                       {'loss': 0.8645, 'learning_rate': 1.9796914646741187e-05, 'epoch': 0.09}
  9%|â–‰         | 480/5198 [2:43:25<29:19:55, 22.38s/it]  9%|â–‰         | 481/5198 [2:43:42<27:04:40, 20.67s/it]                                                       {'loss': 0.9069, 'learning_rate': 1.9795663392290702e-05, 'epoch': 0.09}
  9%|â–‰         | 481/5198 [2:43:42<27:04:40, 20.67s/it]  9%|â–‰         | 482/5198 [2:43:59<25:47:38, 19.69s/it]                                                       {'loss': 0.8559, 'learning_rate': 1.9794408334826415e-05, 'epoch': 0.09}
  9%|â–‰         | 482/5198 [2:43:59<25:47:38, 19.69s/it]  9%|â–‰         | 483/5198 [2:44:17<25:02:51, 19.12s/it]                                                       {'loss': 0.9288, 'learning_rate': 1.979314947483558e-05, 'epoch': 0.09}
  9%|â–‰         | 483/5198 [2:44:17<25:02:51, 19.12s/it]  9%|â–‰         | 484/5198 [2:44:34<24:19:13, 18.57s/it]                                                       {'loss': 0.9707, 'learning_rate': 1.9791886812806932e-05, 'epoch': 0.09}
  9%|â–‰         | 484/5198 [2:44:34<24:19:13, 18.57s/it]  9%|â–‰         | 485/5198 [2:44:51<23:43:26, 18.12s/it]                                                       {'loss': 0.8814, 'learning_rate': 1.9790620349230676e-05, 'epoch': 0.09}
  9%|â–‰         | 485/5198 [2:44:51<23:43:26, 18.12s/it]  9%|â–‰         | 486/5198 [2:45:09<23:37:55, 18.06s/it]                                                       {'loss': 0.9204, 'learning_rate': 1.9789350084598504e-05, 'epoch': 0.09}
  9%|â–‰         | 486/5198 [2:45:09<23:37:55, 18.06s/it]  9%|â–‰         | 487/5198 [2:45:26<23:11:52, 17.73s/it]                                                       {'loss': 0.8648, 'learning_rate': 1.9788076019403565e-05, 'epoch': 0.09}
  9%|â–‰         | 487/5198 [2:45:26<23:11:52, 17.73s/it]  9%|â–‰         | 488/5198 [2:45:44<23:11:30, 17.73s/it]                                                       {'loss': 0.9588, 'learning_rate': 1.9786798154140507e-05, 'epoch': 0.09}
  9%|â–‰         | 488/5198 [2:45:44<23:11:30, 17.73s/it]  9%|â–‰         | 489/5198 [2:46:02<23:08:34, 17.69s/it]                                                       {'loss': 0.3841, 'learning_rate': 1.9785516489305437e-05, 'epoch': 0.09}
  9%|â–‰         | 489/5198 [2:46:02<23:08:34, 17.69s/it]  9%|â–‰         | 490/5198 [2:46:19<23:06:17, 17.67s/it]                                                       {'loss': 0.9267, 'learning_rate': 1.9784231025395936e-05, 'epoch': 0.09}
  9%|â–‰         | 490/5198 [2:46:19<23:06:17, 17.67s/it]  9%|â–‰         | 491/5198 [2:46:37<23:10:10, 17.72s/it]                                                       {'loss': 0.9316, 'learning_rate': 1.9782941762911075e-05, 'epoch': 0.09}
  9%|â–‰         | 491/5198 [2:46:37<23:10:10, 17.72s/it]  9%|â–‰         | 492/5198 [2:46:55<23:25:18, 17.92s/it]                                                       {'loss': 0.9301, 'learning_rate': 1.9781648702351383e-05, 'epoch': 0.09}
  9%|â–‰         | 492/5198 [2:46:55<23:25:18, 17.92s/it]  9%|â–‰         | 493/5198 [2:47:14<23:42:49, 18.14s/it]                                                       {'loss': 0.9433, 'learning_rate': 1.9780351844218874e-05, 'epoch': 0.09}
  9%|â–‰         | 493/5198 [2:47:14<23:42:49, 18.14s/it] 10%|â–‰         | 494/5198 [2:47:32<23:41:56, 18.14s/it]                                                       {'loss': 0.9047, 'learning_rate': 1.977905118901703e-05, 'epoch': 0.1}
 10%|â–‰         | 494/5198 [2:47:32<23:41:56, 18.14s/it] 10%|â–‰         | 495/5198 [2:47:49<23:20:46, 17.87s/it]                                                       {'loss': 0.8833, 'learning_rate': 1.977774673725081e-05, 'epoch': 0.1}
 10%|â–‰         | 495/5198 [2:47:49<23:20:46, 17.87s/it] 10%|â–‰         | 496/5198 [2:48:07<23:06:34, 17.69s/it]                                                       {'loss': 0.3369, 'learning_rate': 1.977643848942665e-05, 'epoch': 0.1}
 10%|â–‰         | 496/5198 [2:48:07<23:06:34, 17.69s/it] 10%|â–‰         | 497/5198 [2:48:25<23:08:46, 17.73s/it]                                                       {'loss': 0.8933, 'learning_rate': 1.977512644605246e-05, 'epoch': 0.1}
 10%|â–‰         | 497/5198 [2:48:25<23:08:46, 17.73s/it] 10%|â–‰         | 498/5198 [2:48:43<23:22:48, 17.91s/it]                                                       {'loss': 0.8852, 'learning_rate': 1.9773810607637612e-05, 'epoch': 0.1}
 10%|â–‰         | 498/5198 [2:48:43<23:22:48, 17.91s/it] 10%|â–‰         | 499/5198 [2:49:00<23:15:18, 17.82s/it]                                                       {'loss': 0.9228, 'learning_rate': 1.9772490974692962e-05, 'epoch': 0.1}
 10%|â–‰         | 499/5198 [2:49:00<23:15:18, 17.82s/it] 10%|â–‰         | 500/5198 [2:49:18<23:03:04, 17.66s/it]                                                       {'loss': 0.9559, 'learning_rate': 1.9771167547730844e-05, 'epoch': 0.1}
 10%|â–‰         | 500/5198 [2:49:18<23:03:04, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 10%|â–‰         | 501/5198 [2:50:51<52:41:36, 40.39s/it]                                                       {'loss': 0.9584, 'learning_rate': 1.976984032726505e-05, 'epoch': 0.1}
 10%|â–‰         | 501/5198 [2:50:51<52:41:36, 40.39s/it] 10%|â–‰         | 502/5198 [2:51:08<43:38:43, 33.46s/it]                                                       {'loss': 0.9465, 'learning_rate': 1.976850931381086e-05, 'epoch': 0.1}
 10%|â–‰         | 502/5198 [2:51:08<43:38:43, 33.46s/it] 10%|â–‰         | 503/5198 [2:51:25<37:07:18, 28.46s/it]                                                       {'loss': 0.9019, 'learning_rate': 1.976717450788501e-05, 'epoch': 0.1}
 10%|â–‰         | 503/5198 [2:51:25<37:07:18, 28.46s/it] 10%|â–‰         | 504/5198 [2:51:42<32:38:40, 25.04s/it]                                                       {'loss': 0.8772, 'learning_rate': 1.9765835910005726e-05, 'epoch': 0.1}
 10%|â–‰         | 504/5198 [2:51:42<32:38:40, 25.04s/it] 10%|â–‰         | 505/5198 [2:52:01<30:02:35, 23.05s/it]                                                       {'loss': 0.8771, 'learning_rate': 1.9764493520692685e-05, 'epoch': 0.1}
 10%|â–‰         | 505/5198 [2:52:01<30:02:35, 23.05s/it] 10%|â–‰         | 506/5198 [2:52:18<27:56:58, 21.44s/it]                                                       {'loss': 1.0009, 'learning_rate': 1.9763147340467067e-05, 'epoch': 0.1}
 10%|â–‰         | 506/5198 [2:52:18<27:56:58, 21.44s/it] 10%|â–‰         | 507/5198 [2:52:36<26:17:10, 20.17s/it]                                                       {'loss': 0.869, 'learning_rate': 1.9761797369851498e-05, 'epoch': 0.1}
 10%|â–‰         | 507/5198 [2:52:36<26:17:10, 20.17s/it] 10%|â–‰         | 508/5198 [2:52:54<25:33:40, 19.62s/it]                                                       {'loss': 0.8956, 'learning_rate': 1.9760443609370074e-05, 'epoch': 0.1}
 10%|â–‰         | 508/5198 [2:52:54<25:33:40, 19.62s/it] 10%|â–‰         | 509/5198 [2:53:12<24:51:30, 19.09s/it]                                                       {'loss': 0.9108, 'learning_rate': 1.975908605954838e-05, 'epoch': 0.1}
 10%|â–‰         | 509/5198 [2:53:12<24:51:30, 19.09s/it] 10%|â–‰         | 510/5198 [2:53:30<24:36:35, 18.90s/it]                                                       {'loss': 0.9468, 'learning_rate': 1.9757724720913466e-05, 'epoch': 0.1}
 10%|â–‰         | 510/5198 [2:53:30<24:36:35, 18.90s/it] 10%|â–‰         | 511/5198 [2:53:48<24:00:45, 18.44s/it]                                                       {'loss': 0.9103, 'learning_rate': 1.9756359593993845e-05, 'epoch': 0.1}
 10%|â–‰         | 511/5198 [2:53:48<24:00:45, 18.44s/it] 10%|â–‰         | 512/5198 [2:54:05<23:30:21, 18.06s/it]                                                       {'loss': 0.3031, 'learning_rate': 1.975499067931951e-05, 'epoch': 0.1}
 10%|â–‰         | 512/5198 [2:54:05<23:30:21, 18.06s/it] 10%|â–‰         | 513/5198 [2:54:22<23:11:55, 17.83s/it]                                                       {'loss': 0.9563, 'learning_rate': 1.975361797742192e-05, 'epoch': 0.1}
 10%|â–‰         | 513/5198 [2:54:22<23:11:55, 17.83s/it] 10%|â–‰         | 514/5198 [2:54:40<23:08:28, 17.79s/it]                                                       {'loss': 0.9331, 'learning_rate': 1.9752241488834002e-05, 'epoch': 0.1}
 10%|â–‰         | 514/5198 [2:54:40<23:08:28, 17.79s/it] 10%|â–‰         | 515/5198 [2:54:57<22:56:06, 17.63s/it]                                                       {'loss': 0.9215, 'learning_rate': 1.975086121409016e-05, 'epoch': 0.1}
 10%|â–‰         | 515/5198 [2:54:57<22:56:06, 17.63s/it] 10%|â–‰         | 516/5198 [2:55:15<22:57:40, 17.65s/it]                                                       {'loss': 0.8729, 'learning_rate': 1.974947715372626e-05, 'epoch': 0.1}
 10%|â–‰         | 516/5198 [2:55:15<22:57:40, 17.65s/it] 10%|â–‰         | 517/5198 [2:55:33<23:02:16, 17.72s/it]                                                       {'loss': 0.8677, 'learning_rate': 1.974808930827965e-05, 'epoch': 0.1}
 10%|â–‰         | 517/5198 [2:55:33<23:02:16, 17.72s/it] 10%|â–‰         | 518/5198 [2:55:50<22:44:41, 17.50s/it]                                                       {'loss': 0.898, 'learning_rate': 1.9746697678289128e-05, 'epoch': 0.1}
 10%|â–‰         | 518/5198 [2:55:50<22:44:41, 17.50s/it] 10%|â–‰         | 519/5198 [2:56:07<22:43:57, 17.49s/it]                                                       {'loss': 0.9091, 'learning_rate': 1.9745302264294982e-05, 'epoch': 0.1}
 10%|â–‰         | 519/5198 [2:56:07<22:43:57, 17.49s/it] 10%|â–ˆ         | 520/5198 [2:56:25<22:50:14, 17.57s/it]                                                       {'loss': 0.9178, 'learning_rate': 1.9743903066838954e-05, 'epoch': 0.1}
 10%|â–ˆ         | 520/5198 [2:56:25<22:50:14, 17.57s/it] 10%|â–ˆ         | 521/5198 [2:56:43<22:59:16, 17.69s/it]                                                       {'loss': 0.9361, 'learning_rate': 1.9742500086464266e-05, 'epoch': 0.1}
 10%|â–ˆ         | 521/5198 [2:56:43<22:59:16, 17.69s/it] 10%|â–ˆ         | 522/5198 [2:57:00<22:47:16, 17.54s/it]                                                       {'loss': 0.3243, 'learning_rate': 1.9741093323715597e-05, 'epoch': 0.1}
 10%|â–ˆ         | 522/5198 [2:57:00<22:47:16, 17.54s/it] 10%|â–ˆ         | 523/5198 [2:57:17<22:39:48, 17.45s/it]                                                       {'loss': 0.9358, 'learning_rate': 1.9739682779139107e-05, 'epoch': 0.1}
 10%|â–ˆ         | 523/5198 [2:57:17<22:39:48, 17.45s/it] 10%|â–ˆ         | 524/5198 [2:57:35<22:41:47, 17.48s/it]                                                       {'loss': 0.9118, 'learning_rate': 1.9738268453282414e-05, 'epoch': 0.1}
 10%|â–ˆ         | 524/5198 [2:57:35<22:41:47, 17.48s/it] 10%|â–ˆ         | 525/5198 [2:57:52<22:41:14, 17.48s/it]                                                       {'loss': 0.9301, 'learning_rate': 1.9736850346694608e-05, 'epoch': 0.1}
 10%|â–ˆ         | 525/5198 [2:57:52<22:41:14, 17.48s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 10%|â–ˆ         | 526/5198 [2:59:27<52:55:52, 40.79s/it]                                                       {'loss': 0.946, 'learning_rate': 1.973542845992625e-05, 'epoch': 0.1}
 10%|â–ˆ         | 526/5198 [2:59:27<52:55:52, 40.79s/it] 10%|â–ˆ         | 527/5198 [2:59:45<44:03:11, 33.95s/it]                                                       {'loss': 0.8796, 'learning_rate': 1.9734002793529362e-05, 'epoch': 0.1}
 10%|â–ˆ         | 527/5198 [2:59:45<44:03:11, 33.95s/it] 10%|â–ˆ         | 528/5198 [3:00:03<37:46:21, 29.12s/it]                                                       {'loss': 0.891, 'learning_rate': 1.9732573348057437e-05, 'epoch': 0.1}
 10%|â–ˆ         | 528/5198 [3:00:03<37:46:21, 29.12s/it] 10%|â–ˆ         | 529/5198 [3:00:21<33:12:49, 25.61s/it]                                                       {'loss': 0.9623, 'learning_rate': 1.973114012406544e-05, 'epoch': 0.1}
 10%|â–ˆ         | 529/5198 [3:00:21<33:12:49, 25.61s/it] 10%|â–ˆ         | 530/5198 [3:00:39<30:15:31, 23.34s/it]                                                       {'loss': 0.9132, 'learning_rate': 1.9729703122109788e-05, 'epoch': 0.1}
 10%|â–ˆ         | 530/5198 [3:00:39<30:15:31, 23.34s/it] 10%|â–ˆ         | 531/5198 [3:00:56<27:46:38, 21.43s/it]                                                       {'loss': 0.8613, 'learning_rate': 1.9728262342748384e-05, 'epoch': 0.1}
 10%|â–ˆ         | 531/5198 [3:00:56<27:46:38, 21.43s/it] 10%|â–ˆ         | 532/5198 [3:01:12<25:49:30, 19.93s/it]                                                       {'loss': 0.9257, 'learning_rate': 1.9726817786540584e-05, 'epoch': 0.1}
 10%|â–ˆ         | 532/5198 [3:01:12<25:49:30, 19.93s/it] 10%|â–ˆ         | 533/5198 [3:01:30<24:56:51, 19.25s/it]                                                       {'loss': 0.9259, 'learning_rate': 1.9725369454047215e-05, 'epoch': 0.1}
 10%|â–ˆ         | 533/5198 [3:01:30<24:56:51, 19.25s/it] 10%|â–ˆ         | 534/5198 [3:01:47<24:11:15, 18.67s/it]                                                       {'loss': 0.9602, 'learning_rate': 1.9723917345830568e-05, 'epoch': 0.1}
 10%|â–ˆ         | 534/5198 [3:01:47<24:11:15, 18.67s/it] 10%|â–ˆ         | 535/5198 [3:02:05<23:53:16, 18.44s/it]                                                       {'loss': 0.8982, 'learning_rate': 1.9722461462454405e-05, 'epoch': 0.1}
 10%|â–ˆ         | 535/5198 [3:02:05<23:53:16, 18.44s/it] 10%|â–ˆ         | 536/5198 [3:02:24<23:57:55, 18.51s/it]                                                       {'loss': 0.8932, 'learning_rate': 1.9721001804483947e-05, 'epoch': 0.1}
 10%|â–ˆ         | 536/5198 [3:02:24<23:57:55, 18.51s/it] 10%|â–ˆ         | 537/5198 [3:02:42<23:47:26, 18.38s/it]                                                       {'loss': 0.9892, 'learning_rate': 1.9719538372485887e-05, 'epoch': 0.1}
 10%|â–ˆ         | 537/5198 [3:02:42<23:47:26, 18.38s/it] 10%|â–ˆ         | 538/5198 [3:02:59<23:11:41, 17.92s/it]                                                       {'loss': 0.8783, 'learning_rate': 1.9718071167028376e-05, 'epoch': 0.1}
 10%|â–ˆ         | 538/5198 [3:02:59<23:11:41, 17.92s/it] 10%|â–ˆ         | 539/5198 [3:03:15<22:40:35, 17.52s/it]                                                       {'loss': 0.899, 'learning_rate': 1.9716600188681038e-05, 'epoch': 0.1}
 10%|â–ˆ         | 539/5198 [3:03:15<22:40:35, 17.52s/it] 10%|â–ˆ         | 540/5198 [3:03:33<22:44:31, 17.58s/it]                                                       {'loss': 0.9155, 'learning_rate': 1.971512543801495e-05, 'epoch': 0.1}
 10%|â–ˆ         | 540/5198 [3:03:33<22:44:31, 17.58s/it] 10%|â–ˆ         | 541/5198 [3:03:50<22:35:45, 17.47s/it]                                                       {'loss': 0.9332, 'learning_rate': 1.9713646915602663e-05, 'epoch': 0.1}
 10%|â–ˆ         | 541/5198 [3:03:50<22:35:45, 17.47s/it] 10%|â–ˆ         | 542/5198 [3:04:08<22:42:26, 17.56s/it]                                                       {'loss': 0.9167, 'learning_rate': 1.9712164622018197e-05, 'epoch': 0.1}
 10%|â–ˆ         | 542/5198 [3:04:08<22:42:26, 17.56s/it] 10%|â–ˆ         | 543/5198 [3:04:26<22:52:16, 17.69s/it]                                                       {'loss': 0.8764, 'learning_rate': 1.9710678557837024e-05, 'epoch': 0.1}
 10%|â–ˆ         | 543/5198 [3:04:26<22:52:16, 17.69s/it] 10%|â–ˆ         | 544/5198 [3:04:43<22:27:36, 17.37s/it]                                                       {'loss': 0.9079, 'learning_rate': 1.9709188723636088e-05, 'epoch': 0.1}
 10%|â–ˆ         | 544/5198 [3:04:43<22:27:36, 17.37s/it] 10%|â–ˆ         | 545/5198 [3:05:01<22:50:54, 17.68s/it]                                                       {'loss': 0.9123, 'learning_rate': 1.970769511999379e-05, 'epoch': 0.1}
 10%|â–ˆ         | 545/5198 [3:05:01<22:50:54, 17.68s/it] 11%|â–ˆ         | 546/5198 [3:05:19<22:57:14, 17.76s/it]                                                       {'loss': 0.8954, 'learning_rate': 1.9706197747490004e-05, 'epoch': 0.11}
 11%|â–ˆ         | 546/5198 [3:05:19<22:57:14, 17.76s/it] 11%|â–ˆ         | 547/5198 [3:05:36<22:52:14, 17.70s/it]                                                       {'loss': 0.9006, 'learning_rate': 1.9704696606706055e-05, 'epoch': 0.11}
 11%|â–ˆ         | 547/5198 [3:05:36<22:52:14, 17.70s/it] 11%|â–ˆ         | 548/5198 [3:05:54<22:52:59, 17.72s/it]                                                       {'loss': 0.9435, 'learning_rate': 1.9703191698224742e-05, 'epoch': 0.11}
 11%|â–ˆ         | 548/5198 [3:05:54<22:52:59, 17.72s/it] 11%|â–ˆ         | 549/5198 [3:06:12<22:54:29, 17.74s/it]                                                       {'loss': 0.9389, 'learning_rate': 1.9701683022630323e-05, 'epoch': 0.11}
 11%|â–ˆ         | 549/5198 [3:06:12<22:54:29, 17.74s/it] 11%|â–ˆ         | 550/5198 [3:06:29<22:40:55, 17.57s/it]                                                       {'loss': 0.9534, 'learning_rate': 1.9700170580508514e-05, 'epoch': 0.11}
 11%|â–ˆ         | 550/5198 [3:06:29<22:40:55, 17.57s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 11%|â–ˆ         | 551/5198 [3:07:59<50:33:27, 39.17s/it]                                                       {'loss': 0.9254, 'learning_rate': 1.9698654372446495e-05, 'epoch': 0.11}
 11%|â–ˆ         | 551/5198 [3:07:59<50:33:27, 39.17s/it] 11%|â–ˆ         | 552/5198 [3:08:16<42:10:07, 32.67s/it]                                                       {'loss': 0.9156, 'learning_rate': 1.969713439903292e-05, 'epoch': 0.11}
 11%|â–ˆ         | 552/5198 [3:08:16<42:10:07, 32.67s/it] 11%|â–ˆ         | 553/5198 [3:08:34<36:23:35, 28.21s/it]                                                       {'loss': 0.9018, 'learning_rate': 1.9695610660857886e-05, 'epoch': 0.11}
 11%|â–ˆ         | 553/5198 [3:08:34<36:23:35, 28.21s/it] 11%|â–ˆ         | 554/5198 [3:08:54<33:08:27, 25.69s/it]                                                       {'loss': 0.9508, 'learning_rate': 1.9694083158512965e-05, 'epoch': 0.11}
 11%|â–ˆ         | 554/5198 [3:08:54<33:08:27, 25.69s/it] 11%|â–ˆ         | 555/5198 [3:09:12<30:09:26, 23.38s/it]                                                       {'loss': 0.9531, 'learning_rate': 1.9692551892591185e-05, 'epoch': 0.11}
 11%|â–ˆ         | 555/5198 [3:09:12<30:09:26, 23.38s/it] 11%|â–ˆ         | 556/5198 [3:09:29<27:54:55, 21.65s/it]                                                       {'loss': 0.9399, 'learning_rate': 1.9691016863687037e-05, 'epoch': 0.11}
 11%|â–ˆ         | 556/5198 [3:09:29<27:54:55, 21.65s/it] 11%|â–ˆ         | 557/5198 [3:09:47<26:23:09, 20.47s/it]                                                       {'loss': 0.8706, 'learning_rate': 1.968947807239647e-05, 'epoch': 0.11}
 11%|â–ˆ         | 557/5198 [3:09:47<26:23:09, 20.47s/it] 11%|â–ˆ         | 558/5198 [3:10:05<25:27:19, 19.75s/it]                                                       {'loss': 0.8832, 'learning_rate': 1.9687935519316897e-05, 'epoch': 0.11}
 11%|â–ˆ         | 558/5198 [3:10:05<25:27:19, 19.75s/it] 11%|â–ˆ         | 559/5198 [3:10:23<24:47:48, 19.24s/it]                                                       {'loss': 0.8987, 'learning_rate': 1.9686389205047186e-05, 'epoch': 0.11}
 11%|â–ˆ         | 559/5198 [3:10:23<24:47:48, 19.24s/it] 11%|â–ˆ         | 560/5198 [3:10:40<23:48:01, 18.47s/it]                                                       {'loss': 0.9371, 'learning_rate': 1.9684839130187678e-05, 'epoch': 0.11}
 11%|â–ˆ         | 560/5198 [3:10:40<23:48:01, 18.47s/it] 11%|â–ˆ         | 561/5198 [3:10:57<23:08:49, 17.97s/it]                                                       {'loss': 0.8805, 'learning_rate': 1.968328529534016e-05, 'epoch': 0.11}
 11%|â–ˆ         | 561/5198 [3:10:57<23:08:49, 17.97s/it] 11%|â–ˆ         | 562/5198 [3:11:15<23:07:03, 17.95s/it]                                                       {'loss': 0.8971, 'learning_rate': 1.9681727701107885e-05, 'epoch': 0.11}
 11%|â–ˆ         | 562/5198 [3:11:15<23:07:03, 17.95s/it] 11%|â–ˆ         | 563/5198 [3:11:32<23:00:24, 17.87s/it]                                                       {'loss': 0.8787, 'learning_rate': 1.9680166348095568e-05, 'epoch': 0.11}
 11%|â–ˆ         | 563/5198 [3:11:32<23:00:24, 17.87s/it] 11%|â–ˆ         | 564/5198 [3:11:51<23:11:10, 18.01s/it]                                                       {'loss': 0.8666, 'learning_rate': 1.967860123690937e-05, 'epoch': 0.11}
 11%|â–ˆ         | 564/5198 [3:11:51<23:11:10, 18.01s/it] 11%|â–ˆ         | 565/5198 [3:12:09<23:11:43, 18.02s/it]                                                       {'loss': 0.8897, 'learning_rate': 1.9677032368156934e-05, 'epoch': 0.11}
 11%|â–ˆ         | 565/5198 [3:12:09<23:11:43, 18.02s/it] 11%|â–ˆ         | 566/5198 [3:12:27<23:12:36, 18.04s/it]                                                       {'loss': 0.8931, 'learning_rate': 1.967545974244734e-05, 'epoch': 0.11}
 11%|â–ˆ         | 566/5198 [3:12:27<23:12:36, 18.04s/it] 11%|â–ˆ         | 567/5198 [3:12:44<22:47:19, 17.72s/it]                                                       {'loss': 0.9465, 'learning_rate': 1.9673883360391138e-05, 'epoch': 0.11}
 11%|â–ˆ         | 567/5198 [3:12:44<22:47:19, 17.72s/it] 11%|â–ˆ         | 568/5198 [3:13:02<23:04:16, 17.94s/it]                                                       {'loss': 0.9361, 'learning_rate': 1.9672303222600333e-05, 'epoch': 0.11}
 11%|â–ˆ         | 568/5198 [3:13:02<23:04:16, 17.94s/it] 11%|â–ˆ         | 569/5198 [3:13:19<22:43:28, 17.67s/it]                                                       {'loss': 0.9468, 'learning_rate': 1.967071932968839e-05, 'epoch': 0.11}
 11%|â–ˆ         | 569/5198 [3:13:19<22:43:28, 17.67s/it] 11%|â–ˆ         | 570/5198 [3:13:38<23:08:41, 18.00s/it]                                                       {'loss': 0.8478, 'learning_rate': 1.9669131682270232e-05, 'epoch': 0.11}
 11%|â–ˆ         | 570/5198 [3:13:39<23:08:41, 18.00s/it] 11%|â–ˆ         | 571/5198 [3:13:56<23:00:20, 17.90s/it]                                                       {'loss': 0.8462, 'learning_rate': 1.9667540280962235e-05, 'epoch': 0.11}
 11%|â–ˆ         | 571/5198 [3:13:56<23:00:20, 17.90s/it] 11%|â–ˆ         | 572/5198 [3:14:13<22:50:53, 17.78s/it]                                                       {'loss': 0.9282, 'learning_rate': 1.966594512638224e-05, 'epoch': 0.11}
 11%|â–ˆ         | 572/5198 [3:14:13<22:50:53, 17.78s/it] 11%|â–ˆ         | 573/5198 [3:14:30<22:34:05, 17.57s/it]                                                       {'loss': 0.9174, 'learning_rate': 1.9664346219149538e-05, 'epoch': 0.11}
 11%|â–ˆ         | 573/5198 [3:14:30<22:34:05, 17.57s/it] 11%|â–ˆ         | 574/5198 [3:14:48<22:25:35, 17.46s/it]                                                       {'loss': 0.8543, 'learning_rate': 1.966274355988488e-05, 'epoch': 0.11}
 11%|â–ˆ         | 574/5198 [3:14:48<22:25:35, 17.46s/it] 11%|â–ˆ         | 575/5198 [3:15:06<22:40:50, 17.66s/it]                                                       {'loss': 0.9178, 'learning_rate': 1.9661137149210473e-05, 'epoch': 0.11}
 11%|â–ˆ         | 575/5198 [3:15:06<22:40:50, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 11%|â–ˆ         | 576/5198 [3:16:33<49:34:00, 38.61s/it]                                                       {'loss': 0.3092, 'learning_rate': 1.9659526987749987e-05, 'epoch': 0.11}
 11%|â–ˆ         | 576/5198 [3:16:33<49:34:00, 38.61s/it] 11%|â–ˆ         | 577/5198 [3:16:51<41:35:40, 32.40s/it]                                                       {'loss': 0.363, 'learning_rate': 1.9657913076128532e-05, 'epoch': 0.11}
 11%|â–ˆ         | 577/5198 [3:16:51<41:35:40, 32.40s/it] 11%|â–ˆ         | 578/5198 [3:17:08<35:45:58, 27.87s/it]                                                       {'loss': 0.8899, 'learning_rate': 1.965629541497269e-05, 'epoch': 0.11}
 11%|â–ˆ         | 578/5198 [3:17:08<35:45:58, 27.87s/it] 11%|â–ˆ         | 579/5198 [3:17:26<31:55:47, 24.89s/it]                                                       {'loss': 0.9057, 'learning_rate': 1.9654674004910493e-05, 'epoch': 0.11}
 11%|â–ˆ         | 579/5198 [3:17:26<31:55:47, 24.89s/it] 11%|â–ˆ         | 580/5198 [3:17:44<29:05:38, 22.68s/it]                                                       {'loss': 0.9393, 'learning_rate': 1.9653048846571427e-05, 'epoch': 0.11}
 11%|â–ˆ         | 580/5198 [3:17:44<29:05:38, 22.68s/it] 11%|â–ˆ         | 581/5198 [3:18:01<27:09:00, 21.17s/it]                                                       {'loss': 0.8881, 'learning_rate': 1.9651419940586437e-05, 'epoch': 0.11}
 11%|â–ˆ         | 581/5198 [3:18:01<27:09:00, 21.17s/it] 11%|â–ˆ         | 582/5198 [3:18:19<25:55:25, 20.22s/it]                                                       {'loss': 0.9326, 'learning_rate': 1.964978728758791e-05, 'epoch': 0.11}
 11%|â–ˆ         | 582/5198 [3:18:19<25:55:25, 20.22s/it] 11%|â–ˆ         | 583/5198 [3:18:37<24:53:20, 19.41s/it]                                                       {'loss': 0.9473, 'learning_rate': 1.9648150888209715e-05, 'epoch': 0.11}
 11%|â–ˆ         | 583/5198 [3:18:37<24:53:20, 19.41s/it] 11%|â–ˆ         | 584/5198 [3:18:54<24:01:08, 18.74s/it]                                                       {'loss': 0.944, 'learning_rate': 1.9646510743087144e-05, 'epoch': 0.11}
 11%|â–ˆ         | 584/5198 [3:18:54<24:01:08, 18.74s/it] 11%|â–ˆâ–        | 585/5198 [3:19:12<23:35:56, 18.42s/it]                                                       {'loss': 0.8898, 'learning_rate': 1.964486685285697e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 585/5198 [3:19:12<23:35:56, 18.42s/it] 11%|â–ˆâ–        | 586/5198 [3:19:29<23:11:02, 18.10s/it]                                                       {'loss': 0.9034, 'learning_rate': 1.9643219218157395e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 586/5198 [3:19:29<23:11:02, 18.10s/it] 11%|â–ˆâ–        | 587/5198 [3:19:47<23:06:52, 18.05s/it]                                                       {'loss': 0.9113, 'learning_rate': 1.9641567839628092e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 587/5198 [3:19:47<23:06:52, 18.05s/it] 11%|â–ˆâ–        | 588/5198 [3:20:04<22:42:08, 17.73s/it]                                                       {'loss': 0.9105, 'learning_rate': 1.963991271791019e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 588/5198 [3:20:04<22:42:08, 17.73s/it] 11%|â–ˆâ–        | 589/5198 [3:20:22<22:45:03, 17.77s/it]                                                       {'loss': 0.8631, 'learning_rate': 1.9638253853646255e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 589/5198 [3:20:22<22:45:03, 17.77s/it] 11%|â–ˆâ–        | 590/5198 [3:20:39<22:39:37, 17.70s/it]                                                       {'loss': 0.928, 'learning_rate': 1.9636591247480323e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 590/5198 [3:20:39<22:39:37, 17.70s/it] 11%|â–ˆâ–        | 591/5198 [3:20:57<22:34:58, 17.65s/it]                                                       {'loss': 0.354, 'learning_rate': 1.9634924900057867e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 591/5198 [3:20:57<22:34:58, 17.65s/it] 11%|â–ˆâ–        | 592/5198 [3:21:15<22:47:12, 17.81s/it]                                                       {'loss': 0.9096, 'learning_rate': 1.963325481202583e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 592/5198 [3:21:15<22:47:12, 17.81s/it] 11%|â–ˆâ–        | 593/5198 [3:21:32<22:31:36, 17.61s/it]                                                       {'loss': 0.8964, 'learning_rate': 1.963158098403259e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 593/5198 [3:21:32<22:31:36, 17.61s/it] 11%|â–ˆâ–        | 594/5198 [3:21:50<22:26:57, 17.55s/it]                                                       {'loss': 0.8662, 'learning_rate': 1.9629903416727987e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 594/5198 [3:21:50<22:26:57, 17.55s/it] 11%|â–ˆâ–        | 595/5198 [3:22:08<22:39:58, 17.73s/it]                                                       {'loss': 0.8715, 'learning_rate': 1.962822211076331e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 595/5198 [3:22:08<22:39:58, 17.73s/it] 11%|â–ˆâ–        | 596/5198 [3:22:26<22:37:19, 17.70s/it]                                                       {'loss': 0.9132, 'learning_rate': 1.96265370667913e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 596/5198 [3:22:26<22:37:19, 17.70s/it] 11%|â–ˆâ–        | 597/5198 [3:22:43<22:24:18, 17.53s/it]                                                       {'loss': 0.8698, 'learning_rate': 1.9624848285466146e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 597/5198 [3:22:43<22:24:18, 17.53s/it] 12%|â–ˆâ–        | 598/5198 [3:22:59<22:01:38, 17.24s/it]                                                       {'loss': 0.8839, 'learning_rate': 1.9623155767443498e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 598/5198 [3:22:59<22:01:38, 17.24s/it] 12%|â–ˆâ–        | 599/5198 [3:23:18<22:32:45, 17.65s/it]                                                       {'loss': 0.89, 'learning_rate': 1.9621459513380445e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 599/5198 [3:23:18<22:32:45, 17.65s/it] 12%|â–ˆâ–        | 600/5198 [3:23:36<22:46:52, 17.84s/it]                                                       {'loss': 0.9523, 'learning_rate': 1.9619759523935532e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 600/5198 [3:23:36<22:46:52, 17.84s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 12%|â–ˆâ–        | 601/5198 [3:25:03<49:11:54, 38.53s/it]                                                       {'loss': 0.8399, 'learning_rate': 1.9618055799768757e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 601/5198 [3:25:03<49:11:54, 38.53s/it] 12%|â–ˆâ–        | 602/5198 [3:25:20<41:09:17, 32.24s/it]                                                       {'loss': 0.8895, 'learning_rate': 1.961634834154156e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 602/5198 [3:25:20<41:09:17, 32.24s/it] 12%|â–ˆâ–        | 603/5198 [3:25:38<35:38:55, 27.93s/it]                                                       {'loss': 0.9053, 'learning_rate': 1.9614637149916834e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 603/5198 [3:25:38<35:38:55, 27.93s/it] 12%|â–ˆâ–        | 604/5198 [3:25:56<31:35:58, 24.76s/it]                                                       {'loss': 0.9132, 'learning_rate': 1.9612922225558924e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 604/5198 [3:25:56<31:35:58, 24.76s/it] 12%|â–ˆâ–        | 605/5198 [3:26:14<29:03:39, 22.78s/it]                                                       {'loss': 0.8966, 'learning_rate': 1.961120356913363e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 605/5198 [3:26:14<29:03:39, 22.78s/it] 12%|â–ˆâ–        | 606/5198 [3:26:32<27:14:58, 21.36s/it]                                                       {'loss': 0.9413, 'learning_rate': 1.960948118130818e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 606/5198 [3:26:32<27:14:58, 21.36s/it] 12%|â–ˆâ–        | 607/5198 [3:26:49<25:36:36, 20.08s/it]                                                       {'loss': 0.9031, 'learning_rate': 1.9607755062751273e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 607/5198 [3:26:49<25:36:36, 20.08s/it] 12%|â–ˆâ–        | 608/5198 [3:27:06<24:25:45, 19.16s/it]                                                       {'loss': 0.3271, 'learning_rate': 1.9606025214133046e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 608/5198 [3:27:06<24:25:45, 19.16s/it] 12%|â–ˆâ–        | 609/5198 [3:27:23<23:36:39, 18.52s/it]                                                       {'loss': 0.3182, 'learning_rate': 1.9604291636125084e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 609/5198 [3:27:23<23:36:39, 18.52s/it] 12%|â–ˆâ–        | 610/5198 [3:27:42<23:38:42, 18.55s/it]                                                       {'loss': 0.8581, 'learning_rate': 1.960255432940043e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 610/5198 [3:27:42<23:38:42, 18.55s/it] 12%|â–ˆâ–        | 611/5198 [3:28:00<23:35:11, 18.51s/it]                                                       {'loss': 0.8842, 'learning_rate': 1.9600813294633552e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 611/5198 [3:28:00<23:35:11, 18.51s/it] 12%|â–ˆâ–        | 612/5198 [3:28:18<23:16:43, 18.27s/it]                                                       {'loss': 0.871, 'learning_rate': 1.9599068532500394e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 612/5198 [3:28:18<23:16:43, 18.27s/it] 12%|â–ˆâ–        | 613/5198 [3:28:35<22:53:17, 17.97s/it]                                                       {'loss': 0.9226, 'learning_rate': 1.9597320043678322e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 613/5198 [3:28:35<22:53:17, 17.97s/it] 12%|â–ˆâ–        | 614/5198 [3:28:52<22:29:10, 17.66s/it]                                                       {'loss': 0.8786, 'learning_rate': 1.9595567828846166e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 614/5198 [3:28:52<22:29:10, 17.66s/it] 12%|â–ˆâ–        | 615/5198 [3:29:11<22:59:01, 18.05s/it]                                                       {'loss': 0.9156, 'learning_rate': 1.9593811888684192e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 615/5198 [3:29:11<22:59:01, 18.05s/it] 12%|â–ˆâ–        | 616/5198 [3:29:29<22:49:20, 17.93s/it]                                                       {'loss': 0.8715, 'learning_rate': 1.9592052223874115e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 616/5198 [3:29:29<22:49:20, 17.93s/it] 12%|â–ˆâ–        | 617/5198 [3:29:46<22:29:17, 17.67s/it]                                                       {'loss': 0.8922, 'learning_rate': 1.959028883509911e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 617/5198 [3:29:46<22:29:17, 17.67s/it] 12%|â–ˆâ–        | 618/5198 [3:30:03<22:18:28, 17.53s/it]                                                       {'loss': 0.8848, 'learning_rate': 1.9588521723043764e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 618/5198 [3:30:03<22:18:28, 17.53s/it] 12%|â–ˆâ–        | 619/5198 [3:30:21<22:30:05, 17.69s/it]                                                       {'loss': 0.9105, 'learning_rate': 1.958675088839415e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 619/5198 [3:30:21<22:30:05, 17.69s/it] 12%|â–ˆâ–        | 620/5198 [3:30:39<22:28:33, 17.67s/it]                                                       {'loss': 0.8901, 'learning_rate': 1.9584976331837758e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 620/5198 [3:30:39<22:28:33, 17.67s/it] 12%|â–ˆâ–        | 621/5198 [3:30:55<22:09:07, 17.42s/it]                                                       {'loss': 0.9205, 'learning_rate': 1.9583198054063535e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 621/5198 [3:30:55<22:09:07, 17.42s/it] 12%|â–ˆâ–        | 622/5198 [3:31:13<22:07:05, 17.40s/it]                                                       {'loss': 0.8992, 'learning_rate': 1.9581416055761865e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 622/5198 [3:31:13<22:07:05, 17.40s/it] 12%|â–ˆâ–        | 623/5198 [3:31:31<22:20:34, 17.58s/it]                                                       {'loss': 0.9642, 'learning_rate': 1.9579630337624585e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 623/5198 [3:31:31<22:20:34, 17.58s/it] 12%|â–ˆâ–        | 624/5198 [3:31:48<22:11:26, 17.47s/it]                                                       {'loss': 0.9351, 'learning_rate': 1.9577840900344974e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 624/5198 [3:31:48<22:11:26, 17.47s/it] 12%|â–ˆâ–        | 625/5198 [3:32:06<22:16:16, 17.53s/it]                                                       {'loss': 0.892, 'learning_rate': 1.9576047744617752e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 625/5198 [3:32:06<22:16:16, 17.53s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 12%|â–ˆâ–        | 626/5198 [3:33:32<48:23:44, 38.11s/it]                                                       {'loss': 0.8485, 'learning_rate': 1.957425087113908e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 626/5198 [3:33:32<48:23:44, 38.11s/it] 12%|â–ˆâ–        | 627/5198 [3:33:50<40:37:11, 31.99s/it]                                                       {'loss': 0.9481, 'learning_rate': 1.9572450280606568e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 627/5198 [3:33:50<40:37:11, 31.99s/it] 12%|â–ˆâ–        | 628/5198 [3:34:08<35:24:40, 27.89s/it]                                                       {'loss': 0.8871, 'learning_rate': 1.9570645973719273e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 628/5198 [3:34:08<35:24:40, 27.89s/it] 12%|â–ˆâ–        | 629/5198 [3:34:24<31:01:03, 24.44s/it]                                                       {'loss': 0.8766, 'learning_rate': 1.9568837951177677e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 629/5198 [3:34:24<31:01:03, 24.44s/it] 12%|â–ˆâ–        | 630/5198 [3:34:42<28:36:18, 22.54s/it]                                                       {'loss': 0.9172, 'learning_rate': 1.9567026213683728e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 630/5198 [3:34:42<28:36:18, 22.54s/it] 12%|â–ˆâ–        | 631/5198 [3:35:00<26:53:24, 21.20s/it]                                                       {'loss': 0.9249, 'learning_rate': 1.9565210761940798e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 631/5198 [3:35:00<26:53:24, 21.20s/it] 12%|â–ˆâ–        | 632/5198 [3:35:18<25:22:50, 20.01s/it]                                                       {'loss': 0.9312, 'learning_rate': 1.956339159665371e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 632/5198 [3:35:18<25:22:50, 20.01s/it] 12%|â–ˆâ–        | 633/5198 [3:35:35<24:24:41, 19.25s/it]                                                       {'loss': 0.9163, 'learning_rate': 1.956156871852873e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 633/5198 [3:35:35<24:24:41, 19.25s/it] 12%|â–ˆâ–        | 634/5198 [3:35:53<23:51:54, 18.82s/it]                                                       {'loss': 0.9095, 'learning_rate': 1.9559742128273558e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 634/5198 [3:35:53<23:51:54, 18.82s/it] 12%|â–ˆâ–        | 635/5198 [3:36:11<23:25:50, 18.49s/it]                                                       {'loss': 0.882, 'learning_rate': 1.9557911826597337e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 635/5198 [3:36:11<23:25:50, 18.49s/it] 12%|â–ˆâ–        | 636/5198 [3:36:28<23:07:24, 18.25s/it]                                                       {'loss': 0.8945, 'learning_rate': 1.9556077814210662e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 636/5198 [3:36:28<23:07:24, 18.25s/it] 12%|â–ˆâ–        | 637/5198 [3:36:46<22:48:45, 18.01s/it]                                                       {'loss': 0.8752, 'learning_rate': 1.955424009182555e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 637/5198 [3:36:46<22:48:45, 18.01s/it] 12%|â–ˆâ–        | 638/5198 [3:37:05<23:05:13, 18.23s/it]                                                       {'loss': 0.8358, 'learning_rate': 1.955239866015547e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 638/5198 [3:37:05<23:05:13, 18.23s/it] 12%|â–ˆâ–        | 639/5198 [3:37:22<22:57:37, 18.13s/it]                                                       {'loss': 0.8902, 'learning_rate': 1.9550553519915335e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 639/5198 [3:37:22<22:57:37, 18.13s/it] 12%|â–ˆâ–        | 640/5198 [3:37:39<22:16:51, 17.60s/it]                                                       {'loss': 0.9097, 'learning_rate': 1.954870467182149e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 640/5198 [3:37:39<22:16:51, 17.60s/it] 12%|â–ˆâ–        | 641/5198 [3:37:56<22:11:44, 17.53s/it]                                                       {'loss': 0.8782, 'learning_rate': 1.954685211659172e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 641/5198 [3:37:56<22:11:44, 17.53s/it] 12%|â–ˆâ–        | 642/5198 [3:38:14<22:21:38, 17.67s/it]                                                       {'loss': 0.9214, 'learning_rate': 1.9544995854945248e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 642/5198 [3:38:14<22:21:38, 17.67s/it] 12%|â–ˆâ–        | 643/5198 [3:38:32<22:29:47, 17.78s/it]                                                       {'loss': 0.8996, 'learning_rate': 1.954313588760274e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 643/5198 [3:38:32<22:29:47, 17.78s/it] 12%|â–ˆâ–        | 644/5198 [3:38:50<22:34:00, 17.84s/it]                                                       {'loss': 0.8382, 'learning_rate': 1.9541272215286304e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 644/5198 [3:38:50<22:34:00, 17.84s/it] 12%|â–ˆâ–        | 645/5198 [3:39:08<22:25:49, 17.74s/it]                                                       {'loss': 0.8748, 'learning_rate': 1.9539404838719477e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 645/5198 [3:39:08<22:25:49, 17.74s/it] 12%|â–ˆâ–        | 646/5198 [3:39:25<22:07:56, 17.50s/it]                                                       {'loss': 0.8517, 'learning_rate': 1.9537533758627242e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 646/5198 [3:39:25<22:07:56, 17.50s/it] 12%|â–ˆâ–        | 647/5198 [3:39:42<22:05:47, 17.48s/it]                                                       {'loss': 0.9243, 'learning_rate': 1.953565897573601e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 647/5198 [3:39:42<22:05:47, 17.48s/it] 12%|â–ˆâ–        | 648/5198 [3:40:00<22:10:11, 17.54s/it]                                                       {'loss': 0.851, 'learning_rate': 1.9533780490773645e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 648/5198 [3:40:00<22:10:11, 17.54s/it] 12%|â–ˆâ–        | 649/5198 [3:40:18<22:25:47, 17.75s/it]                                                       {'loss': 0.8391, 'learning_rate': 1.9531898304469435e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 649/5198 [3:40:18<22:25:47, 17.75s/it] 13%|â–ˆâ–Ž        | 650/5198 [3:40:36<22:40:16, 17.95s/it]                                                       {'loss': 0.8929, 'learning_rate': 1.953001241755411e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 650/5198 [3:40:36<22:40:16, 17.95s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 13%|â–ˆâ–Ž        | 651/5198 [3:42:04<49:10:23, 38.93s/it]                                                       {'loss': 0.9296, 'learning_rate': 1.952812283075984e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 651/5198 [3:42:04<49:10:23, 38.93s/it] 13%|â–ˆâ–Ž        | 652/5198 [3:42:21<40:55:00, 32.40s/it]                                                       {'loss': 0.8922, 'learning_rate': 1.952622954482022e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 652/5198 [3:42:21<40:55:00, 32.40s/it] 13%|â–ˆâ–Ž        | 653/5198 [3:42:39<35:24:15, 28.04s/it]                                                       {'loss': 0.9614, 'learning_rate': 1.9524332560470293e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 653/5198 [3:42:39<35:24:15, 28.04s/it] 13%|â–ˆâ–Ž        | 654/5198 [3:42:56<31:14:21, 24.75s/it]                                                       {'loss': 0.8926, 'learning_rate': 1.9522431878446536e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 654/5198 [3:42:56<31:14:21, 24.75s/it] 13%|â–ˆâ–Ž        | 655/5198 [3:43:14<28:25:35, 22.53s/it]                                                       {'loss': 0.9653, 'learning_rate': 1.9520527499486856e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 655/5198 [3:43:14<28:25:35, 22.53s/it] 13%|â–ˆâ–Ž        | 656/5198 [3:43:31<26:18:46, 20.86s/it]                                                       {'loss': 0.3633, 'learning_rate': 1.95186194243306e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 656/5198 [3:43:31<26:18:46, 20.86s/it] 13%|â–ˆâ–Ž        | 657/5198 [3:43:48<24:56:36, 19.77s/it]                                                       {'loss': 0.8834, 'learning_rate': 1.9516707653718546e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 657/5198 [3:43:48<24:56:36, 19.77s/it] 13%|â–ˆâ–Ž        | 658/5198 [3:44:05<24:01:17, 19.05s/it]                                                       {'loss': 0.8402, 'learning_rate': 1.9514792188392914e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 658/5198 [3:44:05<24:01:17, 19.05s/it] 13%|â–ˆâ–Ž        | 659/5198 [3:44:23<23:26:02, 18.59s/it]                                                       {'loss': 0.8855, 'learning_rate': 1.9512873029097347e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 659/5198 [3:44:23<23:26:02, 18.59s/it] 13%|â–ˆâ–Ž        | 660/5198 [3:44:40<22:51:53, 18.14s/it]                                                       {'loss': 0.873, 'learning_rate': 1.9510950176576933e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 660/5198 [3:44:40<22:51:53, 18.14s/it] 13%|â–ˆâ–Ž        | 661/5198 [3:44:57<22:21:18, 17.74s/it]                                                       {'loss': 0.8908, 'learning_rate': 1.950902363157819e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 661/5198 [3:44:57<22:21:18, 17.74s/it] 13%|â–ˆâ–Ž        | 662/5198 [3:45:15<22:26:53, 17.82s/it]                                                       {'loss': 0.8645, 'learning_rate': 1.950709339484907e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 662/5198 [3:45:15<22:26:53, 17.82s/it] 13%|â–ˆâ–Ž        | 663/5198 [3:45:32<22:13:56, 17.65s/it]                                                       {'loss': 0.9303, 'learning_rate': 1.9505159467138954e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 663/5198 [3:45:32<22:13:56, 17.65s/it] 13%|â–ˆâ–Ž        | 664/5198 [3:45:49<22:06:25, 17.55s/it]                                                       {'loss': 0.8591, 'learning_rate': 1.9503221849198655e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 664/5198 [3:45:49<22:06:25, 17.55s/it] 13%|â–ˆâ–Ž        | 665/5198 [3:46:07<22:01:41, 17.49s/it]                                                       {'loss': 0.8915, 'learning_rate': 1.9501280541780435e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 665/5198 [3:46:07<22:01:41, 17.49s/it] 13%|â–ˆâ–Ž        | 666/5198 [3:46:24<21:49:06, 17.33s/it]                                                       {'loss': 0.9018, 'learning_rate': 1.9499335545637968e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 666/5198 [3:46:24<21:49:06, 17.33s/it] 13%|â–ˆâ–Ž        | 667/5198 [3:46:41<21:58:16, 17.46s/it]                                                       {'loss': 0.9397, 'learning_rate': 1.949738686152637e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 667/5198 [3:46:41<21:58:16, 17.46s/it] 13%|â–ˆâ–Ž        | 668/5198 [3:46:59<22:05:40, 17.56s/it]                                                       {'loss': 0.8731, 'learning_rate': 1.9495434490202188e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 668/5198 [3:46:59<22:05:40, 17.56s/it] 13%|â–ˆâ–Ž        | 669/5198 [3:47:17<22:17:37, 17.72s/it]                                                       {'loss': 0.929, 'learning_rate': 1.94934784324234e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 669/5198 [3:47:17<22:17:37, 17.72s/it] 13%|â–ˆâ–Ž        | 670/5198 [3:47:36<22:30:53, 17.90s/it]                                                       {'loss': 0.8514, 'learning_rate': 1.9491518688949417e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 670/5198 [3:47:36<22:30:53, 17.90s/it] 13%|â–ˆâ–Ž        | 671/5198 [3:47:53<22:17:08, 17.72s/it]                                                       {'loss': 0.8534, 'learning_rate': 1.9489555260541074e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 671/5198 [3:47:53<22:17:08, 17.72s/it] 13%|â–ˆâ–Ž        | 672/5198 [3:48:11<22:19:51, 17.76s/it]                                                       {'loss': 0.916, 'learning_rate': 1.948758814796064e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 672/5198 [3:48:11<22:19:51, 17.76s/it] 13%|â–ˆâ–Ž        | 673/5198 [3:48:28<22:07:51, 17.61s/it]                                                       {'loss': 0.3728, 'learning_rate': 1.9485617351971827e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 673/5198 [3:48:28<22:07:51, 17.61s/it] 13%|â–ˆâ–Ž        | 674/5198 [3:48:45<21:55:41, 17.45s/it]                                                       {'loss': 0.8272, 'learning_rate': 1.9483642873339753e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 674/5198 [3:48:45<21:55:41, 17.45s/it] 13%|â–ˆâ–Ž        | 675/5198 [3:49:04<22:18:44, 17.76s/it]                                                       {'loss': 0.8855, 'learning_rate': 1.9481664712830987e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 675/5198 [3:49:04<22:18:44, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 13%|â–ˆâ–Ž        | 676/5198 [3:50:30<48:07:55, 38.32s/it]                                                       {'loss': 0.3695, 'learning_rate': 1.9479682871213515e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 676/5198 [3:50:30<48:07:55, 38.32s/it] 13%|â–ˆâ–Ž        | 677/5198 [3:50:47<40:09:58, 31.98s/it]                                                       {'loss': 0.9132, 'learning_rate': 1.9477697349256756e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 677/5198 [3:50:47<40:09:58, 31.98s/it] 13%|â–ˆâ–Ž        | 678/5198 [3:51:05<34:57:10, 27.84s/it]                                                       {'loss': 0.8728, 'learning_rate': 1.947570814773156e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 678/5198 [3:51:05<34:57:10, 27.84s/it] 13%|â–ˆâ–Ž        | 679/5198 [3:51:22<30:56:46, 24.65s/it]                                                       {'loss': 0.3427, 'learning_rate': 1.9473715267410206e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 679/5198 [3:51:22<30:56:46, 24.65s/it] 13%|â–ˆâ–Ž        | 680/5198 [3:51:40<28:07:39, 22.41s/it]                                                       {'loss': 0.9399, 'learning_rate': 1.9471718709066392e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 680/5198 [3:51:40<28:07:39, 22.41s/it] 13%|â–ˆâ–Ž        | 681/5198 [3:51:57<26:21:35, 21.01s/it]                                                       {'loss': 0.94, 'learning_rate': 1.9469718473475256e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 681/5198 [3:51:57<26:21:35, 21.01s/it] 13%|â–ˆâ–Ž        | 682/5198 [3:52:15<24:59:15, 19.92s/it]                                                       {'loss': 0.9402, 'learning_rate': 1.9467714561413358e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 682/5198 [3:52:15<24:59:15, 19.92s/it] 13%|â–ˆâ–Ž        | 683/5198 [3:52:32<23:48:54, 18.99s/it]                                                       {'loss': 0.936, 'learning_rate': 1.9465706973658683e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 683/5198 [3:52:32<23:48:54, 18.99s/it] 13%|â–ˆâ–Ž        | 684/5198 [3:52:49<23:24:41, 18.67s/it]                                                       {'loss': 0.8994, 'learning_rate': 1.9463695710990648e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 684/5198 [3:52:49<23:24:41, 18.67s/it] 13%|â–ˆâ–Ž        | 685/5198 [3:53:06<22:46:29, 18.17s/it]                                                       {'loss': 0.8948, 'learning_rate': 1.946168077419009e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 685/5198 [3:53:06<22:46:29, 18.17s/it] 13%|â–ˆâ–Ž        | 686/5198 [3:53:24<22:37:54, 18.06s/it]                                                       {'loss': 0.9047, 'learning_rate': 1.9459662164039283e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 686/5198 [3:53:24<22:37:54, 18.06s/it] 13%|â–ˆâ–Ž        | 687/5198 [3:53:42<22:25:44, 17.90s/it]                                                       {'loss': 0.9167, 'learning_rate': 1.9457639881321917e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 687/5198 [3:53:42<22:25:44, 17.90s/it] 13%|â–ˆâ–Ž        | 688/5198 [3:53:59<22:06:50, 17.65s/it]                                                       {'loss': 0.9217, 'learning_rate': 1.9455613926823115e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 688/5198 [3:53:59<22:06:50, 17.65s/it] 13%|â–ˆâ–Ž        | 689/5198 [3:54:17<22:16:56, 17.79s/it]                                                       {'loss': 0.9021, 'learning_rate': 1.945358430132942e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 689/5198 [3:54:17<22:16:56, 17.79s/it] 13%|â–ˆâ–Ž        | 690/5198 [3:54:35<22:26:47, 17.93s/it]                                                       {'loss': 0.9329, 'learning_rate': 1.9451551005628803e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 690/5198 [3:54:35<22:26:47, 17.93s/it] 13%|â–ˆâ–Ž        | 691/5198 [3:54:52<22:06:11, 17.66s/it]                                                       {'loss': 0.3313, 'learning_rate': 1.9449514040510654e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 691/5198 [3:54:52<22:06:11, 17.66s/it] 13%|â–ˆâ–Ž        | 692/5198 [3:55:10<22:10:53, 17.72s/it]                                                       {'loss': 0.8828, 'learning_rate': 1.9447473406765803e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 692/5198 [3:55:10<22:10:53, 17.72s/it] 13%|â–ˆâ–Ž        | 693/5198 [3:55:29<22:31:58, 18.01s/it]                                                       {'loss': 0.865, 'learning_rate': 1.9445429105186487e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 693/5198 [3:55:29<22:31:58, 18.01s/it] 13%|â–ˆâ–Ž        | 694/5198 [3:55:46<22:04:46, 17.65s/it]                                                       {'loss': 0.876, 'learning_rate': 1.9443381136566382e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 694/5198 [3:55:46<22:04:46, 17.65s/it] 13%|â–ˆâ–Ž        | 695/5198 [3:56:04<22:20:44, 17.86s/it]                                                       {'loss': 0.8714, 'learning_rate': 1.9441329501700568e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 695/5198 [3:56:04<22:20:44, 17.86s/it] 13%|â–ˆâ–Ž        | 696/5198 [3:56:22<22:30:34, 18.00s/it]                                                       {'loss': 0.8697, 'learning_rate': 1.943927420138557e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 696/5198 [3:56:22<22:30:34, 18.00s/it] 13%|â–ˆâ–Ž        | 697/5198 [3:56:40<22:33:21, 18.04s/it]                                                       {'loss': 0.9119, 'learning_rate': 1.9437215236419322e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 697/5198 [3:56:40<22:33:21, 18.04s/it] 13%|â–ˆâ–Ž        | 698/5198 [3:56:59<22:49:46, 18.26s/it]                                                       {'loss': 0.8936, 'learning_rate': 1.9435152607601187e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 698/5198 [3:56:59<22:49:46, 18.26s/it] 13%|â–ˆâ–Ž        | 699/5198 [3:57:16<22:22:06, 17.90s/it]                                                       {'loss': 0.886, 'learning_rate': 1.943308631573195e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 699/5198 [3:57:16<22:22:06, 17.90s/it] 13%|â–ˆâ–Ž        | 700/5198 [3:57:34<22:25:21, 17.95s/it]                                                       {'loss': 0.8896, 'learning_rate': 1.9431016361613816e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 700/5198 [3:57:34<22:25:21, 17.95s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 13%|â–ˆâ–Ž        | 701/5198 [3:59:04<49:22:17, 39.52s/it]                                                       {'loss': 0.9082, 'learning_rate': 1.9428942746050406e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 701/5198 [3:59:04<49:22:17, 39.52s/it] 14%|â–ˆâ–Ž        | 702/5198 [3:59:22<41:10:57, 32.98s/it]                                                       {'loss': 0.3213, 'learning_rate': 1.9426865469846773e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 702/5198 [3:59:22<41:10:57, 32.98s/it] 14%|â–ˆâ–Ž        | 703/5198 [3:59:40<35:42:05, 28.59s/it]                                                       {'loss': 0.9212, 'learning_rate': 1.9424784533809393e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 703/5198 [3:59:40<35:42:05, 28.59s/it] 14%|â–ˆâ–Ž        | 704/5198 [3:59:58<31:38:34, 25.35s/it]                                                       {'loss': 0.9133, 'learning_rate': 1.942269993874615e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 704/5198 [3:59:58<31:38:34, 25.35s/it] 14%|â–ˆâ–Ž        | 705/5198 [4:00:15<28:40:17, 22.97s/it]                                                       {'loss': 0.9538, 'learning_rate': 1.9420611685466358e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 705/5198 [4:00:15<28:40:17, 22.97s/it] 14%|â–ˆâ–Ž        | 706/5198 [4:00:34<27:02:02, 21.67s/it]                                                       {'loss': 0.866, 'learning_rate': 1.9418519774780748e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 706/5198 [4:00:34<27:02:02, 21.67s/it] 14%|â–ˆâ–Ž        | 707/5198 [4:00:52<25:44:19, 20.63s/it]                                                       {'loss': 0.8872, 'learning_rate': 1.9416424207501474e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 707/5198 [4:00:52<25:44:19, 20.63s/it] 14%|â–ˆâ–Ž        | 708/5198 [4:01:11<24:50:22, 19.92s/it]                                                       {'loss': 0.863, 'learning_rate': 1.9414324984442102e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 708/5198 [4:01:11<24:50:22, 19.92s/it] 14%|â–ˆâ–Ž        | 709/5198 [4:01:29<24:08:10, 19.36s/it]                                                       {'loss': 0.8569, 'learning_rate': 1.9412222106417632e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 709/5198 [4:01:29<24:08:10, 19.36s/it] 14%|â–ˆâ–Ž        | 710/5198 [4:01:47<23:50:40, 19.13s/it]                                                       {'loss': 0.9347, 'learning_rate': 1.9410115574244462e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 710/5198 [4:01:47<23:50:40, 19.13s/it] 14%|â–ˆâ–Ž        | 711/5198 [4:02:05<23:17:47, 18.69s/it]                                                       {'loss': 0.8852, 'learning_rate': 1.9408005388740433e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 711/5198 [4:02:05<23:17:47, 18.69s/it] 14%|â–ˆâ–Ž        | 712/5198 [4:02:22<22:40:26, 18.20s/it]                                                       {'loss': 0.9317, 'learning_rate': 1.9405891550724778e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 712/5198 [4:02:22<22:40:26, 18.20s/it] 14%|â–ˆâ–Ž        | 713/5198 [4:02:40<22:37:33, 18.16s/it]                                                       {'loss': 0.9233, 'learning_rate': 1.940377406101817e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 713/5198 [4:02:40<22:37:33, 18.16s/it] 14%|â–ˆâ–Ž        | 714/5198 [4:02:57<22:08:33, 17.78s/it]                                                       {'loss': 0.8984, 'learning_rate': 1.9401652920442694e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 714/5198 [4:02:57<22:08:33, 17.78s/it] 14%|â–ˆâ–        | 715/5198 [4:03:15<22:23:52, 17.99s/it]                                                       {'loss': 0.982, 'learning_rate': 1.9399528129821842e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 715/5198 [4:03:16<22:23:52, 17.99s/it] 14%|â–ˆâ–        | 716/5198 [4:03:33<22:20:06, 17.94s/it]                                                       {'loss': 0.9019, 'learning_rate': 1.939739968998054e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 716/5198 [4:03:33<22:20:06, 17.94s/it] 14%|â–ˆâ–        | 717/5198 [4:03:51<22:20:55, 17.95s/it]                                                       {'loss': 0.9463, 'learning_rate': 1.939526760174511e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 717/5198 [4:03:51<22:20:55, 17.95s/it] 14%|â–ˆâ–        | 718/5198 [4:04:08<22:06:05, 17.76s/it]                                                       {'loss': 0.9498, 'learning_rate': 1.939313186594331e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 718/5198 [4:04:08<22:06:05, 17.76s/it] 14%|â–ˆâ–        | 719/5198 [4:04:26<21:59:07, 17.67s/it]                                                       {'loss': 0.8578, 'learning_rate': 1.9390992483404308e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 719/5198 [4:04:26<21:59:07, 17.67s/it] 14%|â–ˆâ–        | 720/5198 [4:04:43<21:54:19, 17.61s/it]                                                       {'loss': 0.9156, 'learning_rate': 1.938884945495868e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 720/5198 [4:04:43<21:54:19, 17.61s/it] 14%|â–ˆâ–        | 721/5198 [4:05:01<21:51:12, 17.57s/it]                                                       {'loss': 0.9061, 'learning_rate': 1.9386702781438425e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 721/5198 [4:05:01<21:51:12, 17.57s/it] 14%|â–ˆâ–        | 722/5198 [4:05:19<21:59:49, 17.69s/it]                                                       {'loss': 0.9401, 'learning_rate': 1.938455246367696e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 722/5198 [4:05:19<21:59:49, 17.69s/it] 14%|â–ˆâ–        | 723/5198 [4:05:37<22:16:07, 17.91s/it]                                                       {'loss': 0.8741, 'learning_rate': 1.9382398502509107e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 723/5198 [4:05:37<22:16:07, 17.91s/it] 14%|â–ˆâ–        | 724/5198 [4:05:55<22:11:33, 17.86s/it]                                                       {'loss': 0.8917, 'learning_rate': 1.938024089877111e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 724/5198 [4:05:55<22:11:33, 17.86s/it] 14%|â–ˆâ–        | 725/5198 [4:06:12<21:57:01, 17.67s/it]                                                       {'loss': 0.9091, 'learning_rate': 1.9378079653300624e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 725/5198 [4:06:12<21:57:01, 17.67s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 14%|â–ˆâ–        | 726/5198 [4:07:39<47:41:58, 38.40s/it]                                                       {'loss': 0.8904, 'learning_rate': 1.9375914766936723e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 726/5198 [4:07:39<47:41:58, 38.40s/it] 14%|â–ˆâ–        | 727/5198 [4:07:57<39:59:07, 32.20s/it]                                                       {'loss': 0.9249, 'learning_rate': 1.9373746240519884e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 727/5198 [4:07:57<39:59:07, 32.20s/it] 14%|â–ˆâ–        | 728/5198 [4:08:15<34:54:52, 28.12s/it]                                                       {'loss': 0.9216, 'learning_rate': 1.937157407489201e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 728/5198 [4:08:15<34:54:52, 28.12s/it] 14%|â–ˆâ–        | 729/5198 [4:08:34<31:21:54, 25.27s/it]                                                       {'loss': 0.8877, 'learning_rate': 1.9369398270896403e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 729/5198 [4:08:34<31:21:54, 25.27s/it] 14%|â–ˆâ–        | 730/5198 [4:08:51<28:28:48, 22.95s/it]                                                       {'loss': 0.3111, 'learning_rate': 1.936721882937779e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 730/5198 [4:08:51<28:28:48, 22.95s/it] 14%|â–ˆâ–        | 731/5198 [4:09:09<26:32:41, 21.39s/it]                                                       {'loss': 0.9059, 'learning_rate': 1.9365035751182307e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 731/5198 [4:09:09<26:32:41, 21.39s/it] 14%|â–ˆâ–        | 732/5198 [4:09:26<24:52:03, 20.05s/it]                                                       {'loss': 0.8937, 'learning_rate': 1.93628490371575e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 732/5198 [4:09:26<24:52:03, 20.05s/it] 14%|â–ˆâ–        | 733/5198 [4:09:43<23:47:06, 19.18s/it]                                                       {'loss': 0.9119, 'learning_rate': 1.9360658688152322e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 733/5198 [4:09:43<23:47:06, 19.18s/it] 14%|â–ˆâ–        | 734/5198 [4:10:01<23:20:41, 18.83s/it]                                                       {'loss': 0.8667, 'learning_rate': 1.9358464705017143e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 734/5198 [4:10:01<23:20:41, 18.83s/it] 14%|â–ˆâ–        | 735/5198 [4:10:18<22:41:18, 18.30s/it]                                                       {'loss': 0.9268, 'learning_rate': 1.9356267088603745e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 735/5198 [4:10:18<22:41:18, 18.30s/it] 14%|â–ˆâ–        | 736/5198 [4:10:37<22:45:41, 18.36s/it]                                                       {'loss': 0.8424, 'learning_rate': 1.9354065839765316e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 736/5198 [4:10:37<22:45:41, 18.36s/it] 14%|â–ˆâ–        | 737/5198 [4:10:54<22:16:45, 17.98s/it]                                                       {'loss': 0.9135, 'learning_rate': 1.9351860959356462e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 737/5198 [4:10:54<22:16:45, 17.98s/it] 14%|â–ˆâ–        | 738/5198 [4:11:11<22:06:28, 17.84s/it]                                                       {'loss': 0.8655, 'learning_rate': 1.9349652448233187e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 738/5198 [4:11:11<22:06:28, 17.84s/it] 14%|â–ˆâ–        | 739/5198 [4:11:29<22:00:35, 17.77s/it]                                                       {'loss': 0.8929, 'learning_rate': 1.934744030725291e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 739/5198 [4:11:29<22:00:35, 17.77s/it] 14%|â–ˆâ–        | 740/5198 [4:11:48<22:17:54, 18.01s/it]                                                       {'loss': 0.8727, 'learning_rate': 1.934522453727447e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 740/5198 [4:11:48<22:17:54, 18.01s/it] 14%|â–ˆâ–        | 741/5198 [4:12:05<21:57:24, 17.73s/it]                                                       {'loss': 0.8703, 'learning_rate': 1.93430051391581e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 741/5198 [4:12:05<21:57:24, 17.73s/it] 14%|â–ˆâ–        | 742/5198 [4:12:23<22:01:28, 17.79s/it]                                                       {'loss': 0.8987, 'learning_rate': 1.934078211376544e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 742/5198 [4:12:23<22:01:28, 17.79s/it] 14%|â–ˆâ–        | 743/5198 [4:12:40<21:47:11, 17.61s/it]                                                       {'loss': 0.8996, 'learning_rate': 1.9338555461959554e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 743/5198 [4:12:40<21:47:11, 17.61s/it] 14%|â–ˆâ–        | 744/5198 [4:12:57<21:48:12, 17.62s/it]                                                       {'loss': 0.9047, 'learning_rate': 1.93363251846049e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 744/5198 [4:12:57<21:48:12, 17.62s/it] 14%|â–ˆâ–        | 745/5198 [4:13:16<22:02:34, 17.82s/it]                                                       {'loss': 0.8882, 'learning_rate': 1.9334091282567352e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 745/5198 [4:13:16<22:02:34, 17.82s/it] 14%|â–ˆâ–        | 746/5198 [4:13:34<22:01:08, 17.81s/it]                                                       {'loss': 0.9125, 'learning_rate': 1.9331853756714185e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 746/5198 [4:13:34<22:01:08, 17.81s/it] 14%|â–ˆâ–        | 747/5198 [4:13:51<21:55:11, 17.73s/it]                                                       {'loss': 0.8932, 'learning_rate': 1.9329612607914088e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 747/5198 [4:13:51<21:55:11, 17.73s/it] 14%|â–ˆâ–        | 748/5198 [4:14:09<22:06:29, 17.89s/it]                                                       {'loss': 0.3404, 'learning_rate': 1.9327367837037142e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 748/5198 [4:14:09<22:06:29, 17.89s/it] 14%|â–ˆâ–        | 749/5198 [4:14:26<21:41:11, 17.55s/it]                                                       {'loss': 0.3162, 'learning_rate': 1.9325119444954855e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 749/5198 [4:14:26<21:41:11, 17.55s/it] 14%|â–ˆâ–        | 750/5198 [4:14:44<21:47:53, 17.64s/it]                                                       {'loss': 0.9128, 'learning_rate': 1.9322867432540126e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 750/5198 [4:14:44<21:47:53, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 14%|â–ˆâ–        | 751/5198 [4:16:10<47:15:03, 38.25s/it]                                                       {'loss': 0.9348, 'learning_rate': 1.9320611800667268e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 751/5198 [4:16:10<47:15:03, 38.25s/it] 14%|â–ˆâ–        | 752/5198 [4:16:28<39:36:24, 32.07s/it]                                                       {'loss': 0.9112, 'learning_rate': 1.9318352550211986e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 752/5198 [4:16:28<39:36:24, 32.07s/it] 14%|â–ˆâ–        | 753/5198 [4:16:45<34:09:44, 27.67s/it]                                                       {'loss': 0.859, 'learning_rate': 1.9316089682051403e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 753/5198 [4:16:45<34:09:44, 27.67s/it] 15%|â–ˆâ–        | 754/5198 [4:17:03<30:19:43, 24.57s/it]                                                       {'loss': 0.8895, 'learning_rate': 1.9313823197064042e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 754/5198 [4:17:03<30:19:43, 24.57s/it] 15%|â–ˆâ–        | 755/5198 [4:17:19<27:23:00, 22.19s/it]                                                       {'loss': 0.8873, 'learning_rate': 1.9311553096129835e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 755/5198 [4:17:19<27:23:00, 22.19s/it] 15%|â–ˆâ–        | 756/5198 [4:17:36<25:29:30, 20.66s/it]                                                       {'loss': 0.8987, 'learning_rate': 1.9309279380130112e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 756/5198 [4:17:36<25:29:30, 20.66s/it] 15%|â–ˆâ–        | 757/5198 [4:17:53<24:07:38, 19.56s/it]                                                       {'loss': 0.9719, 'learning_rate': 1.93070020499476e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 757/5198 [4:17:53<24:07:38, 19.56s/it] 15%|â–ˆâ–        | 758/5198 [4:18:11<23:34:08, 19.11s/it]                                                       {'loss': 0.9544, 'learning_rate': 1.930472110646645e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 758/5198 [4:18:11<23:34:08, 19.11s/it] 15%|â–ˆâ–        | 759/5198 [4:18:30<23:11:07, 18.80s/it]                                                       {'loss': 0.9007, 'learning_rate': 1.9302436550572187e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 759/5198 [4:18:30<23:11:07, 18.80s/it] 15%|â–ˆâ–        | 760/5198 [4:18:47<22:48:27, 18.50s/it]                                                       {'loss': 0.9116, 'learning_rate': 1.930014838315177e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 760/5198 [4:18:47<22:48:27, 18.50s/it] 15%|â–ˆâ–        | 761/5198 [4:19:05<22:26:15, 18.20s/it]                                                       {'loss': 0.8692, 'learning_rate': 1.9297856605093534e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 761/5198 [4:19:05<22:26:15, 18.20s/it] 15%|â–ˆâ–        | 762/5198 [4:19:23<22:20:48, 18.14s/it]                                                       {'loss': 0.8795, 'learning_rate': 1.9295561217287226e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 762/5198 [4:19:23<22:20:48, 18.14s/it] 15%|â–ˆâ–        | 763/5198 [4:19:40<22:06:55, 17.95s/it]                                                       {'loss': 0.9318, 'learning_rate': 1.9293262220624002e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 763/5198 [4:19:40<22:06:55, 17.95s/it] 15%|â–ˆâ–        | 764/5198 [4:19:57<21:42:28, 17.62s/it]                                                       {'loss': 0.8803, 'learning_rate': 1.9290959615996407e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 764/5198 [4:19:57<21:42:28, 17.62s/it] 15%|â–ˆâ–        | 765/5198 [4:20:15<21:49:30, 17.72s/it]                                                       {'loss': 0.9089, 'learning_rate': 1.9288653404298392e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 765/5198 [4:20:15<21:49:30, 17.72s/it] 15%|â–ˆâ–        | 766/5198 [4:20:32<21:27:28, 17.43s/it]                                                       {'loss': 0.8571, 'learning_rate': 1.9286343586425307e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 766/5198 [4:20:32<21:27:28, 17.43s/it] 15%|â–ˆâ–        | 767/5198 [4:20:48<21:04:24, 17.12s/it]                                                       {'loss': 0.8995, 'learning_rate': 1.9284030163273907e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 767/5198 [4:20:48<21:04:24, 17.12s/it] 15%|â–ˆâ–        | 768/5198 [4:21:06<21:08:34, 17.18s/it]                                                       {'loss': 0.8968, 'learning_rate': 1.9281713135742333e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 768/5198 [4:21:06<21:08:34, 17.18s/it] 15%|â–ˆâ–        | 769/5198 [4:21:24<21:32:37, 17.51s/it]                                                       {'loss': 0.8962, 'learning_rate': 1.9279392504730147e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 769/5198 [4:21:24<21:32:37, 17.51s/it] 15%|â–ˆâ–        | 770/5198 [4:21:41<21:29:02, 17.47s/it]                                                       {'loss': 0.9618, 'learning_rate': 1.9277068271138287e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 770/5198 [4:21:41<21:29:02, 17.47s/it] 15%|â–ˆâ–        | 771/5198 [4:22:00<21:58:14, 17.87s/it]                                                       {'loss': 0.8758, 'learning_rate': 1.9274740435869107e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 771/5198 [4:22:00<21:58:14, 17.87s/it] 15%|â–ˆâ–        | 772/5198 [4:22:18<21:50:37, 17.77s/it]                                                       {'loss': 0.8834, 'learning_rate': 1.927240899982635e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 772/5198 [4:22:18<21:50:37, 17.77s/it] 15%|â–ˆâ–        | 773/5198 [4:22:35<21:41:28, 17.65s/it]                                                       {'loss': 0.9005, 'learning_rate': 1.9270073963915162e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 773/5198 [4:22:35<21:41:28, 17.65s/it] 15%|â–ˆâ–        | 774/5198 [4:22:53<21:47:37, 17.73s/it]                                                       {'loss': 0.9263, 'learning_rate': 1.9267735329042086e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 774/5198 [4:22:53<21:47:37, 17.73s/it] 15%|â–ˆâ–        | 775/5198 [4:23:10<21:37:45, 17.60s/it]                                                       {'loss': 0.8658, 'learning_rate': 1.9265393096115056e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 775/5198 [4:23:10<21:37:45, 17.60s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 15%|â–ˆâ–        | 776/5198 [4:24:37<47:16:16, 38.48s/it]                                                       {'loss': 0.8729, 'learning_rate': 1.926304726604341e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 776/5198 [4:24:37<47:16:16, 38.48s/it] 15%|â–ˆâ–        | 777/5198 [4:24:55<39:25:11, 32.10s/it]                                                       {'loss': 0.9035, 'learning_rate': 1.9260697839737875e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 777/5198 [4:24:55<39:25:11, 32.10s/it] 15%|â–ˆâ–        | 778/5198 [4:25:13<34:23:34, 28.01s/it]                                                       {'loss': 0.8449, 'learning_rate': 1.925834481811059e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 778/5198 [4:25:13<34:23:34, 28.01s/it] 15%|â–ˆâ–        | 779/5198 [4:25:31<30:38:42, 24.97s/it]                                                       {'loss': 0.9188, 'learning_rate': 1.9255988202075065e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 779/5198 [4:25:31<30:38:42, 24.97s/it] 15%|â–ˆâ–Œ        | 780/5198 [4:25:49<28:11:15, 22.97s/it]                                                       {'loss': 0.8588, 'learning_rate': 1.925362799254623e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 780/5198 [4:25:49<28:11:15, 22.97s/it] 15%|â–ˆâ–Œ        | 781/5198 [4:26:08<26:36:25, 21.69s/it]                                                       {'loss': 0.906, 'learning_rate': 1.9251264190440398e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 781/5198 [4:26:08<26:36:25, 21.69s/it] 15%|â–ˆâ–Œ        | 782/5198 [4:26:27<25:29:28, 20.78s/it]                                                       {'loss': 0.8537, 'learning_rate': 1.9248896796675277e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 782/5198 [4:26:27<25:29:28, 20.78s/it] 15%|â–ˆâ–Œ        | 783/5198 [4:26:44<24:16:24, 19.79s/it]                                                       {'loss': 0.9082, 'learning_rate': 1.924652581216997e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 783/5198 [4:26:44<24:16:24, 19.79s/it] 15%|â–ˆâ–Œ        | 784/5198 [4:27:02<23:42:03, 19.33s/it]                                                       {'loss': 0.8797, 'learning_rate': 1.9244151237844975e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 784/5198 [4:27:02<23:42:03, 19.33s/it] 15%|â–ˆâ–Œ        | 785/5198 [4:27:20<23:02:33, 18.80s/it]                                                       {'loss': 0.8612, 'learning_rate': 1.9241773074622182e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 785/5198 [4:27:20<23:02:33, 18.80s/it] 15%|â–ˆâ–Œ        | 786/5198 [4:27:37<22:31:39, 18.38s/it]                                                       {'loss': 0.9212, 'learning_rate': 1.923939132342488e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 786/5198 [4:27:37<22:31:39, 18.38s/it] 15%|â–ˆâ–Œ        | 787/5198 [4:27:54<21:50:40, 17.83s/it]                                                       {'loss': 0.7859, 'learning_rate': 1.923700598517775e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 787/5198 [4:27:54<21:50:40, 17.83s/it] 15%|â–ˆâ–Œ        | 788/5198 [4:28:12<22:05:44, 18.04s/it]                                                       {'loss': 0.8533, 'learning_rate': 1.923461706080685e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 788/5198 [4:28:12<22:05:44, 18.04s/it] 15%|â–ˆâ–Œ        | 789/5198 [4:28:30<21:59:50, 17.96s/it]                                                       {'loss': 0.4051, 'learning_rate': 1.923222455123965e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 789/5198 [4:28:30<21:59:50, 17.96s/it] 15%|â–ˆâ–Œ        | 790/5198 [4:28:47<21:37:43, 17.66s/it]                                                       {'loss': 0.9176, 'learning_rate': 1.9229828457405005e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 790/5198 [4:28:47<21:37:43, 17.66s/it] 15%|â–ˆâ–Œ        | 791/5198 [4:29:05<21:39:39, 17.69s/it]                                                       {'loss': 0.3784, 'learning_rate': 1.9227428780233162e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 791/5198 [4:29:05<21:39:39, 17.69s/it] 15%|â–ˆâ–Œ        | 792/5198 [4:29:22<21:26:55, 17.53s/it]                                                       {'loss': 0.9308, 'learning_rate': 1.922502552065576e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 792/5198 [4:29:22<21:26:55, 17.53s/it] 15%|â–ˆâ–Œ        | 793/5198 [4:29:39<21:08:30, 17.28s/it]                                                       {'loss': 0.3307, 'learning_rate': 1.922261867960582e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 793/5198 [4:29:39<21:08:30, 17.28s/it] 15%|â–ˆâ–Œ        | 794/5198 [4:29:55<20:45:33, 16.97s/it]                                                       {'loss': 0.8853, 'learning_rate': 1.9220208258017763e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 794/5198 [4:29:55<20:45:33, 16.97s/it] 15%|â–ˆâ–Œ        | 795/5198 [4:30:13<21:01:34, 17.19s/it]                                                       {'loss': 0.9048, 'learning_rate': 1.92177942568274e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 795/5198 [4:30:13<21:01:34, 17.19s/it] 15%|â–ˆâ–Œ        | 796/5198 [4:30:30<20:56:42, 17.13s/it]                                                       {'loss': 0.9422, 'learning_rate': 1.921537667697193e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 796/5198 [4:30:30<20:56:42, 17.13s/it] 15%|â–ˆâ–Œ        | 797/5198 [4:30:47<21:04:20, 17.24s/it]                                                       {'loss': 0.8951, 'learning_rate': 1.9212955519389938e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 797/5198 [4:30:47<21:04:20, 17.24s/it] 15%|â–ˆâ–Œ        | 798/5198 [4:31:05<21:08:15, 17.29s/it]                                                       {'loss': 0.9252, 'learning_rate': 1.9210530785021405e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 798/5198 [4:31:05<21:08:15, 17.29s/it] 15%|â–ˆâ–Œ        | 799/5198 [4:31:22<21:01:38, 17.21s/it]                                                       {'loss': 0.8482, 'learning_rate': 1.9208102474807692e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 799/5198 [4:31:22<21:01:38, 17.21s/it] 15%|â–ˆâ–Œ        | 800/5198 [4:31:40<21:16:43, 17.42s/it]                                                       {'loss': 0.9146, 'learning_rate': 1.920567058969155e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 800/5198 [4:31:40<21:16:43, 17.42s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 15%|â–ˆâ–Œ        | 801/5198 [4:33:19<51:16:47, 41.98s/it]                                                       {'loss': 0.8729, 'learning_rate': 1.920323513061713e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 801/5198 [4:33:19<51:16:47, 41.98s/it] 15%|â–ˆâ–Œ        | 802/5198 [4:33:37<42:22:33, 34.70s/it]                                                       {'loss': 0.8314, 'learning_rate': 1.9200796098529956e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 802/5198 [4:33:37<42:22:33, 34.70s/it] 15%|â–ˆâ–Œ        | 803/5198 [4:33:54<36:07:37, 29.59s/it]                                                       {'loss': 0.8614, 'learning_rate': 1.919835349437694e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 803/5198 [4:33:54<36:07:37, 29.59s/it] 15%|â–ˆâ–Œ        | 804/5198 [4:34:12<31:47:53, 26.05s/it]                                                       {'loss': 0.8874, 'learning_rate': 1.9195907319106394e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 804/5198 [4:34:12<31:47:53, 26.05s/it] 15%|â–ˆâ–Œ        | 805/5198 [4:34:30<28:47:01, 23.59s/it]                                                       {'loss': 0.8435, 'learning_rate': 1.9193457573667996e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 805/5198 [4:34:30<28:47:01, 23.59s/it] 16%|â–ˆâ–Œ        | 806/5198 [4:34:48<26:49:08, 21.98s/it]                                                       {'loss': 0.8189, 'learning_rate': 1.919100425901283e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 806/5198 [4:34:48<26:49:08, 21.98s/it] 16%|â–ˆâ–Œ        | 807/5198 [4:35:05<25:03:32, 20.54s/it]                                                       {'loss': 0.9421, 'learning_rate': 1.9188547376093355e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 807/5198 [4:35:05<25:03:32, 20.54s/it] 16%|â–ˆâ–Œ        | 808/5198 [4:35:24<24:18:57, 19.94s/it]                                                       {'loss': 0.8849, 'learning_rate': 1.918608692586342e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 808/5198 [4:35:24<24:18:57, 19.94s/it] 16%|â–ˆâ–Œ        | 809/5198 [4:35:42<23:47:25, 19.51s/it]                                                       {'loss': 0.8746, 'learning_rate': 1.918362290927825e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 809/5198 [4:35:42<23:47:25, 19.51s/it] 16%|â–ˆâ–Œ        | 810/5198 [4:35:59<22:45:54, 18.68s/it]                                                       {'loss': 0.8878, 'learning_rate': 1.9181155327294468e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 810/5198 [4:35:59<22:45:54, 18.68s/it] 16%|â–ˆâ–Œ        | 811/5198 [4:36:17<22:25:43, 18.41s/it]                                                       {'loss': 0.8629, 'learning_rate': 1.9178684180870072e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 811/5198 [4:36:17<22:25:43, 18.41s/it] 16%|â–ˆâ–Œ        | 812/5198 [4:36:35<22:10:30, 18.20s/it]                                                       {'loss': 0.9014, 'learning_rate': 1.9176209470964446e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 812/5198 [4:36:35<22:10:30, 18.20s/it] 16%|â–ˆâ–Œ        | 813/5198 [4:36:53<22:06:55, 18.16s/it]                                                       {'loss': 0.8348, 'learning_rate': 1.9173731198538354e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 813/5198 [4:36:53<22:06:55, 18.16s/it] 16%|â–ˆâ–Œ        | 814/5198 [4:37:09<21:26:08, 17.60s/it]                                                       {'loss': 0.9131, 'learning_rate': 1.9171249364553956e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 814/5198 [4:37:09<21:26:08, 17.60s/it] 16%|â–ˆâ–Œ        | 815/5198 [4:37:27<21:46:03, 17.88s/it]                                                       {'loss': 0.9131, 'learning_rate': 1.9168763969974773e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 815/5198 [4:37:27<21:46:03, 17.88s/it] 16%|â–ˆâ–Œ        | 816/5198 [4:37:46<22:03:55, 18.13s/it]                                                       {'loss': 0.8597, 'learning_rate': 1.916627501576573e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 816/5198 [4:37:46<22:03:55, 18.13s/it] 16%|â–ˆâ–Œ        | 817/5198 [4:38:03<21:32:19, 17.70s/it]                                                       {'loss': 0.9271, 'learning_rate': 1.916378250289312e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 817/5198 [4:38:03<21:32:19, 17.70s/it] 16%|â–ˆâ–Œ        | 818/5198 [4:38:21<21:38:02, 17.78s/it]                                                       {'loss': 0.8886, 'learning_rate': 1.9161286432324628e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 818/5198 [4:38:21<21:38:02, 17.78s/it] 16%|â–ˆâ–Œ        | 819/5198 [4:38:38<21:32:46, 17.71s/it]                                                       {'loss': 0.901, 'learning_rate': 1.9158786805029307e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 819/5198 [4:38:38<21:32:46, 17.71s/it] 16%|â–ˆâ–Œ        | 820/5198 [4:38:56<21:40:01, 17.82s/it]                                                       {'loss': 0.8785, 'learning_rate': 1.9156283621977603e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 820/5198 [4:38:56<21:40:01, 17.82s/it] 16%|â–ˆâ–Œ        | 821/5198 [4:39:15<21:47:27, 17.92s/it]                                                       {'loss': 0.8726, 'learning_rate': 1.9153776884141336e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 821/5198 [4:39:15<21:47:27, 17.92s/it] 16%|â–ˆâ–Œ        | 822/5198 [4:39:33<21:50:41, 17.97s/it]                                                       {'loss': 0.8447, 'learning_rate': 1.915126659249371e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 822/5198 [4:39:33<21:50:41, 17.97s/it] 16%|â–ˆâ–Œ        | 823/5198 [4:39:50<21:44:01, 17.88s/it]                                                       {'loss': 0.9119, 'learning_rate': 1.9148752748009304e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 823/5198 [4:39:50<21:44:01, 17.88s/it] 16%|â–ˆâ–Œ        | 824/5198 [4:40:09<21:50:10, 17.97s/it]                                                       {'loss': 0.9406, 'learning_rate': 1.914623535166408e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 824/5198 [4:40:09<21:50:10, 17.97s/it] 16%|â–ˆâ–Œ        | 825/5198 [4:40:27<22:01:51, 18.14s/it]                                                       {'loss': 0.9093, 'learning_rate': 1.9143714404435382e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 825/5198 [4:40:27<22:01:51, 18.14s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 16%|â–ˆâ–Œ        | 826/5198 [4:41:59<49:05:36, 40.42s/it]                                                       {'loss': 0.9309, 'learning_rate': 1.9141189907301922e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 826/5198 [4:41:59<49:05:36, 40.42s/it] 16%|â–ˆâ–Œ        | 827/5198 [4:42:17<40:53:12, 33.67s/it]                                                       {'loss': 0.8711, 'learning_rate': 1.9138661861243802e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 827/5198 [4:42:17<40:53:12, 33.67s/it] 16%|â–ˆâ–Œ        | 828/5198 [4:42:35<35:02:18, 28.86s/it]                                                       {'loss': 0.9082, 'learning_rate': 1.913613026724249e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 828/5198 [4:42:35<35:02:18, 28.86s/it] 16%|â–ˆâ–Œ        | 829/5198 [4:42:52<30:51:48, 25.43s/it]                                                       {'loss': 0.8532, 'learning_rate': 1.9133595126280848e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 829/5198 [4:42:52<30:51:48, 25.43s/it] 16%|â–ˆâ–Œ        | 830/5198 [4:43:11<28:14:57, 23.28s/it]                                                       {'loss': 0.8719, 'learning_rate': 1.9131056439343095e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 830/5198 [4:43:11<28:14:57, 23.28s/it] 16%|â–ˆâ–Œ        | 831/5198 [4:43:28<26:12:30, 21.61s/it]                                                       {'loss': 0.8637, 'learning_rate': 1.9128514207414838e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 831/5198 [4:43:28<26:12:30, 21.61s/it] 16%|â–ˆâ–Œ        | 832/5198 [4:43:46<24:38:47, 20.32s/it]                                                       {'loss': 0.833, 'learning_rate': 1.9125968431483068e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 832/5198 [4:43:46<24:38:47, 20.32s/it] 16%|â–ˆâ–Œ        | 833/5198 [4:44:04<23:49:46, 19.65s/it]                                                       {'loss': 0.9116, 'learning_rate': 1.9123419112536132e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 833/5198 [4:44:04<23:49:46, 19.65s/it] 16%|â–ˆâ–Œ        | 834/5198 [4:44:22<23:20:27, 19.25s/it]                                                       {'loss': 0.9546, 'learning_rate': 1.912086625156377e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 834/5198 [4:44:22<23:20:27, 19.25s/it] 16%|â–ˆâ–Œ        | 835/5198 [4:44:40<22:58:19, 18.95s/it]                                                       {'loss': 0.8668, 'learning_rate': 1.911830984955709e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 835/5198 [4:44:40<22:58:19, 18.95s/it] 16%|â–ˆâ–Œ        | 836/5198 [4:44:57<22:14:06, 18.35s/it]                                                       {'loss': 0.9, 'learning_rate': 1.911574990750857e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 836/5198 [4:44:57<22:14:06, 18.35s/it] 16%|â–ˆâ–Œ        | 837/5198 [4:45:16<22:11:15, 18.32s/it]                                                       {'loss': 0.9072, 'learning_rate': 1.9113186426412073e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 837/5198 [4:45:16<22:11:15, 18.32s/it] 16%|â–ˆâ–Œ        | 838/5198 [4:45:33<21:53:34, 18.08s/it]                                                       {'loss': 0.3737, 'learning_rate': 1.9110619407262828e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 838/5198 [4:45:33<21:53:34, 18.08s/it] 16%|â–ˆâ–Œ        | 839/5198 [4:45:50<21:36:58, 17.85s/it]                                                       {'loss': 0.9182, 'learning_rate': 1.9108048851057447e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 839/5198 [4:45:50<21:36:58, 17.85s/it] 16%|â–ˆâ–Œ        | 840/5198 [4:46:09<21:55:09, 18.11s/it]                                                       {'loss': 0.8799, 'learning_rate': 1.9105474758793897e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 840/5198 [4:46:09<21:55:09, 18.11s/it] 16%|â–ˆâ–Œ        | 841/5198 [4:46:26<21:32:48, 17.80s/it]                                                       {'loss': 0.9261, 'learning_rate': 1.9102897131471536e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 841/5198 [4:46:26<21:32:48, 17.80s/it] 16%|â–ˆâ–Œ        | 842/5198 [4:46:44<21:38:02, 17.88s/it]                                                       {'loss': 0.8758, 'learning_rate': 1.9100315970091088e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 842/5198 [4:46:44<21:38:02, 17.88s/it] 16%|â–ˆâ–Œ        | 843/5198 [4:47:02<21:40:45, 17.92s/it]                                                       {'loss': 0.89, 'learning_rate': 1.9097731275654645e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 843/5198 [4:47:02<21:40:45, 17.92s/it] 16%|â–ˆâ–Œ        | 844/5198 [4:47:20<21:35:58, 17.86s/it]                                                       {'loss': 0.8997, 'learning_rate': 1.909514304916568e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 844/5198 [4:47:20<21:35:58, 17.86s/it] 16%|â–ˆâ–‹        | 845/5198 [4:47:37<21:23:01, 17.68s/it]                                                       {'loss': 0.8975, 'learning_rate': 1.9092551291629026e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 845/5198 [4:47:37<21:23:01, 17.68s/it] 16%|â–ˆâ–‹        | 846/5198 [4:47:55<21:17:57, 17.62s/it]                                                       {'loss': 0.3387, 'learning_rate': 1.9089956004050893e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 846/5198 [4:47:55<21:17:57, 17.62s/it] 16%|â–ˆâ–‹        | 847/5198 [4:48:12<21:09:32, 17.51s/it]                                                       {'loss': 0.3414, 'learning_rate': 1.908735718743887e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 847/5198 [4:48:12<21:09:32, 17.51s/it] 16%|â–ˆâ–‹        | 848/5198 [4:48:30<21:29:48, 17.79s/it]                                                       {'loss': 0.9412, 'learning_rate': 1.908475484280189e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 848/5198 [4:48:30<21:29:48, 17.79s/it] 16%|â–ˆâ–‹        | 849/5198 [4:48:49<21:35:08, 17.87s/it]                                                       {'loss': 0.907, 'learning_rate': 1.908214897115029e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 849/5198 [4:48:49<21:35:08, 17.87s/it] 16%|â–ˆâ–‹        | 850/5198 [4:49:07<21:37:22, 17.90s/it]                                                       {'loss': 0.8711, 'learning_rate': 1.907953957349575e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 850/5198 [4:49:07<21:37:22, 17.90s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 16%|â–ˆâ–‹        | 851/5198 [4:50:42<49:47:33, 41.24s/it]                                                       {'loss': 0.9222, 'learning_rate': 1.907692665085133e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 851/5198 [4:50:42<49:47:33, 41.24s/it] 16%|â–ˆâ–‹        | 852/5198 [4:51:00<41:14:55, 34.17s/it]                                                       {'loss': 0.8917, 'learning_rate': 1.9074310204231457e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 852/5198 [4:51:00<41:14:55, 34.17s/it] 16%|â–ˆâ–‹        | 853/5198 [4:51:17<35:07:32, 29.10s/it]                                                       {'loss': 0.9241, 'learning_rate': 1.9071690234651923e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 853/5198 [4:51:17<35:07:32, 29.10s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2254 > 2048). Running this sequence through the model will result in indexing errors
 16%|â–ˆâ–‹        | 854/5198 [4:51:34<30:45:03, 25.48s/it]                                                       {'loss': 0.7859, 'learning_rate': 1.9069066743129893e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 854/5198 [4:51:34<30:45:03, 25.48s/it] 16%|â–ˆâ–‹        | 855/5198 [4:51:52<27:54:53, 23.14s/it]                                                       {'loss': 0.8683, 'learning_rate': 1.90664397306839e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 855/5198 [4:51:52<27:54:53, 23.14s/it] 16%|â–ˆâ–‹        | 856/5198 [4:52:10<26:04:54, 21.62s/it]                                                       {'loss': 0.8616, 'learning_rate': 1.9063809198333832e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 856/5198 [4:52:10<26:04:54, 21.62s/it] 16%|â–ˆâ–‹        | 857/5198 [4:52:28<24:38:03, 20.43s/it]                                                       {'loss': 0.876, 'learning_rate': 1.9061175147100957e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 857/5198 [4:52:28<24:38:03, 20.43s/it] 17%|â–ˆâ–‹        | 858/5198 [4:52:45<23:40:13, 19.63s/it]                                                       {'loss': 0.9339, 'learning_rate': 1.905853757800791e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 858/5198 [4:52:45<23:40:13, 19.63s/it] 17%|â–ˆâ–‹        | 859/5198 [4:53:03<23:06:15, 19.17s/it]                                                       {'loss': 0.8973, 'learning_rate': 1.9055896492078675e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 859/5198 [4:53:04<23:06:15, 19.17s/it] 17%|â–ˆâ–‹        | 860/5198 [4:53:22<22:47:14, 18.91s/it]                                                       {'loss': 0.8571, 'learning_rate': 1.905325189033862e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 860/5198 [4:53:22<22:47:14, 18.91s/it] 17%|â–ˆâ–‹        | 861/5198 [4:53:39<22:21:06, 18.55s/it]                                                       {'loss': 0.8992, 'learning_rate': 1.905060377381447e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 861/5198 [4:53:39<22:21:06, 18.55s/it] 17%|â–ˆâ–‹        | 862/5198 [4:53:57<21:50:46, 18.14s/it]                                                       {'loss': 0.9664, 'learning_rate': 1.904795214353431e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 862/5198 [4:53:57<21:50:46, 18.14s/it] 17%|â–ˆâ–‹        | 863/5198 [4:54:15<21:54:55, 18.20s/it]                                                       {'loss': 0.8486, 'learning_rate': 1.90452970005276e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 863/5198 [4:54:15<21:54:55, 18.20s/it] 17%|â–ˆâ–‹        | 864/5198 [4:54:33<21:54:55, 18.20s/it]                                                       {'loss': 0.9158, 'learning_rate': 1.9042638345825155e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 864/5198 [4:54:33<21:54:55, 18.20s/it] 17%|â–ˆâ–‹        | 865/5198 [4:54:50<21:29:59, 17.86s/it]                                                       {'loss': 0.8701, 'learning_rate': 1.9039976180459158e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 865/5198 [4:54:50<21:29:59, 17.86s/it] 17%|â–ˆâ–‹        | 866/5198 [4:55:08<21:16:25, 17.68s/it]                                                       {'loss': 0.3121, 'learning_rate': 1.9037310505463153e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 866/5198 [4:55:08<21:16:25, 17.68s/it] 17%|â–ˆâ–‹        | 867/5198 [4:55:25<21:10:15, 17.60s/it]                                                       {'loss': 0.9146, 'learning_rate': 1.9034641321872043e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 867/5198 [4:55:25<21:10:15, 17.60s/it] 17%|â–ˆâ–‹        | 868/5198 [4:55:43<21:27:42, 17.84s/it]                                                       {'loss': 0.9128, 'learning_rate': 1.9031968630722104e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 868/5198 [4:55:43<21:27:42, 17.84s/it] 17%|â–ˆâ–‹        | 869/5198 [4:56:01<21:28:31, 17.86s/it]                                                       {'loss': 0.8801, 'learning_rate': 1.902929243305096e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 869/5198 [4:56:01<21:28:31, 17.86s/it] 17%|â–ˆâ–‹        | 870/5198 [4:56:19<21:28:51, 17.87s/it]                                                       {'loss': 0.8614, 'learning_rate': 1.902661272989761e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 870/5198 [4:56:19<21:28:51, 17.87s/it] 17%|â–ˆâ–‹        | 871/5198 [4:56:38<21:39:29, 18.02s/it]                                                       {'loss': 0.8783, 'learning_rate': 1.9023929522302394e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 871/5198 [4:56:38<21:39:29, 18.02s/it] 17%|â–ˆâ–‹        | 872/5198 [4:56:55<21:37:11, 17.99s/it]                                                       {'loss': 0.843, 'learning_rate': 1.9021242811307044e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 872/5198 [4:56:55<21:37:11, 17.99s/it] 17%|â–ˆâ–‹        | 873/5198 [4:57:15<22:03:03, 18.35s/it]                                                       {'loss': 0.8703, 'learning_rate': 1.901855259795462e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 873/5198 [4:57:15<22:03:03, 18.35s/it] 17%|â–ˆâ–‹        | 874/5198 [4:57:32<21:51:32, 18.20s/it]                                                       {'loss': 0.8765, 'learning_rate': 1.9015858883289556e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 874/5198 [4:57:32<21:51:32, 18.20s/it] 17%|â–ˆâ–‹        | 875/5198 [4:57:51<21:50:03, 18.18s/it]                                                       {'loss': 0.8671, 'learning_rate': 1.9013161668357655e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 875/5198 [4:57:51<21:50:03, 18.18s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 17%|â–ˆâ–‹        | 876/5198 [4:59:17<46:23:56, 38.65s/it]                                                       {'loss': 0.8442, 'learning_rate': 1.901046095420606e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 876/5198 [4:59:17<46:23:56, 38.65s/it] 17%|â–ˆâ–‹        | 877/5198 [4:59:35<38:54:57, 32.42s/it]                                                       {'loss': 0.877, 'learning_rate': 1.9007756741883284e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 877/5198 [4:59:35<38:54:57, 32.42s/it] 17%|â–ˆâ–‹        | 878/5198 [4:59:51<33:11:01, 27.65s/it]                                                       {'loss': 0.8857, 'learning_rate': 1.9005049032439193e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 878/5198 [4:59:51<33:11:01, 27.65s/it] 17%|â–ˆâ–‹        | 879/5198 [5:00:09<29:40:37, 24.74s/it]                                                       {'loss': 0.9001, 'learning_rate': 1.9002337826925012e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 879/5198 [5:00:09<29:40:37, 24.74s/it] 17%|â–ˆâ–‹        | 880/5198 [5:00:27<27:08:33, 22.63s/it]                                                       {'loss': 0.8657, 'learning_rate': 1.899962312639333e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 880/5198 [5:00:27<27:08:33, 22.63s/it] 17%|â–ˆâ–‹        | 881/5198 [5:00:45<25:16:32, 21.08s/it]                                                       {'loss': 0.3375, 'learning_rate': 1.8996904931898085e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 881/5198 [5:00:45<25:16:32, 21.08s/it] 17%|â–ˆâ–‹        | 882/5198 [5:01:02<23:50:39, 19.89s/it]                                                       {'loss': 0.8341, 'learning_rate': 1.899418324449457e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 882/5198 [5:01:02<23:50:39, 19.89s/it] 17%|â–ˆâ–‹        | 883/5198 [5:01:18<22:38:56, 18.90s/it]                                                       {'loss': 0.8892, 'learning_rate': 1.8991458065239444e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 883/5198 [5:01:18<22:38:56, 18.90s/it] 17%|â–ˆâ–‹        | 884/5198 [5:01:36<22:09:25, 18.49s/it]                                                       {'loss': 0.9007, 'learning_rate': 1.8988729395190712e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 884/5198 [5:01:36<22:09:25, 18.49s/it] 17%|â–ˆâ–‹        | 885/5198 [5:01:54<22:02:41, 18.40s/it]                                                       {'loss': 0.9142, 'learning_rate': 1.8985997235407735e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 885/5198 [5:01:54<22:02:41, 18.40s/it] 17%|â–ˆâ–‹        | 886/5198 [5:02:11<21:31:48, 17.98s/it]                                                       {'loss': 0.9273, 'learning_rate': 1.898326158695124e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 886/5198 [5:02:11<21:31:48, 17.98s/it] 17%|â–ˆâ–‹        | 887/5198 [5:02:29<21:28:53, 17.94s/it]                                                       {'loss': 0.8544, 'learning_rate': 1.8980522450883287e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 887/5198 [5:02:29<21:28:53, 17.94s/it] 17%|â–ˆâ–‹        | 888/5198 [5:02:46<21:05:37, 17.62s/it]                                                       {'loss': 0.916, 'learning_rate': 1.8977779828267314e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 888/5198 [5:02:46<21:05:37, 17.62s/it] 17%|â–ˆâ–‹        | 889/5198 [5:03:04<21:19:31, 17.82s/it]                                                       {'loss': 0.8342, 'learning_rate': 1.8975033720168094e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 889/5198 [5:03:04<21:19:31, 17.82s/it] 17%|â–ˆâ–‹        | 890/5198 [5:03:22<21:32:55, 18.01s/it]                                                       {'loss': 0.8906, 'learning_rate': 1.897228412765177e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 890/5198 [5:03:22<21:32:55, 18.01s/it] 17%|â–ˆâ–‹        | 891/5198 [5:03:39<21:04:59, 17.62s/it]                                                       {'loss': 0.9026, 'learning_rate': 1.896953105178582e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 891/5198 [5:03:39<21:04:59, 17.62s/it] 17%|â–ˆâ–‹        | 892/5198 [5:03:57<21:00:03, 17.56s/it]                                                       {'loss': 0.9091, 'learning_rate': 1.8966774493639084e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 892/5198 [5:03:57<21:00:03, 17.56s/it] 17%|â–ˆâ–‹        | 893/5198 [5:04:14<21:03:47, 17.61s/it]                                                       {'loss': 0.9471, 'learning_rate': 1.896401445428176e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 893/5198 [5:04:14<21:03:47, 17.61s/it] 17%|â–ˆâ–‹        | 894/5198 [5:04:33<21:22:07, 17.87s/it]                                                       {'loss': 0.8098, 'learning_rate': 1.896125093478538e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 894/5198 [5:04:33<21:22:07, 17.87s/it] 17%|â–ˆâ–‹        | 895/5198 [5:04:50<21:14:16, 17.77s/it]                                                       {'loss': 0.8596, 'learning_rate': 1.895848393622284e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 895/5198 [5:04:50<21:14:16, 17.77s/it] 17%|â–ˆâ–‹        | 896/5198 [5:05:07<20:54:41, 17.50s/it]                                                       {'loss': 0.8513, 'learning_rate': 1.895571345966839e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 896/5198 [5:05:07<20:54:41, 17.50s/it] 17%|â–ˆâ–‹        | 897/5198 [5:05:25<21:02:57, 17.62s/it]                                                       {'loss': 0.3475, 'learning_rate': 1.8952939506197622e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 897/5198 [5:05:25<21:02:57, 17.62s/it] 17%|â–ˆâ–‹        | 898/5198 [5:05:44<21:24:49, 17.93s/it]                                                       {'loss': 0.8672, 'learning_rate': 1.8950162076887477e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 898/5198 [5:05:44<21:24:49, 17.93s/it] 17%|â–ˆâ–‹        | 899/5198 [5:06:02<21:29:24, 18.00s/it]                                                       {'loss': 0.8641, 'learning_rate': 1.894738117281625e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 899/5198 [5:06:02<21:29:24, 18.00s/it] 17%|â–ˆâ–‹        | 900/5198 [5:06:20<21:38:11, 18.12s/it]                                                       {'loss': 0.9094, 'learning_rate': 1.8944596795063584e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 900/5198 [5:06:20<21:38:11, 18.12s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 17%|â–ˆâ–‹        | 901/5198 [5:07:55<48:56:14, 41.00s/it]                                                       {'loss': 0.9099, 'learning_rate': 1.894180894471047e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 901/5198 [5:07:55<48:56:14, 41.00s/it] 17%|â–ˆâ–‹        | 902/5198 [5:08:12<40:18:50, 33.78s/it]                                                       {'loss': 0.8878, 'learning_rate': 1.8939017622839253e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 902/5198 [5:08:12<40:18:50, 33.78s/it] 17%|â–ˆâ–‹        | 903/5198 [5:08:29<34:31:42, 28.94s/it]                                                       {'loss': 0.8723, 'learning_rate': 1.8936222830533613e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 903/5198 [5:08:29<34:31:42, 28.94s/it] 17%|â–ˆâ–‹        | 904/5198 [5:08:48<30:49:31, 25.84s/it]                                                       {'loss': 0.8536, 'learning_rate': 1.8933424568878586e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 904/5198 [5:08:48<30:49:31, 25.84s/it] 17%|â–ˆâ–‹        | 905/5198 [5:09:05<27:40:19, 23.21s/it]                                                       {'loss': 0.9316, 'learning_rate': 1.8930622838960555e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 905/5198 [5:09:05<27:40:19, 23.21s/it] 17%|â–ˆâ–‹        | 906/5198 [5:09:22<25:39:19, 21.52s/it]                                                       {'loss': 0.9106, 'learning_rate': 1.8927817641867244e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 906/5198 [5:09:22<25:39:19, 21.52s/it] 17%|â–ˆâ–‹        | 907/5198 [5:09:40<24:05:45, 20.22s/it]                                                       {'loss': 0.8333, 'learning_rate': 1.8925008978687737e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 907/5198 [5:09:40<24:05:45, 20.22s/it] 17%|â–ˆâ–‹        | 908/5198 [5:09:56<22:51:28, 19.18s/it]                                                       {'loss': 0.3593, 'learning_rate': 1.8922196850512446e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 908/5198 [5:09:56<22:51:28, 19.18s/it] 17%|â–ˆâ–‹        | 909/5198 [5:10:14<22:15:02, 18.68s/it]                                                       {'loss': 0.9001, 'learning_rate': 1.8919381258433135e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 909/5198 [5:10:14<22:15:02, 18.68s/it] 18%|â–ˆâ–Š        | 910/5198 [5:10:32<21:51:37, 18.35s/it]                                                       {'loss': 0.8869, 'learning_rate': 1.8916562203542916e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 910/5198 [5:10:32<21:51:37, 18.35s/it] 18%|â–ˆâ–Š        | 911/5198 [5:10:50<21:50:29, 18.34s/it]                                                       {'loss': 0.8894, 'learning_rate': 1.8913739686936244e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 911/5198 [5:10:50<21:50:29, 18.34s/it] 18%|â–ˆâ–Š        | 912/5198 [5:11:07<21:32:15, 18.09s/it]                                                       {'loss': 0.9013, 'learning_rate': 1.8910913709708918e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 912/5198 [5:11:07<21:32:15, 18.09s/it] 18%|â–ˆâ–Š        | 913/5198 [5:11:25<21:19:00, 17.91s/it]                                                       {'loss': 0.8677, 'learning_rate': 1.8908084272958077e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 913/5198 [5:11:25<21:19:00, 17.91s/it] 18%|â–ˆâ–Š        | 914/5198 [5:11:42<21:10:46, 17.80s/it]                                                       {'loss': 0.84, 'learning_rate': 1.8905251377782206e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 914/5198 [5:11:42<21:10:46, 17.80s/it] 18%|â–ˆâ–Š        | 915/5198 [5:12:00<21:10:44, 17.80s/it]                                                       {'loss': 0.9057, 'learning_rate': 1.8902415025281136e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 915/5198 [5:12:00<21:10:44, 17.80s/it] 18%|â–ˆâ–Š        | 916/5198 [5:12:17<20:55:13, 17.59s/it]                                                       {'loss': 0.8712, 'learning_rate': 1.889957521655603e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 916/5198 [5:12:17<20:55:13, 17.59s/it] 18%|â–ˆâ–Š        | 917/5198 [5:12:35<20:57:51, 17.63s/it]                                                       {'loss': 0.8527, 'learning_rate': 1.8896731952709408e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 917/5198 [5:12:35<20:57:51, 17.63s/it] 18%|â–ˆâ–Š        | 918/5198 [5:12:53<21:00:03, 17.66s/it]                                                       {'loss': 0.9026, 'learning_rate': 1.8893885234845117e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 918/5198 [5:12:53<21:00:03, 17.66s/it] 18%|â–ˆâ–Š        | 919/5198 [5:13:10<20:51:28, 17.55s/it]                                                       {'loss': 0.8692, 'learning_rate': 1.8891035064068354e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 919/5198 [5:13:10<20:51:28, 17.55s/it] 18%|â–ˆâ–Š        | 920/5198 [5:13:29<21:23:09, 18.00s/it]                                                       {'loss': 0.8839, 'learning_rate': 1.888818144148565e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 920/5198 [5:13:29<21:23:09, 18.00s/it] 18%|â–ˆâ–Š        | 921/5198 [5:13:48<21:33:03, 18.14s/it]                                                       {'loss': 0.8128, 'learning_rate': 1.888532436820488e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 921/5198 [5:13:48<21:33:03, 18.14s/it] 18%|â–ˆâ–Š        | 922/5198 [5:14:06<21:36:47, 18.20s/it]                                                       {'loss': 0.8234, 'learning_rate': 1.8882463845335263e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 922/5198 [5:14:06<21:36:47, 18.20s/it] 18%|â–ˆâ–Š        | 923/5198 [5:14:24<21:28:04, 18.08s/it]                                                       {'loss': 0.8388, 'learning_rate': 1.8879599873987343e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 923/5198 [5:14:24<21:28:04, 18.08s/it] 18%|â–ˆâ–Š        | 924/5198 [5:14:41<21:21:29, 17.99s/it]                                                       {'loss': 0.8487, 'learning_rate': 1.8876732455273022e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 924/5198 [5:14:41<21:21:29, 17.99s/it] 18%|â–ˆâ–Š        | 925/5198 [5:14:59<21:11:33, 17.85s/it]                                                       {'loss': 0.9093, 'learning_rate': 1.8873861590305527e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 925/5198 [5:14:59<21:11:33, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 18%|â–ˆâ–Š        | 926/5198 [5:16:38<50:04:06, 42.19s/it]                                                       {'loss': 0.3711, 'learning_rate': 1.8870987280199428e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 926/5198 [5:16:38<50:04:06, 42.19s/it] 18%|â–ˆâ–Š        | 927/5198 [5:16:56<41:32:30, 35.02s/it]                                                       {'loss': 0.9157, 'learning_rate': 1.886810952607063e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 927/5198 [5:16:56<41:32:30, 35.02s/it] 18%|â–ˆâ–Š        | 928/5198 [5:17:14<35:33:37, 29.98s/it]                                                       {'loss': 0.8667, 'learning_rate': 1.8865228329036372e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 928/5198 [5:17:14<35:33:37, 29.98s/it] 18%|â–ˆâ–Š        | 929/5198 [5:17:32<30:56:44, 26.10s/it]                                                       {'loss': 0.9386, 'learning_rate': 1.886234369021524e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 929/5198 [5:17:32<30:56:44, 26.10s/it] 18%|â–ˆâ–Š        | 930/5198 [5:17:49<27:49:58, 23.48s/it]                                                       {'loss': 0.8766, 'learning_rate': 1.885945561072715e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 930/5198 [5:17:49<27:49:58, 23.48s/it] 18%|â–ˆâ–Š        | 931/5198 [5:18:07<25:46:49, 21.75s/it]                                                       {'loss': 0.8837, 'learning_rate': 1.885656409169335e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 931/5198 [5:18:07<25:46:49, 21.75s/it] 18%|â–ˆâ–Š        | 932/5198 [5:18:25<24:26:37, 20.63s/it]                                                       {'loss': 0.9189, 'learning_rate': 1.885366913423643e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 932/5198 [5:18:25<24:26:37, 20.63s/it] 18%|â–ˆâ–Š        | 933/5198 [5:18:42<23:25:44, 19.78s/it]                                                       {'loss': 0.9241, 'learning_rate': 1.8850770739480312e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 933/5198 [5:18:42<23:25:44, 19.78s/it] 18%|â–ˆâ–Š        | 934/5198 [5:19:00<22:39:56, 19.14s/it]                                                       {'loss': 0.8221, 'learning_rate': 1.8847868908550252e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 934/5198 [5:19:00<22:39:56, 19.14s/it] 18%|â–ˆâ–Š        | 935/5198 [5:19:18<22:08:50, 18.70s/it]                                                       {'loss': 0.844, 'learning_rate': 1.8844963642572837e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 935/5198 [5:19:18<22:08:50, 18.70s/it] 18%|â–ˆâ–Š        | 936/5198 [5:19:36<21:50:17, 18.45s/it]                                                       {'loss': 0.8807, 'learning_rate': 1.8842054942676e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 936/5198 [5:19:36<21:50:17, 18.45s/it] 18%|â–ˆâ–Š        | 937/5198 [5:19:53<21:35:37, 18.24s/it]                                                       {'loss': 0.8414, 'learning_rate': 1.8839142809988987e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 937/5198 [5:19:53<21:35:37, 18.24s/it] 18%|â–ˆâ–Š        | 938/5198 [5:20:12<21:38:32, 18.29s/it]                                                       {'loss': 0.8245, 'learning_rate': 1.88362272456424e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 938/5198 [5:20:12<21:38:32, 18.29s/it] 18%|â–ˆâ–Š        | 939/5198 [5:20:29<21:19:01, 18.02s/it]                                                       {'loss': 0.8586, 'learning_rate': 1.8833308250768153e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 939/5198 [5:20:29<21:19:01, 18.02s/it] 18%|â–ˆâ–Š        | 940/5198 [5:20:47<21:11:47, 17.92s/it]                                                       {'loss': 0.8633, 'learning_rate': 1.8830385826499507e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 940/5198 [5:20:47<21:11:47, 17.92s/it] 18%|â–ˆâ–Š        | 941/5198 [5:21:03<20:39:58, 17.48s/it]                                                       {'loss': 0.8662, 'learning_rate': 1.882745997397104e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 941/5198 [5:21:03<20:39:58, 17.48s/it] 18%|â–ˆâ–Š        | 942/5198 [5:21:21<20:53:23, 17.67s/it]                                                       {'loss': 0.8586, 'learning_rate': 1.8824530694318675e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 942/5198 [5:21:21<20:53:23, 17.67s/it] 18%|â–ˆâ–Š        | 943/5198 [5:21:40<21:08:31, 17.89s/it]                                                       {'loss': 0.8648, 'learning_rate': 1.882159798867966e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 943/5198 [5:21:40<21:08:31, 17.89s/it] 18%|â–ˆâ–Š        | 944/5198 [5:21:58<21:12:32, 17.95s/it]                                                       {'loss': 0.9437, 'learning_rate': 1.8818661858192562e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 944/5198 [5:21:58<21:12:32, 17.95s/it] 18%|â–ˆâ–Š        | 945/5198 [5:22:16<21:06:47, 17.87s/it]                                                       {'loss': 0.8943, 'learning_rate': 1.88157223039973e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 945/5198 [5:22:16<21:06:47, 17.87s/it] 18%|â–ˆâ–Š        | 946/5198 [5:22:33<20:49:03, 17.63s/it]                                                       {'loss': 0.3646, 'learning_rate': 1.8812779327235106e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 946/5198 [5:22:33<20:49:03, 17.63s/it] 18%|â–ˆâ–Š        | 947/5198 [5:22:50<20:43:30, 17.55s/it]                                                       {'loss': 0.8855, 'learning_rate': 1.880983292904854e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 947/5198 [5:22:50<20:43:30, 17.55s/it] 18%|â–ˆâ–Š        | 948/5198 [5:23:09<21:03:56, 17.84s/it]                                                       {'loss': 0.8877, 'learning_rate': 1.88068831105815e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 948/5198 [5:23:09<21:03:56, 17.84s/it] 18%|â–ˆâ–Š        | 949/5198 [5:23:26<20:55:16, 17.73s/it]                                                       {'loss': 0.8901, 'learning_rate': 1.8803929872979214e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 949/5198 [5:23:26<20:55:16, 17.73s/it] 18%|â–ˆâ–Š        | 950/5198 [5:23:43<20:48:09, 17.63s/it]                                                       {'loss': 0.8693, 'learning_rate': 1.8800973217388215e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 950/5198 [5:23:43<20:48:09, 17.63s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 18%|â–ˆâ–Š        | 951/5198 [5:25:13<46:10:11, 39.14s/it]                                                       {'loss': 0.8897, 'learning_rate': 1.879801314495639e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 951/5198 [5:25:13<46:10:11, 39.14s/it] 18%|â–ˆâ–Š        | 952/5198 [5:25:31<38:55:29, 33.00s/it]                                                       {'loss': 0.9114, 'learning_rate': 1.879504965683294e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 952/5198 [5:25:31<38:55:29, 33.00s/it] 18%|â–ˆâ–Š        | 953/5198 [5:25:49<33:18:07, 28.24s/it]                                                       {'loss': 0.8297, 'learning_rate': 1.8792082754168385e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 953/5198 [5:25:49<33:18:07, 28.24s/it] 18%|â–ˆâ–Š        | 954/5198 [5:26:06<29:28:28, 25.00s/it]                                                       {'loss': 0.8328, 'learning_rate': 1.878911243811459e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 954/5198 [5:26:06<29:28:28, 25.00s/it] 18%|â–ˆâ–Š        | 955/5198 [5:26:24<27:04:30, 22.97s/it]                                                       {'loss': 0.8018, 'learning_rate': 1.8786138709824726e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 955/5198 [5:26:24<27:04:30, 22.97s/it] 18%|â–ˆâ–Š        | 956/5198 [5:26:43<25:27:42, 21.61s/it]                                                       {'loss': 0.8437, 'learning_rate': 1.8783161570453295e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 956/5198 [5:26:43<25:27:42, 21.61s/it] 18%|â–ˆâ–Š        | 957/5198 [5:27:01<24:13:30, 20.56s/it]                                                       {'loss': 0.8811, 'learning_rate': 1.878018102115614e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 957/5198 [5:27:01<24:13:30, 20.56s/it] 18%|â–ˆâ–Š        | 958/5198 [5:27:19<23:15:09, 19.74s/it]                                                       {'loss': 0.8535, 'learning_rate': 1.8777197063090394e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 958/5198 [5:27:19<23:15:09, 19.74s/it] 18%|â–ˆâ–Š        | 959/5198 [5:27:36<22:29:03, 19.09s/it]                                                       {'loss': 0.859, 'learning_rate': 1.877420969741454e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 959/5198 [5:27:36<22:29:03, 19.09s/it] 18%|â–ˆâ–Š        | 960/5198 [5:27:55<22:13:16, 18.88s/it]                                                       {'loss': 0.879, 'learning_rate': 1.877121892528838e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 960/5198 [5:27:55<22:13:16, 18.88s/it] 18%|â–ˆâ–Š        | 961/5198 [5:28:12<21:51:55, 18.58s/it]                                                       {'loss': 0.8768, 'learning_rate': 1.876822474787303e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 961/5198 [5:28:12<21:51:55, 18.58s/it] 19%|â–ˆâ–Š        | 962/5198 [5:28:31<21:48:48, 18.54s/it]                                                       {'loss': 0.9025, 'learning_rate': 1.8765227166330933e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 962/5198 [5:28:31<21:48:48, 18.54s/it] 19%|â–ˆâ–Š        | 963/5198 [5:28:48<21:17:38, 18.10s/it]                                                       {'loss': 0.8911, 'learning_rate': 1.8762226181825857e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 963/5198 [5:28:48<21:17:38, 18.10s/it] 19%|â–ˆâ–Š        | 964/5198 [5:29:05<20:57:28, 17.82s/it]                                                       {'loss': 0.9289, 'learning_rate': 1.875922179552288e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 964/5198 [5:29:05<20:57:28, 17.82s/it] 19%|â–ˆâ–Š        | 965/5198 [5:29:23<21:03:36, 17.91s/it]                                                       {'loss': 0.9227, 'learning_rate': 1.875621400858842e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 965/5198 [5:29:23<21:03:36, 17.91s/it] 19%|â–ˆâ–Š        | 966/5198 [5:29:40<20:45:19, 17.66s/it]                                                       {'loss': 0.8667, 'learning_rate': 1.875320282219019e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 966/5198 [5:29:40<20:45:19, 17.66s/it] 19%|â–ˆâ–Š        | 967/5198 [5:29:58<20:50:08, 17.73s/it]                                                       {'loss': 0.8912, 'learning_rate': 1.8750188237497247e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 967/5198 [5:29:58<20:50:08, 17.73s/it] 19%|â–ˆâ–Š        | 968/5198 [5:30:15<20:39:36, 17.58s/it]                                                       {'loss': 0.8016, 'learning_rate': 1.874717025567995e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 968/5198 [5:30:15<20:39:36, 17.58s/it] 19%|â–ˆâ–Š        | 969/5198 [5:30:33<20:38:24, 17.57s/it]                                                       {'loss': 0.9152, 'learning_rate': 1.874414887790999e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 969/5198 [5:30:33<20:38:24, 17.57s/it] 19%|â–ˆâ–Š        | 970/5198 [5:30:51<20:39:36, 17.59s/it]                                                       {'loss': 0.9173, 'learning_rate': 1.8741124105360363e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 970/5198 [5:30:51<20:39:36, 17.59s/it] 19%|â–ˆâ–Š        | 971/5198 [5:31:09<20:56:02, 17.83s/it]                                                       {'loss': 0.8044, 'learning_rate': 1.873809593920539e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 971/5198 [5:31:09<20:56:02, 17.83s/it] 19%|â–ˆâ–Š        | 972/5198 [5:31:26<20:45:22, 17.68s/it]                                                       {'loss': 0.9153, 'learning_rate': 1.8735064380620717e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 972/5198 [5:31:26<20:45:22, 17.68s/it] 19%|â–ˆâ–Š        | 973/5198 [5:31:44<20:42:55, 17.65s/it]                                                       {'loss': 0.347, 'learning_rate': 1.873202943078329e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 973/5198 [5:31:44<20:42:55, 17.65s/it] 19%|â–ˆâ–Š        | 974/5198 [5:32:02<20:42:45, 17.65s/it]                                                       {'loss': 0.87, 'learning_rate': 1.8728991090871387e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 974/5198 [5:32:02<20:42:45, 17.65s/it] 19%|â–ˆâ–‰        | 975/5198 [5:32:20<20:51:58, 17.79s/it]                                                       {'loss': 0.8652, 'learning_rate': 1.8725949362064596e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 975/5198 [5:32:20<20:51:58, 17.79s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 19%|â–ˆâ–‰        | 976/5198 [5:33:48<45:37:22, 38.90s/it]                                                       {'loss': 0.9861, 'learning_rate': 1.8722904245543817e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 976/5198 [5:33:48<45:37:22, 38.90s/it] 19%|â–ˆâ–‰        | 977/5198 [5:34:05<37:54:44, 32.33s/it]                                                       {'loss': 0.8773, 'learning_rate': 1.871985574249127e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 977/5198 [5:34:05<37:54:44, 32.33s/it] 19%|â–ˆâ–‰        | 978/5198 [5:34:23<33:00:20, 28.16s/it]                                                       {'loss': 0.8631, 'learning_rate': 1.8716803854090495e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 978/5198 [5:34:23<33:00:20, 28.16s/it] 19%|â–ˆâ–‰        | 979/5198 [5:34:40<29:06:47, 24.84s/it]                                                       {'loss': 0.8498, 'learning_rate': 1.8713748581526334e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 979/5198 [5:34:40<29:06:47, 24.84s/it] 19%|â–ˆâ–‰        | 980/5198 [5:34:59<26:45:57, 22.84s/it]                                                       {'loss': 0.8331, 'learning_rate': 1.871068992598495e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 980/5198 [5:34:59<26:45:57, 22.84s/it] 19%|â–ˆâ–‰        | 981/5198 [5:35:16<24:59:18, 21.33s/it]                                                       {'loss': 0.865, 'learning_rate': 1.8707627888653816e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 981/5198 [5:35:16<24:59:18, 21.33s/it] 19%|â–ˆâ–‰        | 982/5198 [5:35:33<23:23:52, 19.98s/it]                                                       {'loss': 0.8636, 'learning_rate': 1.8704562470721728e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 982/5198 [5:35:33<23:23:52, 19.98s/it] 19%|â–ˆâ–‰        | 983/5198 [5:35:50<22:19:56, 19.07s/it]                                                       {'loss': 0.8536, 'learning_rate': 1.870149367337878e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 983/5198 [5:35:50<22:19:56, 19.07s/it] 19%|â–ˆâ–‰        | 984/5198 [5:36:08<21:47:01, 18.61s/it]                                                       {'loss': 0.8941, 'learning_rate': 1.8698421497816386e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 984/5198 [5:36:08<21:47:01, 18.61s/it] 19%|â–ˆâ–‰        | 985/5198 [5:36:26<21:31:34, 18.39s/it]                                                       {'loss': 0.9033, 'learning_rate': 1.869534594522727e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 985/5198 [5:36:26<21:31:34, 18.39s/it] 19%|â–ˆâ–‰        | 986/5198 [5:36:43<21:15:54, 18.18s/it]                                                       {'loss': 0.9327, 'learning_rate': 1.8692267016805473e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 986/5198 [5:36:43<21:15:54, 18.18s/it] 19%|â–ˆâ–‰        | 987/5198 [5:37:00<20:43:32, 17.72s/it]                                                       {'loss': 0.92, 'learning_rate': 1.8689184713746333e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 987/5198 [5:37:00<20:43:32, 17.72s/it] 19%|â–ˆâ–‰        | 988/5198 [5:37:17<20:32:54, 17.57s/it]                                                       {'loss': 0.86, 'learning_rate': 1.868609903724651e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 988/5198 [5:37:17<20:32:54, 17.57s/it] 19%|â–ˆâ–‰        | 989/5198 [5:37:35<20:45:17, 17.75s/it]                                                       {'loss': 0.8971, 'learning_rate': 1.8683009988503972e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 989/5198 [5:37:35<20:45:17, 17.75s/it] 19%|â–ˆâ–‰        | 990/5198 [5:37:53<20:37:22, 17.64s/it]                                                       {'loss': 0.8796, 'learning_rate': 1.867991756871799e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 990/5198 [5:37:53<20:37:22, 17.64s/it] 19%|â–ˆâ–‰        | 991/5198 [5:38:10<20:29:22, 17.53s/it]                                                       {'loss': 0.9488, 'learning_rate': 1.867682177908915e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 991/5198 [5:38:10<20:29:22, 17.53s/it] 19%|â–ˆâ–‰        | 992/5198 [5:38:28<20:32:19, 17.58s/it]                                                       {'loss': 0.8368, 'learning_rate': 1.867372262081934e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 992/5198 [5:38:28<20:32:19, 17.58s/it] 19%|â–ˆâ–‰        | 993/5198 [5:38:45<20:17:51, 17.38s/it]                                                       {'loss': 0.8891, 'learning_rate': 1.8670620095111766e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 993/5198 [5:38:45<20:17:51, 17.38s/it] 19%|â–ˆâ–‰        | 994/5198 [5:39:03<20:40:02, 17.70s/it]                                                       {'loss': 0.795, 'learning_rate': 1.8667514203170934e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 994/5198 [5:39:03<20:40:02, 17.70s/it] 19%|â–ˆâ–‰        | 995/5198 [5:39:21<20:45:37, 17.78s/it]                                                       {'loss': 0.9401, 'learning_rate': 1.8664404946202658e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 995/5198 [5:39:21<20:45:37, 17.78s/it] 19%|â–ˆâ–‰        | 996/5198 [5:39:38<20:39:10, 17.69s/it]                                                       {'loss': 0.8431, 'learning_rate': 1.8661292325414058e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 996/5198 [5:39:38<20:39:10, 17.69s/it] 19%|â–ˆâ–‰        | 997/5198 [5:39:56<20:31:25, 17.59s/it]                                                       {'loss': 0.8795, 'learning_rate': 1.865817634201356e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 997/5198 [5:39:56<20:31:25, 17.59s/it] 19%|â–ˆâ–‰        | 998/5198 [5:40:14<20:47:35, 17.82s/it]                                                       {'loss': 0.8431, 'learning_rate': 1.8655056997210893e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 998/5198 [5:40:14<20:47:35, 17.82s/it] 19%|â–ˆâ–‰        | 999/5198 [5:40:32<20:39:11, 17.71s/it]                                                       {'loss': 0.828, 'learning_rate': 1.8651934292217097e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 999/5198 [5:40:32<20:39:11, 17.71s/it] 19%|â–ˆâ–‰        | 1000/5198 [5:40:50<20:45:21, 17.80s/it]                                                        {'loss': 0.8544, 'learning_rate': 1.864880822824452e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1000/5198 [5:40:50<20:45:21, 17.80s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 19%|â–ˆâ–‰        | 1001/5198 [5:42:25<47:45:41, 40.97s/it]                                                        {'loss': 0.8421, 'learning_rate': 1.8645678806506795e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1001/5198 [5:42:25<47:45:41, 40.97s/it] 19%|â–ˆâ–‰        | 1002/5198 [5:42:42<39:33:14, 33.94s/it]                                                        {'loss': 0.3427, 'learning_rate': 1.864254602821888e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1002/5198 [5:42:42<39:33:14, 33.94s/it] 19%|â–ˆâ–‰        | 1003/5198 [5:43:00<33:48:04, 29.01s/it]                                                        {'loss': 0.9083, 'learning_rate': 1.8639409894597026e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1003/5198 [5:43:00<33:48:04, 29.01s/it] 19%|â–ˆâ–‰        | 1004/5198 [5:43:18<30:06:44, 25.85s/it]                                                        {'loss': 0.8291, 'learning_rate': 1.8636270406858786e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1004/5198 [5:43:18<30:06:44, 25.85s/it] 19%|â–ˆâ–‰        | 1005/5198 [5:43:36<27:21:55, 23.50s/it]                                                        {'loss': 0.9014, 'learning_rate': 1.8633127566223023e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1005/5198 [5:43:36<27:21:55, 23.50s/it] 19%|â–ˆâ–‰        | 1006/5198 [5:43:54<25:30:17, 21.90s/it]                                                        {'loss': 0.9286, 'learning_rate': 1.862998137390989e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1006/5198 [5:43:54<25:30:17, 21.90s/it] 19%|â–ˆâ–‰        | 1007/5198 [5:44:12<24:09:06, 20.75s/it]                                                        {'loss': 0.8769, 'learning_rate': 1.8626831831140845e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1007/5198 [5:44:12<24:09:06, 20.75s/it] 19%|â–ˆâ–‰        | 1008/5198 [5:44:30<22:53:09, 19.66s/it]                                                        {'loss': 0.8857, 'learning_rate': 1.8623678939138652e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1008/5198 [5:44:30<22:53:09, 19.66s/it] 19%|â–ˆâ–‰        | 1009/5198 [5:44:46<21:39:58, 18.62s/it]                                                        {'loss': 0.924, 'learning_rate': 1.8620522699127374e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1009/5198 [5:44:46<21:39:58, 18.62s/it] 19%|â–ˆâ–‰        | 1010/5198 [5:45:04<21:42:51, 18.67s/it]                                                        {'loss': 0.8688, 'learning_rate': 1.8617363112332376e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1010/5198 [5:45:04<21:42:51, 18.67s/it] 19%|â–ˆâ–‰        | 1011/5198 [5:45:22<21:21:45, 18.37s/it]                                                        {'loss': 0.8881, 'learning_rate': 1.8614200179980307e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1011/5198 [5:45:22<21:21:45, 18.37s/it] 19%|â–ˆâ–‰        | 1012/5198 [5:45:40<21:12:04, 18.23s/it]                                                        {'loss': 0.8952, 'learning_rate': 1.8611033903299136e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1012/5198 [5:45:40<21:12:04, 18.23s/it] 19%|â–ˆâ–‰        | 1013/5198 [5:45:57<20:54:10, 17.98s/it]                                                        {'loss': 0.3901, 'learning_rate': 1.8607864283518116e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1013/5198 [5:45:57<20:54:10, 17.98s/it] 20%|â–ˆâ–‰        | 1014/5198 [5:46:15<20:47:54, 17.90s/it]                                                        {'loss': 0.8212, 'learning_rate': 1.8604691321867804e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1014/5198 [5:46:15<20:47:54, 17.90s/it] 20%|â–ˆâ–‰        | 1015/5198 [5:46:33<20:36:40, 17.74s/it]                                                        {'loss': 0.9311, 'learning_rate': 1.8601515019580053e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1015/5198 [5:46:33<20:36:40, 17.74s/it] 20%|â–ˆâ–‰        | 1016/5198 [5:46:51<20:49:04, 17.92s/it]                                                        {'loss': 0.8736, 'learning_rate': 1.8598335377888012e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1016/5198 [5:46:51<20:49:04, 17.92s/it] 20%|â–ˆâ–‰        | 1017/5198 [5:47:08<20:41:37, 17.82s/it]                                                        {'loss': 0.9129, 'learning_rate': 1.8595152398026128e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1017/5198 [5:47:08<20:41:37, 17.82s/it] 20%|â–ˆâ–‰        | 1018/5198 [5:47:27<21:04:21, 18.15s/it]                                                        {'loss': 0.8786, 'learning_rate': 1.8591966081230142e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1018/5198 [5:47:27<21:04:21, 18.15s/it] 20%|â–ˆâ–‰        | 1019/5198 [5:47:45<20:50:18, 17.95s/it]                                                        {'loss': 0.8836, 'learning_rate': 1.8588776428737095e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1019/5198 [5:47:45<20:50:18, 17.95s/it] 20%|â–ˆâ–‰        | 1020/5198 [5:48:03<20:43:46, 17.86s/it]                                                        {'loss': 0.8955, 'learning_rate': 1.858558344178532e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1020/5198 [5:48:03<20:43:46, 17.86s/it] 20%|â–ˆâ–‰        | 1021/5198 [5:48:21<20:58:44, 18.08s/it]                                                        {'loss': 0.9276, 'learning_rate': 1.8582387121614437e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1021/5198 [5:48:21<20:58:44, 18.08s/it] 20%|â–ˆâ–‰        | 1022/5198 [5:48:38<20:35:42, 17.75s/it]                                                        {'loss': 0.8718, 'learning_rate': 1.857918746946538e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1022/5198 [5:48:38<20:35:42, 17.75s/it] 20%|â–ˆâ–‰        | 1023/5198 [5:48:56<20:39:30, 17.81s/it]                                                        {'loss': 0.9185, 'learning_rate': 1.8575984486580353e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1023/5198 [5:48:56<20:39:30, 17.81s/it] 20%|â–ˆâ–‰        | 1024/5198 [5:49:14<20:43:24, 17.87s/it]                                                        {'loss': 0.8873, 'learning_rate': 1.857277817420287e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1024/5198 [5:49:14<20:43:24, 17.87s/it] 20%|â–ˆâ–‰        | 1025/5198 [5:49:31<20:29:56, 17.68s/it]                                                        {'loss': 0.943, 'learning_rate': 1.8569568533577727e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1025/5198 [5:49:31<20:29:56, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 20%|â–ˆâ–‰        | 1026/5198 [5:51:10<48:34:15, 41.91s/it]                                                        {'loss': 0.8519, 'learning_rate': 1.8566355565951023e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1026/5198 [5:51:10<48:34:15, 41.91s/it] 20%|â–ˆâ–‰        | 1027/5198 [5:51:27<40:07:52, 34.64s/it]                                                        {'loss': 0.9012, 'learning_rate': 1.8563139272570142e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1027/5198 [5:51:27<40:07:52, 34.64s/it] 20%|â–ˆâ–‰        | 1028/5198 [5:51:44<34:01:18, 29.37s/it]                                                        {'loss': 0.9535, 'learning_rate': 1.8559919654683756e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1028/5198 [5:51:44<34:01:18, 29.37s/it] 20%|â–ˆâ–‰        | 1029/5198 [5:52:02<29:45:27, 25.70s/it]                                                        {'loss': 0.9058, 'learning_rate': 1.8556696713541833e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1029/5198 [5:52:02<29:45:27, 25.70s/it] 20%|â–ˆâ–‰        | 1030/5198 [5:52:21<27:23:26, 23.66s/it]                                                        {'loss': 0.9004, 'learning_rate': 1.855347045039563e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1030/5198 [5:52:21<27:23:26, 23.66s/it] 20%|â–ˆâ–‰        | 1031/5198 [5:52:37<24:57:27, 21.56s/it]                                                        {'loss': 0.8779, 'learning_rate': 1.8550240866497697e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1031/5198 [5:52:37<24:57:27, 21.56s/it] 20%|â–ˆâ–‰        | 1032/5198 [5:52:55<23:39:10, 20.44s/it]                                                        {'loss': 0.9223, 'learning_rate': 1.854700796310186e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1032/5198 [5:52:55<23:39:10, 20.44s/it] 20%|â–ˆâ–‰        | 1033/5198 [5:53:13<22:44:19, 19.65s/it]                                                        {'loss': 0.9373, 'learning_rate': 1.8543771741463254e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1033/5198 [5:53:13<22:44:19, 19.65s/it] 20%|â–ˆâ–‰        | 1034/5198 [5:53:30<21:53:37, 18.93s/it]                                                        {'loss': 0.9478, 'learning_rate': 1.8540532202838286e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1034/5198 [5:53:30<21:53:37, 18.93s/it] 20%|â–ˆâ–‰        | 1035/5198 [5:53:47<21:21:22, 18.47s/it]                                                        {'loss': 0.8572, 'learning_rate': 1.8537289348484658e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1035/5198 [5:53:47<21:21:22, 18.47s/it] 20%|â–ˆâ–‰        | 1036/5198 [5:54:06<21:17:19, 18.41s/it]                                                        {'loss': 0.8716, 'learning_rate': 1.8534043179661357e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1036/5198 [5:54:06<21:17:19, 18.41s/it] 20%|â–ˆâ–‰        | 1037/5198 [5:54:24<21:22:10, 18.49s/it]                                                        {'loss': 0.8309, 'learning_rate': 1.8530793697628658e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1037/5198 [5:54:24<21:22:10, 18.49s/it] 20%|â–ˆâ–‰        | 1038/5198 [5:54:41<20:48:54, 18.01s/it]                                                        {'loss': 0.9058, 'learning_rate': 1.8527540903648122e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1038/5198 [5:54:41<20:48:54, 18.01s/it] 20%|â–ˆâ–‰        | 1039/5198 [5:54:59<20:34:37, 17.81s/it]                                                        {'loss': 0.8833, 'learning_rate': 1.8524284798982595e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1039/5198 [5:54:59<20:34:37, 17.81s/it] 20%|â–ˆâ–ˆ        | 1040/5198 [5:55:16<20:26:59, 17.71s/it]                                                        {'loss': 0.8892, 'learning_rate': 1.852102538489621e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1040/5198 [5:55:16<20:26:59, 17.71s/it] 20%|â–ˆâ–ˆ        | 1041/5198 [5:55:33<20:09:40, 17.46s/it]                                                        {'loss': 0.9762, 'learning_rate': 1.8517762662654383e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1041/5198 [5:55:33<20:09:40, 17.46s/it] 20%|â–ˆâ–ˆ        | 1042/5198 [5:55:51<20:11:03, 17.48s/it]                                                        {'loss': 0.8884, 'learning_rate': 1.851449663352381e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1042/5198 [5:55:51<20:11:03, 17.48s/it] 20%|â–ˆâ–ˆ        | 1043/5198 [5:56:08<20:17:16, 17.58s/it]                                                        {'loss': 0.8512, 'learning_rate': 1.851122729877249e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1043/5198 [5:56:08<20:17:16, 17.58s/it] 20%|â–ˆâ–ˆ        | 1044/5198 [5:56:26<20:22:17, 17.65s/it]                                                        {'loss': 0.8436, 'learning_rate': 1.8507954659669677e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1044/5198 [5:56:26<20:22:17, 17.65s/it] 20%|â–ˆâ–ˆ        | 1045/5198 [5:56:44<20:26:45, 17.72s/it]                                                        {'loss': 0.8206, 'learning_rate': 1.850467871748593e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1045/5198 [5:56:44<20:26:45, 17.72s/it] 20%|â–ˆâ–ˆ        | 1046/5198 [5:57:01<20:13:26, 17.54s/it]                                                        {'loss': 0.9318, 'learning_rate': 1.850139947349308e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1046/5198 [5:57:01<20:13:26, 17.54s/it] 20%|â–ˆâ–ˆ        | 1047/5198 [5:57:19<20:22:07, 17.66s/it]                                                        {'loss': 0.3271, 'learning_rate': 1.8498116928964244e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1047/5198 [5:57:19<20:22:07, 17.66s/it] 20%|â–ˆâ–ˆ        | 1048/5198 [5:57:37<20:21:02, 17.65s/it]                                                        {'loss': 0.807, 'learning_rate': 1.849483108517381e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1048/5198 [5:57:37<20:21:02, 17.65s/it] 20%|â–ˆâ–ˆ        | 1049/5198 [5:57:55<20:27:46, 17.76s/it]                                                        {'loss': 0.859, 'learning_rate': 1.849154194339747e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1049/5198 [5:57:55<20:27:46, 17.76s/it] 20%|â–ˆâ–ˆ        | 1050/5198 [5:58:13<20:34:07, 17.85s/it]                                                        {'loss': 0.8666, 'learning_rate': 1.8488249504912173e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1050/5198 [5:58:13<20:34:07, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 20%|â–ˆâ–ˆ        | 1051/5198 [5:59:39<44:08:34, 38.32s/it]                                                        {'loss': 0.9194, 'learning_rate': 1.8484953770996163e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1051/5198 [5:59:39<44:08:34, 38.32s/it] 20%|â–ˆâ–ˆ        | 1052/5198 [5:59:56<36:47:55, 31.95s/it]                                                        {'loss': 0.9219, 'learning_rate': 1.848165474292895e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1052/5198 [5:59:56<36:47:55, 31.95s/it] 20%|â–ˆâ–ˆ        | 1053/5198 [6:00:14<31:58:54, 27.78s/it]                                                        {'loss': 0.8695, 'learning_rate': 1.8478352421991334e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1053/5198 [6:00:14<31:58:54, 27.78s/it] 20%|â–ˆâ–ˆ        | 1054/5198 [6:00:32<28:29:11, 24.75s/it]                                                        {'loss': 0.3404, 'learning_rate': 1.847504680946539e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1054/5198 [6:00:32<28:29:11, 24.75s/it] 20%|â–ˆâ–ˆ        | 1055/5198 [6:00:49<25:59:55, 22.59s/it]                                                        {'loss': 0.9065, 'learning_rate': 1.847173790663447e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1055/5198 [6:00:49<25:59:55, 22.59s/it] 20%|â–ˆâ–ˆ        | 1056/5198 [6:01:07<24:29:15, 21.28s/it]                                                        {'loss': 0.8305, 'learning_rate': 1.8468425714783206e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1056/5198 [6:01:08<24:29:15, 21.28s/it] 20%|â–ˆâ–ˆ        | 1057/5198 [6:01:26<23:23:55, 20.34s/it]                                                        {'loss': 0.8954, 'learning_rate': 1.84651102351975e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1057/5198 [6:01:26<23:23:55, 20.34s/it] 20%|â–ˆâ–ˆ        | 1058/5198 [6:01:45<22:55:10, 19.93s/it]                                                        {'loss': 0.9183, 'learning_rate': 1.846179146916454e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1058/5198 [6:01:45<22:55:10, 19.93s/it] 20%|â–ˆâ–ˆ        | 1059/5198 [6:02:01<21:50:06, 18.99s/it]                                                        {'loss': 0.881, 'learning_rate': 1.8458469417972783e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1059/5198 [6:02:01<21:50:06, 18.99s/it] 20%|â–ˆâ–ˆ        | 1060/5198 [6:02:20<21:32:43, 18.74s/it]                                                        {'loss': 0.9049, 'learning_rate': 1.8455144082911965e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1060/5198 [6:02:20<21:32:43, 18.74s/it] 20%|â–ˆâ–ˆ        | 1061/5198 [6:02:37<21:14:33, 18.49s/it]                                                        {'loss': 0.9108, 'learning_rate': 1.8451815465273097e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1061/5198 [6:02:37<21:14:33, 18.49s/it] 20%|â–ˆâ–ˆ        | 1062/5198 [6:02:54<20:41:58, 18.02s/it]                                                        {'loss': 0.9241, 'learning_rate': 1.8448483566348456e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1062/5198 [6:02:54<20:41:58, 18.02s/it] 20%|â–ˆâ–ˆ        | 1063/5198 [6:03:12<20:38:02, 17.96s/it]                                                        {'loss': 0.341, 'learning_rate': 1.8445148387431605e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1063/5198 [6:03:12<20:38:02, 17.96s/it] 20%|â–ˆâ–ˆ        | 1064/5198 [6:03:30<20:25:33, 17.79s/it]                                                        {'loss': 0.3483, 'learning_rate': 1.8441809929817382e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1064/5198 [6:03:30<20:25:33, 17.79s/it] 20%|â–ˆâ–ˆ        | 1065/5198 [6:03:47<20:20:58, 17.73s/it]                                                        {'loss': 0.8857, 'learning_rate': 1.8438468194801876e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1065/5198 [6:03:47<20:20:58, 17.73s/it] 21%|â–ˆâ–ˆ        | 1066/5198 [6:04:05<20:16:21, 17.66s/it]                                                        {'loss': 0.9109, 'learning_rate': 1.8435123183682475e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1066/5198 [6:04:05<20:16:21, 17.66s/it] 21%|â–ˆâ–ˆ        | 1067/5198 [6:04:22<20:12:24, 17.61s/it]                                                        {'loss': 0.8586, 'learning_rate': 1.8431774897757824e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1067/5198 [6:04:22<20:12:24, 17.61s/it] 21%|â–ˆâ–ˆ        | 1068/5198 [6:04:39<19:52:44, 17.33s/it]                                                        {'loss': 0.8618, 'learning_rate': 1.8428423338327847e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1068/5198 [6:04:39<19:52:44, 17.33s/it] 21%|â–ˆâ–ˆ        | 1069/5198 [6:04:56<19:53:29, 17.34s/it]                                                        {'loss': 0.8459, 'learning_rate': 1.8425068506693727e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1069/5198 [6:04:56<19:53:29, 17.34s/it] 21%|â–ˆâ–ˆ        | 1070/5198 [6:05:14<19:57:04, 17.40s/it]                                                        {'loss': 0.8611, 'learning_rate': 1.842171040415793e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1070/5198 [6:05:14<19:57:04, 17.40s/it] 21%|â–ˆâ–ˆ        | 1071/5198 [6:05:32<20:20:53, 17.75s/it]                                                        {'loss': 0.8763, 'learning_rate': 1.8418349032024185e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1071/5198 [6:05:32<20:20:53, 17.75s/it] 21%|â–ˆâ–ˆ        | 1072/5198 [6:05:51<20:36:30, 17.98s/it]                                                        {'loss': 0.8161, 'learning_rate': 1.8414984391597492e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1072/5198 [6:05:51<20:36:30, 17.98s/it] 21%|â–ˆâ–ˆ        | 1073/5198 [6:06:07<20:06:43, 17.55s/it]                                                        {'loss': 0.8975, 'learning_rate': 1.8411616484184126e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1073/5198 [6:06:07<20:06:43, 17.55s/it] 21%|â–ˆâ–ˆ        | 1074/5198 [6:06:25<20:02:16, 17.49s/it]                                                        {'loss': 0.9167, 'learning_rate': 1.8408245311091618e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1074/5198 [6:06:25<20:02:16, 17.49s/it] 21%|â–ˆâ–ˆ        | 1075/5198 [6:06:43<20:10:08, 17.61s/it]                                                        {'loss': 0.8932, 'learning_rate': 1.8404870873628774e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1075/5198 [6:06:43<20:10:08, 17.61s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 21%|â–ˆâ–ˆ        | 1076/5198 [6:08:07<42:59:49, 37.55s/it]                                                        {'loss': 0.823, 'learning_rate': 1.8401493173105675e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1076/5198 [6:08:07<42:59:49, 37.55s/it] 21%|â–ˆâ–ˆ        | 1077/5198 [6:08:23<35:49:59, 31.30s/it]                                                        {'loss': 0.9357, 'learning_rate': 1.8398112210833648e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1077/5198 [6:08:23<35:49:59, 31.30s/it] 21%|â–ˆâ–ˆ        | 1078/5198 [6:08:41<31:15:02, 27.31s/it]                                                        {'loss': 0.8455, 'learning_rate': 1.8394727988125308e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1078/5198 [6:08:41<31:15:02, 27.31s/it] 21%|â–ˆâ–ˆ        | 1079/5198 [6:08:58<27:38:23, 24.16s/it]                                                        {'loss': 0.91, 'learning_rate': 1.8391340506294524e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1079/5198 [6:08:58<27:38:23, 24.16s/it] 21%|â–ˆâ–ˆ        | 1080/5198 [6:09:16<25:24:39, 22.21s/it]                                                        {'loss': 0.8937, 'learning_rate': 1.8387949766656434e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1080/5198 [6:09:16<25:24:39, 22.21s/it] 21%|â–ˆâ–ˆ        | 1081/5198 [6:09:33<23:45:23, 20.77s/it]                                                        {'loss': 0.9134, 'learning_rate': 1.8384555770527438e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1081/5198 [6:09:33<23:45:23, 20.77s/it] 21%|â–ˆâ–ˆ        | 1082/5198 [6:09:51<22:42:00, 19.85s/it]                                                        {'loss': 0.8864, 'learning_rate': 1.8381158519225204e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1082/5198 [6:09:51<22:42:00, 19.85s/it] 21%|â–ˆâ–ˆ        | 1083/5198 [6:10:08<21:46:00, 19.04s/it]                                                        {'loss': 0.8703, 'learning_rate': 1.8377758014068662e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1083/5198 [6:10:08<21:46:00, 19.04s/it] 21%|â–ˆâ–ˆ        | 1084/5198 [6:10:26<21:19:57, 18.67s/it]                                                        {'loss': 0.8453, 'learning_rate': 1.8374354256378e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1084/5198 [6:10:26<21:19:57, 18.67s/it] 21%|â–ˆâ–ˆ        | 1085/5198 [6:10:44<21:00:59, 18.40s/it]                                                        {'loss': 0.8648, 'learning_rate': 1.837094724747468e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1085/5198 [6:10:44<21:00:59, 18.40s/it] 21%|â–ˆâ–ˆ        | 1086/5198 [6:11:02<20:53:05, 18.28s/it]                                                        {'loss': 0.8707, 'learning_rate': 1.8367536988681422e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1086/5198 [6:11:02<20:53:05, 18.28s/it] 21%|â–ˆâ–ˆ        | 1087/5198 [6:11:19<20:36:56, 18.05s/it]                                                        {'loss': 0.919, 'learning_rate': 1.83641234813222e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1087/5198 [6:11:19<20:36:56, 18.05s/it] 21%|â–ˆâ–ˆ        | 1088/5198 [6:11:37<20:32:32, 17.99s/it]                                                        {'loss': 0.88, 'learning_rate': 1.8360706726722253e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1088/5198 [6:11:37<20:32:32, 17.99s/it] 21%|â–ˆâ–ˆ        | 1089/5198 [6:11:56<20:54:58, 18.33s/it]                                                        {'loss': 0.8042, 'learning_rate': 1.835728672620809e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1089/5198 [6:11:56<20:54:58, 18.33s/it] 21%|â–ˆâ–ˆ        | 1090/5198 [6:12:14<20:45:55, 18.20s/it]                                                        {'loss': 0.8837, 'learning_rate': 1.8353863481107473e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1090/5198 [6:12:14<20:45:55, 18.20s/it] 21%|â–ˆâ–ˆ        | 1091/5198 [6:12:33<20:52:38, 18.30s/it]                                                        {'loss': 0.8562, 'learning_rate': 1.835043699274942e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1091/5198 [6:12:33<20:52:38, 18.30s/it] 21%|â–ˆâ–ˆ        | 1092/5198 [6:12:50<20:40:44, 18.13s/it]                                                        {'loss': 0.8482, 'learning_rate': 1.8347007262464206e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1092/5198 [6:12:50<20:40:44, 18.13s/it] 21%|â–ˆâ–ˆ        | 1093/5198 [6:13:08<20:33:03, 18.02s/it]                                                        {'loss': 0.9057, 'learning_rate': 1.8343574291583385e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1093/5198 [6:13:08<20:33:03, 18.02s/it] 21%|â–ˆâ–ˆ        | 1094/5198 [6:13:26<20:33:39, 18.04s/it]                                                        {'loss': 0.8601, 'learning_rate': 1.8340138081439743e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1094/5198 [6:13:26<20:33:39, 18.04s/it] 21%|â–ˆâ–ˆ        | 1095/5198 [6:13:44<20:22:38, 17.88s/it]                                                        {'loss': 0.8562, 'learning_rate': 1.833669863336734e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1095/5198 [6:13:44<20:22:38, 17.88s/it] 21%|â–ˆâ–ˆ        | 1096/5198 [6:14:01<20:17:50, 17.81s/it]                                                        {'loss': 0.8239, 'learning_rate': 1.833325594870148e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1096/5198 [6:14:01<20:17:50, 17.81s/it] 21%|â–ˆâ–ˆ        | 1097/5198 [6:14:19<20:22:57, 17.89s/it]                                                        {'loss': 0.8393, 'learning_rate': 1.8329810028778747e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1097/5198 [6:14:19<20:22:57, 17.89s/it] 21%|â–ˆâ–ˆ        | 1098/5198 [6:14:36<20:02:36, 17.60s/it]                                                        {'loss': 0.889, 'learning_rate': 1.8326360874936952e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1098/5198 [6:14:36<20:02:36, 17.60s/it] 21%|â–ˆâ–ˆ        | 1099/5198 [6:14:54<20:01:30, 17.59s/it]                                                        {'loss': 0.9284, 'learning_rate': 1.8322908488515182e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1099/5198 [6:14:54<20:01:30, 17.59s/it] 21%|â–ˆâ–ˆ        | 1100/5198 [6:15:12<20:11:42, 17.74s/it]                                                        {'loss': 0.8842, 'learning_rate': 1.8319452870853772e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1100/5198 [6:15:12<20:11:42, 17.74s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 21%|â–ˆâ–ˆ        | 1101/5198 [6:16:40<44:15:36, 38.89s/it]                                                        {'loss': 0.9404, 'learning_rate': 1.8315994023294306e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1101/5198 [6:16:40<44:15:36, 38.89s/it] 21%|â–ˆâ–ˆ        | 1102/5198 [6:16:57<36:46:44, 32.33s/it]                                                        {'loss': 0.8739, 'learning_rate': 1.8312531947179634e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1102/5198 [6:16:57<36:46:44, 32.33s/it] 21%|â–ˆâ–ˆ        | 1103/5198 [6:17:15<31:37:17, 27.80s/it]                                                        {'loss': 0.8486, 'learning_rate': 1.8309066643853854e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1103/5198 [6:17:15<31:37:17, 27.80s/it] 21%|â–ˆâ–ˆ        | 1104/5198 [6:17:32<28:08:36, 24.75s/it]                                                        {'loss': 0.9077, 'learning_rate': 1.8305598114662312e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1104/5198 [6:17:32<28:08:36, 24.75s/it] 21%|â–ˆâ–ˆâ–       | 1105/5198 [6:17:50<25:39:59, 22.58s/it]                                                        {'loss': 0.8835, 'learning_rate': 1.830212636095161e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1105/5198 [6:17:50<25:39:59, 22.58s/it] 21%|â–ˆâ–ˆâ–       | 1106/5198 [6:18:07<24:00:21, 21.12s/it]                                                        {'loss': 0.9077, 'learning_rate': 1.8298651384069605e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1106/5198 [6:18:07<24:00:21, 21.12s/it] 21%|â–ˆâ–ˆâ–       | 1107/5198 [6:18:26<22:58:49, 20.22s/it]                                                        {'loss': 0.8727, 'learning_rate': 1.8295173185365405e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1107/5198 [6:18:26<22:58:49, 20.22s/it] 21%|â–ˆâ–ˆâ–       | 1108/5198 [6:18:43<21:53:53, 19.27s/it]                                                        {'loss': 0.9362, 'learning_rate': 1.829169176618936e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1108/5198 [6:18:43<21:53:53, 19.27s/it] 21%|â–ˆâ–ˆâ–       | 1109/5198 [6:19:01<21:33:49, 18.99s/it]                                                        {'loss': 0.8949, 'learning_rate': 1.828820712789308e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1109/5198 [6:19:01<21:33:49, 18.99s/it] 21%|â–ˆâ–ˆâ–       | 1110/5198 [6:19:19<21:05:13, 18.57s/it]                                                        {'loss': 0.842, 'learning_rate': 1.828471927182942e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1110/5198 [6:19:19<21:05:13, 18.57s/it] 21%|â–ˆâ–ˆâ–       | 1111/5198 [6:19:36<20:46:53, 18.31s/it]                                                        {'loss': 0.8718, 'learning_rate': 1.828122819935249e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1111/5198 [6:19:36<20:46:53, 18.31s/it] 21%|â–ˆâ–ˆâ–       | 1112/5198 [6:19:54<20:34:10, 18.12s/it]                                                        {'loss': 0.3744, 'learning_rate': 1.8277733911817642e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1112/5198 [6:19:54<20:34:10, 18.12s/it] 21%|â–ˆâ–ˆâ–       | 1113/5198 [6:20:12<20:27:48, 18.03s/it]                                                        {'loss': 0.8962, 'learning_rate': 1.8274236410581478e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1113/5198 [6:20:12<20:27:48, 18.03s/it] 21%|â–ˆâ–ˆâ–       | 1114/5198 [6:20:29<20:21:26, 17.94s/it]                                                        {'loss': 0.8709, 'learning_rate': 1.827073569700185e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1114/5198 [6:20:29<20:21:26, 17.94s/it] 21%|â–ˆâ–ˆâ–       | 1115/5198 [6:20:46<19:59:54, 17.63s/it]                                                        {'loss': 0.8597, 'learning_rate': 1.8267231772437854e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1115/5198 [6:20:46<19:59:54, 17.63s/it] 21%|â–ˆâ–ˆâ–       | 1116/5198 [6:21:03<19:44:54, 17.42s/it]                                                        {'loss': 0.3409, 'learning_rate': 1.8263724638249834e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1116/5198 [6:21:03<19:44:54, 17.42s/it] 21%|â–ˆâ–ˆâ–       | 1117/5198 [6:21:21<19:57:44, 17.61s/it]                                                        {'loss': 0.3493, 'learning_rate': 1.8260214295799382e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1117/5198 [6:21:21<19:57:44, 17.61s/it] 22%|â–ˆâ–ˆâ–       | 1118/5198 [6:21:40<20:11:59, 17.82s/it]                                                        {'loss': 0.8614, 'learning_rate': 1.825670074644933e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1118/5198 [6:21:40<20:11:59, 17.82s/it] 22%|â–ˆâ–ˆâ–       | 1119/5198 [6:21:57<19:52:28, 17.54s/it]                                                        {'loss': 0.3541, 'learning_rate': 1.8253183991563768e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1119/5198 [6:21:57<19:52:28, 17.54s/it] 22%|â–ˆâ–ˆâ–       | 1120/5198 [6:22:15<20:07:18, 17.76s/it]                                                        {'loss': 0.8583, 'learning_rate': 1.824966403250801e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1120/5198 [6:22:15<20:07:18, 17.76s/it] 22%|â–ˆâ–ˆâ–       | 1121/5198 [6:22:33<20:12:03, 17.84s/it]                                                        {'loss': 0.9232, 'learning_rate': 1.8246140870648633e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1121/5198 [6:22:33<20:12:03, 17.84s/it] 22%|â–ˆâ–ˆâ–       | 1122/5198 [6:22:51<20:22:26, 17.99s/it]                                                        {'loss': 0.8726, 'learning_rate': 1.8242614507353446e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1122/5198 [6:22:51<20:22:26, 17.99s/it] 22%|â–ˆâ–ˆâ–       | 1123/5198 [6:23:09<20:17:20, 17.92s/it]                                                        {'loss': 0.7835, 'learning_rate': 1.8239084943991507e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1123/5198 [6:23:09<20:17:20, 17.92s/it] 22%|â–ˆâ–ˆâ–       | 1124/5198 [6:23:26<19:56:08, 17.62s/it]                                                        {'loss': 0.847, 'learning_rate': 1.823555218193311e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1124/5198 [6:23:26<19:56:08, 17.62s/it] 22%|â–ˆâ–ˆâ–       | 1125/5198 [6:23:44<19:57:45, 17.64s/it]                                                        {'loss': 0.8838, 'learning_rate': 1.8232016222549797e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1125/5198 [6:23:44<19:57:45, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 1126/5198 [6:25:11<43:45:20, 38.68s/it]                                                        {'loss': 0.868, 'learning_rate': 1.8228477067214352e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1126/5198 [6:25:11<43:45:20, 38.68s/it] 22%|â–ˆâ–ˆâ–       | 1127/5198 [6:25:29<36:41:48, 32.45s/it]                                                        {'loss': 0.8651, 'learning_rate': 1.8224934717300794e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1127/5198 [6:25:29<36:41:48, 32.45s/it] 22%|â–ˆâ–ˆâ–       | 1128/5198 [6:25:48<31:55:05, 28.23s/it]                                                        {'loss': 0.8195, 'learning_rate': 1.8221389174184385e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1128/5198 [6:25:48<31:55:05, 28.23s/it] 22%|â–ˆâ–ˆâ–       | 1129/5198 [6:26:05<28:23:35, 25.12s/it]                                                        {'loss': 0.8933, 'learning_rate': 1.8217840439241633e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1129/5198 [6:26:05<28:23:35, 25.12s/it] 22%|â–ˆâ–ˆâ–       | 1130/5198 [6:26:23<25:54:33, 22.93s/it]                                                        {'loss': 0.4196, 'learning_rate': 1.8214288513850267e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1130/5198 [6:26:23<25:54:33, 22.93s/it] 22%|â–ˆâ–ˆâ–       | 1131/5198 [6:26:40<23:53:25, 21.15s/it]                                                        {'loss': 0.8472, 'learning_rate': 1.8210733399389277e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1131/5198 [6:26:40<23:53:25, 21.15s/it] 22%|â–ˆâ–ˆâ–       | 1132/5198 [6:26:58<22:37:04, 20.03s/it]                                                        {'loss': 0.3558, 'learning_rate': 1.820717509723888e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1132/5198 [6:26:58<22:37:04, 20.03s/it] 22%|â–ˆâ–ˆâ–       | 1133/5198 [6:27:16<22:05:40, 19.57s/it]                                                        {'loss': 0.9254, 'learning_rate': 1.8203613608780525e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1133/5198 [6:27:16<22:05:40, 19.57s/it] 22%|â–ˆâ–ˆâ–       | 1134/5198 [6:27:35<21:48:14, 19.31s/it]                                                        {'loss': 0.8108, 'learning_rate': 1.8200048935396908e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1134/5198 [6:27:35<21:48:14, 19.31s/it] 22%|â–ˆâ–ˆâ–       | 1135/5198 [6:27:53<21:25:48, 18.99s/it]                                                        {'loss': 0.8839, 'learning_rate': 1.819648107847196e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1135/5198 [6:27:53<21:25:48, 18.99s/it] 22%|â–ˆâ–ˆâ–       | 1136/5198 [6:28:11<21:03:36, 18.66s/it]                                                        {'loss': 0.8758, 'learning_rate': 1.8192910039390844e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1136/5198 [6:28:11<21:03:36, 18.66s/it] 22%|â–ˆâ–ˆâ–       | 1137/5198 [6:28:28<20:33:11, 18.22s/it]                                                        {'loss': 0.8315, 'learning_rate': 1.8189335819539963e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1137/5198 [6:28:28<20:33:11, 18.22s/it] 22%|â–ˆâ–ˆâ–       | 1138/5198 [6:28:45<20:04:37, 17.80s/it]                                                        {'loss': 0.8364, 'learning_rate': 1.8185758420306947e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1138/5198 [6:28:45<20:04:37, 17.80s/it] 22%|â–ˆâ–ˆâ–       | 1139/5198 [6:29:03<20:15:07, 17.96s/it]                                                        {'loss': 0.9079, 'learning_rate': 1.818217784308067e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1139/5198 [6:29:03<20:15:07, 17.96s/it] 22%|â–ˆâ–ˆâ–       | 1140/5198 [6:29:21<20:09:03, 17.88s/it]                                                        {'loss': 0.8812, 'learning_rate': 1.817859408925123e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1140/5198 [6:29:21<20:09:03, 17.88s/it] 22%|â–ˆâ–ˆâ–       | 1141/5198 [6:29:38<19:51:51, 17.63s/it]                                                        {'loss': 0.8746, 'learning_rate': 1.817500716020997e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1141/5198 [6:29:38<19:51:51, 17.63s/it] 22%|â–ˆâ–ˆâ–       | 1142/5198 [6:29:56<20:05:56, 17.84s/it]                                                        {'loss': 0.8758, 'learning_rate': 1.8171417057349457e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1142/5198 [6:29:56<20:05:56, 17.84s/it] 22%|â–ˆâ–ˆâ–       | 1143/5198 [6:30:14<19:53:03, 17.65s/it]                                                        {'loss': 0.3505, 'learning_rate': 1.816782378206349e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1143/5198 [6:30:14<19:53:03, 17.65s/it] 22%|â–ˆâ–ˆâ–       | 1144/5198 [6:30:31<19:52:09, 17.64s/it]                                                        {'loss': 0.8421, 'learning_rate': 1.8164227335747108e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1144/5198 [6:30:31<19:52:09, 17.64s/it] 22%|â–ˆâ–ˆâ–       | 1145/5198 [6:30:50<20:10:51, 17.93s/it]                                                        {'loss': 0.9017, 'learning_rate': 1.8160627719796568e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1145/5198 [6:30:50<20:10:51, 17.93s/it] 22%|â–ˆâ–ˆâ–       | 1146/5198 [6:31:08<20:13:07, 17.96s/it]                                                        {'loss': 0.8807, 'learning_rate': 1.815702493560937e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1146/5198 [6:31:08<20:13:07, 17.96s/it] 22%|â–ˆâ–ˆâ–       | 1147/5198 [6:31:26<20:05:50, 17.86s/it]                                                        {'loss': 0.8616, 'learning_rate': 1.8153418984584238e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1147/5198 [6:31:26<20:05:50, 17.86s/it] 22%|â–ˆâ–ˆâ–       | 1148/5198 [6:31:43<19:59:47, 17.77s/it]                                                        {'loss': 0.8411, 'learning_rate': 1.8149809868121125e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1148/5198 [6:31:43<19:59:47, 17.77s/it] 22%|â–ˆâ–ˆâ–       | 1149/5198 [6:32:00<19:38:42, 17.47s/it]                                                        {'loss': 0.3645, 'learning_rate': 1.8146197587621217e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1149/5198 [6:32:00<19:38:42, 17.47s/it] 22%|â–ˆâ–ˆâ–       | 1150/5198 [6:32:18<19:42:45, 17.53s/it]                                                        {'loss': 0.8946, 'learning_rate': 1.814258214448692e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1150/5198 [6:32:18<19:42:45, 17.53s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 1151/5198 [6:33:43<42:32:51, 37.85s/it]                                                        {'loss': 0.8154, 'learning_rate': 1.8138963540121878e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1151/5198 [6:33:43<42:32:51, 37.85s/it] 22%|â–ˆâ–ˆâ–       | 1152/5198 [6:34:00<35:42:04, 31.77s/it]                                                        {'loss': 0.3657, 'learning_rate': 1.813534177593096e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1152/5198 [6:34:00<35:42:04, 31.77s/it] 22%|â–ˆâ–ˆâ–       | 1153/5198 [6:34:17<30:33:27, 27.20s/it]                                                        {'loss': 0.9244, 'learning_rate': 1.8131716853320254e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1153/5198 [6:34:17<30:33:27, 27.20s/it] 22%|â–ˆâ–ˆâ–       | 1154/5198 [6:34:35<27:25:40, 24.42s/it]                                                        {'loss': 0.9096, 'learning_rate': 1.8128088773697086e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1154/5198 [6:34:35<27:25:40, 24.42s/it] 22%|â–ˆâ–ˆâ–       | 1155/5198 [6:34:53<25:09:05, 22.40s/it]                                                        {'loss': 0.8809, 'learning_rate': 1.8124457538469996e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1155/5198 [6:34:53<25:09:05, 22.40s/it] 22%|â–ˆâ–ˆâ–       | 1156/5198 [6:35:10<23:27:34, 20.89s/it]                                                        {'loss': 0.8721, 'learning_rate': 1.8120823149048753e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1156/5198 [6:35:10<23:27:34, 20.89s/it] 22%|â–ˆâ–ˆâ–       | 1157/5198 [6:35:27<22:18:31, 19.87s/it]                                                        {'loss': 0.9357, 'learning_rate': 1.811718560684436e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1157/5198 [6:35:27<22:18:31, 19.87s/it] 22%|â–ˆâ–ˆâ–       | 1158/5198 [6:35:46<21:49:54, 19.45s/it]                                                        {'loss': 0.8498, 'learning_rate': 1.8113544913269025e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1158/5198 [6:35:46<21:49:54, 19.45s/it] 22%|â–ˆâ–ˆâ–       | 1159/5198 [6:36:04<21:20:08, 19.02s/it]                                                        {'loss': 0.8508, 'learning_rate': 1.8109901069736202e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1159/5198 [6:36:04<21:20:08, 19.02s/it] 22%|â–ˆâ–ˆâ–       | 1160/5198 [6:36:20<20:25:09, 18.20s/it]                                                        {'loss': 0.9029, 'learning_rate': 1.8106254077660552e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1160/5198 [6:36:20<20:25:09, 18.20s/it] 22%|â–ˆâ–ˆâ–       | 1161/5198 [6:36:38<20:16:16, 18.08s/it]                                                        {'loss': 0.3296, 'learning_rate': 1.810260393845796e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1161/5198 [6:36:38<20:16:16, 18.08s/it] 22%|â–ˆâ–ˆâ–       | 1162/5198 [6:36:56<20:09:41, 17.98s/it]                                                        {'loss': 0.3251, 'learning_rate': 1.809895065354554e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1162/5198 [6:36:56<20:09:41, 17.98s/it] 22%|â–ˆâ–ˆâ–       | 1163/5198 [6:37:13<19:54:49, 17.77s/it]                                                        {'loss': 0.9084, 'learning_rate': 1.8095294224341622e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1163/5198 [6:37:13<19:54:49, 17.77s/it] 22%|â–ˆâ–ˆâ–       | 1164/5198 [6:37:30<19:41:04, 17.57s/it]                                                        {'loss': 0.8742, 'learning_rate': 1.8091634652265755e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1164/5198 [6:37:30<19:41:04, 17.57s/it] 22%|â–ˆâ–ˆâ–       | 1165/5198 [6:37:47<19:37:14, 17.51s/it]                                                        {'loss': 0.8814, 'learning_rate': 1.8087971938738715e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1165/5198 [6:37:47<19:37:14, 17.51s/it] 22%|â–ˆâ–ˆâ–       | 1166/5198 [6:38:05<19:41:11, 17.58s/it]                                                        {'loss': 0.8926, 'learning_rate': 1.808430608518249e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1166/5198 [6:38:05<19:41:11, 17.58s/it] 22%|â–ˆâ–ˆâ–       | 1167/5198 [6:38:23<19:52:19, 17.75s/it]                                                        {'loss': 0.8335, 'learning_rate': 1.808063709302029e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1167/5198 [6:38:23<19:52:19, 17.75s/it] 22%|â–ˆâ–ˆâ–       | 1168/5198 [6:38:41<19:58:55, 17.85s/it]                                                        {'loss': 0.8896, 'learning_rate': 1.807696496367655e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1168/5198 [6:38:41<19:58:55, 17.85s/it] 22%|â–ˆâ–ˆâ–       | 1169/5198 [6:38:59<19:55:14, 17.80s/it]                                                        {'loss': 0.3596, 'learning_rate': 1.8073289698576913e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1169/5198 [6:38:59<19:55:14, 17.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1170/5198 [6:39:17<20:02:09, 17.91s/it]                                                        {'loss': 0.8994, 'learning_rate': 1.8069611299148236e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1170/5198 [6:39:17<20:02:09, 17.91s/it] 23%|â–ˆâ–ˆâ–Ž       | 1171/5198 [6:39:35<19:54:49, 17.80s/it]                                                        {'loss': 0.9294, 'learning_rate': 1.8065929766818617e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1171/5198 [6:39:35<19:54:49, 17.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1172/5198 [6:39:52<19:38:19, 17.56s/it]                                                        {'loss': 0.8374, 'learning_rate': 1.806224510301734e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1172/5198 [6:39:52<19:38:19, 17.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 1173/5198 [6:40:10<19:48:18, 17.71s/it]                                                        {'loss': 0.8213, 'learning_rate': 1.8058557309174926e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1173/5198 [6:40:10<19:48:18, 17.71s/it] 23%|â–ˆâ–ˆâ–Ž       | 1174/5198 [6:40:28<19:53:44, 17.80s/it]                                                        {'loss': 0.8345, 'learning_rate': 1.8054866386723096e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1174/5198 [6:40:28<19:53:44, 17.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1175/5198 [6:40:45<19:41:47, 17.63s/it]                                                        {'loss': 0.3444, 'learning_rate': 1.80511723370948e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1175/5198 [6:40:45<19:41:47, 17.63s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 23%|â–ˆâ–ˆâ–Ž       | 1176/5198 [6:42:07<41:14:29, 36.91s/it]                                                        {'loss': 0.8121, 'learning_rate': 1.804747516172419e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1176/5198 [6:42:07<41:14:29, 36.91s/it] 23%|â–ˆâ–ˆâ–Ž       | 1177/5198 [6:42:24<34:38:59, 31.02s/it]                                                        {'loss': 0.8712, 'learning_rate': 1.8043774862046644e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1177/5198 [6:42:24<34:38:59, 31.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1178/5198 [6:42:41<29:53:19, 26.77s/it]                                                        {'loss': 0.8643, 'learning_rate': 1.804007143949874e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1178/5198 [6:42:41<29:53:19, 26.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 1179/5198 [6:42:59<26:51:02, 24.05s/it]                                                        {'loss': 0.8975, 'learning_rate': 1.8036364895518272e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1179/5198 [6:42:59<26:51:02, 24.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 1180/5198 [6:43:16<24:36:31, 22.05s/it]                                                        {'loss': 0.828, 'learning_rate': 1.8032655231544253e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1180/5198 [6:43:16<24:36:31, 22.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 1181/5198 [6:43:33<22:54:08, 20.52s/it]                                                        {'loss': 0.8961, 'learning_rate': 1.8028942449016903e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1181/5198 [6:43:33<22:54:08, 20.52s/it] 23%|â–ˆâ–ˆâ–Ž       | 1182/5198 [6:43:51<21:53:33, 19.62s/it]                                                        {'loss': 0.9121, 'learning_rate': 1.8025226549377647e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1182/5198 [6:43:51<21:53:33, 19.62s/it] 23%|â–ˆâ–ˆâ–Ž       | 1183/5198 [6:44:08<21:15:04, 19.05s/it]                                                        {'loss': 0.8612, 'learning_rate': 1.8021507534069133e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1183/5198 [6:44:08<21:15:04, 19.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 1184/5198 [6:44:26<20:43:28, 18.59s/it]                                                        {'loss': 0.3451, 'learning_rate': 1.8017785404535198e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1184/5198 [6:44:26<20:43:28, 18.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 1185/5198 [6:44:43<20:13:31, 18.14s/it]                                                        {'loss': 0.8378, 'learning_rate': 1.8014060162220916e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1185/5198 [6:44:43<20:13:31, 18.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 1186/5198 [6:45:00<19:48:44, 17.78s/it]                                                        {'loss': 0.9184, 'learning_rate': 1.801033180857254e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1186/5198 [6:45:00<19:48:44, 17.78s/it] 23%|â–ˆâ–ˆâ–Ž       | 1187/5198 [6:45:19<20:05:13, 18.03s/it]                                                        {'loss': 0.8316, 'learning_rate': 1.8006600345037558e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1187/5198 [6:45:19<20:05:13, 18.03s/it] 23%|â–ˆâ–ˆâ–Ž       | 1188/5198 [6:45:36<19:46:15, 17.75s/it]                                                        {'loss': 0.8942, 'learning_rate': 1.8002865773064644e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1188/5198 [6:45:36<19:46:15, 17.75s/it] 23%|â–ˆâ–ˆâ–Ž       | 1189/5198 [6:45:53<19:43:06, 17.71s/it]                                                        {'loss': 0.8487, 'learning_rate': 1.799912809410369e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1189/5198 [6:45:53<19:43:06, 17.71s/it] 23%|â–ˆâ–ˆâ–Ž       | 1190/5198 [6:46:12<19:56:25, 17.91s/it]                                                        {'loss': 0.8986, 'learning_rate': 1.799538730960579e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1190/5198 [6:46:12<19:56:25, 17.91s/it] 23%|â–ˆâ–ˆâ–Ž       | 1191/5198 [6:46:29<19:48:48, 17.80s/it]                                                        {'loss': 0.8734, 'learning_rate': 1.799164342102325e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1191/5198 [6:46:29<19:48:48, 17.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1192/5198 [6:46:48<20:03:14, 18.02s/it]                                                        {'loss': 0.8335, 'learning_rate': 1.7987896429809573e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1192/5198 [6:46:48<20:03:14, 18.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1193/5198 [6:47:06<20:12:39, 18.17s/it]                                                        {'loss': 0.8553, 'learning_rate': 1.798414633741947e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1193/5198 [6:47:06<20:12:39, 18.17s/it] 23%|â–ˆâ–ˆâ–Ž       | 1194/5198 [6:47:23<19:52:51, 17.88s/it]                                                        {'loss': 0.8819, 'learning_rate': 1.7980393145308857e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1194/5198 [6:47:23<19:52:51, 17.88s/it] 23%|â–ˆâ–ˆâ–Ž       | 1195/5198 [6:47:41<19:51:58, 17.87s/it]                                                        {'loss': 0.8615, 'learning_rate': 1.797663685493485e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1195/5198 [6:47:41<19:51:58, 17.87s/it] 23%|â–ˆâ–ˆâ–Ž       | 1196/5198 [6:47:59<19:42:56, 17.74s/it]                                                        {'loss': 0.8664, 'learning_rate': 1.7972877467755777e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1196/5198 [6:47:59<19:42:56, 17.74s/it] 23%|â–ˆâ–ˆâ–Ž       | 1197/5198 [6:48:17<19:49:19, 17.84s/it]                                                        {'loss': 0.8623, 'learning_rate': 1.7969114985231152e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1197/5198 [6:48:17<19:49:19, 17.84s/it] 23%|â–ˆâ–ˆâ–Ž       | 1198/5198 [6:48:36<20:05:54, 18.09s/it]                                                        {'loss': 0.8437, 'learning_rate': 1.796534940882171e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1198/5198 [6:48:36<20:05:54, 18.09s/it] 23%|â–ˆâ–ˆâ–Ž       | 1199/5198 [6:48:54<20:10:21, 18.16s/it]                                                        {'loss': 0.8769, 'learning_rate': 1.7961580739989365e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1199/5198 [6:48:54<20:10:21, 18.16s/it] 23%|â–ˆâ–ˆâ–Ž       | 1200/5198 [6:49:11<19:43:44, 17.76s/it]                                                        {'loss': 0.8759, 'learning_rate': 1.795780898019726e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1200/5198 [6:49:11<19:43:44, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 23%|â–ˆâ–ˆâ–Ž       | 1201/5198 [6:50:38<42:48:56, 38.56s/it]                                                        {'loss': 0.8092, 'learning_rate': 1.795403413090971e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1201/5198 [6:50:38<42:48:56, 38.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 1202/5198 [6:50:55<35:44:37, 32.20s/it]                                                        {'loss': 0.3517, 'learning_rate': 1.7950256193592243e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1202/5198 [6:50:55<35:44:37, 32.20s/it] 23%|â–ˆâ–ˆâ–Ž       | 1203/5198 [6:51:14<31:10:15, 28.09s/it]                                                        {'loss': 0.8932, 'learning_rate': 1.794647516971159e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1203/5198 [6:51:14<31:10:15, 28.09s/it] 23%|â–ˆâ–ˆâ–Ž       | 1204/5198 [6:51:31<27:35:44, 24.87s/it]                                                        {'loss': 0.8422, 'learning_rate': 1.7942691060735666e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1204/5198 [6:51:31<27:35:44, 24.87s/it] 23%|â–ˆâ–ˆâ–Ž       | 1205/5198 [6:51:48<25:04:17, 22.60s/it]                                                        {'loss': 0.9123, 'learning_rate': 1.79389038681336e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1205/5198 [6:51:48<25:04:17, 22.60s/it] 23%|â–ˆâ–ˆâ–Ž       | 1206/5198 [6:52:05<23:12:21, 20.93s/it]                                                        {'loss': 0.8891, 'learning_rate': 1.7935113593375707e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1206/5198 [6:52:05<23:12:21, 20.93s/it] 23%|â–ˆâ–ˆâ–Ž       | 1207/5198 [6:52:22<21:54:46, 19.77s/it]                                                        {'loss': 0.8901, 'learning_rate': 1.7931320237933503e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1207/5198 [6:52:22<21:54:46, 19.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 1208/5198 [6:52:40<21:02:47, 18.99s/it]                                                        {'loss': 0.8673, 'learning_rate': 1.79275238032797e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1208/5198 [6:52:40<21:02:47, 18.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1209/5198 [6:52:58<20:44:48, 18.72s/it]                                                        {'loss': 0.8951, 'learning_rate': 1.7923724290888205e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1209/5198 [6:52:58<20:44:48, 18.72s/it] 23%|â–ˆâ–ˆâ–Ž       | 1210/5198 [6:53:16<20:28:00, 18.48s/it]                                                        {'loss': 0.875, 'learning_rate': 1.791992170223412e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1210/5198 [6:53:16<20:28:00, 18.48s/it] 23%|â–ˆâ–ˆâ–Ž       | 1211/5198 [6:53:33<20:11:30, 18.23s/it]                                                        {'loss': 0.8545, 'learning_rate': 1.791611603879374e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1211/5198 [6:53:33<20:11:30, 18.23s/it] 23%|â–ˆâ–ˆâ–Ž       | 1212/5198 [6:53:51<20:01:22, 18.08s/it]                                                        {'loss': 0.9391, 'learning_rate': 1.791230730204455e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1212/5198 [6:53:51<20:01:22, 18.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 1213/5198 [6:54:09<19:54:03, 17.98s/it]                                                        {'loss': 0.8437, 'learning_rate': 1.7908495493465236e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1213/5198 [6:54:09<19:54:03, 17.98s/it] 23%|â–ˆâ–ˆâ–Ž       | 1214/5198 [6:54:26<19:35:41, 17.71s/it]                                                        {'loss': 0.8227, 'learning_rate': 1.7904680614535675e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1214/5198 [6:54:26<19:35:41, 17.71s/it] 23%|â–ˆâ–ˆâ–Ž       | 1215/5198 [6:54:43<19:25:43, 17.56s/it]                                                        {'loss': 0.8326, 'learning_rate': 1.7900862666736935e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1215/5198 [6:54:43<19:25:43, 17.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 1216/5198 [6:55:01<19:27:25, 17.59s/it]                                                        {'loss': 0.3429, 'learning_rate': 1.789704165155127e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1216/5198 [6:55:01<19:27:25, 17.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 1217/5198 [6:55:18<19:13:21, 17.38s/it]                                                        {'loss': 0.9449, 'learning_rate': 1.7893217570462134e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1217/5198 [6:55:18<19:13:21, 17.38s/it] 23%|â–ˆâ–ˆâ–Ž       | 1218/5198 [6:55:36<19:40:50, 17.80s/it]                                                        {'loss': 0.8148, 'learning_rate': 1.7889390424954168e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1218/5198 [6:55:36<19:40:50, 17.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 1219/5198 [6:55:54<19:36:59, 17.75s/it]                                                        {'loss': 0.8516, 'learning_rate': 1.78855602165132e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1219/5198 [6:55:54<19:36:59, 17.75s/it] 23%|â–ˆâ–ˆâ–Ž       | 1220/5198 [6:56:11<19:29:38, 17.64s/it]                                                        {'loss': 0.8763, 'learning_rate': 1.7881726946626244e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1220/5198 [6:56:11<19:29:38, 17.64s/it] 23%|â–ˆâ–ˆâ–Ž       | 1221/5198 [6:56:29<19:32:35, 17.69s/it]                                                        {'loss': 0.8439, 'learning_rate': 1.787789061678151e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1221/5198 [6:56:29<19:32:35, 17.69s/it] 24%|â–ˆâ–ˆâ–Ž       | 1222/5198 [6:56:47<19:29:11, 17.64s/it]                                                        {'loss': 0.8392, 'learning_rate': 1.78740512284684e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1222/5198 [6:56:47<19:29:11, 17.64s/it] 24%|â–ˆâ–ˆâ–Ž       | 1223/5198 [6:57:04<19:18:40, 17.49s/it]                                                        {'loss': 0.8361, 'learning_rate': 1.787020878317749e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1223/5198 [6:57:04<19:18:40, 17.49s/it] 24%|â–ˆâ–ˆâ–Ž       | 1224/5198 [6:57:21<19:16:15, 17.46s/it]                                                        {'loss': 0.8461, 'learning_rate': 1.7866363282400555e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1224/5198 [6:57:21<19:16:15, 17.46s/it] 24%|â–ˆâ–ˆâ–Ž       | 1225/5198 [6:57:39<19:17:17, 17.48s/it]                                                        {'loss': 0.3511, 'learning_rate': 1.7862514727630543e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1225/5198 [6:57:39<19:17:17, 17.48s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 24%|â–ˆâ–ˆâ–Ž       | 1226/5198 [6:59:03<41:24:57, 37.54s/it]                                                        {'loss': 0.8417, 'learning_rate': 1.7858663120361597e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1226/5198 [6:59:03<41:24:57, 37.54s/it] 24%|â–ˆâ–ˆâ–Ž       | 1227/5198 [6:59:21<34:53:19, 31.63s/it]                                                        {'loss': 0.3456, 'learning_rate': 1.785480846208905e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1227/5198 [6:59:21<34:53:19, 31.63s/it] 24%|â–ˆâ–ˆâ–Ž       | 1228/5198 [6:59:38<30:08:41, 27.34s/it]                                                        {'loss': 0.8412, 'learning_rate': 1.7850950754309405e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1228/5198 [6:59:38<30:08:41, 27.34s/it] 24%|â–ˆâ–ˆâ–Ž       | 1229/5198 [6:59:55<26:40:25, 24.19s/it]                                                        {'loss': 0.9379, 'learning_rate': 1.7847089998520365e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1229/5198 [6:59:55<26:40:25, 24.19s/it] 24%|â–ˆâ–ˆâ–Ž       | 1230/5198 [7:00:12<24:20:20, 22.08s/it]                                                        {'loss': 0.9202, 'learning_rate': 1.7843226196220803e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1230/5198 [7:00:12<24:20:20, 22.08s/it] 24%|â–ˆâ–ˆâ–Ž       | 1231/5198 [7:00:30<22:50:36, 20.73s/it]                                                        {'loss': 0.8453, 'learning_rate': 1.783935934891078e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1231/5198 [7:00:30<22:50:36, 20.73s/it] 24%|â–ˆâ–ˆâ–Ž       | 1232/5198 [7:00:48<22:00:29, 19.98s/it]                                                        {'loss': 0.8865, 'learning_rate': 1.7835489458091544e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1232/5198 [7:00:48<22:00:29, 19.98s/it] 24%|â–ˆâ–ˆâ–Ž       | 1233/5198 [7:01:05<21:10:02, 19.22s/it]                                                        {'loss': 0.8605, 'learning_rate': 1.7831616525265515e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1233/5198 [7:01:05<21:10:02, 19.22s/it] 24%|â–ˆâ–ˆâ–Ž       | 1234/5198 [7:01:23<20:42:37, 18.81s/it]                                                        {'loss': 0.8748, 'learning_rate': 1.7827740551936296e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 1234/5198 [7:01:23<20:42:37, 18.81s/it] 24%|â–ˆâ–ˆâ–       | 1235/5198 [7:01:40<20:08:05, 18.29s/it]                                                        {'loss': 0.8649, 'learning_rate': 1.7823861539608686e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1235/5198 [7:01:40<20:08:05, 18.29s/it] 24%|â–ˆâ–ˆâ–       | 1236/5198 [7:01:58<19:56:09, 18.11s/it]                                                        {'loss': 0.8024, 'learning_rate': 1.7819979489788638e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1236/5198 [7:01:58<19:56:09, 18.11s/it] 24%|â–ˆâ–ˆâ–       | 1237/5198 [7:02:16<19:48:26, 18.00s/it]                                                        {'loss': 0.8487, 'learning_rate': 1.7816094403983298e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1237/5198 [7:02:16<19:48:26, 18.00s/it] 24%|â–ˆâ–ˆâ–       | 1238/5198 [7:02:33<19:39:03, 17.86s/it]                                                        {'loss': 0.9073, 'learning_rate': 1.7812206283701002e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1238/5198 [7:02:33<19:39:03, 17.86s/it] 24%|â–ˆâ–ˆâ–       | 1239/5198 [7:02:50<19:16:55, 17.53s/it]                                                        {'loss': 0.8844, 'learning_rate': 1.7808315130451244e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1239/5198 [7:02:50<19:16:55, 17.53s/it] 24%|â–ˆâ–ˆâ–       | 1240/5198 [7:03:07<19:06:21, 17.38s/it]                                                        {'loss': 0.8851, 'learning_rate': 1.78044209457447e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1240/5198 [7:03:07<19:06:21, 17.38s/it] 24%|â–ˆâ–ˆâ–       | 1241/5198 [7:03:25<19:17:57, 17.56s/it]                                                        {'loss': 0.8126, 'learning_rate': 1.7800523731093232e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1241/5198 [7:03:25<19:17:57, 17.56s/it] 24%|â–ˆâ–ˆâ–       | 1242/5198 [7:03:43<19:23:36, 17.65s/it]                                                        {'loss': 0.3394, 'learning_rate': 1.7796623488009875e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1242/5198 [7:03:43<19:23:36, 17.65s/it] 24%|â–ˆâ–ˆâ–       | 1243/5198 [7:04:01<19:26:41, 17.70s/it]                                                        {'loss': 0.8318, 'learning_rate': 1.7792720218008826e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1243/5198 [7:04:01<19:26:41, 17.70s/it] 24%|â–ˆâ–ˆâ–       | 1244/5198 [7:04:19<19:29:05, 17.74s/it]                                                        {'loss': 0.8499, 'learning_rate': 1.7788813922605488e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1244/5198 [7:04:19<19:29:05, 17.74s/it] 24%|â–ˆâ–ˆâ–       | 1245/5198 [7:04:38<19:54:33, 18.13s/it]                                                        {'loss': 0.9008, 'learning_rate': 1.7784904603316402e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1245/5198 [7:04:38<19:54:33, 18.13s/it] 24%|â–ˆâ–ˆâ–       | 1246/5198 [7:04:56<19:52:58, 18.11s/it]                                                        {'loss': 0.8654, 'learning_rate': 1.7780992261659305e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1246/5198 [7:04:56<19:52:58, 18.11s/it] 24%|â–ˆâ–ˆâ–       | 1247/5198 [7:05:14<19:45:44, 18.01s/it]                                                        {'loss': 0.8415, 'learning_rate': 1.777707689915311e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1247/5198 [7:05:14<19:45:44, 18.01s/it] 24%|â–ˆâ–ˆâ–       | 1248/5198 [7:05:31<19:43:48, 17.98s/it]                                                        {'loss': 0.8517, 'learning_rate': 1.777315851731789e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1248/5198 [7:05:31<19:43:48, 17.98s/it] 24%|â–ˆâ–ˆâ–       | 1249/5198 [7:05:50<19:55:43, 18.17s/it]                                                        {'loss': 0.8983, 'learning_rate': 1.7769237117674893e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1249/5198 [7:05:50<19:55:43, 18.17s/it] 24%|â–ˆâ–ˆâ–       | 1250/5198 [7:06:08<19:44:44, 18.01s/it]                                                        {'loss': 0.8394, 'learning_rate': 1.7765312701746543e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1250/5198 [7:06:08<19:44:44, 18.01s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 24%|â–ˆâ–ˆâ–       | 1251/5198 [7:07:31<41:15:13, 37.63s/it]                                                        {'loss': 0.873, 'learning_rate': 1.7761385271056436e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1251/5198 [7:07:31<41:15:13, 37.63s/it] 24%|â–ˆâ–ˆâ–       | 1252/5198 [7:07:49<34:45:14, 31.71s/it]                                                        {'loss': 0.9302, 'learning_rate': 1.7757454827129338e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1252/5198 [7:07:49<34:45:14, 31.71s/it] 24%|â–ˆâ–ˆâ–       | 1253/5198 [7:08:07<30:12:36, 27.57s/it]                                                        {'loss': 0.8372, 'learning_rate': 1.7753521371491174e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1253/5198 [7:08:07<30:12:36, 27.57s/it] 24%|â–ˆâ–ˆâ–       | 1254/5198 [7:08:25<26:57:51, 24.61s/it]                                                        {'loss': 0.8561, 'learning_rate': 1.7749584905669057e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1254/5198 [7:08:25<26:57:51, 24.61s/it] 24%|â–ˆâ–ˆâ–       | 1255/5198 [7:08:41<24:17:21, 22.18s/it]                                                        {'loss': 0.3285, 'learning_rate': 1.774564543119125e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1255/5198 [7:08:41<24:17:21, 22.18s/it] 24%|â–ˆâ–ˆâ–       | 1256/5198 [7:08:59<22:57:19, 20.96s/it]                                                        {'loss': 0.8537, 'learning_rate': 1.7741702949587196e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1256/5198 [7:08:59<22:57:19, 20.96s/it] 24%|â–ˆâ–ˆâ–       | 1257/5198 [7:09:17<21:47:20, 19.90s/it]                                                        {'loss': 0.8744, 'learning_rate': 1.7737757462387507e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1257/5198 [7:09:17<21:47:20, 19.90s/it] 24%|â–ˆâ–ˆâ–       | 1258/5198 [7:09:34<21:03:40, 19.24s/it]                                                        {'loss': 0.8421, 'learning_rate': 1.7733808971123946e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1258/5198 [7:09:34<21:03:40, 19.24s/it] 24%|â–ˆâ–ˆâ–       | 1259/5198 [7:09:52<20:35:31, 18.82s/it]                                                        {'loss': 0.857, 'learning_rate': 1.7729857477329463e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1259/5198 [7:09:52<20:35:31, 18.82s/it] 24%|â–ˆâ–ˆâ–       | 1260/5198 [7:10:10<20:20:38, 18.60s/it]                                                        {'loss': 0.8716, 'learning_rate': 1.7725902982538162e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1260/5198 [7:10:10<20:20:38, 18.60s/it] 24%|â–ˆâ–ˆâ–       | 1261/5198 [7:10:28<20:08:53, 18.42s/it]                                                        {'loss': 0.8968, 'learning_rate': 1.772194548828531e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1261/5198 [7:10:28<20:08:53, 18.42s/it] 24%|â–ˆâ–ˆâ–       | 1262/5198 [7:10:46<19:53:31, 18.19s/it]                                                        {'loss': 0.8948, 'learning_rate': 1.7717984996107346e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1262/5198 [7:10:46<19:53:31, 18.19s/it] 24%|â–ˆâ–ˆâ–       | 1263/5198 [7:11:04<19:49:07, 18.13s/it]                                                        {'loss': 0.8587, 'learning_rate': 1.771402150754187e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1263/5198 [7:11:04<19:49:07, 18.13s/it] 24%|â–ˆâ–ˆâ–       | 1264/5198 [7:11:22<19:44:01, 18.06s/it]                                                        {'loss': 0.8715, 'learning_rate': 1.7710055024127637e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1264/5198 [7:11:22<19:44:01, 18.06s/it] 24%|â–ˆâ–ˆâ–       | 1265/5198 [7:11:40<19:50:46, 18.17s/it]                                                        {'loss': 0.8671, 'learning_rate': 1.7706085547404582e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1265/5198 [7:11:40<19:50:46, 18.17s/it] 24%|â–ˆâ–ˆâ–       | 1266/5198 [7:11:58<19:33:32, 17.91s/it]                                                        {'loss': 0.8789, 'learning_rate': 1.770211307891379e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1266/5198 [7:11:58<19:33:32, 17.91s/it] 24%|â–ˆâ–ˆâ–       | 1267/5198 [7:12:14<19:11:46, 17.58s/it]                                                        {'loss': 0.9349, 'learning_rate': 1.769813762019751e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1267/5198 [7:12:14<19:11:46, 17.58s/it] 24%|â–ˆâ–ˆâ–       | 1268/5198 [7:12:32<19:16:48, 17.66s/it]                                                        {'loss': 0.8724, 'learning_rate': 1.769415917279915e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1268/5198 [7:12:32<19:16:48, 17.66s/it] 24%|â–ˆâ–ˆâ–       | 1269/5198 [7:12:49<18:59:15, 17.40s/it]                                                        {'loss': 0.7998, 'learning_rate': 1.7690177738263284e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1269/5198 [7:12:49<18:59:15, 17.40s/it] 24%|â–ˆâ–ˆâ–       | 1270/5198 [7:13:07<19:02:09, 17.45s/it]                                                        {'loss': 0.9086, 'learning_rate': 1.7686193318135635e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1270/5198 [7:13:07<19:02:09, 17.45s/it] 24%|â–ˆâ–ˆâ–       | 1271/5198 [7:13:24<18:57:11, 17.37s/it]                                                        {'loss': 0.3532, 'learning_rate': 1.76822059139631e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1271/5198 [7:13:24<18:57:11, 17.37s/it] 24%|â–ˆâ–ˆâ–       | 1272/5198 [7:13:42<19:11:35, 17.60s/it]                                                        {'loss': 0.878, 'learning_rate': 1.7678215527293724e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1272/5198 [7:13:42<19:11:35, 17.60s/it] 24%|â–ˆâ–ˆâ–       | 1273/5198 [7:14:01<19:31:53, 17.91s/it]                                                        {'loss': 0.8427, 'learning_rate': 1.767422215967671e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 1273/5198 [7:14:01<19:31:53, 17.91s/it] 25%|â–ˆâ–ˆâ–       | 1274/5198 [7:14:19<19:34:41, 17.96s/it]                                                        {'loss': 0.8433, 'learning_rate': 1.767022581266242e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1274/5198 [7:14:19<19:34:41, 17.96s/it] 25%|â–ˆâ–ˆâ–       | 1275/5198 [7:14:36<19:27:58, 17.86s/it]                                                        {'loss': 0.8827, 'learning_rate': 1.766622648780238e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1275/5198 [7:14:36<19:27:58, 17.86s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 25%|â–ˆâ–ˆâ–       | 1276/5198 [7:15:56<39:49:06, 36.55s/it]                                                        {'loss': 0.8402, 'learning_rate': 1.766222418664926e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1276/5198 [7:15:56<39:49:06, 36.55s/it] 25%|â–ˆâ–ˆâ–       | 1277/5198 [7:16:14<33:42:28, 30.95s/it]                                                        {'loss': 0.8207, 'learning_rate': 1.765821891075689e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1277/5198 [7:16:14<33:42:28, 30.95s/it] 25%|â–ˆâ–ˆâ–       | 1278/5198 [7:16:32<29:32:02, 27.12s/it]                                                        {'loss': 0.8727, 'learning_rate': 1.7654210661680263e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1278/5198 [7:16:32<29:32:02, 27.12s/it] 25%|â–ˆâ–ˆâ–       | 1279/5198 [7:16:51<26:41:30, 24.52s/it]                                                        {'loss': 0.8568, 'learning_rate': 1.765019944097551e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1279/5198 [7:16:51<26:41:30, 24.52s/it] 25%|â–ˆâ–ˆâ–       | 1280/5198 [7:17:09<24:39:39, 22.66s/it]                                                        {'loss': 0.836, 'learning_rate': 1.7646185250199936e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1280/5198 [7:17:09<24:39:39, 22.66s/it] 25%|â–ˆâ–ˆâ–       | 1281/5198 [7:17:27<22:54:47, 21.06s/it]                                                        {'loss': 0.821, 'learning_rate': 1.7642168090911976e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1281/5198 [7:17:27<22:54:47, 21.06s/it] 25%|â–ˆâ–ˆâ–       | 1282/5198 [7:17:45<21:56:42, 20.17s/it]                                                        {'loss': 0.8806, 'learning_rate': 1.763814796467124e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1282/5198 [7:17:45<21:56:42, 20.17s/it] 25%|â–ˆâ–ˆâ–       | 1283/5198 [7:18:02<20:57:26, 19.27s/it]                                                        {'loss': 0.8652, 'learning_rate': 1.763412487303847e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1283/5198 [7:18:02<20:57:26, 19.27s/it] 25%|â–ˆâ–ˆâ–       | 1284/5198 [7:18:19<20:10:41, 18.56s/it]                                                        {'loss': 0.8513, 'learning_rate': 1.7630098817575578e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1284/5198 [7:18:19<20:10:41, 18.56s/it] 25%|â–ˆâ–ˆâ–       | 1285/5198 [7:18:36<19:53:41, 18.30s/it]                                                        {'loss': 0.8934, 'learning_rate': 1.762606979984561e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1285/5198 [7:18:36<19:53:41, 18.30s/it] 25%|â–ˆâ–ˆâ–       | 1286/5198 [7:18:54<19:44:06, 18.16s/it]                                                        {'loss': 0.8735, 'learning_rate': 1.7622037821412775e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1286/5198 [7:18:54<19:44:06, 18.16s/it] 25%|â–ˆâ–ˆâ–       | 1287/5198 [7:19:12<19:28:18, 17.92s/it]                                                        {'loss': 0.8817, 'learning_rate': 1.7618002883842426e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1287/5198 [7:19:12<19:28:18, 17.92s/it] 25%|â–ˆâ–ˆâ–       | 1288/5198 [7:19:30<19:34:56, 18.03s/it]                                                        {'loss': 0.8925, 'learning_rate': 1.7613964988701057e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1288/5198 [7:19:30<19:34:56, 18.03s/it] 25%|â–ˆâ–ˆâ–       | 1289/5198 [7:19:47<19:15:45, 17.74s/it]                                                        {'loss': 0.3544, 'learning_rate': 1.7609924137556326e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1289/5198 [7:19:47<19:15:45, 17.74s/it] 25%|â–ˆâ–ˆâ–       | 1290/5198 [7:20:06<19:35:14, 18.04s/it]                                                        {'loss': 0.8503, 'learning_rate': 1.7605880331977022e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1290/5198 [7:20:06<19:35:14, 18.04s/it] 25%|â–ˆâ–ˆâ–       | 1291/5198 [7:20:24<19:39:51, 18.12s/it]                                                        {'loss': 0.867, 'learning_rate': 1.76018335735331e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1291/5198 [7:20:24<19:39:51, 18.12s/it] 25%|â–ˆâ–ˆâ–       | 1292/5198 [7:20:42<19:26:59, 17.93s/it]                                                        {'loss': 0.8867, 'learning_rate': 1.7597783863795644e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1292/5198 [7:20:42<19:26:59, 17.93s/it] 25%|â–ˆâ–ˆâ–       | 1293/5198 [7:20:59<19:20:56, 17.84s/it]                                                        {'loss': 0.9264, 'learning_rate': 1.7593731204336895e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1293/5198 [7:20:59<19:20:56, 17.84s/it] 25%|â–ˆâ–ˆâ–       | 1294/5198 [7:21:17<19:11:46, 17.70s/it]                                                        {'loss': 0.3581, 'learning_rate': 1.7589675596730233e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1294/5198 [7:21:17<19:11:46, 17.70s/it] 25%|â–ˆâ–ˆâ–       | 1295/5198 [7:21:34<18:58:30, 17.50s/it]                                                        {'loss': 0.8635, 'learning_rate': 1.758561704255018e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1295/5198 [7:21:34<18:58:30, 17.50s/it] 25%|â–ˆâ–ˆâ–       | 1296/5198 [7:21:51<18:55:54, 17.47s/it]                                                        {'loss': 0.9136, 'learning_rate': 1.7581555543372413e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1296/5198 [7:21:51<18:55:54, 17.47s/it] 25%|â–ˆâ–ˆâ–       | 1297/5198 [7:22:08<18:55:21, 17.46s/it]                                                        {'loss': 0.9208, 'learning_rate': 1.7577491100773744e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1297/5198 [7:22:08<18:55:21, 17.46s/it] 25%|â–ˆâ–ˆâ–       | 1298/5198 [7:22:25<18:45:12, 17.31s/it]                                                        {'loss': 0.3511, 'learning_rate': 1.7573423716332128e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1298/5198 [7:22:25<18:45:12, 17.31s/it] 25%|â–ˆâ–ˆâ–       | 1299/5198 [7:22:43<18:53:30, 17.44s/it]                                                        {'loss': 0.8913, 'learning_rate': 1.7569353391626665e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 1299/5198 [7:22:43<18:53:30, 17.44s/it] 25%|â–ˆâ–ˆâ–Œ       | 1300/5198 [7:23:01<19:06:17, 17.64s/it]                                                        {'loss': 0.8292, 'learning_rate': 1.7565280128237595e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1300/5198 [7:23:01<19:06:17, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 25%|â–ˆâ–ˆâ–Œ       | 1301/5198 [7:24:22<39:43:05, 36.69s/it]                                                        {'loss': 0.3617, 'learning_rate': 1.75612039277463e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1301/5198 [7:24:22<39:43:05, 36.69s/it] 25%|â–ˆâ–ˆâ–Œ       | 1302/5198 [7:24:40<33:22:47, 30.84s/it]                                                        {'loss': 0.8758, 'learning_rate': 1.75571247917353e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1302/5198 [7:24:40<33:22:47, 30.84s/it] 25%|â–ˆâ–ˆâ–Œ       | 1303/5198 [7:24:57<29:04:03, 26.87s/it]                                                        {'loss': 0.8302, 'learning_rate': 1.7553042721788255e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1303/5198 [7:24:57<29:04:03, 26.87s/it] 25%|â–ˆâ–ˆâ–Œ       | 1304/5198 [7:25:15<25:59:28, 24.03s/it]                                                        {'loss': 0.9, 'learning_rate': 1.754895771948997e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1304/5198 [7:25:15<25:59:28, 24.03s/it] 25%|â–ˆâ–ˆâ–Œ       | 1305/5198 [7:25:33<24:05:38, 22.28s/it]                                                        {'loss': 0.8431, 'learning_rate': 1.754486978642637e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1305/5198 [7:25:33<24:05:38, 22.28s/it] 25%|â–ˆâ–ˆâ–Œ       | 1306/5198 [7:25:51<22:42:53, 21.01s/it]                                                        {'loss': 0.8839, 'learning_rate': 1.7540778924184553e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1306/5198 [7:25:51<22:42:53, 21.01s/it] 25%|â–ˆâ–ˆâ–Œ       | 1307/5198 [7:26:09<21:46:48, 20.15s/it]                                                        {'loss': 0.8588, 'learning_rate': 1.7536685134352717e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1307/5198 [7:26:09<21:46:48, 20.15s/it] 25%|â–ˆâ–ˆâ–Œ       | 1308/5198 [7:26:26<20:50:07, 19.28s/it]                                                        {'loss': 0.351, 'learning_rate': 1.7532588418520215e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1308/5198 [7:26:26<20:50:07, 19.28s/it] 25%|â–ˆâ–ˆâ–Œ       | 1309/5198 [7:26:45<20:33:01, 19.02s/it]                                                        {'loss': 0.9345, 'learning_rate': 1.7528488778277535e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1309/5198 [7:26:45<20:33:01, 19.02s/it] 25%|â–ˆâ–ˆâ–Œ       | 1310/5198 [7:27:02<19:57:52, 18.49s/it]                                                        {'loss': 0.8465, 'learning_rate': 1.75243862152163e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1310/5198 [7:27:02<19:57:52, 18.49s/it] 25%|â–ˆâ–ˆâ–Œ       | 1311/5198 [7:27:19<19:36:28, 18.16s/it]                                                        {'loss': 0.8821, 'learning_rate': 1.752028073092926e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1311/5198 [7:27:19<19:36:28, 18.16s/it] 25%|â–ˆâ–ˆâ–Œ       | 1312/5198 [7:27:37<19:36:58, 18.17s/it]                                                        {'loss': 0.8564, 'learning_rate': 1.7516172327010314e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1312/5198 [7:27:37<19:36:58, 18.17s/it] 25%|â–ˆâ–ˆâ–Œ       | 1313/5198 [7:27:55<19:29:11, 18.06s/it]                                                        {'loss': 0.8309, 'learning_rate': 1.751206100505448e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1313/5198 [7:27:55<19:29:11, 18.06s/it] 25%|â–ˆâ–ˆâ–Œ       | 1314/5198 [7:28:14<19:32:48, 18.12s/it]                                                        {'loss': 0.8624, 'learning_rate': 1.7507946766657914e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1314/5198 [7:28:14<19:32:48, 18.12s/it] 25%|â–ˆâ–ˆâ–Œ       | 1315/5198 [7:28:32<19:36:21, 18.18s/it]                                                        {'loss': 0.8674, 'learning_rate': 1.7503829613417905e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1315/5198 [7:28:32<19:36:21, 18.18s/it] 25%|â–ˆâ–ˆâ–Œ       | 1316/5198 [7:28:50<19:38:29, 18.21s/it]                                                        {'loss': 0.8574, 'learning_rate': 1.749970954693288e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1316/5198 [7:28:50<19:38:29, 18.21s/it] 25%|â–ˆâ–ˆâ–Œ       | 1317/5198 [7:29:08<19:24:09, 18.00s/it]                                                        {'loss': 0.9012, 'learning_rate': 1.7495586568802384e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1317/5198 [7:29:08<19:24:09, 18.00s/it] 25%|â–ˆâ–ˆâ–Œ       | 1318/5198 [7:29:25<19:10:32, 17.79s/it]                                                        {'loss': 0.8636, 'learning_rate': 1.7491460680627105e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1318/5198 [7:29:25<19:10:32, 17.79s/it] 25%|â–ˆâ–ˆâ–Œ       | 1319/5198 [7:29:42<19:01:38, 17.66s/it]                                                        {'loss': 0.9096, 'learning_rate': 1.7487331884008845e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1319/5198 [7:29:42<19:01:38, 17.66s/it] 25%|â–ˆâ–ˆâ–Œ       | 1320/5198 [7:30:00<19:12:19, 17.83s/it]                                                        {'loss': 0.8745, 'learning_rate': 1.7483200180550554e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1320/5198 [7:30:01<19:12:19, 17.83s/it] 25%|â–ˆâ–ˆâ–Œ       | 1321/5198 [7:30:17<18:55:01, 17.57s/it]                                                        {'loss': 0.8499, 'learning_rate': 1.74790655718563e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1321/5198 [7:30:17<18:55:01, 17.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 1322/5198 [7:30:35<18:59:38, 17.64s/it]                                                        {'loss': 0.836, 'learning_rate': 1.747492805953128e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1322/5198 [7:30:35<18:59:38, 17.64s/it] 25%|â–ˆâ–ˆâ–Œ       | 1323/5198 [7:30:53<18:56:27, 17.60s/it]                                                        {'loss': 0.8783, 'learning_rate': 1.7470787645181818e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1323/5198 [7:30:53<18:56:27, 17.60s/it] 25%|â–ˆâ–ˆâ–Œ       | 1324/5198 [7:31:09<18:34:57, 17.27s/it]                                                        {'loss': 0.3415, 'learning_rate': 1.7466644330415362e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1324/5198 [7:31:09<18:34:57, 17.27s/it] 25%|â–ˆâ–ˆâ–Œ       | 1325/5198 [7:31:28<19:08:13, 17.79s/it]                                                        {'loss': 0.8191, 'learning_rate': 1.7462498116840496e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 1325/5198 [7:31:28<19:08:13, 17.79s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 26%|â–ˆâ–ˆâ–Œ       | 1326/5198 [7:32:49<39:34:30, 36.80s/it]                                                        {'loss': 0.8279, 'learning_rate': 1.745834900606692e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1326/5198 [7:32:49<39:34:30, 36.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 1327/5198 [7:33:06<33:07:13, 30.80s/it]                                                        {'loss': 0.8328, 'learning_rate': 1.7454196999705458e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1327/5198 [7:33:06<33:07:13, 30.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 1328/5198 [7:33:23<28:40:54, 26.68s/it]                                                        {'loss': 0.9121, 'learning_rate': 1.7450042099368066e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1328/5198 [7:33:23<28:40:54, 26.68s/it] 26%|â–ˆâ–ˆâ–Œ       | 1329/5198 [7:33:42<26:00:19, 24.20s/it]                                                        {'loss': 0.8511, 'learning_rate': 1.7445884306667823e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1329/5198 [7:33:42<26:00:19, 24.20s/it] 26%|â–ˆâ–ˆâ–Œ       | 1330/5198 [7:33:58<23:35:17, 21.95s/it]                                                        {'loss': 0.831, 'learning_rate': 1.7441723623218917e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1330/5198 [7:33:58<23:35:17, 21.95s/it] 26%|â–ˆâ–ˆâ–Œ       | 1331/5198 [7:34:15<21:50:20, 20.33s/it]                                                        {'loss': 0.938, 'learning_rate': 1.7437560050636678e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1331/5198 [7:34:15<21:50:20, 20.33s/it] 26%|â–ˆâ–ˆâ–Œ       | 1332/5198 [7:34:33<20:58:40, 19.53s/it]                                                        {'loss': 0.8318, 'learning_rate': 1.7433393590537543e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1332/5198 [7:34:33<20:58:40, 19.53s/it] 26%|â–ˆâ–ˆâ–Œ       | 1333/5198 [7:34:50<20:13:47, 18.84s/it]                                                        {'loss': 0.8543, 'learning_rate': 1.7429224244539077e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1333/5198 [7:34:50<20:13:47, 18.84s/it] 26%|â–ˆâ–ˆâ–Œ       | 1334/5198 [7:35:08<19:50:46, 18.49s/it]                                                        {'loss': 0.8909, 'learning_rate': 1.7425052014259965e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1334/5198 [7:35:08<19:50:46, 18.49s/it] 26%|â–ˆâ–ˆâ–Œ       | 1335/5198 [7:35:25<19:25:15, 18.10s/it]                                                        {'loss': 0.8564, 'learning_rate': 1.7420876901320006e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1335/5198 [7:35:25<19:25:15, 18.10s/it] 26%|â–ˆâ–ˆâ–Œ       | 1336/5198 [7:35:42<19:03:08, 17.76s/it]                                                        {'loss': 0.8875, 'learning_rate': 1.7416698907340128e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1336/5198 [7:35:42<19:03:08, 17.76s/it] 26%|â–ˆâ–ˆâ–Œ       | 1337/5198 [7:36:00<19:09:02, 17.86s/it]                                                        {'loss': 0.9106, 'learning_rate': 1.741251803394237e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1337/5198 [7:36:00<19:09:02, 17.86s/it] 26%|â–ˆâ–ˆâ–Œ       | 1338/5198 [7:36:18<19:07:27, 17.84s/it]                                                        {'loss': 0.8686, 'learning_rate': 1.740833428274989e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1338/5198 [7:36:18<19:07:27, 17.84s/it] 26%|â–ˆâ–ˆâ–Œ       | 1339/5198 [7:36:35<18:50:25, 17.58s/it]                                                        {'loss': 0.8599, 'learning_rate': 1.7404147655386966e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1339/5198 [7:36:35<18:50:25, 17.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 1340/5198 [7:36:52<18:57:39, 17.69s/it]                                                        {'loss': 0.843, 'learning_rate': 1.739995815347899e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1340/5198 [7:36:52<18:57:39, 17.69s/it] 26%|â–ˆâ–ˆâ–Œ       | 1341/5198 [7:37:11<19:06:11, 17.83s/it]                                                        {'loss': 0.9001, 'learning_rate': 1.739576577865247e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1341/5198 [7:37:11<19:06:11, 17.83s/it] 26%|â–ˆâ–ˆâ–Œ       | 1342/5198 [7:37:28<18:49:38, 17.58s/it]                                                        {'loss': 0.8564, 'learning_rate': 1.739157053253503e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1342/5198 [7:37:28<18:49:38, 17.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 1343/5198 [7:37:46<19:02:40, 17.78s/it]                                                        {'loss': 0.8529, 'learning_rate': 1.738737241675541e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1343/5198 [7:37:46<19:02:40, 17.78s/it] 26%|â–ˆâ–ˆâ–Œ       | 1344/5198 [7:38:03<18:50:56, 17.61s/it]                                                        {'loss': 0.8645, 'learning_rate': 1.7383171432943466e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1344/5198 [7:38:03<18:50:56, 17.61s/it] 26%|â–ˆâ–ˆâ–Œ       | 1345/5198 [7:38:20<18:39:27, 17.43s/it]                                                        {'loss': 0.8377, 'learning_rate': 1.737896758273016e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1345/5198 [7:38:20<18:39:27, 17.43s/it] 26%|â–ˆâ–ˆâ–Œ       | 1346/5198 [7:38:38<18:46:31, 17.55s/it]                                                        {'loss': 0.904, 'learning_rate': 1.7374760867747574e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1346/5198 [7:38:38<18:46:31, 17.55s/it] 26%|â–ˆâ–ˆâ–Œ       | 1347/5198 [7:38:55<18:39:31, 17.44s/it]                                                        {'loss': 0.8702, 'learning_rate': 1.7370551289628895e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1347/5198 [7:38:55<18:39:31, 17.44s/it] 26%|â–ˆâ–ˆâ–Œ       | 1348/5198 [7:39:13<18:55:26, 17.70s/it]                                                        {'loss': 0.8538, 'learning_rate': 1.7366338850008432e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1348/5198 [7:39:13<18:55:26, 17.70s/it] 26%|â–ˆâ–ˆâ–Œ       | 1349/5198 [7:39:31<18:58:35, 17.75s/it]                                                        {'loss': 0.8254, 'learning_rate': 1.73621235505216e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1349/5198 [7:39:31<18:58:35, 17.75s/it] 26%|â–ˆâ–ˆâ–Œ       | 1350/5198 [7:39:48<18:43:29, 17.52s/it]                                                        {'loss': 0.888, 'learning_rate': 1.7357905392804918e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1350/5198 [7:39:48<18:43:29, 17.52s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 26%|â–ˆâ–ˆâ–Œ       | 1351/5198 [7:41:09<39:08:23, 36.63s/it]                                                        {'loss': 0.8539, 'learning_rate': 1.735368437849602e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1351/5198 [7:41:09<39:08:23, 36.63s/it] 26%|â–ˆâ–ˆâ–Œ       | 1352/5198 [7:41:27<33:04:25, 30.96s/it]                                                        {'loss': 0.3327, 'learning_rate': 1.7349460509233654e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1352/5198 [7:41:27<33:04:25, 30.96s/it] 26%|â–ˆâ–ˆâ–Œ       | 1353/5198 [7:41:45<28:56:44, 27.10s/it]                                                        {'loss': 0.8371, 'learning_rate': 1.734523378665767e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1353/5198 [7:41:45<28:56:44, 27.10s/it] 26%|â–ˆâ–ˆâ–Œ       | 1354/5198 [7:42:02<25:37:27, 24.00s/it]                                                        {'loss': 0.8738, 'learning_rate': 1.7341004212409026e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1354/5198 [7:42:02<25:37:27, 24.00s/it] 26%|â–ˆâ–ˆâ–Œ       | 1355/5198 [7:42:19<23:26:55, 21.97s/it]                                                        {'loss': 0.9241, 'learning_rate': 1.7336771788129785e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1355/5198 [7:42:19<23:26:55, 21.97s/it] 26%|â–ˆâ–ˆâ–Œ       | 1356/5198 [7:42:37<21:57:37, 20.58s/it]                                                        {'loss': 0.9407, 'learning_rate': 1.7332536515463126e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1356/5198 [7:42:37<21:57:37, 20.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 1357/5198 [7:42:55<21:08:15, 19.81s/it]                                                        {'loss': 0.921, 'learning_rate': 1.7328298396053324e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1357/5198 [7:42:55<21:08:15, 19.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 1358/5198 [7:43:12<20:19:38, 19.06s/it]                                                        {'loss': 0.8699, 'learning_rate': 1.7324057431545768e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1358/5198 [7:43:12<20:19:38, 19.06s/it] 26%|â–ˆâ–ˆâ–Œ       | 1359/5198 [7:43:29<19:40:19, 18.45s/it]                                                        {'loss': 0.892, 'learning_rate': 1.7319813623586935e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1359/5198 [7:43:29<19:40:19, 18.45s/it] 26%|â–ˆâ–ˆâ–Œ       | 1360/5198 [7:43:47<19:40:43, 18.46s/it]                                                        {'loss': 0.8869, 'learning_rate': 1.7315566973824433e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1360/5198 [7:43:47<19:40:43, 18.46s/it] 26%|â–ˆâ–ˆâ–Œ       | 1361/5198 [7:44:05<19:18:26, 18.11s/it]                                                        {'loss': 0.8293, 'learning_rate': 1.7311317483906946e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1361/5198 [7:44:05<19:18:26, 18.11s/it] 26%|â–ˆâ–ˆâ–Œ       | 1362/5198 [7:44:23<19:19:45, 18.14s/it]                                                        {'loss': 0.8793, 'learning_rate': 1.730706515548427e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1362/5198 [7:44:23<19:19:45, 18.14s/it] 26%|â–ˆâ–ˆâ–Œ       | 1363/5198 [7:44:41<19:09:40, 17.99s/it]                                                        {'loss': 0.8692, 'learning_rate': 1.730280999020732e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1363/5198 [7:44:41<19:09:40, 17.99s/it] 26%|â–ˆâ–ˆâ–Œ       | 1364/5198 [7:44:58<19:03:41, 17.90s/it]                                                        {'loss': 0.837, 'learning_rate': 1.729855198972808e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 1364/5198 [7:44:58<19:03:41, 17.90s/it] 26%|â–ˆâ–ˆâ–‹       | 1365/5198 [7:45:17<19:10:55, 18.02s/it]                                                        {'loss': 0.8725, 'learning_rate': 1.729429115569967e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1365/5198 [7:45:17<19:10:55, 18.02s/it] 26%|â–ˆâ–ˆâ–‹       | 1366/5198 [7:45:35<19:17:57, 18.13s/it]                                                        {'loss': 0.9015, 'learning_rate': 1.729002748977628e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1366/5198 [7:45:35<19:17:57, 18.13s/it] 26%|â–ˆâ–ˆâ–‹       | 1367/5198 [7:45:53<19:10:53, 18.02s/it]                                                        {'loss': 0.8214, 'learning_rate': 1.7285760993613215e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1367/5198 [7:45:53<19:10:53, 18.02s/it] 26%|â–ˆâ–ˆâ–‹       | 1368/5198 [7:46:11<19:08:42, 18.00s/it]                                                        {'loss': 0.9051, 'learning_rate': 1.7281491668866874e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1368/5198 [7:46:11<19:08:42, 18.00s/it] 26%|â–ˆâ–ˆâ–‹       | 1369/5198 [7:46:29<19:08:16, 17.99s/it]                                                        {'loss': 0.858, 'learning_rate': 1.727721951719476e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1369/5198 [7:46:29<19:08:16, 17.99s/it] 26%|â–ˆâ–ˆâ–‹       | 1370/5198 [7:46:47<19:09:49, 18.02s/it]                                                        {'loss': 0.865, 'learning_rate': 1.7272944540255468e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1370/5198 [7:46:47<19:09:49, 18.02s/it] 26%|â–ˆâ–ˆâ–‹       | 1371/5198 [7:47:03<18:38:00, 17.53s/it]                                                        {'loss': 0.8938, 'learning_rate': 1.726866673970869e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1371/5198 [7:47:03<18:38:00, 17.53s/it] 26%|â–ˆâ–ˆâ–‹       | 1372/5198 [7:47:20<18:30:14, 17.41s/it]                                                        {'loss': 0.8874, 'learning_rate': 1.7264386117215216e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1372/5198 [7:47:20<18:30:14, 17.41s/it] 26%|â–ˆâ–ˆâ–‹       | 1373/5198 [7:47:37<18:15:07, 17.18s/it]                                                        {'loss': 0.8145, 'learning_rate': 1.7260102674436933e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1373/5198 [7:47:37<18:15:07, 17.18s/it] 26%|â–ˆâ–ˆâ–‹       | 1374/5198 [7:47:54<18:07:04, 17.06s/it]                                                        {'loss': 0.3639, 'learning_rate': 1.7255816413036818e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1374/5198 [7:47:54<18:07:04, 17.06s/it] 26%|â–ˆâ–ˆâ–‹       | 1375/5198 [7:48:12<18:30:42, 17.43s/it]                                                        {'loss': 0.9168, 'learning_rate': 1.7251527334678946e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1375/5198 [7:48:12<18:30:42, 17.43s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 26%|â–ˆâ–ˆâ–‹       | 1376/5198 [7:49:33<38:51:48, 36.61s/it]                                                        {'loss': 0.354, 'learning_rate': 1.7247235441028486e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1376/5198 [7:49:33<38:51:48, 36.61s/it] 26%|â–ˆâ–ˆâ–‹       | 1377/5198 [7:49:52<32:59:03, 31.08s/it]                                                        {'loss': 0.8625, 'learning_rate': 1.7242940733751696e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 1377/5198 [7:49:52<32:59:03, 31.08s/it] 27%|â–ˆâ–ˆâ–‹       | 1378/5198 [7:50:11<29:10:03, 27.49s/it]                                                        {'loss': 0.8642, 'learning_rate': 1.7238643214515934e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1378/5198 [7:50:11<29:10:03, 27.49s/it] 27%|â–ˆâ–ˆâ–‹       | 1379/5198 [7:50:27<25:43:04, 24.24s/it]                                                        {'loss': 0.9054, 'learning_rate': 1.7234342884989642e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1379/5198 [7:50:27<25:43:04, 24.24s/it] 27%|â–ˆâ–ˆâ–‹       | 1380/5198 [7:50:44<23:24:55, 22.08s/it]                                                        {'loss': 0.9162, 'learning_rate': 1.7230039746842352e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1380/5198 [7:50:44<23:24:55, 22.08s/it] 27%|â–ˆâ–ˆâ–‹       | 1381/5198 [7:51:02<21:59:06, 20.74s/it]                                                        {'loss': 0.8814, 'learning_rate': 1.7225733801744698e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1381/5198 [7:51:02<21:59:06, 20.74s/it] 27%|â–ˆâ–ˆâ–‹       | 1382/5198 [7:51:21<21:21:16, 20.15s/it]                                                        {'loss': 0.8089, 'learning_rate': 1.7221425051368394e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1382/5198 [7:51:21<21:21:16, 20.15s/it] 27%|â–ˆâ–ˆâ–‹       | 1383/5198 [7:51:38<20:32:59, 19.39s/it]                                                        {'loss': 0.8447, 'learning_rate': 1.7217113497386245e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1383/5198 [7:51:38<20:32:59, 19.39s/it] 27%|â–ˆâ–ˆâ–‹       | 1384/5198 [7:51:56<19:59:01, 18.86s/it]                                                        {'loss': 0.8984, 'learning_rate': 1.721279914147214e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1384/5198 [7:51:56<19:59:01, 18.86s/it] 27%|â–ˆâ–ˆâ–‹       | 1385/5198 [7:52:14<19:34:44, 18.49s/it]                                                        {'loss': 0.8363, 'learning_rate': 1.7208481985301065e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1385/5198 [7:52:14<19:34:44, 18.49s/it] 27%|â–ˆâ–ˆâ–‹       | 1386/5198 [7:52:32<19:32:36, 18.46s/it]                                                        {'loss': 0.8612, 'learning_rate': 1.7204162030549093e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1386/5198 [7:52:32<19:32:36, 18.46s/it] 27%|â–ˆâ–ˆâ–‹       | 1387/5198 [7:52:50<19:15:19, 18.19s/it]                                                        {'loss': 0.8673, 'learning_rate': 1.7199839278893368e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1387/5198 [7:52:50<19:15:19, 18.19s/it] 27%|â–ˆâ–ˆâ–‹       | 1388/5198 [7:53:07<18:59:17, 17.94s/it]                                                        {'loss': 0.871, 'learning_rate': 1.719551373201214e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1388/5198 [7:53:07<18:59:17, 17.94s/it] 27%|â–ˆâ–ˆâ–‹       | 1389/5198 [7:53:25<18:55:44, 17.89s/it]                                                        {'loss': 0.8069, 'learning_rate': 1.7191185391584736e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1389/5198 [7:53:25<18:55:44, 17.89s/it] 27%|â–ˆâ–ˆâ–‹       | 1390/5198 [7:53:42<18:52:04, 17.84s/it]                                                        {'loss': 0.3542, 'learning_rate': 1.7186854259291558e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1390/5198 [7:53:42<18:52:04, 17.84s/it] 27%|â–ˆâ–ˆâ–‹       | 1391/5198 [7:54:01<19:02:44, 18.01s/it]                                                        {'loss': 0.8508, 'learning_rate': 1.7182520336814105e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1391/5198 [7:54:01<19:02:44, 18.01s/it] 27%|â–ˆâ–ˆâ–‹       | 1392/5198 [7:54:19<19:07:28, 18.09s/it]                                                        {'loss': 0.8234, 'learning_rate': 1.717818362583496e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1392/5198 [7:54:19<19:07:28, 18.09s/it] 27%|â–ˆâ–ˆâ–‹       | 1393/5198 [7:54:37<18:57:20, 17.93s/it]                                                        {'loss': 0.878, 'learning_rate': 1.7173844128037777e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1393/5198 [7:54:37<18:57:20, 17.93s/it] 27%|â–ˆâ–ˆâ–‹       | 1394/5198 [7:54:54<18:55:29, 17.91s/it]                                                        {'loss': 0.378, 'learning_rate': 1.71695018451073e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1394/5198 [7:54:54<18:55:29, 17.91s/it] 27%|â–ˆâ–ˆâ–‹       | 1395/5198 [7:55:11<18:36:00, 17.61s/it]                                                        {'loss': 0.8557, 'learning_rate': 1.7165156778729355e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1395/5198 [7:55:11<18:36:00, 17.61s/it] 27%|â–ˆâ–ˆâ–‹       | 1396/5198 [7:55:28<18:25:16, 17.44s/it]                                                        {'loss': 0.926, 'learning_rate': 1.7160808930590845e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1396/5198 [7:55:28<18:25:16, 17.44s/it] 27%|â–ˆâ–ˆâ–‹       | 1397/5198 [7:55:45<18:11:46, 17.23s/it]                                                        {'loss': 0.332, 'learning_rate': 1.7156458302379753e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1397/5198 [7:55:45<18:11:46, 17.23s/it] 27%|â–ˆâ–ˆâ–‹       | 1398/5198 [7:56:03<18:30:28, 17.53s/it]                                                        {'loss': 0.8716, 'learning_rate': 1.7152104895785147e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1398/5198 [7:56:03<18:30:28, 17.53s/it] 27%|â–ˆâ–ˆâ–‹       | 1399/5198 [7:56:20<18:19:40, 17.37s/it]                                                        {'loss': 0.8683, 'learning_rate': 1.7147748712497162e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1399/5198 [7:56:20<18:19:40, 17.37s/it] 27%|â–ˆâ–ˆâ–‹       | 1400/5198 [7:56:39<18:39:19, 17.68s/it]                                                        {'loss': 0.8411, 'learning_rate': 1.7143389754207026e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1400/5198 [7:56:39<18:39:19, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 27%|â–ˆâ–ˆâ–‹       | 1401/5198 [7:58:01<39:02:10, 37.01s/it]                                                        {'loss': 0.8604, 'learning_rate': 1.713902802260703e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1401/5198 [7:58:01<39:02:10, 37.01s/it] 27%|â–ˆâ–ˆâ–‹       | 1402/5198 [7:58:19<32:53:13, 31.19s/it]                                                        {'loss': 0.8547, 'learning_rate': 1.7134663519390557e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1402/5198 [7:58:19<32:53:13, 31.19s/it] 27%|â–ˆâ–ˆâ–‹       | 1403/5198 [7:58:36<28:35:17, 27.12s/it]                                                        {'loss': 0.8057, 'learning_rate': 1.7130296246252048e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1403/5198 [7:58:36<28:35:17, 27.12s/it] 27%|â–ˆâ–ˆâ–‹       | 1404/5198 [7:58:54<25:46:08, 24.45s/it]                                                        {'loss': 0.8674, 'learning_rate': 1.7125926204887034e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1404/5198 [7:58:54<25:46:08, 24.45s/it] 27%|â–ˆâ–ˆâ–‹       | 1405/5198 [7:59:12<23:43:50, 22.52s/it]                                                        {'loss': 0.8522, 'learning_rate': 1.712155339699211e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1405/5198 [7:59:12<23:43:50, 22.52s/it] 27%|â–ˆâ–ˆâ–‹       | 1406/5198 [7:59:31<22:31:46, 21.39s/it]                                                        {'loss': 0.8399, 'learning_rate': 1.7117177824264962e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1406/5198 [7:59:31<22:31:46, 21.39s/it] 27%|â–ˆâ–ˆâ–‹       | 1407/5198 [7:59:50<21:34:23, 20.49s/it]                                                        {'loss': 0.8474, 'learning_rate': 1.7112799488404327e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1407/5198 [7:59:50<21:34:23, 20.49s/it] 27%|â–ˆâ–ˆâ–‹       | 1408/5198 [8:00:07<20:38:52, 19.61s/it]                                                        {'loss': 0.8485, 'learning_rate': 1.7108418391110033e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1408/5198 [8:00:07<20:38:52, 19.61s/it] 27%|â–ˆâ–ˆâ–‹       | 1409/5198 [8:00:25<20:01:42, 19.03s/it]                                                        {'loss': 0.8654, 'learning_rate': 1.7104034534082968e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1409/5198 [8:00:25<20:01:42, 19.03s/it] 27%|â–ˆâ–ˆâ–‹       | 1410/5198 [8:00:42<19:33:25, 18.59s/it]                                                        {'loss': 0.8908, 'learning_rate': 1.7099647919025096e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1410/5198 [8:00:42<19:33:25, 18.59s/it] 27%|â–ˆâ–ˆâ–‹       | 1411/5198 [8:00:59<19:05:44, 18.15s/it]                                                        {'loss': 0.8718, 'learning_rate': 1.7095258547639456e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1411/5198 [8:00:59<19:05:44, 18.15s/it] 27%|â–ˆâ–ˆâ–‹       | 1412/5198 [8:01:17<18:59:30, 18.06s/it]                                                        {'loss': 0.819, 'learning_rate': 1.709086642163015e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1412/5198 [8:01:17<18:59:30, 18.06s/it] 27%|â–ˆâ–ˆâ–‹       | 1413/5198 [8:01:35<18:52:47, 17.96s/it]                                                        {'loss': 0.8695, 'learning_rate': 1.7086471542702355e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1413/5198 [8:01:35<18:52:47, 17.96s/it] 27%|â–ˆâ–ˆâ–‹       | 1414/5198 [8:01:52<18:42:56, 17.81s/it]                                                        {'loss': 0.358, 'learning_rate': 1.708207391256231e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1414/5198 [8:01:52<18:42:56, 17.81s/it] 27%|â–ˆâ–ˆâ–‹       | 1415/5198 [8:02:10<18:45:29, 17.85s/it]                                                        {'loss': 0.8829, 'learning_rate': 1.707767353291733e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1415/5198 [8:02:10<18:45:29, 17.85s/it] 27%|â–ˆâ–ˆâ–‹       | 1416/5198 [8:02:28<18:48:40, 17.91s/it]                                                        {'loss': 0.8616, 'learning_rate': 1.7073270405475796e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1416/5198 [8:02:28<18:48:40, 17.91s/it] 27%|â–ˆâ–ˆâ–‹       | 1417/5198 [8:02:47<18:55:01, 18.01s/it]                                                        {'loss': 0.8161, 'learning_rate': 1.7068864531947147e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1417/5198 [8:02:47<18:55:01, 18.01s/it] 27%|â–ˆâ–ˆâ–‹       | 1418/5198 [8:03:04<18:46:01, 17.87s/it]                                                        {'loss': 0.8737, 'learning_rate': 1.70644559140419e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1418/5198 [8:03:04<18:46:01, 17.87s/it] 27%|â–ˆâ–ˆâ–‹       | 1419/5198 [8:03:21<18:32:28, 17.66s/it]                                                        {'loss': 0.855, 'learning_rate': 1.706004455347163e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1419/5198 [8:03:21<18:32:28, 17.66s/it] 27%|â–ˆâ–ˆâ–‹       | 1420/5198 [8:03:39<18:39:15, 17.78s/it]                                                        {'loss': 0.8356, 'learning_rate': 1.705563045194898e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1420/5198 [8:03:39<18:39:15, 17.78s/it] 27%|â–ˆâ–ˆâ–‹       | 1421/5198 [8:03:57<18:27:30, 17.59s/it]                                                        {'loss': 0.3146, 'learning_rate': 1.7051213611187657e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1421/5198 [8:03:57<18:27:30, 17.59s/it] 27%|â–ˆâ–ˆâ–‹       | 1422/5198 [8:04:14<18:30:00, 17.64s/it]                                                        {'loss': 0.8298, 'learning_rate': 1.704679403290243e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1422/5198 [8:04:14<18:30:00, 17.64s/it] 27%|â–ˆâ–ˆâ–‹       | 1423/5198 [8:04:32<18:36:45, 17.75s/it]                                                        {'loss': 0.8461, 'learning_rate': 1.7042371718809132e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1423/5198 [8:04:32<18:36:45, 17.75s/it] 27%|â–ˆâ–ˆâ–‹       | 1424/5198 [8:04:51<18:47:15, 17.92s/it]                                                        {'loss': 0.8509, 'learning_rate': 1.7037946670624652e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1424/5198 [8:04:51<18:47:15, 17.92s/it] 27%|â–ˆâ–ˆâ–‹       | 1425/5198 [8:05:08<18:30:33, 17.66s/it]                                                        {'loss': 0.3643, 'learning_rate': 1.7033518890066956e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1425/5198 [8:05:08<18:30:33, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 27%|â–ˆâ–ˆâ–‹       | 1426/5198 [8:06:34<40:02:31, 38.22s/it]                                                        {'loss': 0.367, 'learning_rate': 1.7029088378855055e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1426/5198 [8:06:34<40:02:31, 38.22s/it] 27%|â–ˆâ–ˆâ–‹       | 1427/5198 [8:06:52<33:35:46, 32.07s/it]                                                        {'loss': 0.8681, 'learning_rate': 1.7024655138709025e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1427/5198 [8:06:52<33:35:46, 32.07s/it] 27%|â–ˆâ–ˆâ–‹       | 1428/5198 [8:07:10<29:16:29, 27.95s/it]                                                        {'loss': 0.8555, 'learning_rate': 1.7020219171350004e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1428/5198 [8:07:10<29:16:29, 27.95s/it] 27%|â–ˆâ–ˆâ–‹       | 1429/5198 [8:07:28<26:05:04, 24.91s/it]                                                        {'loss': 0.8787, 'learning_rate': 1.7015780478500187e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 1429/5198 [8:07:28<26:05:04, 24.91s/it] 28%|â–ˆâ–ˆâ–Š       | 1430/5198 [8:07:45<23:43:17, 22.66s/it]                                                        {'loss': 0.8669, 'learning_rate': 1.701133906188283e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1430/5198 [8:07:45<23:43:17, 22.66s/it] 28%|â–ˆâ–ˆâ–Š       | 1431/5198 [8:08:03<22:10:09, 21.19s/it]                                                        {'loss': 0.929, 'learning_rate': 1.700689492322224e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1431/5198 [8:08:03<22:10:09, 21.19s/it] 28%|â–ˆâ–ˆâ–Š       | 1432/5198 [8:08:22<21:23:52, 20.45s/it]                                                        {'loss': 0.8766, 'learning_rate': 1.700244806424379e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1432/5198 [8:08:22<21:23:52, 20.45s/it] 28%|â–ˆâ–ˆâ–Š       | 1433/5198 [8:08:40<20:38:27, 19.74s/it]                                                        {'loss': 0.8623, 'learning_rate': 1.6997998486673893e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1433/5198 [8:08:40<20:38:27, 19.74s/it] 28%|â–ˆâ–ˆâ–Š       | 1434/5198 [8:08:58<20:13:22, 19.34s/it]                                                        {'loss': 0.8271, 'learning_rate': 1.699354619224004e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1434/5198 [8:08:58<20:13:22, 19.34s/it] 28%|â–ˆâ–ˆâ–Š       | 1435/5198 [8:09:16<19:51:36, 19.00s/it]                                                        {'loss': 0.8165, 'learning_rate': 1.698909118267076e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1435/5198 [8:09:16<19:51:36, 19.00s/it] 28%|â–ˆâ–ˆâ–Š       | 1436/5198 [8:09:34<19:22:05, 18.53s/it]                                                        {'loss': 0.8675, 'learning_rate': 1.6984633459695646e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1436/5198 [8:09:34<19:22:05, 18.53s/it] 28%|â–ˆâ–ˆâ–Š       | 1437/5198 [8:09:51<18:47:59, 18.00s/it]                                                        {'loss': 0.9472, 'learning_rate': 1.6980173025045328e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1437/5198 [8:09:51<18:47:59, 18.00s/it] 28%|â–ˆâ–ˆâ–Š       | 1438/5198 [8:10:09<18:45:48, 17.97s/it]                                                        {'loss': 0.3629, 'learning_rate': 1.697570988045151e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1438/5198 [8:10:09<18:45:48, 17.97s/it] 28%|â–ˆâ–ˆâ–Š       | 1439/5198 [8:10:26<18:39:35, 17.87s/it]                                                        {'loss': 0.8945, 'learning_rate': 1.6971244027646937e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1439/5198 [8:10:26<18:39:35, 17.87s/it] 28%|â–ˆâ–ˆâ–Š       | 1440/5198 [8:10:43<18:16:47, 17.51s/it]                                                        {'loss': 0.3857, 'learning_rate': 1.69667754683654e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1440/5198 [8:10:43<18:16:47, 17.51s/it] 28%|â–ˆâ–ˆâ–Š       | 1441/5198 [8:11:01<18:19:59, 17.57s/it]                                                        {'loss': 0.8695, 'learning_rate': 1.6962304204341758e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1441/5198 [8:11:01<18:19:59, 17.57s/it] 28%|â–ˆâ–ˆâ–Š       | 1442/5198 [8:11:18<18:23:28, 17.63s/it]                                                        {'loss': 0.9175, 'learning_rate': 1.6957830237311904e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1442/5198 [8:11:18<18:23:28, 17.63s/it] 28%|â–ˆâ–ˆâ–Š       | 1443/5198 [8:11:36<18:33:18, 17.79s/it]                                                        {'loss': 0.8318, 'learning_rate': 1.6953353569012784e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1443/5198 [8:11:36<18:33:18, 17.79s/it] 28%|â–ˆâ–ˆâ–Š       | 1444/5198 [8:11:54<18:23:19, 17.63s/it]                                                        {'loss': 0.8748, 'learning_rate': 1.6948874201182402e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1444/5198 [8:11:54<18:23:19, 17.63s/it] 28%|â–ˆâ–ˆâ–Š       | 1445/5198 [8:12:12<18:34:41, 17.82s/it]                                                        {'loss': 0.9375, 'learning_rate': 1.6944392135559798e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1445/5198 [8:12:12<18:34:41, 17.82s/it] 28%|â–ˆâ–ˆâ–Š       | 1446/5198 [8:12:30<18:29:26, 17.74s/it]                                                        {'loss': 0.8706, 'learning_rate': 1.6939907373885062e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1446/5198 [8:12:30<18:29:26, 17.74s/it] 28%|â–ˆâ–ˆâ–Š       | 1447/5198 [8:12:46<18:09:09, 17.42s/it]                                                        {'loss': 0.9026, 'learning_rate': 1.6935419917899335e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1447/5198 [8:12:46<18:09:09, 17.42s/it] 28%|â–ˆâ–ˆâ–Š       | 1448/5198 [8:13:04<18:21:12, 17.62s/it]                                                        {'loss': 0.8939, 'learning_rate': 1.6930929769344807e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1448/5198 [8:13:04<18:21:12, 17.62s/it] 28%|â–ˆâ–ˆâ–Š       | 1449/5198 [8:13:23<18:43:13, 17.98s/it]                                                        {'loss': 0.8772, 'learning_rate': 1.69264369299647e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1449/5198 [8:13:23<18:43:13, 17.98s/it] 28%|â–ˆâ–ˆâ–Š       | 1450/5198 [8:13:41<18:42:10, 17.96s/it]                                                        {'loss': 0.8989, 'learning_rate': 1.692194140150329e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1450/5198 [8:13:41<18:42:10, 17.96s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 28%|â–ˆâ–ˆâ–Š       | 1451/5198 [8:15:06<39:33:31, 38.01s/it]                                                        {'loss': 0.8876, 'learning_rate': 1.69174431857059e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1451/5198 [8:15:06<39:33:31, 38.01s/it] 28%|â–ˆâ–ˆâ–Š       | 1452/5198 [8:15:23<33:11:19, 31.90s/it]                                                        {'loss': 0.8648, 'learning_rate': 1.6912942284318898e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1452/5198 [8:15:23<33:11:19, 31.90s/it] 28%|â–ˆâ–ˆâ–Š       | 1453/5198 [8:15:41<28:46:41, 27.66s/it]                                                        {'loss': 0.8944, 'learning_rate': 1.6908438699089674e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1453/5198 [8:15:41<28:46:41, 27.66s/it] 28%|â–ˆâ–ˆâ–Š       | 1454/5198 [8:15:58<25:19:54, 24.36s/it]                                                        {'loss': 0.8113, 'learning_rate': 1.690393243176668e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1454/5198 [8:15:58<25:19:54, 24.36s/it] 28%|â–ˆâ–ˆâ–Š       | 1455/5198 [8:16:16<23:22:11, 22.48s/it]                                                        {'loss': 0.9267, 'learning_rate': 1.6899423484099413e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1455/5198 [8:16:16<23:22:11, 22.48s/it] 28%|â–ˆâ–ˆâ–Š       | 1456/5198 [8:16:34<21:58:10, 21.14s/it]                                                        {'loss': 0.8825, 'learning_rate': 1.6894911857838394e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1456/5198 [8:16:34<21:58:10, 21.14s/it] 28%|â–ˆâ–ˆâ–Š       | 1457/5198 [8:16:52<20:56:50, 20.16s/it]                                                        {'loss': 0.8384, 'learning_rate': 1.689039755473519e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1457/5198 [8:16:52<20:56:50, 20.16s/it] 28%|â–ˆâ–ˆâ–Š       | 1458/5198 [8:17:10<20:09:39, 19.41s/it]                                                        {'loss': 0.7705, 'learning_rate': 1.6885880576542417e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1458/5198 [8:17:10<20:09:39, 19.41s/it] 28%|â–ˆâ–ˆâ–Š       | 1459/5198 [8:17:26<19:21:27, 18.64s/it]                                                        {'loss': 0.8665, 'learning_rate': 1.6881360925013712e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1459/5198 [8:17:26<19:21:27, 18.64s/it] 28%|â–ˆâ–ˆâ–Š       | 1460/5198 [8:17:45<19:16:30, 18.56s/it]                                                        {'loss': 0.8274, 'learning_rate': 1.6876838601903765e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1460/5198 [8:17:45<19:16:30, 18.56s/it] 28%|â–ˆâ–ˆâ–Š       | 1461/5198 [8:18:03<19:08:43, 18.44s/it]                                                        {'loss': 0.7952, 'learning_rate': 1.6872313608968296e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1461/5198 [8:18:03<19:08:43, 18.44s/it] 28%|â–ˆâ–ˆâ–Š       | 1462/5198 [8:18:21<18:53:38, 18.21s/it]                                                        {'loss': 0.8793, 'learning_rate': 1.6867785947964065e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1462/5198 [8:18:21<18:53:38, 18.21s/it] 28%|â–ˆâ–ˆâ–Š       | 1463/5198 [8:18:38<18:41:13, 18.01s/it]                                                        {'loss': 0.8496, 'learning_rate': 1.6863255620648866e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1463/5198 [8:18:38<18:41:13, 18.01s/it] 28%|â–ˆâ–ˆâ–Š       | 1464/5198 [8:18:56<18:40:43, 18.01s/it]                                                        {'loss': 0.8513, 'learning_rate': 1.685872262878152e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1464/5198 [8:18:56<18:40:43, 18.01s/it] 28%|â–ˆâ–ˆâ–Š       | 1465/5198 [8:19:14<18:31:18, 17.86s/it]                                                        {'loss': 0.9298, 'learning_rate': 1.6854186974121903e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1465/5198 [8:19:14<18:31:18, 17.86s/it] 28%|â–ˆâ–ˆâ–Š       | 1466/5198 [8:19:32<18:31:19, 17.87s/it]                                                        {'loss': 0.8943, 'learning_rate': 1.68496486584309e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1466/5198 [8:19:32<18:31:19, 17.87s/it] 28%|â–ˆâ–ˆâ–Š       | 1467/5198 [8:19:49<18:18:51, 17.67s/it]                                                        {'loss': 0.3484, 'learning_rate': 1.6845107683470453e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1467/5198 [8:19:49<18:18:51, 17.67s/it] 28%|â–ˆâ–ˆâ–Š       | 1468/5198 [8:20:07<18:27:53, 17.82s/it]                                                        {'loss': 0.8468, 'learning_rate': 1.6840564051003517e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1468/5198 [8:20:07<18:27:53, 17.82s/it] 28%|â–ˆâ–ˆâ–Š       | 1469/5198 [8:20:24<18:12:38, 17.58s/it]                                                        {'loss': 0.3591, 'learning_rate': 1.6836017762794087e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1469/5198 [8:20:25<18:12:38, 17.58s/it] 28%|â–ˆâ–ˆâ–Š       | 1470/5198 [8:20:42<18:28:13, 17.84s/it]                                                        {'loss': 0.8239, 'learning_rate': 1.6831468820607192e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1470/5198 [8:20:42<18:28:13, 17.84s/it] 28%|â–ˆâ–ˆâ–Š       | 1471/5198 [8:21:00<18:17:35, 17.67s/it]                                                        {'loss': 0.9376, 'learning_rate': 1.6826917226208886e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1471/5198 [8:21:00<18:17:35, 17.67s/it] 28%|â–ˆâ–ˆâ–Š       | 1472/5198 [8:21:18<18:28:57, 17.86s/it]                                                        {'loss': 0.8205, 'learning_rate': 1.6822362981366257e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1472/5198 [8:21:18<18:28:57, 17.86s/it] 28%|â–ˆâ–ˆâ–Š       | 1473/5198 [8:21:36<18:27:04, 17.83s/it]                                                        {'loss': 0.8262, 'learning_rate': 1.6817806087847417e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1473/5198 [8:21:36<18:27:04, 17.83s/it] 28%|â–ˆâ–ˆâ–Š       | 1474/5198 [8:21:54<18:32:03, 17.92s/it]                                                        {'loss': 0.8256, 'learning_rate': 1.681324654742151e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1474/5198 [8:21:54<18:32:03, 17.92s/it] 28%|â–ˆâ–ˆâ–Š       | 1475/5198 [8:22:11<18:19:57, 17.73s/it]                                                        {'loss': 0.8754, 'learning_rate': 1.6808684361858706e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1475/5198 [8:22:11<18:19:57, 17.73s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 28%|â–ˆâ–ˆâ–Š       | 1476/5198 [8:23:38<39:42:07, 38.40s/it]                                                        {'loss': 0.8555, 'learning_rate': 1.6804119532930202e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1476/5198 [8:23:38<39:42:07, 38.40s/it] 28%|â–ˆâ–ˆâ–Š       | 1477/5198 [8:23:55<33:05:10, 32.01s/it]                                                        {'loss': 0.8788, 'learning_rate': 1.6799552062408225e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1477/5198 [8:23:55<33:05:10, 32.01s/it] 28%|â–ˆâ–ˆâ–Š       | 1478/5198 [8:24:13<28:37:28, 27.70s/it]                                                        {'loss': 0.865, 'learning_rate': 1.6794981952066018e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1478/5198 [8:24:13<28:37:28, 27.70s/it] 28%|â–ˆâ–ˆâ–Š       | 1479/5198 [8:24:30<25:31:51, 24.71s/it]                                                        {'loss': 0.368, 'learning_rate': 1.6790409203677863e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1479/5198 [8:24:30<25:31:51, 24.71s/it] 28%|â–ˆâ–ˆâ–Š       | 1480/5198 [8:24:48<23:18:18, 22.57s/it]                                                        {'loss': 0.8657, 'learning_rate': 1.6785833819019052e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1480/5198 [8:24:48<23:18:18, 22.57s/it] 28%|â–ˆâ–ˆâ–Š       | 1481/5198 [8:25:06<21:50:48, 21.16s/it]                                                        {'loss': 0.8589, 'learning_rate': 1.678125579986591e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 1481/5198 [8:25:06<21:50:48, 21.16s/it] 29%|â–ˆâ–ˆâ–Š       | 1482/5198 [8:25:22<20:24:54, 19.78s/it]                                                        {'loss': 0.8531, 'learning_rate': 1.677667514799578e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1482/5198 [8:25:22<20:24:54, 19.78s/it] 29%|â–ˆâ–ˆâ–Š       | 1483/5198 [8:25:40<19:52:29, 19.26s/it]                                                        {'loss': 0.8731, 'learning_rate': 1.6772091865187032e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1483/5198 [8:25:40<19:52:29, 19.26s/it] 29%|â–ˆâ–ˆâ–Š       | 1484/5198 [8:25:57<19:10:02, 18.58s/it]                                                        {'loss': 0.8713, 'learning_rate': 1.676750595321905e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1484/5198 [8:25:57<19:10:02, 18.58s/it] 29%|â–ˆâ–ˆâ–Š       | 1485/5198 [8:26:15<18:58:18, 18.39s/it]                                                        {'loss': 0.8273, 'learning_rate': 1.6762917413872246e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1485/5198 [8:26:15<18:58:18, 18.39s/it] 29%|â–ˆâ–ˆâ–Š       | 1486/5198 [8:26:32<18:23:08, 17.83s/it]                                                        {'loss': 0.8573, 'learning_rate': 1.675832624892805e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1486/5198 [8:26:32<18:23:08, 17.83s/it] 29%|â–ˆâ–ˆâ–Š       | 1487/5198 [8:26:50<18:21:03, 17.80s/it]                                                        {'loss': 0.8205, 'learning_rate': 1.6753732460168907e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1487/5198 [8:26:50<18:21:03, 17.80s/it] 29%|â–ˆâ–ˆâ–Š       | 1488/5198 [8:27:07<18:21:57, 17.82s/it]                                                        {'loss': 0.8384, 'learning_rate': 1.674913604937828e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1488/5198 [8:27:07<18:21:57, 17.82s/it] 29%|â–ˆâ–ˆâ–Š       | 1489/5198 [8:27:25<18:11:55, 17.66s/it]                                                        {'loss': 0.8917, 'learning_rate': 1.6744537018340662e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1489/5198 [8:27:25<18:11:55, 17.66s/it] 29%|â–ˆâ–ˆâ–Š       | 1490/5198 [8:27:42<18:02:23, 17.51s/it]                                                        {'loss': 0.838, 'learning_rate': 1.6739935368841555e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1490/5198 [8:27:42<18:02:23, 17.51s/it] 29%|â–ˆâ–ˆâ–Š       | 1491/5198 [8:27:59<18:03:03, 17.53s/it]                                                        {'loss': 0.882, 'learning_rate': 1.6735331102667475e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1491/5198 [8:27:59<18:03:03, 17.53s/it] 29%|â–ˆâ–ˆâ–Š       | 1492/5198 [8:28:17<18:09:30, 17.64s/it]                                                        {'loss': 0.9039, 'learning_rate': 1.6730724221605955e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1492/5198 [8:28:17<18:09:30, 17.64s/it] 29%|â–ˆâ–ˆâ–Š       | 1493/5198 [8:28:34<17:55:57, 17.42s/it]                                                        {'loss': 0.8771, 'learning_rate': 1.6726114727445547e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1493/5198 [8:28:34<17:55:57, 17.42s/it] 29%|â–ˆâ–ˆâ–Š       | 1494/5198 [8:28:52<18:06:51, 17.61s/it]                                                        {'loss': 0.8167, 'learning_rate': 1.6721502621975813e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 1494/5198 [8:28:52<18:06:51, 17.61s/it] 29%|â–ˆâ–ˆâ–‰       | 1495/5198 [8:29:11<18:19:31, 17.82s/it]                                                        {'loss': 0.9001, 'learning_rate': 1.6716887906987332e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1495/5198 [8:29:11<18:19:31, 17.82s/it] 29%|â–ˆâ–ˆâ–‰       | 1496/5198 [8:29:28<18:14:11, 17.73s/it]                                                        {'loss': 0.8556, 'learning_rate': 1.6712270584271703e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1496/5198 [8:29:28<18:14:11, 17.73s/it] 29%|â–ˆâ–ˆâ–‰       | 1497/5198 [8:29:46<18:19:43, 17.83s/it]                                                        {'loss': 0.8537, 'learning_rate': 1.670765065562152e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1497/5198 [8:29:46<18:19:43, 17.83s/it] 29%|â–ˆâ–ˆâ–‰       | 1498/5198 [8:30:04<18:17:04, 17.79s/it]                                                        {'loss': 0.3604, 'learning_rate': 1.67030281228304e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1498/5198 [8:30:04<18:17:04, 17.79s/it] 29%|â–ˆâ–ˆâ–‰       | 1499/5198 [8:30:22<18:21:41, 17.87s/it]                                                        {'loss': 0.8219, 'learning_rate': 1.6698402987692968e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1499/5198 [8:30:22<18:21:41, 17.87s/it] 29%|â–ˆâ–ˆâ–‰       | 1500/5198 [8:30:39<18:03:36, 17.58s/it]                                                        {'loss': 0.8723, 'learning_rate': 1.6693775252004866e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1500/5198 [8:30:39<18:03:36, 17.58s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 29%|â–ˆâ–ˆâ–‰       | 1501/5198 [8:32:03<38:41:09, 37.67s/it]                                                        {'loss': 0.8513, 'learning_rate': 1.668914491756274e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1501/5198 [8:32:03<38:41:09, 37.67s/it] 29%|â–ˆâ–ˆâ–‰       | 1502/5198 [8:32:22<32:41:53, 31.85s/it]                                                        {'loss': 0.8409, 'learning_rate': 1.668451198616424e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1502/5198 [8:32:22<32:41:53, 31.85s/it] 29%|â–ˆâ–ˆâ–‰       | 1503/5198 [8:32:39<28:17:27, 27.56s/it]                                                        {'loss': 0.8508, 'learning_rate': 1.6679876459608033e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1503/5198 [8:32:39<28:17:27, 27.56s/it] 29%|â–ˆâ–ˆâ–‰       | 1504/5198 [8:32:57<25:21:29, 24.71s/it]                                                        {'loss': 0.8645, 'learning_rate': 1.667523833969379e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1504/5198 [8:32:57<25:21:29, 24.71s/it] 29%|â–ˆâ–ˆâ–‰       | 1505/5198 [8:33:14<23:01:48, 22.45s/it]                                                        {'loss': 0.8845, 'learning_rate': 1.667059762822219e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1505/5198 [8:33:14<23:01:48, 22.45s/it] 29%|â–ˆâ–ˆâ–‰       | 1506/5198 [8:33:32<21:39:53, 21.12s/it]                                                        {'loss': 0.7809, 'learning_rate': 1.666595432699491e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1506/5198 [8:33:32<21:39:53, 21.12s/it] 29%|â–ˆâ–ˆâ–‰       | 1507/5198 [8:33:49<20:23:52, 19.90s/it]                                                        {'loss': 0.8984, 'learning_rate': 1.6661308437814652e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1507/5198 [8:33:49<20:23:52, 19.90s/it] 29%|â–ˆâ–ˆâ–‰       | 1508/5198 [8:34:07<19:42:18, 19.22s/it]                                                        {'loss': 0.8451, 'learning_rate': 1.6656659962485097e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1508/5198 [8:34:07<19:42:18, 19.22s/it] 29%|â–ˆâ–ˆâ–‰       | 1509/5198 [8:34:25<19:22:37, 18.91s/it]                                                        {'loss': 0.7685, 'learning_rate': 1.6652008902810952e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1509/5198 [8:34:25<19:22:37, 18.91s/it] 29%|â–ˆâ–ˆâ–‰       | 1510/5198 [8:34:43<19:00:15, 18.55s/it]                                                        {'loss': 0.8646, 'learning_rate': 1.6647355260597915e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1510/5198 [8:34:43<19:00:15, 18.55s/it] 29%|â–ˆâ–ˆâ–‰       | 1511/5198 [8:35:00<18:32:22, 18.10s/it]                                                        {'loss': 0.8369, 'learning_rate': 1.664269903765269e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1511/5198 [8:35:00<18:32:22, 18.10s/it] 29%|â–ˆâ–ˆâ–‰       | 1512/5198 [8:35:18<18:34:58, 18.15s/it]                                                        {'loss': 0.8239, 'learning_rate': 1.6638040235782983e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1512/5198 [8:35:18<18:34:58, 18.15s/it] 29%|â–ˆâ–ˆâ–‰       | 1513/5198 [8:35:36<18:25:27, 18.00s/it]                                                        {'loss': 0.8642, 'learning_rate': 1.6633378856797505e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1513/5198 [8:35:36<18:25:27, 18.00s/it] 29%|â–ˆâ–ˆâ–‰       | 1514/5198 [8:35:53<18:10:13, 17.76s/it]                                                        {'loss': 0.8863, 'learning_rate': 1.662871490250596e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1514/5198 [8:35:53<18:10:13, 17.76s/it] 29%|â–ˆâ–ˆâ–‰       | 1515/5198 [8:36:10<17:54:19, 17.50s/it]                                                        {'loss': 0.8992, 'learning_rate': 1.662404837471905e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1515/5198 [8:36:10<17:54:19, 17.50s/it] 29%|â–ˆâ–ˆâ–‰       | 1516/5198 [8:36:28<17:59:20, 17.59s/it]                                                        {'loss': 0.8846, 'learning_rate': 1.66193792752485e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1516/5198 [8:36:28<17:59:20, 17.59s/it] 29%|â–ˆâ–ˆâ–‰       | 1517/5198 [8:36:45<17:59:37, 17.60s/it]                                                        {'loss': 0.9194, 'learning_rate': 1.6614707605906995e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1517/5198 [8:36:45<17:59:37, 17.60s/it] 29%|â–ˆâ–ˆâ–‰       | 1518/5198 [8:37:04<18:07:33, 17.73s/it]                                                        {'loss': 0.883, 'learning_rate': 1.661003336850825e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1518/5198 [8:37:04<18:07:33, 17.73s/it] 29%|â–ˆâ–ˆâ–‰       | 1519/5198 [8:37:21<18:08:45, 17.76s/it]                                                        {'loss': 0.8509, 'learning_rate': 1.660535656486696e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1519/5198 [8:37:21<18:08:45, 17.76s/it] 29%|â–ˆâ–ˆâ–‰       | 1520/5198 [8:37:38<17:54:32, 17.53s/it]                                                        {'loss': 0.9087, 'learning_rate': 1.660067719679882e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1520/5198 [8:37:38<17:54:32, 17.53s/it] 29%|â–ˆâ–ˆâ–‰       | 1521/5198 [8:37:55<17:44:00, 17.36s/it]                                                        {'loss': 0.8933, 'learning_rate': 1.6595995266120528e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1521/5198 [8:37:55<17:44:00, 17.36s/it] 29%|â–ˆâ–ˆâ–‰       | 1522/5198 [8:38:13<17:47:57, 17.43s/it]                                                        {'loss': 0.8821, 'learning_rate': 1.6591310774649766e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1522/5198 [8:38:13<17:47:57, 17.43s/it] 29%|â–ˆâ–ˆâ–‰       | 1523/5198 [8:38:30<17:41:27, 17.33s/it]                                                        {'loss': 0.8868, 'learning_rate': 1.6586623724205216e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1523/5198 [8:38:30<17:41:27, 17.33s/it] 29%|â–ˆâ–ˆâ–‰       | 1524/5198 [8:38:47<17:39:12, 17.30s/it]                                                        {'loss': 0.8471, 'learning_rate': 1.6581934116606554e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1524/5198 [8:38:47<17:39:12, 17.30s/it] 29%|â–ˆâ–ˆâ–‰       | 1525/5198 [8:39:05<17:54:27, 17.55s/it]                                                        {'loss': 0.8196, 'learning_rate': 1.657724195367444e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1525/5198 [8:39:05<17:54:27, 17.55s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 29%|â–ˆâ–ˆâ–‰       | 1526/5198 [8:40:28<37:52:46, 37.14s/it]                                                        {'loss': 0.8579, 'learning_rate': 1.657254723723054e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1526/5198 [8:40:28<37:52:46, 37.14s/it] 29%|â–ˆâ–ˆâ–‰       | 1527/5198 [8:40:45<31:39:10, 31.04s/it]                                                        {'loss': 0.8643, 'learning_rate': 1.6567849969097505e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1527/5198 [8:40:45<31:39:10, 31.04s/it] 29%|â–ˆâ–ˆâ–‰       | 1528/5198 [8:41:02<27:12:19, 26.69s/it]                                                        {'loss': 0.9025, 'learning_rate': 1.6563150151098973e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1528/5198 [8:41:02<27:12:19, 26.69s/it] 29%|â–ˆâ–ˆâ–‰       | 1529/5198 [8:41:18<24:09:07, 23.70s/it]                                                        {'loss': 0.8035, 'learning_rate': 1.6558447785059577e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1529/5198 [8:41:18<24:09:07, 23.70s/it]WARNING: tokenization mismatch: 1 vs. 1440. (ignored)
 29%|â–ˆâ–ˆâ–‰       | 1530/5198 [8:41:36<22:19:27, 21.91s/it]                                                        {'loss': 0.8704, 'learning_rate': 1.655374287280494e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1530/5198 [8:41:36<22:19:27, 21.91s/it] 29%|â–ˆâ–ˆâ–‰       | 1531/5198 [8:41:54<21:06:16, 20.72s/it]                                                        {'loss': 0.8138, 'learning_rate': 1.6549035416161662e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1531/5198 [8:41:54<21:06:16, 20.72s/it] 29%|â–ˆâ–ˆâ–‰       | 1532/5198 [8:42:12<20:18:49, 19.95s/it]                                                        {'loss': 0.8917, 'learning_rate': 1.654432541695735e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1532/5198 [8:42:12<20:18:49, 19.95s/it] 29%|â–ˆâ–ˆâ–‰       | 1533/5198 [8:42:29<19:20:09, 18.99s/it]                                                        {'loss': 0.8779, 'learning_rate': 1.653961287702058e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 1533/5198 [8:42:29<19:20:09, 18.99s/it] 30%|â–ˆâ–ˆâ–‰       | 1534/5198 [8:42:46<18:52:58, 18.55s/it]                                                        {'loss': 0.3647, 'learning_rate': 1.653489779818093e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1534/5198 [8:42:46<18:52:58, 18.55s/it] 30%|â–ˆâ–ˆâ–‰       | 1535/5198 [8:43:03<18:24:16, 18.09s/it]                                                        {'loss': 0.8907, 'learning_rate': 1.6530180182268946e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1535/5198 [8:43:03<18:24:16, 18.09s/it] 30%|â–ˆâ–ˆâ–‰       | 1536/5198 [8:43:21<18:07:16, 17.81s/it]                                                        {'loss': 0.9074, 'learning_rate': 1.652546003111618e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1536/5198 [8:43:21<18:07:16, 17.81s/it] 30%|â–ˆâ–ˆâ–‰       | 1537/5198 [8:43:40<18:36:12, 18.29s/it]                                                        {'loss': 0.8547, 'learning_rate': 1.652073734655515e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1537/5198 [8:43:40<18:36:12, 18.29s/it] 30%|â–ˆâ–ˆâ–‰       | 1538/5198 [8:43:58<18:36:46, 18.31s/it]                                                        {'loss': 0.8489, 'learning_rate': 1.6516012130419366e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1538/5198 [8:43:59<18:36:46, 18.31s/it] 30%|â–ˆâ–ˆâ–‰       | 1539/5198 [8:44:18<18:56:01, 18.63s/it]                                                        {'loss': 0.8114, 'learning_rate': 1.6511284384543317e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1539/5198 [8:44:18<18:56:01, 18.63s/it] 30%|â–ˆâ–ˆâ–‰       | 1540/5198 [8:44:35<18:34:54, 18.29s/it]                                                        {'loss': 0.355, 'learning_rate': 1.6506554110762483e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1540/5198 [8:44:35<18:34:54, 18.29s/it] 30%|â–ˆâ–ˆâ–‰       | 1541/5198 [8:44:52<18:11:06, 17.90s/it]                                                        {'loss': 0.8544, 'learning_rate': 1.650182131091332e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1541/5198 [8:44:52<18:11:06, 17.90s/it] 30%|â–ˆâ–ˆâ–‰       | 1542/5198 [8:45:10<18:03:52, 17.79s/it]                                                        {'loss': 0.8797, 'learning_rate': 1.6497085986833252e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1542/5198 [8:45:10<18:03:52, 17.79s/it] 30%|â–ˆâ–ˆâ–‰       | 1543/5198 [8:45:27<17:50:47, 17.58s/it]                                                        {'loss': 0.3425, 'learning_rate': 1.6492348140360704e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1543/5198 [8:45:27<17:50:47, 17.58s/it] 30%|â–ˆâ–ˆâ–‰       | 1544/5198 [8:45:45<17:52:35, 17.61s/it]                                                        {'loss': 0.8274, 'learning_rate': 1.6487607773335074e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1544/5198 [8:45:45<17:52:35, 17.61s/it] 30%|â–ˆâ–ˆâ–‰       | 1545/5198 [8:46:02<17:46:22, 17.52s/it]                                                        {'loss': 0.9001, 'learning_rate': 1.648286488759673e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1545/5198 [8:46:02<17:46:22, 17.52s/it] 30%|â–ˆâ–ˆâ–‰       | 1546/5198 [8:46:19<17:41:20, 17.44s/it]                                                        {'loss': 0.89, 'learning_rate': 1.6478119484987026e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1546/5198 [8:46:19<17:41:20, 17.44s/it] 30%|â–ˆâ–ˆâ–‰       | 1547/5198 [8:46:37<17:44:57, 17.50s/it]                                                        {'loss': 0.824, 'learning_rate': 1.6473371567348287e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1547/5198 [8:46:37<17:44:57, 17.50s/it] 30%|â–ˆâ–ˆâ–‰       | 1548/5198 [8:46:55<18:00:13, 17.76s/it]                                                        {'loss': 0.8965, 'learning_rate': 1.6468621136523823e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1548/5198 [8:46:55<18:00:13, 17.76s/it] 30%|â–ˆâ–ˆâ–‰       | 1549/5198 [8:47:13<18:00:52, 17.77s/it]                                                        {'loss': 0.8323, 'learning_rate': 1.646386819435791e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1549/5198 [8:47:13<18:00:52, 17.77s/it] 30%|â–ˆâ–ˆâ–‰       | 1550/5198 [8:47:31<17:58:24, 17.74s/it]                                                        {'loss': 0.8076, 'learning_rate': 1.6459112742695807e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1550/5198 [8:47:31<17:58:24, 17.74s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 30%|â–ˆâ–ˆâ–‰       | 1551/5198 [8:48:58<39:02:02, 38.53s/it]                                                        {'loss': 0.8873, 'learning_rate': 1.6454354783383748e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1551/5198 [8:48:58<39:02:02, 38.53s/it] 30%|â–ˆâ–ˆâ–‰       | 1552/5198 [8:49:15<32:41:03, 32.27s/it]                                                        {'loss': 0.8085, 'learning_rate': 1.644959431826893e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1552/5198 [8:49:15<32:41:03, 32.27s/it] 30%|â–ˆâ–ˆâ–‰       | 1553/5198 [8:49:33<28:22:37, 28.03s/it]                                                        {'loss': 0.8573, 'learning_rate': 1.6444831349199528e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1553/5198 [8:49:33<28:22:37, 28.03s/it] 30%|â–ˆâ–ˆâ–‰       | 1554/5198 [8:49:51<25:10:20, 24.87s/it]                                                        {'loss': 0.8843, 'learning_rate': 1.6440065878024697e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1554/5198 [8:49:51<25:10:20, 24.87s/it] 30%|â–ˆâ–ˆâ–‰       | 1555/5198 [8:50:11<23:41:43, 23.42s/it]                                                        {'loss': 0.9081, 'learning_rate': 1.6435297906594553e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1555/5198 [8:50:11<23:41:43, 23.42s/it] 30%|â–ˆâ–ˆâ–‰       | 1556/5198 [8:50:29<22:09:36, 21.90s/it]                                                        {'loss': 0.8525, 'learning_rate': 1.643052743676019e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1556/5198 [8:50:29<22:09:36, 21.90s/it] 30%|â–ˆâ–ˆâ–‰       | 1557/5198 [8:50:47<20:54:12, 20.67s/it]                                                        {'loss': 0.8475, 'learning_rate': 1.6425754470373667e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1557/5198 [8:50:47<20:54:12, 20.67s/it] 30%|â–ˆâ–ˆâ–‰       | 1558/5198 [8:51:05<20:03:10, 19.83s/it]                                                        {'loss': 0.8038, 'learning_rate': 1.642097900928801e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1558/5198 [8:51:05<20:03:10, 19.83s/it] 30%|â–ˆâ–ˆâ–‰       | 1559/5198 [8:51:24<19:47:45, 19.58s/it]                                                        {'loss': 0.8766, 'learning_rate': 1.6416201055357225e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 1559/5198 [8:51:24<19:47:45, 19.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1560/5198 [8:51:41<19:03:12, 18.85s/it]                                                        {'loss': 0.8747, 'learning_rate': 1.641142061043627e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1560/5198 [8:51:41<19:03:12, 18.85s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1561/5198 [8:51:59<18:38:32, 18.45s/it]                                                        {'loss': 0.8744, 'learning_rate': 1.640663767638108e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1561/5198 [8:51:59<18:38:32, 18.45s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1562/5198 [8:52:17<18:28:33, 18.29s/it]                                                        {'loss': 0.8802, 'learning_rate': 1.6401852255048564e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1562/5198 [8:52:17<18:28:33, 18.29s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1563/5198 [8:52:34<18:20:07, 18.16s/it]                                                        {'loss': 0.8745, 'learning_rate': 1.6397064348296578e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1563/5198 [8:52:34<18:20:07, 18.16s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1564/5198 [8:52:52<18:11:00, 18.01s/it]                                                        {'loss': 0.8945, 'learning_rate': 1.6392273957983955e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1564/5198 [8:52:52<18:11:00, 18.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1565/5198 [8:53:10<18:07:37, 17.96s/it]                                                        {'loss': 0.9001, 'learning_rate': 1.638748108597049e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1565/5198 [8:53:10<18:07:37, 17.96s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1566/5198 [8:53:28<18:03:06, 17.89s/it]                                                        {'loss': 0.9102, 'learning_rate': 1.6382685734116934e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1566/5198 [8:53:28<18:03:06, 17.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1567/5198 [8:53:46<18:12:17, 18.05s/it]                                                        {'loss': 0.8863, 'learning_rate': 1.6377887904285018e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1567/5198 [8:53:46<18:12:17, 18.05s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1568/5198 [8:54:04<18:04:43, 17.93s/it]                                                        {'loss': 0.8109, 'learning_rate': 1.637308759833742e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1568/5198 [8:54:04<18:04:43, 17.93s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1569/5198 [8:54:22<18:10:33, 18.03s/it]                                                        {'loss': 0.9175, 'learning_rate': 1.6368284818137787e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1569/5198 [8:54:22<18:10:33, 18.03s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1570/5198 [8:54:39<18:01:35, 17.89s/it]                                                        {'loss': 0.8154, 'learning_rate': 1.636347956555072e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1570/5198 [8:54:39<18:01:35, 17.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1571/5198 [8:54:57<18:02:36, 17.91s/it]                                                        {'loss': 0.9033, 'learning_rate': 1.635867184244178e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1571/5198 [8:54:57<18:02:36, 17.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1572/5198 [8:55:15<17:57:06, 17.82s/it]                                                        {'loss': 0.8958, 'learning_rate': 1.63538616506775e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1572/5198 [8:55:15<17:57:06, 17.82s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1573/5198 [8:55:33<18:03:27, 17.93s/it]                                                        {'loss': 0.838, 'learning_rate': 1.6349048992125358e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1573/5198 [8:55:33<18:03:27, 17.93s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1574/5198 [8:55:51<18:03:08, 17.93s/it]                                                        {'loss': 0.8364, 'learning_rate': 1.634423386865379e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1574/5198 [8:55:51<18:03:08, 17.93s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1575/5198 [8:56:08<17:34:05, 17.46s/it]                                                        {'loss': 0.8407, 'learning_rate': 1.6339416282132196e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1575/5198 [8:56:08<17:34:05, 17.46s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 30%|â–ˆâ–ˆâ–ˆ       | 1576/5198 [8:57:32<37:42:09, 37.47s/it]                                                        {'loss': 0.8823, 'learning_rate': 1.633459623443093e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1576/5198 [8:57:32<37:42:09, 37.47s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1577/5198 [8:57:50<31:50:40, 31.66s/it]                                                        {'loss': 0.89, 'learning_rate': 1.6329773727421297e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1577/5198 [8:57:50<31:50:40, 31.66s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1578/5198 [8:58:08<27:37:29, 27.47s/it]                                                        {'loss': 0.8267, 'learning_rate': 1.6324948762975567e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1578/5198 [8:58:08<27:37:29, 27.47s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1579/5198 [8:58:25<24:38:05, 24.51s/it]                                                        {'loss': 0.8667, 'learning_rate': 1.632012134296695e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1579/5198 [8:58:25<24:38:05, 24.51s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1580/5198 [8:58:42<22:22:43, 22.27s/it]                                                        {'loss': 0.9144, 'learning_rate': 1.6315291469269617e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1580/5198 [8:58:42<22:22:43, 22.27s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1581/5198 [8:58:59<20:45:33, 20.66s/it]                                                        {'loss': 0.8683, 'learning_rate': 1.63104591437587e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1581/5198 [8:58:59<20:45:33, 20.66s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1582/5198 [8:59:17<19:51:40, 19.77s/it]                                                        {'loss': 0.9099, 'learning_rate': 1.6305624368310265e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1582/5198 [8:59:17<19:51:40, 19.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1583/5198 [8:59:34<19:12:46, 19.13s/it]                                                        {'loss': 0.8401, 'learning_rate': 1.630078714480134e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1583/5198 [8:59:34<19:12:46, 19.13s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1584/5198 [8:59:52<18:46:44, 18.71s/it]                                                        {'loss': 0.3886, 'learning_rate': 1.6295947475109904e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1584/5198 [8:59:52<18:46:44, 18.71s/it] 30%|â–ˆâ–ˆâ–ˆ       | 1585/5198 [9:00:09<18:17:05, 18.22s/it]                                                        {'loss': 0.9185, 'learning_rate': 1.629110536111488e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 1585/5198 [9:00:09<18:17:05, 18.22s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1586/5198 [9:00:26<18:00:15, 17.94s/it]                                                        {'loss': 0.8587, 'learning_rate': 1.628626080469615e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1586/5198 [9:00:26<18:00:15, 17.94s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1587/5198 [9:00:44<17:49:44, 17.77s/it]                                                        {'loss': 0.353, 'learning_rate': 1.628141380773453e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1587/5198 [9:00:44<17:49:44, 17.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1588/5198 [9:01:01<17:38:10, 17.59s/it]                                                        {'loss': 0.9076, 'learning_rate': 1.6276564372111797e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1588/5198 [9:01:01<17:38:10, 17.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1589/5198 [9:01:19<17:42:28, 17.66s/it]                                                        {'loss': 0.3676, 'learning_rate': 1.6271712499710663e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1589/5198 [9:01:19<17:42:28, 17.66s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1590/5198 [9:01:36<17:28:29, 17.44s/it]                                                        {'loss': 0.8157, 'learning_rate': 1.62668581924148e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1590/5198 [9:01:36<17:28:29, 17.44s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1591/5198 [9:01:54<17:45:04, 17.72s/it]                                                        {'loss': 0.8726, 'learning_rate': 1.6262001452108807e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1591/5198 [9:01:54<17:45:04, 17.72s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1592/5198 [9:02:10<17:18:29, 17.28s/it]                                                        {'loss': 0.8903, 'learning_rate': 1.6257142280678247e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1592/5198 [9:02:10<17:18:29, 17.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1593/5198 [9:02:28<17:25:02, 17.39s/it]                                                        {'loss': 0.8522, 'learning_rate': 1.6252280680009613e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1593/5198 [9:02:28<17:25:02, 17.39s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1594/5198 [9:02:45<17:16:10, 17.25s/it]                                                        {'loss': 0.8735, 'learning_rate': 1.6247416651990343e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1594/5198 [9:02:45<17:16:10, 17.25s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1595/5198 [9:03:02<17:10:55, 17.17s/it]                                                        {'loss': 0.8534, 'learning_rate': 1.624255019850883e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1595/5198 [9:03:02<17:10:55, 17.17s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1596/5198 [9:03:19<17:08:15, 17.13s/it]                                                        {'loss': 0.973, 'learning_rate': 1.6237681321454387e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1596/5198 [9:03:19<17:08:15, 17.13s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1597/5198 [9:03:37<17:28:41, 17.47s/it]                                                        {'loss': 0.8852, 'learning_rate': 1.623281002271729e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1597/5198 [9:03:37<17:28:41, 17.47s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1598/5198 [9:03:55<17:34:16, 17.57s/it]                                                        {'loss': 0.3409, 'learning_rate': 1.6227936304188738e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1598/5198 [9:03:55<17:34:16, 17.57s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1599/5198 [9:04:13<17:35:41, 17.60s/it]                                                        {'loss': 0.8803, 'learning_rate': 1.622306016776088e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1599/5198 [9:04:13<17:35:41, 17.60s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1600/5198 [9:04:29<17:18:54, 17.32s/it]                                                        {'loss': 0.8831, 'learning_rate': 1.6218181615326795e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1600/5198 [9:04:29<17:18:54, 17.32s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 31%|â–ˆâ–ˆâ–ˆ       | 1601/5198 [9:05:55<37:47:48, 37.83s/it]                                                        {'loss': 0.8351, 'learning_rate': 1.6213300648780515e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1601/5198 [9:05:55<37:47:48, 37.83s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1602/5198 [9:06:12<31:37:23, 31.66s/it]                                                        {'loss': 0.8679, 'learning_rate': 1.620841727001699e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1602/5198 [9:06:12<31:37:23, 31.66s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1603/5198 [9:06:30<27:18:03, 27.34s/it]                                                        {'loss': 0.8171, 'learning_rate': 1.6203531480932114e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1603/5198 [9:06:30<27:18:03, 27.34s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1604/5198 [9:06:48<24:37:26, 24.67s/it]                                                        {'loss': 0.8304, 'learning_rate': 1.619864328342273e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1604/5198 [9:06:48<24:37:26, 24.67s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1605/5198 [9:07:06<22:34:06, 22.61s/it]                                                        {'loss': 0.8132, 'learning_rate': 1.6193752679386593e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1605/5198 [9:07:06<22:34:06, 22.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1606/5198 [9:07:24<21:08:15, 21.18s/it]                                                        {'loss': 0.7908, 'learning_rate': 1.6188859670722414e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1606/5198 [9:07:24<21:08:15, 21.18s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1607/5198 [9:07:42<20:16:27, 20.32s/it]                                                        {'loss': 0.8393, 'learning_rate': 1.6183964259329817e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1607/5198 [9:07:42<20:16:27, 20.32s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1608/5198 [9:07:59<19:22:20, 19.43s/it]                                                        {'loss': 0.8325, 'learning_rate': 1.6179066447109376e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1608/5198 [9:07:59<19:22:20, 19.43s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1609/5198 [9:08:18<19:05:21, 19.15s/it]                                                        {'loss': 0.8241, 'learning_rate': 1.6174166235962588e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1609/5198 [9:08:18<19:05:21, 19.15s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1610/5198 [9:08:36<18:46:32, 18.84s/it]                                                        {'loss': 0.8835, 'learning_rate': 1.6169263627791886e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1610/5198 [9:08:36<18:46:32, 18.84s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1611/5198 [9:08:54<18:29:31, 18.56s/it]                                                        {'loss': 0.913, 'learning_rate': 1.616435862450063e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1611/5198 [9:08:54<18:29:31, 18.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1612/5198 [9:09:12<18:26:34, 18.51s/it]                                                        {'loss': 0.8822, 'learning_rate': 1.615945122799311e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1612/5198 [9:09:12<18:26:34, 18.51s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1613/5198 [9:09:30<18:12:46, 18.29s/it]                                                        {'loss': 0.8307, 'learning_rate': 1.6154541440174547e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1613/5198 [9:09:30<18:12:46, 18.29s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1614/5198 [9:09:49<18:17:19, 18.37s/it]                                                        {'loss': 0.8831, 'learning_rate': 1.614962926295109e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1614/5198 [9:09:49<18:17:19, 18.37s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1615/5198 [9:10:06<17:56:50, 18.03s/it]                                                        {'loss': 0.8563, 'learning_rate': 1.6144714698229814e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1615/5198 [9:10:06<17:56:50, 18.03s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1616/5198 [9:10:23<17:48:01, 17.89s/it]                                                        {'loss': 0.9238, 'learning_rate': 1.6139797747918725e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1616/5198 [9:10:23<17:48:01, 17.89s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1617/5198 [9:10:41<17:47:34, 17.89s/it]                                                        {'loss': 0.8915, 'learning_rate': 1.613487841392675e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1617/5198 [9:10:41<17:47:34, 17.89s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1618/5198 [9:10:59<17:44:45, 17.85s/it]                                                        {'loss': 0.8697, 'learning_rate': 1.612995669816375e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1618/5198 [9:10:59<17:44:45, 17.85s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1619/5198 [9:11:16<17:30:07, 17.60s/it]                                                        {'loss': 0.8475, 'learning_rate': 1.6125032602540492e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1619/5198 [9:11:16<17:30:07, 17.60s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1620/5198 [9:11:34<17:35:27, 17.70s/it]                                                        {'loss': 0.7894, 'learning_rate': 1.6120106128968686e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1620/5198 [9:11:34<17:35:27, 17.70s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1621/5198 [9:11:52<17:34:08, 17.68s/it]                                                        {'loss': 0.8084, 'learning_rate': 1.6115177279360965e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1621/5198 [9:11:52<17:34:08, 17.68s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1622/5198 [9:12:09<17:33:27, 17.68s/it]                                                        {'loss': 0.8392, 'learning_rate': 1.611024605563087e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1622/5198 [9:12:09<17:33:27, 17.68s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1623/5198 [9:12:27<17:35:58, 17.72s/it]                                                        {'loss': 0.8858, 'learning_rate': 1.610531245969287e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1623/5198 [9:12:27<17:35:58, 17.72s/it] 31%|â–ˆâ–ˆâ–ˆ       | 1624/5198 [9:12:44<17:26:43, 17.57s/it]                                                        {'loss': 0.8558, 'learning_rate': 1.6100376493462368e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 1624/5198 [9:12:44<17:26:43, 17.57s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1625/5198 [9:13:03<17:39:47, 17.80s/it]                                                        {'loss': 0.9065, 'learning_rate': 1.6095438158855668e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1625/5198 [9:13:03<17:39:47, 17.80s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 31%|â–ˆâ–ˆâ–ˆâ–      | 1626/5198 [9:14:33<39:13:14, 39.53s/it]                                                        {'loss': 0.8539, 'learning_rate': 1.609049745779e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1626/5198 [9:14:33<39:13:14, 39.53s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1627/5198 [9:14:51<32:47:33, 33.06s/it]                                                        {'loss': 0.8569, 'learning_rate': 1.6085554392183517e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1627/5198 [9:14:51<32:47:33, 33.06s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1628/5198 [9:15:08<27:57:14, 28.19s/it]                                                        {'loss': 0.8511, 'learning_rate': 1.608060896395529e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1628/5198 [9:15:08<27:57:14, 28.19s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1629/5198 [9:15:26<24:57:13, 25.17s/it]                                                        {'loss': 0.8391, 'learning_rate': 1.60756611750253e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1629/5198 [9:15:26<24:57:13, 25.17s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1630/5198 [9:15:43<22:34:57, 22.79s/it]                                                        {'loss': 0.8083, 'learning_rate': 1.6070711027314446e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1630/5198 [9:15:43<22:34:57, 22.79s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1631/5198 [9:16:00<20:50:23, 21.03s/it]                                                        {'loss': 0.9002, 'learning_rate': 1.606575852274456e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1631/5198 [9:16:00<20:50:23, 21.03s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1632/5198 [9:16:18<19:49:14, 20.01s/it]                                                        {'loss': 0.8258, 'learning_rate': 1.6060803663238357e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1632/5198 [9:16:18<19:49:14, 20.01s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1633/5198 [9:16:35<18:53:24, 19.08s/it]                                                        {'loss': 0.87, 'learning_rate': 1.6055846450719498e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1633/5198 [9:16:35<18:53:24, 19.08s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1634/5198 [9:16:52<18:25:43, 18.61s/it]                                                        {'loss': 0.8261, 'learning_rate': 1.6050886887112535e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1634/5198 [9:16:52<18:25:43, 18.61s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1635/5198 [9:17:09<17:58:41, 18.16s/it]                                                        {'loss': 0.9403, 'learning_rate': 1.6045924974342945e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1635/5198 [9:17:09<17:58:41, 18.16s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1636/5198 [9:17:26<17:32:31, 17.73s/it]                                                        {'loss': 0.8608, 'learning_rate': 1.604096071433711e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1636/5198 [9:17:26<17:32:31, 17.73s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 1637/5198 [9:17:42<17:10:25, 17.36s/it]                                                        {'loss': 0.8885, 'learning_rate': 1.6035994109022333e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 1637/5198 [9:17:42<17:10:25, 17.36s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1638/5198 [9:17:59<17:01:47, 17.22s/it]                                                        {'loss': 0.8921, 'learning_rate': 1.6031025160326814e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1638/5198 [9:17:59<17:01:47, 17.22s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1639/5198 [9:18:16<16:58:07, 17.16s/it]                                                        {'loss': 0.9154, 'learning_rate': 1.6026053870179678e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1639/5198 [9:18:16<16:58:07, 17.16s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1640/5198 [9:18:34<17:12:12, 17.41s/it]                                                        {'loss': 0.8499, 'learning_rate': 1.6021080240510943e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1640/5198 [9:18:34<17:12:12, 17.41s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1641/5198 [9:18:52<17:11:01, 17.39s/it]                                                        {'loss': 0.3381, 'learning_rate': 1.601610427325155e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1641/5198 [9:18:52<17:11:01, 17.39s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1642/5198 [9:19:09<17:19:00, 17.53s/it]                                                        {'loss': 0.809, 'learning_rate': 1.6011125970333333e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1642/5198 [9:19:10<17:19:00, 17.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1643/5198 [9:19:28<17:29:42, 17.72s/it]                                                        {'loss': 0.8027, 'learning_rate': 1.600614533368905e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1643/5198 [9:19:28<17:29:42, 17.72s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1644/5198 [9:19:45<17:31:48, 17.76s/it]                                                        {'loss': 0.9326, 'learning_rate': 1.6001162365252348e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1644/5198 [9:19:46<17:31:48, 17.76s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1645/5198 [9:20:04<17:38:59, 17.88s/it]                                                        {'loss': 0.855, 'learning_rate': 1.5996177066957787e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1645/5198 [9:20:04<17:38:59, 17.88s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1646/5198 [9:20:22<17:43:29, 17.96s/it]                                                        {'loss': 0.8349, 'learning_rate': 1.5991189440740838e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1646/5198 [9:20:22<17:43:29, 17.96s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1647/5198 [9:20:39<17:30:54, 17.76s/it]                                                        {'loss': 0.8455, 'learning_rate': 1.5986199488537867e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1647/5198 [9:20:39<17:30:54, 17.76s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1648/5198 [9:20:56<17:19:24, 17.57s/it]                                                        {'loss': 0.8945, 'learning_rate': 1.598120721228614e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1648/5198 [9:20:56<17:19:24, 17.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1649/5198 [9:21:14<17:18:10, 17.55s/it]                                                        {'loss': 0.8754, 'learning_rate': 1.5976212613923836e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1649/5198 [9:21:14<17:18:10, 17.55s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1650/5198 [9:21:32<17:38:12, 17.90s/it]                                                        {'loss': 0.8282, 'learning_rate': 1.5971215695390026e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1650/5198 [9:21:32<17:38:12, 17.90s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 32%|â–ˆâ–ˆâ–ˆâ–      | 1651/5198 [9:23:01<38:37:02, 39.19s/it]                                                        {'loss': 0.8654, 'learning_rate': 1.5966216458624692e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1651/5198 [9:23:01<38:37:02, 39.19s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1652/5198 [9:23:19<32:21:33, 32.85s/it]                                                        {'loss': 0.8776, 'learning_rate': 1.5961214905568705e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1652/5198 [9:23:19<32:21:33, 32.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1653/5198 [9:23:38<28:14:03, 28.67s/it]                                                        {'loss': 0.8527, 'learning_rate': 1.595621103816384e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1653/5198 [9:23:38<28:14:03, 28.67s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1654/5198 [9:23:55<24:49:08, 25.21s/it]                                                        {'loss': 0.8466, 'learning_rate': 1.5951204858352772e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1654/5198 [9:23:55<24:49:08, 25.21s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1655/5198 [9:24:12<22:22:02, 22.73s/it]                                                        {'loss': 0.8151, 'learning_rate': 1.594619636807907e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1655/5198 [9:24:12<22:22:02, 22.73s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1656/5198 [9:24:29<20:38:33, 20.98s/it]                                                        {'loss': 0.8766, 'learning_rate': 1.5941185569287206e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1656/5198 [9:24:29<20:38:33, 20.98s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1657/5198 [9:24:47<19:44:47, 20.08s/it]                                                        {'loss': 0.8783, 'learning_rate': 1.5936172463922542e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1657/5198 [9:24:47<19:44:47, 20.08s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1658/5198 [9:25:05<19:06:47, 19.44s/it]                                                        {'loss': 0.8087, 'learning_rate': 1.593115705393134e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1658/5198 [9:25:05<19:06:47, 19.44s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1659/5198 [9:25:23<18:39:01, 18.97s/it]                                                        {'loss': 0.9038, 'learning_rate': 1.5926139341260755e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1659/5198 [9:25:23<18:39:01, 18.97s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1660/5198 [9:25:40<18:03:28, 18.37s/it]                                                        {'loss': 0.8453, 'learning_rate': 1.5921119327858835e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1660/5198 [9:25:40<18:03:28, 18.37s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1661/5198 [9:25:57<17:31:30, 17.84s/it]                                                        {'loss': 0.8836, 'learning_rate': 1.5916097015674518e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1661/5198 [9:25:57<17:31:30, 17.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1662/5198 [9:26:14<17:27:49, 17.78s/it]                                                        {'loss': 0.824, 'learning_rate': 1.5911072406657646e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1662/5198 [9:26:14<17:27:49, 17.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1663/5198 [9:26:32<17:31:42, 17.85s/it]                                                        {'loss': 0.8009, 'learning_rate': 1.5906045502758943e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1663/5198 [9:26:32<17:31:42, 17.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1664/5198 [9:26:50<17:25:03, 17.74s/it]                                                        {'loss': 0.8796, 'learning_rate': 1.590101630593002e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1664/5198 [9:26:50<17:25:03, 17.74s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1665/5198 [9:27:08<17:24:59, 17.75s/it]                                                        {'loss': 0.8407, 'learning_rate': 1.5895984818123392e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1665/5198 [9:27:08<17:24:59, 17.75s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1666/5198 [9:27:25<17:26:17, 17.77s/it]                                                        {'loss': 0.3723, 'learning_rate': 1.5890951041292453e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1666/5198 [9:27:25<17:26:17, 17.77s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1667/5198 [9:27:44<17:36:03, 17.94s/it]                                                        {'loss': 0.8533, 'learning_rate': 1.588591497739149e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1667/5198 [9:27:44<17:36:03, 17.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1668/5198 [9:28:01<17:16:45, 17.62s/it]                                                        {'loss': 0.8397, 'learning_rate': 1.5880876628375668e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1668/5198 [9:28:01<17:16:45, 17.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1669/5198 [9:28:18<17:21:15, 17.70s/it]                                                        {'loss': 0.8459, 'learning_rate': 1.587583599620106e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1669/5198 [9:28:18<17:21:15, 17.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1670/5198 [9:28:36<17:14:27, 17.59s/it]                                                        {'loss': 0.9015, 'learning_rate': 1.5870793082824604e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1670/5198 [9:28:36<17:14:27, 17.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1671/5198 [9:28:55<17:34:22, 17.94s/it]                                                        {'loss': 0.8101, 'learning_rate': 1.5865747890204138e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1671/5198 [9:28:55<17:34:22, 17.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1672/5198 [9:29:12<17:31:30, 17.89s/it]                                                        {'loss': 0.853, 'learning_rate': 1.5860700420298377e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1672/5198 [9:29:12<17:31:30, 17.89s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1673/5198 [9:29:30<17:30:30, 17.88s/it]                                                        {'loss': 0.8818, 'learning_rate': 1.5855650675066924e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1673/5198 [9:29:30<17:30:30, 17.88s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1674/5198 [9:29:47<17:09:42, 17.53s/it]                                                        {'loss': 0.8736, 'learning_rate': 1.5850598656470265e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1674/5198 [9:29:47<17:09:42, 17.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1675/5198 [9:30:04<16:53:01, 17.25s/it]                                                        {'loss': 0.8332, 'learning_rate': 1.584554436646976e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1675/5198 [9:30:04<16:53:01, 17.25s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 32%|â–ˆâ–ˆâ–ˆâ–      | 1676/5198 [9:31:34<38:30:01, 39.35s/it]                                                        {'loss': 0.8958, 'learning_rate': 1.5840487807027665e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1676/5198 [9:31:34<38:30:01, 39.35s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1677/5198 [9:31:51<31:54:40, 32.63s/it]                                                        {'loss': 0.8705, 'learning_rate': 1.5835428980107113e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1677/5198 [9:31:51<31:54:40, 32.63s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1678/5198 [9:32:09<27:29:01, 28.11s/it]                                                        {'loss': 0.9135, 'learning_rate': 1.583036788767211e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1678/5198 [9:32:09<27:29:01, 28.11s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1679/5198 [9:32:27<24:25:40, 24.99s/it]                                                        {'loss': 0.8708, 'learning_rate': 1.5825304531687548e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1679/5198 [9:32:27<24:25:40, 24.99s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1680/5198 [9:32:45<22:25:29, 22.95s/it]                                                        {'loss': 0.8614, 'learning_rate': 1.5820238914119195e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1680/5198 [9:32:45<22:25:29, 22.95s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1681/5198 [9:33:02<20:40:16, 21.16s/it]                                                        {'loss': 0.8439, 'learning_rate': 1.5815171036933697e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1681/5198 [9:33:02<20:40:16, 21.16s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1682/5198 [9:33:20<19:43:18, 20.19s/it]                                                        {'loss': 0.3602, 'learning_rate': 1.5810100902098582e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1682/5198 [9:33:20<19:43:18, 20.19s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1683/5198 [9:33:37<18:50:58, 19.31s/it]                                                        {'loss': 0.8692, 'learning_rate': 1.580502851158225e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1683/5198 [9:33:37<18:50:58, 19.31s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1684/5198 [9:33:55<18:29:33, 18.95s/it]                                                        {'loss': 0.8165, 'learning_rate': 1.5799953867353975e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1684/5198 [9:33:55<18:29:33, 18.95s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1685/5198 [9:34:13<18:09:41, 18.61s/it]                                                        {'loss': 0.909, 'learning_rate': 1.579487697138391e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1685/5198 [9:34:13<18:09:41, 18.61s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1686/5198 [9:34:31<17:59:19, 18.44s/it]                                                        {'loss': 0.8489, 'learning_rate': 1.5789797825643086e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1686/5198 [9:34:31<17:59:19, 18.44s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1687/5198 [9:34:49<17:55:43, 18.38s/it]                                                        {'loss': 0.8489, 'learning_rate': 1.5784716432103394e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1687/5198 [9:34:49<17:55:43, 18.38s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1688/5198 [9:35:06<17:33:34, 18.01s/it]                                                        {'loss': 0.8236, 'learning_rate': 1.5779632792737608e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1688/5198 [9:35:06<17:33:34, 18.01s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 1689/5198 [9:35:24<17:26:45, 17.90s/it]                                                        {'loss': 0.3576, 'learning_rate': 1.5774546909519376e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 1689/5198 [9:35:24<17:26:45, 17.90s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1690/5198 [9:35:42<17:30:23, 17.97s/it]                                                        {'loss': 0.9068, 'learning_rate': 1.5769458784423206e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1690/5198 [9:35:42<17:30:23, 17.97s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1691/5198 [9:36:00<17:31:36, 17.99s/it]                                                        {'loss': 0.8916, 'learning_rate': 1.5764368419424488e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1691/5198 [9:36:00<17:31:36, 17.99s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1692/5198 [9:36:17<17:14:52, 17.71s/it]                                                        {'loss': 0.8675, 'learning_rate': 1.575927581649948e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1692/5198 [9:36:17<17:14:52, 17.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1693/5198 [9:36:35<17:21:11, 17.82s/it]                                                        {'loss': 0.873, 'learning_rate': 1.5754180977625303e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1693/5198 [9:36:35<17:21:11, 17.82s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1694/5198 [9:36:54<17:32:24, 18.02s/it]                                                        {'loss': 0.8299, 'learning_rate': 1.574908390477995e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1694/5198 [9:36:54<17:32:24, 18.02s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1695/5198 [9:37:11<17:22:38, 17.86s/it]                                                        {'loss': 0.8655, 'learning_rate': 1.5743984599942273e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1695/5198 [9:37:11<17:22:38, 17.86s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1696/5198 [9:37:30<17:28:50, 17.97s/it]                                                        {'loss': 0.8924, 'learning_rate': 1.5738883065092005e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1696/5198 [9:37:30<17:28:50, 17.97s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1697/5198 [9:37:46<17:03:21, 17.54s/it]                                                        {'loss': 0.8761, 'learning_rate': 1.5733779302209735e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1697/5198 [9:37:46<17:03:21, 17.54s/it]WARNING: tokenization mismatch: 1 vs. 1590. (ignored)
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1698/5198 [9:38:04<17:06:07, 17.59s/it]                                                        {'loss': 0.7847, 'learning_rate': 1.572867331327692e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1698/5198 [9:38:04<17:06:07, 17.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1699/5198 [9:38:21<16:54:20, 17.39s/it]                                                        {'loss': 0.9268, 'learning_rate': 1.5723565100275884e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1699/5198 [9:38:21<16:54:20, 17.39s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1700/5198 [9:38:39<17:12:36, 17.71s/it]                                                        {'loss': 0.8764, 'learning_rate': 1.5718454665189806e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1700/5198 [9:38:39<17:12:36, 17.71s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1701/5198 [9:40:09<38:13:21, 39.35s/it]                                                        {'loss': 0.8224, 'learning_rate': 1.5713342010002733e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1701/5198 [9:40:09<38:13:21, 39.35s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1702/5198 [9:40:26<31:50:36, 32.79s/it]                                                        {'loss': 0.3522, 'learning_rate': 1.5708227136699578e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1702/5198 [9:40:26<31:50:36, 32.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1703/5198 [9:40:44<27:25:28, 28.25s/it]                                                        {'loss': 0.3669, 'learning_rate': 1.5703110047266105e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1703/5198 [9:40:44<27:25:28, 28.25s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1704/5198 [9:41:02<24:20:00, 25.07s/it]                                                        {'loss': 0.8301, 'learning_rate': 1.569799074368895e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1704/5198 [9:41:02<24:20:00, 25.07s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1705/5198 [9:41:20<22:17:58, 22.98s/it]                                                        {'loss': 0.8971, 'learning_rate': 1.5692869227955603e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1705/5198 [9:41:20<22:17:58, 22.98s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1706/5198 [9:41:38<20:46:13, 21.41s/it]                                                        {'loss': 0.825, 'learning_rate': 1.5687745502054407e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1706/5198 [9:41:38<20:46:13, 21.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1707/5198 [9:41:56<19:43:59, 20.35s/it]                                                        {'loss': 0.8418, 'learning_rate': 1.5682619567974575e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1707/5198 [9:41:56<19:43:59, 20.35s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1708/5198 [9:42:13<18:57:04, 19.55s/it]                                                        {'loss': 0.8581, 'learning_rate': 1.567749142770617e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1708/5198 [9:42:13<18:57:04, 19.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1709/5198 [9:42:31<18:22:10, 18.95s/it]                                                        {'loss': 0.8034, 'learning_rate': 1.5672361083240106e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1709/5198 [9:42:31<18:22:10, 18.95s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1710/5198 [9:42:48<17:53:47, 18.47s/it]                                                        {'loss': 0.7983, 'learning_rate': 1.5667228536568167e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1710/5198 [9:42:48<17:53:47, 18.47s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1711/5198 [9:43:05<17:30:09, 18.07s/it]                                                        {'loss': 0.8251, 'learning_rate': 1.566209378968298e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1711/5198 [9:43:05<17:30:09, 18.07s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1712/5198 [9:43:23<17:26:44, 18.02s/it]                                                        {'loss': 0.797, 'learning_rate': 1.565695684457803e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1712/5198 [9:43:23<17:26:44, 18.02s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1713/5198 [9:43:41<17:18:23, 17.88s/it]                                                        {'loss': 0.8564, 'learning_rate': 1.5651817703247666e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1713/5198 [9:43:41<17:18:23, 17.88s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1714/5198 [9:43:58<17:06:27, 17.68s/it]                                                        {'loss': 0.878, 'learning_rate': 1.5646676367687067e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1714/5198 [9:43:58<17:06:27, 17.68s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1715/5198 [9:44:16<17:12:09, 17.78s/it]                                                        {'loss': 0.8635, 'learning_rate': 1.564153283989228e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1715/5198 [9:44:16<17:12:09, 17.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1716/5198 [9:44:34<17:18:18, 17.89s/it]                                                        {'loss': 0.8622, 'learning_rate': 1.5636387121860207e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1716/5198 [9:44:34<17:18:18, 17.89s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1717/5198 [9:44:53<17:29:16, 18.09s/it]                                                        {'loss': 0.8659, 'learning_rate': 1.5631239215588578e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1717/5198 [9:44:53<17:29:16, 18.09s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1718/5198 [9:45:10<17:14:52, 17.84s/it]                                                        {'loss': 0.8891, 'learning_rate': 1.5626089123076004e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1718/5198 [9:45:10<17:14:52, 17.84s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1719/5198 [9:45:28<17:12:23, 17.80s/it]                                                        {'loss': 0.8514, 'learning_rate': 1.5620936846321917e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1719/5198 [9:45:28<17:12:23, 17.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1720/5198 [9:45:45<17:08:21, 17.74s/it]                                                        {'loss': 0.8719, 'learning_rate': 1.561578238732661e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1720/5198 [9:45:45<17:08:21, 17.74s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1721/5198 [9:46:03<17:10:09, 17.78s/it]                                                        {'loss': 0.87, 'learning_rate': 1.561062574809123e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1721/5198 [9:46:03<17:10:09, 17.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1722/5198 [9:46:20<17:00:05, 17.61s/it]                                                        {'loss': 0.871, 'learning_rate': 1.5605466930617747e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1722/5198 [9:46:20<17:00:05, 17.61s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1723/5198 [9:46:37<16:42:56, 17.32s/it]                                                        {'loss': 0.8523, 'learning_rate': 1.5600305936909005e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1723/5198 [9:46:37<16:42:56, 17.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1724/5198 [9:46:54<16:40:55, 17.29s/it]                                                        {'loss': 0.8333, 'learning_rate': 1.559514276896867e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1724/5198 [9:46:54<16:40:55, 17.29s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1725/5198 [9:47:12<16:48:40, 17.43s/it]                                                        {'loss': 0.8189, 'learning_rate': 1.558997742880127e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1725/5198 [9:47:12<16:48:40, 17.43s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1726/5198 [9:48:41<37:36:43, 39.00s/it]                                                        {'loss': 0.8974, 'learning_rate': 1.5584809918412158e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1726/5198 [9:48:41<37:36:43, 39.00s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1727/5198 [9:48:58<31:17:52, 32.46s/it]                                                        {'loss': 0.8392, 'learning_rate': 1.557964023980755e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1727/5198 [9:48:58<31:17:52, 32.46s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1728/5198 [9:49:16<27:00:33, 28.02s/it]                                                        {'loss': 0.853, 'learning_rate': 1.5574468394994486e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1728/5198 [9:49:16<27:00:33, 28.02s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1729/5198 [9:49:34<23:59:38, 24.90s/it]                                                        {'loss': 0.8928, 'learning_rate': 1.5569294385980856e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1729/5198 [9:49:34<23:59:38, 24.90s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1730/5198 [9:49:51<21:56:20, 22.77s/it]                                                        {'loss': 0.8432, 'learning_rate': 1.556411821477539e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1730/5198 [9:49:51<21:56:20, 22.77s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1731/5198 [9:50:09<20:22:12, 21.15s/it]                                                        {'loss': 0.9051, 'learning_rate': 1.5558939883387657e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1731/5198 [9:50:09<20:22:12, 21.15s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1732/5198 [9:50:26<19:17:55, 20.04s/it]                                                        {'loss': 0.8341, 'learning_rate': 1.5553759393828058e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1732/5198 [9:50:26<19:17:55, 20.04s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1733/5198 [9:50:44<18:41:43, 19.42s/it]                                                        {'loss': 0.8352, 'learning_rate': 1.554857674810784e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1733/5198 [9:50:44<18:41:43, 19.42s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1734/5198 [9:51:01<18:00:01, 18.71s/it]                                                        {'loss': 0.8221, 'learning_rate': 1.554339194823909e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1734/5198 [9:51:01<18:00:01, 18.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1735/5198 [9:51:19<17:47:56, 18.50s/it]                                                        {'loss': 0.7806, 'learning_rate': 1.553820499623472e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1735/5198 [9:51:19<17:47:56, 18.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1736/5198 [9:51:37<17:31:22, 18.22s/it]                                                        {'loss': 0.851, 'learning_rate': 1.553301589410848e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1736/5198 [9:51:37<17:31:22, 18.22s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1737/5198 [9:51:54<17:18:28, 18.00s/it]                                                        {'loss': 0.8443, 'learning_rate': 1.5527824643874968e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1737/5198 [9:51:54<17:18:28, 18.00s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1738/5198 [9:52:12<17:05:44, 17.79s/it]                                                        {'loss': 0.7995, 'learning_rate': 1.5522631247549598e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1738/5198 [9:52:12<17:05:44, 17.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1739/5198 [9:52:30<17:07:44, 17.83s/it]                                                        {'loss': 0.87, 'learning_rate': 1.5517435707148628e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1739/5198 [9:52:30<17:07:44, 17.83s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1740/5198 [9:52:47<16:53:46, 17.59s/it]                                                        {'loss': 0.896, 'learning_rate': 1.5512238024689144e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1740/5198 [9:52:47<16:53:46, 17.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1741/5198 [9:53:05<17:03:58, 17.77s/it]                                                        {'loss': 0.846, 'learning_rate': 1.550703820218907e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1741/5198 [9:53:05<17:03:58, 17.77s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1742/5198 [9:53:23<17:02:48, 17.76s/it]                                                        {'loss': 0.8652, 'learning_rate': 1.550183624166715e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1742/5198 [9:53:23<17:02:48, 17.76s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1743/5198 [9:53:40<16:55:50, 17.64s/it]                                                        {'loss': 0.8673, 'learning_rate': 1.549663214514297e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1743/5198 [9:53:40<16:55:50, 17.64s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1744/5198 [9:53:58<16:54:37, 17.63s/it]                                                        {'loss': 0.8948, 'learning_rate': 1.5491425914636934e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1744/5198 [9:53:58<16:54:37, 17.63s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1745/5198 [9:54:15<16:56:30, 17.66s/it]                                                        {'loss': 0.8819, 'learning_rate': 1.5486217552170283e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1745/5198 [9:54:15<16:56:30, 17.66s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1746/5198 [9:54:33<16:55:21, 17.65s/it]                                                        {'loss': 0.8504, 'learning_rate': 1.548100705976508e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1746/5198 [9:54:33<16:55:21, 17.65s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1747/5198 [9:54:50<16:51:30, 17.59s/it]                                                        {'loss': 0.8413, 'learning_rate': 1.5475794439444226e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1747/5198 [9:54:50<16:51:30, 17.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1748/5198 [9:55:08<16:46:42, 17.51s/it]                                                        {'loss': 0.797, 'learning_rate': 1.5470579693231432e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1748/5198 [9:55:08<16:46:42, 17.51s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1749/5198 [9:55:25<16:42:20, 17.44s/it]                                                        {'loss': 0.8233, 'learning_rate': 1.5465362823151245e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1749/5198 [9:55:25<16:42:20, 17.44s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1750/5198 [9:55:43<16:50:02, 17.58s/it]                                                        {'loss': 0.854, 'learning_rate': 1.5460143831229026e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1750/5198 [9:55:43<16:50:02, 17.58s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1751/5198 [9:57:08<36:18:17, 37.92s/it]                                                        {'loss': 0.8267, 'learning_rate': 1.545492271949098e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1751/5198 [9:57:08<36:18:17, 37.92s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1752/5198 [9:57:26<30:25:36, 31.79s/it]                                                        {'loss': 0.888, 'learning_rate': 1.544969948996411e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1752/5198 [9:57:26<30:25:36, 31.79s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1753/5198 [9:57:44<26:28:32, 27.67s/it]                                                        {'loss': 0.8354, 'learning_rate': 1.544447414467626e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1753/5198 [9:57:44<26:28:32, 27.67s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1754/5198 [9:58:02<23:37:28, 24.69s/it]                                                        {'loss': 0.3605, 'learning_rate': 1.5439246685656093e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1754/5198 [9:58:02<23:37:28, 24.69s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1755/5198 [9:58:19<21:38:04, 22.62s/it]                                                        {'loss': 0.346, 'learning_rate': 1.5434017114933082e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1755/5198 [9:58:19<21:38:04, 22.62s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1756/5198 [9:58:36<19:58:30, 20.89s/it]                                                        {'loss': 0.893, 'learning_rate': 1.5428785434537527e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1756/5198 [9:58:36<19:58:30, 20.89s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1757/5198 [9:58:54<19:01:00, 19.90s/it]                                                        {'loss': 0.8446, 'learning_rate': 1.542355164650055e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1757/5198 [9:58:54<19:01:00, 19.90s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1758/5198 [9:59:11<18:22:13, 19.22s/it]                                                        {'loss': 0.8751, 'learning_rate': 1.541831575285408e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1758/5198 [9:59:11<18:22:13, 19.22s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1759/5198 [9:59:28<17:42:55, 18.54s/it]                                                        {'loss': 0.3463, 'learning_rate': 1.541307775563088e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1759/5198 [9:59:28<17:42:55, 18.54s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1760/5198 [9:59:47<17:36:10, 18.43s/it]                                                        {'loss': 0.8577, 'learning_rate': 1.540783765686452e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1760/5198 [9:59:47<17:36:10, 18.43s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1761/5198 [10:00:05<17:37:39, 18.46s/it]                                                         {'loss': 0.8242, 'learning_rate': 1.540259545858938e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1761/5198 [10:00:05<17:37:39, 18.46s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1762/5198 [10:00:23<17:27:52, 18.30s/it]                                                         {'loss': 0.8507, 'learning_rate': 1.539735116284067e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1762/5198 [10:00:23<17:27:52, 18.30s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1763/5198 [10:00:41<17:20:43, 18.18s/it]                                                         {'loss': 0.8507, 'learning_rate': 1.53921047716544e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1763/5198 [10:00:41<17:20:43, 18.18s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1764/5198 [10:00:59<17:15:22, 18.09s/it]                                                         {'loss': 0.8948, 'learning_rate': 1.53868562870674e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1764/5198 [10:00:59<17:15:22, 18.09s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1765/5198 [10:01:16<16:58:16, 17.80s/it]                                                         {'loss': 0.8577, 'learning_rate': 1.5381605711117318e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1765/5198 [10:01:16<16:58:16, 17.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1766/5198 [10:01:33<16:49:54, 17.66s/it]                                                         {'loss': 0.8231, 'learning_rate': 1.5376353045842604e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1766/5198 [10:01:33<16:49:54, 17.66s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1767/5198 [10:01:51<16:57:58, 17.80s/it]                                                         {'loss': 0.9035, 'learning_rate': 1.5371098293282526e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1767/5198 [10:01:51<16:57:58, 17.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1768/5198 [10:02:10<17:06:27, 17.96s/it]                                                         {'loss': 0.8692, 'learning_rate': 1.5365841455477158e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1768/5198 [10:02:10<17:06:27, 17.96s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1769/5198 [10:02:28<17:07:26, 17.98s/it]                                                         {'loss': 0.849, 'learning_rate': 1.5360582534467382e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1769/5198 [10:02:28<17:07:26, 17.98s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1770/5198 [10:02:45<16:52:25, 17.72s/it]                                                         {'loss': 0.8812, 'learning_rate': 1.5355321532294897e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1770/5198 [10:02:45<16:52:25, 17.72s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1771/5198 [10:03:03<17:06:47, 17.98s/it]                                                         {'loss': 0.8762, 'learning_rate': 1.5350058451002204e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1771/5198 [10:03:03<17:06:47, 17.98s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1772/5198 [10:03:21<16:59:43, 17.86s/it]                                                         {'loss': 0.859, 'learning_rate': 1.5344793292632614e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1772/5198 [10:03:21<16:59:43, 17.86s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1773/5198 [10:03:38<16:45:06, 17.61s/it]                                                         {'loss': 0.8472, 'learning_rate': 1.533952605923024e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1773/5198 [10:03:38<16:45:06, 17.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1774/5198 [10:03:56<16:54:08, 17.77s/it]                                                         {'loss': 0.8738, 'learning_rate': 1.5334256752840007e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1774/5198 [10:03:56<16:54:08, 17.77s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1775/5198 [10:04:14<17:01:31, 17.91s/it]                                                         {'loss': 0.8294, 'learning_rate': 1.532898537550764e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1775/5198 [10:04:14<17:01:31, 17.91s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 34%|â–ˆâ–ˆâ–ˆâ–      | 1776/5198 [10:05:41<36:42:48, 38.62s/it]                                                         {'loss': 0.8809, 'learning_rate': 1.532371192927966e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1776/5198 [10:05:41<36:42:48, 38.62s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1777/5198 [10:06:00<30:53:46, 32.51s/it]                                                         {'loss': 0.8288, 'learning_rate': 1.5318436416203412e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1777/5198 [10:06:00<30:53:46, 32.51s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1778/5198 [10:06:17<26:42:12, 28.11s/it]                                                         {'loss': 0.8312, 'learning_rate': 1.531315883832703e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1778/5198 [10:06:17<26:42:12, 28.11s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1779/5198 [10:06:35<23:49:24, 25.08s/it]                                                         {'loss': 0.8403, 'learning_rate': 1.530787919769945e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1779/5198 [10:06:35<23:49:24, 25.08s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1780/5198 [10:06:53<21:41:24, 22.85s/it]                                                         {'loss': 0.8405, 'learning_rate': 1.5302597496370408e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1780/5198 [10:06:53<21:41:24, 22.85s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1781/5198 [10:07:10<19:57:19, 21.02s/it]                                                         {'loss': 0.364, 'learning_rate': 1.5297313736390447e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1781/5198 [10:07:10<19:57:19, 21.02s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1782/5198 [10:07:27<18:59:13, 20.01s/it]                                                         {'loss': 0.8711, 'learning_rate': 1.5292027919810898e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1782/5198 [10:07:27<18:59:13, 20.01s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1783/5198 [10:07:45<18:11:08, 19.17s/it]                                                         {'loss': 0.8089, 'learning_rate': 1.52867400486839e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1783/5198 [10:07:45<18:11:08, 19.17s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1784/5198 [10:08:02<17:47:10, 18.76s/it]                                                         {'loss': 0.8244, 'learning_rate': 1.528145012506239e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1784/5198 [10:08:02<17:47:10, 18.76s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1785/5198 [10:08:20<17:32:54, 18.51s/it]                                                         {'loss': 0.8846, 'learning_rate': 1.5276158151000096e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1785/5198 [10:08:20<17:32:54, 18.51s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1786/5198 [10:08:39<17:25:59, 18.39s/it]                                                         {'loss': 0.8117, 'learning_rate': 1.5270864128551542e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1786/5198 [10:08:39<17:25:59, 18.39s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1787/5198 [10:08:56<17:15:47, 18.22s/it]                                                         {'loss': 0.8916, 'learning_rate': 1.5265568059772053e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1787/5198 [10:08:56<17:15:47, 18.22s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1788/5198 [10:09:14<17:08:00, 18.09s/it]                                                         {'loss': 0.8619, 'learning_rate': 1.5260269946717746e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1788/5198 [10:09:14<17:08:00, 18.09s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1789/5198 [10:09:31<16:52:42, 17.82s/it]                                                         {'loss': 0.8483, 'learning_rate': 1.5254969791445526e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1789/5198 [10:09:31<16:52:42, 17.82s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1790/5198 [10:09:50<17:01:33, 17.99s/it]                                                         {'loss': 0.852, 'learning_rate': 1.5249667596013102e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1790/5198 [10:09:50<17:01:33, 17.99s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1791/5198 [10:10:06<16:39:08, 17.60s/it]                                                         {'loss': 0.8765, 'learning_rate': 1.5244363362478967e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1791/5198 [10:10:06<16:39:08, 17.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1792/5198 [10:10:24<16:37:10, 17.57s/it]                                                         {'loss': 0.8182, 'learning_rate': 1.5239057092902404e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1792/5198 [10:10:24<16:37:10, 17.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 1793/5198 [10:10:42<16:41:41, 17.65s/it]                                                         {'loss': 0.9355, 'learning_rate': 1.523374878934349e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 1793/5198 [10:10:42<16:41:41, 17.65s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1794/5198 [10:11:00<16:54:08, 17.88s/it]                                                         {'loss': 0.8256, 'learning_rate': 1.5228438453863095e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1794/5198 [10:11:00<16:54:08, 17.88s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1795/5198 [10:11:18<16:58:31, 17.96s/it]                                                         {'loss': 0.8695, 'learning_rate': 1.522312608852287e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1795/5198 [10:11:18<16:58:31, 17.96s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1796/5198 [10:11:35<16:32:58, 17.51s/it]                                                         {'loss': 0.8029, 'learning_rate': 1.5217811695385263e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1796/5198 [10:11:35<16:32:58, 17.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1797/5198 [10:11:53<16:52:45, 17.87s/it]                                                         {'loss': 0.9086, 'learning_rate': 1.52124952765135e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1797/5198 [10:11:53<16:52:45, 17.87s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1798/5198 [10:12:10<16:32:13, 17.51s/it]                                                         {'loss': 0.8684, 'learning_rate': 1.5207176833971598e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1798/5198 [10:12:10<16:32:13, 17.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1799/5198 [10:12:27<16:28:09, 17.44s/it]                                                         {'loss': 0.8273, 'learning_rate': 1.520185636982436e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1799/5198 [10:12:27<16:28:09, 17.44s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1800/5198 [10:12:45<16:24:40, 17.39s/it]                                                         {'loss': 0.8381, 'learning_rate': 1.5196533886137376e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1800/5198 [10:12:45<16:24:40, 17.39s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 35%|â–ˆâ–ˆâ–ˆâ–      | 1801/5198 [10:14:12<36:10:20, 38.33s/it]                                                         {'loss': 0.8686, 'learning_rate': 1.5191209384977014e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1801/5198 [10:14:12<36:10:20, 38.33s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1802/5198 [10:14:30<30:27:06, 32.28s/it]                                                         {'loss': 0.7816, 'learning_rate': 1.5185882868410431e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1802/5198 [10:14:30<30:27:06, 32.28s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1803/5198 [10:14:48<26:19:59, 27.92s/it]                                                         {'loss': 0.8904, 'learning_rate': 1.5180554338505564e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1803/5198 [10:14:48<26:19:59, 27.92s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1804/5198 [10:15:06<23:39:16, 25.09s/it]                                                         {'loss': 0.7823, 'learning_rate': 1.517522379733113e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1804/5198 [10:15:06<23:39:16, 25.09s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1805/5198 [10:15:24<21:31:48, 22.84s/it]                                                         {'loss': 0.8261, 'learning_rate': 1.5169891246956629e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1805/5198 [10:15:24<21:31:48, 22.84s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1806/5198 [10:15:42<20:11:50, 21.44s/it]                                                         {'loss': 0.8437, 'learning_rate': 1.5164556689452346e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1806/5198 [10:15:42<20:11:50, 21.44s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1807/5198 [10:15:59<19:03:04, 20.23s/it]                                                         {'loss': 0.866, 'learning_rate': 1.5159220126889329e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1807/5198 [10:15:59<19:03:04, 20.23s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1808/5198 [10:16:17<18:13:20, 19.35s/it]                                                         {'loss': 0.8835, 'learning_rate': 1.5153881561339426e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1808/5198 [10:16:17<18:13:20, 19.35s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1809/5198 [10:16:35<17:56:17, 19.06s/it]                                                         {'loss': 0.8545, 'learning_rate': 1.5148540994875242e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1809/5198 [10:16:35<17:56:17, 19.06s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1810/5198 [10:16:52<17:23:56, 18.49s/it]                                                         {'loss': 0.3504, 'learning_rate': 1.5143198429570181e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1810/5198 [10:16:52<17:23:56, 18.49s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1811/5198 [10:17:10<17:07:35, 18.20s/it]                                                         {'loss': 0.8485, 'learning_rate': 1.5137853867498403e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1811/5198 [10:17:10<17:07:35, 18.20s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1812/5198 [10:17:27<16:51:36, 17.93s/it]                                                         {'loss': 0.361, 'learning_rate': 1.5132507310734847e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1812/5198 [10:17:27<16:51:36, 17.93s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1813/5198 [10:17:45<16:50:56, 17.92s/it]                                                         {'loss': 0.851, 'learning_rate': 1.5127158761355241e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1813/5198 [10:17:45<16:50:56, 17.92s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1814/5198 [10:18:03<16:53:07, 17.96s/it]                                                         {'loss': 0.8293, 'learning_rate': 1.512180822143607e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1814/5198 [10:18:03<16:53:07, 17.96s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1815/5198 [10:18:20<16:42:31, 17.78s/it]                                                         {'loss': 0.8471, 'learning_rate': 1.5116455693054594e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1815/5198 [10:18:20<16:42:31, 17.78s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1816/5198 [10:18:37<16:25:22, 17.48s/it]                                                         {'loss': 0.8704, 'learning_rate': 1.5111101178288858e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1816/5198 [10:18:37<16:25:22, 17.48s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1817/5198 [10:18:55<16:34:56, 17.66s/it]                                                         {'loss': 0.8058, 'learning_rate': 1.510574467921766e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1817/5198 [10:18:55<16:34:56, 17.66s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1818/5198 [10:19:12<16:25:04, 17.49s/it]                                                         {'loss': 0.9006, 'learning_rate': 1.5100386197920585e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1818/5198 [10:19:12<16:25:04, 17.49s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 1819/5198 [10:19:30<16:33:12, 17.64s/it]                                                         {'loss': 0.7593, 'learning_rate': 1.5095025736477977e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 1819/5198 [10:19:30<16:33:12, 17.64s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1820/5198 [10:19:48<16:37:54, 17.72s/it]                                                         {'loss': 0.8757, 'learning_rate': 1.5089663296970952e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1820/5198 [10:19:48<16:37:54, 17.72s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1821/5198 [10:20:06<16:34:17, 17.67s/it]                                                         {'loss': 0.3386, 'learning_rate': 1.5084298881481388e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1821/5198 [10:20:06<16:34:17, 17.67s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1822/5198 [10:20:24<16:41:45, 17.80s/it]                                                         {'loss': 0.7866, 'learning_rate': 1.5078932492091942e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1822/5198 [10:20:24<16:41:45, 17.80s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1823/5198 [10:20:42<16:47:16, 17.91s/it]                                                         {'loss': 0.8051, 'learning_rate': 1.5073564130886032e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1823/5198 [10:20:42<16:47:16, 17.91s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1824/5198 [10:20:59<16:28:26, 17.58s/it]                                                         {'loss': 0.8303, 'learning_rate': 1.506819379994784e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1824/5198 [10:20:59<16:28:26, 17.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1825/5198 [10:21:17<16:34:34, 17.69s/it]                                                         {'loss': 0.8313, 'learning_rate': 1.5062821501362308e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1825/5198 [10:21:17<16:34:34, 17.69s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1826/5198 [10:22:49<37:22:49, 39.91s/it]                                                         {'loss': 0.8745, 'learning_rate': 1.5057447237215152e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1826/5198 [10:22:49<37:22:49, 39.91s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1827/5198 [10:23:06<31:07:30, 33.24s/it]                                                         {'loss': 0.3502, 'learning_rate': 1.5052071009592846e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1827/5198 [10:23:06<31:07:30, 33.24s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1828/5198 [10:23:23<26:35:45, 28.41s/it]                                                         {'loss': 0.8754, 'learning_rate': 1.5046692820582625e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1828/5198 [10:23:23<26:35:45, 28.41s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1829/5198 [10:23:41<23:37:34, 25.25s/it]                                                         {'loss': 0.9091, 'learning_rate': 1.504131267227249e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1829/5198 [10:23:41<23:37:34, 25.25s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1830/5198 [10:23:59<21:34:11, 23.06s/it]                                                         {'loss': 0.8209, 'learning_rate': 1.5035930566751198e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1830/5198 [10:23:59<21:34:11, 23.06s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1831/5198 [10:24:17<20:08:48, 21.54s/it]                                                         {'loss': 0.836, 'learning_rate': 1.5030546506108268e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1831/5198 [10:24:17<20:08:48, 21.54s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1832/5198 [10:24:35<19:10:38, 20.51s/it]                                                         {'loss': 0.8187, 'learning_rate': 1.5025160492433976e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1832/5198 [10:24:35<19:10:38, 20.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1833/5198 [10:24:53<18:21:53, 19.65s/it]                                                         {'loss': 0.8422, 'learning_rate': 1.501977252781936e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1833/5198 [10:24:53<18:21:53, 19.65s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1834/5198 [10:25:11<17:51:15, 19.11s/it]                                                         {'loss': 0.9485, 'learning_rate': 1.5014382614356213e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1834/5198 [10:25:11<17:51:15, 19.11s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1835/5198 [10:25:29<17:36:47, 18.85s/it]                                                         {'loss': 0.8114, 'learning_rate': 1.5008990754137088e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1835/5198 [10:25:29<17:36:47, 18.85s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1836/5198 [10:25:46<17:11:36, 18.41s/it]                                                         {'loss': 0.8495, 'learning_rate': 1.5003596949255284e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1836/5198 [10:25:46<17:11:36, 18.41s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1837/5198 [10:26:04<16:58:27, 18.18s/it]                                                         {'loss': 0.3405, 'learning_rate': 1.4998201201804867e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1837/5198 [10:26:04<16:58:27, 18.18s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1838/5198 [10:26:21<16:34:47, 17.76s/it]                                                         {'loss': 0.89, 'learning_rate': 1.499280351388065e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1838/5198 [10:26:21<16:34:47, 17.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1839/5198 [10:26:39<16:33:18, 17.74s/it]                                                         {'loss': 0.8242, 'learning_rate': 1.49874038875782e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1839/5198 [10:26:39<16:33:18, 17.74s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1840/5198 [10:26:56<16:33:49, 17.76s/it]                                                         {'loss': 0.8871, 'learning_rate': 1.498200232499384e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1840/5198 [10:26:56<16:33:49, 17.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1841/5198 [10:27:14<16:31:31, 17.72s/it]                                                         {'loss': 0.8746, 'learning_rate': 1.4976598828224643e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1841/5198 [10:27:14<16:31:31, 17.72s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1842/5198 [10:27:32<16:33:21, 17.76s/it]                                                         {'loss': 0.8522, 'learning_rate': 1.497119339936843e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1842/5198 [10:27:32<16:33:21, 17.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1843/5198 [10:27:50<16:41:19, 17.91s/it]                                                         {'loss': 0.7637, 'learning_rate': 1.4965786040523779e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1843/5198 [10:27:50<16:41:19, 17.91s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1844/5198 [10:28:08<16:40:26, 17.90s/it]                                                         {'loss': 0.8953, 'learning_rate': 1.496037675379001e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1844/5198 [10:28:08<16:40:26, 17.90s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1845/5198 [10:28:26<16:44:26, 17.97s/it]                                                         {'loss': 0.8786, 'learning_rate': 1.4954965541267192e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1845/5198 [10:28:26<16:44:26, 17.97s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1846/5198 [10:28:43<16:33:45, 17.79s/it]                                                         {'loss': 0.8775, 'learning_rate': 1.494955240505615e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1846/5198 [10:28:43<16:33:45, 17.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1847/5198 [10:29:01<16:22:54, 17.60s/it]                                                         {'loss': 0.8333, 'learning_rate': 1.494413734725844e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1847/5198 [10:29:01<16:22:54, 17.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1848/5198 [10:29:18<16:24:17, 17.63s/it]                                                         {'loss': 0.8362, 'learning_rate': 1.4938720369976385e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1848/5198 [10:29:18<16:24:17, 17.63s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1849/5198 [10:29:36<16:28:29, 17.71s/it]                                                         {'loss': 0.8533, 'learning_rate': 1.4933301475313036e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1849/5198 [10:29:36<16:28:29, 17.71s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1850/5198 [10:29:54<16:28:33, 17.72s/it]                                                         {'loss': 0.3699, 'learning_rate': 1.4927880665372197e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1850/5198 [10:29:54<16:28:33, 17.72s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1851/5198 [10:31:24<36:41:09, 39.46s/it]                                                         {'loss': 0.847, 'learning_rate': 1.4922457942258411e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1851/5198 [10:31:24<36:41:09, 39.46s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1852/5198 [10:31:43<30:49:19, 33.16s/it]                                                         {'loss': 0.8167, 'learning_rate': 1.4917033308076967e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1852/5198 [10:31:43<30:49:19, 33.16s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1853/5198 [10:32:00<26:17:12, 28.29s/it]                                                         {'loss': 0.831, 'learning_rate': 1.4911606764933892e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1853/5198 [10:32:00<26:17:12, 28.29s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1854/5198 [10:32:17<23:20:29, 25.13s/it]                                                         {'loss': 0.7997, 'learning_rate': 1.490617831493596e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1854/5198 [10:32:17<23:20:29, 25.13s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1855/5198 [10:32:35<21:18:16, 22.94s/it]                                                         {'loss': 0.8509, 'learning_rate': 1.4900747960190682e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1855/5198 [10:32:35<21:18:16, 22.94s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1856/5198 [10:32:52<19:37:12, 21.13s/it]                                                         {'loss': 0.776, 'learning_rate': 1.489531570280631e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1856/5198 [10:32:52<19:37:12, 21.13s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1857/5198 [10:33:10<18:47:58, 20.26s/it]                                                         {'loss': 0.8253, 'learning_rate': 1.488988154489183e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1857/5198 [10:33:10<18:47:58, 20.26s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1858/5198 [10:33:29<18:15:54, 19.69s/it]                                                         {'loss': 0.8166, 'learning_rate': 1.4884445488556972e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1858/5198 [10:33:29<18:15:54, 19.69s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1859/5198 [10:33:47<17:58:03, 19.37s/it]                                                         {'loss': 0.8039, 'learning_rate': 1.4879007535912198e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1859/5198 [10:33:47<17:58:03, 19.37s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1860/5198 [10:34:05<17:25:54, 18.80s/it]                                                         {'loss': 0.8381, 'learning_rate': 1.4873567689068708e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1860/5198 [10:34:05<17:25:54, 18.80s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1861/5198 [10:34:22<17:07:15, 18.47s/it]                                                         {'loss': 0.8625, 'learning_rate': 1.4868125950138442e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1861/5198 [10:34:22<17:07:15, 18.47s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1862/5198 [10:34:40<16:47:38, 18.12s/it]                                                         {'loss': 0.8283, 'learning_rate': 1.4862682321234064e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1862/5198 [10:34:40<16:47:38, 18.12s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1863/5198 [10:34:57<16:32:23, 17.85s/it]                                                         {'loss': 0.8741, 'learning_rate': 1.4857236804468983e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1863/5198 [10:34:57<16:32:23, 17.85s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1864/5198 [10:35:15<16:27:14, 17.77s/it]                                                         {'loss': 0.7761, 'learning_rate': 1.4851789401957338e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1864/5198 [10:35:15<16:27:14, 17.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1865/5198 [10:35:33<16:36:26, 17.94s/it]                                                         {'loss': 0.8585, 'learning_rate': 1.4846340115813993e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1865/5198 [10:35:33<16:36:26, 17.94s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1866/5198 [10:35:50<16:29:49, 17.82s/it]                                                         {'loss': 0.8358, 'learning_rate': 1.484088894815455e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1866/5198 [10:35:50<16:29:49, 17.82s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1867/5198 [10:36:08<16:28:33, 17.81s/it]                                                         {'loss': 0.827, 'learning_rate': 1.4835435901095341e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1867/5198 [10:36:08<16:28:33, 17.81s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1868/5198 [10:36:24<16:03:02, 17.35s/it]                                                         {'loss': 0.8562, 'learning_rate': 1.4829980976753426e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1868/5198 [10:36:24<16:03:02, 17.35s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1869/5198 [10:36:42<15:59:57, 17.30s/it]                                                         {'loss': 0.8126, 'learning_rate': 1.4824524177246597e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1869/5198 [10:36:42<15:59:57, 17.30s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1870/5198 [10:36:59<15:57:14, 17.26s/it]                                                         {'loss': 0.8813, 'learning_rate': 1.4819065504693365e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1870/5198 [10:36:59<15:57:14, 17.26s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1871/5198 [10:37:16<15:48:32, 17.11s/it]                                                         {'loss': 0.858, 'learning_rate': 1.4813604961212984e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1871/5198 [10:37:16<15:48:32, 17.11s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1872/5198 [10:37:34<16:06:39, 17.44s/it]                                                         {'loss': 0.8355, 'learning_rate': 1.4808142548925417e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1872/5198 [10:37:34<16:06:39, 17.44s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1873/5198 [10:37:52<16:17:08, 17.63s/it]                                                         {'loss': 0.8719, 'learning_rate': 1.4802678269951365e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1873/5198 [10:37:52<16:17:08, 17.63s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1874/5198 [10:38:10<16:26:49, 17.81s/it]                                                         {'loss': 0.8356, 'learning_rate': 1.4797212126412243e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1874/5198 [10:38:10<16:26:49, 17.81s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1875/5198 [10:38:28<16:27:33, 17.83s/it]                                                         {'loss': 0.7999, 'learning_rate': 1.4791744120430202e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1875/5198 [10:38:28<16:27:33, 17.83s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1876/5198 [10:39:56<35:55:30, 38.93s/it]                                                         {'loss': 0.8831, 'learning_rate': 1.4786274254128112e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1876/5198 [10:39:56<35:55:30, 38.93s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1877/5198 [10:40:13<29:53:50, 32.41s/it]                                                         {'loss': 0.8296, 'learning_rate': 1.4780802529629559e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1877/5198 [10:40:13<29:53:50, 32.41s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1878/5198 [10:40:30<25:38:22, 27.80s/it]                                                         {'loss': 0.8666, 'learning_rate': 1.4775328949058856e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1878/5198 [10:40:30<25:38:22, 27.80s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1879/5198 [10:40:48<22:48:54, 24.75s/it]                                                         {'loss': 0.8891, 'learning_rate': 1.4769853514541037e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1879/5198 [10:40:48<22:48:54, 24.75s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1880/5198 [10:41:05<20:43:42, 22.49s/it]                                                         {'loss': 0.8387, 'learning_rate': 1.4764376228201848e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1880/5198 [10:41:05<20:43:42, 22.49s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1881/5198 [10:41:23<19:22:41, 21.03s/it]                                                         {'loss': 0.8569, 'learning_rate': 1.475889709216777e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1881/5198 [10:41:23<19:22:41, 21.03s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1882/5198 [10:41:40<18:22:18, 19.95s/it]                                                         {'loss': 0.8172, 'learning_rate': 1.4753416108565985e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1882/5198 [10:41:40<18:22:18, 19.95s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1883/5198 [10:41:57<17:33:56, 19.08s/it]                                                         {'loss': 0.88, 'learning_rate': 1.47479332795244e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1883/5198 [10:41:57<17:33:56, 19.08s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1884/5198 [10:42:15<17:12:36, 18.70s/it]                                                         {'loss': 0.842, 'learning_rate': 1.4742448607171644e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1884/5198 [10:42:15<17:12:36, 18.70s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1885/5198 [10:42:33<16:57:52, 18.43s/it]                                                         {'loss': 0.8765, 'learning_rate': 1.473696209363705e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1885/5198 [10:42:33<16:57:52, 18.43s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1886/5198 [10:42:51<16:47:19, 18.25s/it]                                                         {'loss': 0.8737, 'learning_rate': 1.4731473741050673e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1886/5198 [10:42:51<16:47:19, 18.25s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1887/5198 [10:43:09<16:40:45, 18.14s/it]                                                         {'loss': 0.8444, 'learning_rate': 1.4725983551543279e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1887/5198 [10:43:09<16:40:45, 18.14s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1888/5198 [10:43:27<16:49:02, 18.29s/it]                                                         {'loss': 0.8517, 'learning_rate': 1.472049152724635e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1888/5198 [10:43:27<16:49:02, 18.29s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1889/5198 [10:43:46<16:51:30, 18.34s/it]                                                         {'loss': 0.8549, 'learning_rate': 1.471499767029208e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1889/5198 [10:43:46<16:51:30, 18.34s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1890/5198 [10:44:04<16:49:41, 18.31s/it]                                                         {'loss': 0.855, 'learning_rate': 1.470950198281337e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1890/5198 [10:44:04<16:49:41, 18.31s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1891/5198 [10:44:22<16:40:59, 18.16s/it]                                                         {'loss': 0.3716, 'learning_rate': 1.470400446694384e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1891/5198 [10:44:22<16:40:59, 18.16s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1892/5198 [10:44:39<16:30:05, 17.97s/it]                                                         {'loss': 0.8001, 'learning_rate': 1.4698505124817811e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1892/5198 [10:44:39<16:30:05, 17.97s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1893/5198 [10:44:56<16:10:14, 17.61s/it]                                                         {'loss': 0.8235, 'learning_rate': 1.4693003958570318e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1893/5198 [10:44:56<16:10:14, 17.61s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1894/5198 [10:45:14<16:15:51, 17.72s/it]                                                         {'loss': 0.8599, 'learning_rate': 1.4687500970337103e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1894/5198 [10:45:14<16:15:51, 17.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1895/5198 [10:45:32<16:13:42, 17.69s/it]                                                         {'loss': 0.8551, 'learning_rate': 1.4681996162254618e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1895/5198 [10:45:32<16:13:42, 17.69s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1896/5198 [10:45:50<16:25:16, 17.90s/it]                                                         {'loss': 0.8151, 'learning_rate': 1.4676489536460015e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1896/5198 [10:45:50<16:25:16, 17.90s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1897/5198 [10:46:08<16:26:00, 17.92s/it]                                                         {'loss': 0.8287, 'learning_rate': 1.467098109509116e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1897/5198 [10:46:08<16:26:00, 17.92s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1898/5198 [10:46:25<16:04:09, 17.53s/it]                                                         {'loss': 0.8357, 'learning_rate': 1.4665470840286614e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1898/5198 [10:46:25<16:04:09, 17.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1899/5198 [10:46:43<16:08:57, 17.62s/it]                                                         {'loss': 0.9018, 'learning_rate': 1.4659958774185654e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1899/5198 [10:46:43<16:08:57, 17.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1900/5198 [10:47:00<16:01:23, 17.49s/it]                                                         {'loss': 0.8836, 'learning_rate': 1.4654444898928249e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1900/5198 [10:47:00<16:01:23, 17.49s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1901/5198 [10:48:30<35:56:56, 39.25s/it]                                                         {'loss': 0.8564, 'learning_rate': 1.4648929216655077e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1901/5198 [10:48:30<35:56:56, 39.25s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1902/5198 [10:48:48<30:15:03, 33.04s/it]                                                         {'loss': 0.8612, 'learning_rate': 1.4643411729507517e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1902/5198 [10:48:48<30:15:03, 33.04s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1903/5198 [10:49:06<26:00:32, 28.42s/it]                                                         {'loss': 0.8261, 'learning_rate': 1.4637892439627644e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1903/5198 [10:49:06<26:00:32, 28.42s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1904/5198 [10:49:24<23:13:41, 25.39s/it]                                                         {'loss': 0.8855, 'learning_rate': 1.4632371349158241e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1904/5198 [10:49:24<23:13:41, 25.39s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1905/5198 [10:49:42<21:13:53, 23.21s/it]                                                         {'loss': 0.8562, 'learning_rate': 1.4626848460242782e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1905/5198 [10:49:42<21:13:53, 23.21s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1906/5198 [10:49:59<19:30:51, 21.34s/it]                                                         {'loss': 0.833, 'learning_rate': 1.4621323775025444e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1906/5198 [10:49:59<19:30:51, 21.34s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1907/5198 [10:50:16<18:11:14, 19.89s/it]                                                         {'loss': 0.8704, 'learning_rate': 1.4615797295651099e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1907/5198 [10:50:16<18:11:14, 19.89s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1908/5198 [10:50:34<17:34:13, 19.23s/it]                                                         {'loss': 0.3684, 'learning_rate': 1.4610269024265317e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1908/5198 [10:50:34<17:34:13, 19.23s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1909/5198 [10:50:51<17:12:19, 18.83s/it]                                                         {'loss': 0.8899, 'learning_rate': 1.4604738963014365e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1909/5198 [10:50:51<17:12:19, 18.83s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1910/5198 [10:51:09<16:52:21, 18.47s/it]                                                         {'loss': 0.867, 'learning_rate': 1.4599207114045202e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1910/5198 [10:51:09<16:52:21, 18.47s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1911/5198 [10:51:26<16:34:52, 18.16s/it]                                                         {'loss': 0.3424, 'learning_rate': 1.4593673479505482e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1911/5198 [10:51:26<16:34:52, 18.16s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1912/5198 [10:51:44<16:31:29, 18.10s/it]                                                         {'loss': 0.7851, 'learning_rate': 1.4588138061543551e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1912/5198 [10:51:44<16:31:29, 18.10s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1913/5198 [10:52:01<16:13:12, 17.78s/it]                                                         {'loss': 0.8874, 'learning_rate': 1.458260086230845e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1913/5198 [10:52:01<16:13:12, 17.78s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1914/5198 [10:52:19<16:05:42, 17.64s/it]                                                         {'loss': 0.8555, 'learning_rate': 1.4577061883949912e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1914/5198 [10:52:19<16:05:42, 17.64s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1915/5198 [10:52:36<15:59:21, 17.53s/it]                                                         {'loss': 0.8792, 'learning_rate': 1.4571521128618358e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1915/5198 [10:52:36<15:59:21, 17.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1916/5198 [10:52:54<16:11:42, 17.76s/it]                                                         {'loss': 0.819, 'learning_rate': 1.4565978598464895e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1916/5198 [10:52:54<16:11:42, 17.76s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1917/5198 [10:53:11<15:54:08, 17.45s/it]                                                         {'loss': 0.8611, 'learning_rate': 1.4560434295641338e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1917/5198 [10:53:11<15:54:08, 17.45s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1918/5198 [10:53:29<15:54:18, 17.46s/it]                                                         {'loss': 0.8235, 'learning_rate': 1.455488822230016e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1918/5198 [10:53:29<15:54:18, 17.46s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1919/5198 [10:53:47<16:04:54, 17.66s/it]                                                         {'loss': 0.8383, 'learning_rate': 1.4549340380594545e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1919/5198 [10:53:47<16:04:54, 17.66s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1920/5198 [10:54:04<15:59:09, 17.56s/it]                                                         {'loss': 0.8252, 'learning_rate': 1.454379077267836e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1920/5198 [10:54:04<15:59:09, 17.56s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1921/5198 [10:54:21<15:50:26, 17.40s/it]                                                         {'loss': 0.8525, 'learning_rate': 1.4538239400706147e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1921/5198 [10:54:21<15:50:26, 17.40s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1922/5198 [10:54:39<15:54:42, 17.49s/it]                                                         {'loss': 0.8407, 'learning_rate': 1.4532686266833143e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1922/5198 [10:54:39<15:54:42, 17.49s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1923/5198 [10:54:55<15:38:30, 17.19s/it]                                                         {'loss': 0.9021, 'learning_rate': 1.4527131373215265e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1923/5198 [10:54:55<15:38:30, 17.19s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1924/5198 [10:55:14<15:58:12, 17.56s/it]                                                         {'loss': 0.8474, 'learning_rate': 1.4521574722009115e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1924/5198 [10:55:14<15:58:12, 17.56s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1925/5198 [10:55:31<16:00:17, 17.60s/it]                                                         {'loss': 0.8633, 'learning_rate': 1.4516016315371974e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1925/5198 [10:55:31<16:00:17, 17.60s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1926/5198 [10:57:03<36:13:41, 39.86s/it]                                                         {'loss': 0.7965, 'learning_rate': 1.4510456155461807e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1926/5198 [10:57:03<36:13:41, 39.86s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1927/5198 [10:57:21<30:07:24, 33.15s/it]                                                         {'loss': 0.9065, 'learning_rate': 1.4504894244437264e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1927/5198 [10:57:21<30:07:24, 33.15s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1928/5198 [10:57:38<25:41:49, 28.29s/it]                                                         {'loss': 0.8276, 'learning_rate': 1.4499330584457667e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1928/5198 [10:57:38<25:41:49, 28.29s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1929/5198 [10:57:56<23:01:13, 25.35s/it]                                                         {'loss': 0.9028, 'learning_rate': 1.4493765177683017e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1929/5198 [10:57:56<23:01:13, 25.35s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1930/5198 [10:58:13<20:37:06, 22.71s/it]                                                         {'loss': 0.8241, 'learning_rate': 1.4488198026274007e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1930/5198 [10:58:13<20:37:06, 22.71s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1931/5198 [10:58:30<19:06:46, 21.06s/it]                                                         {'loss': 0.8678, 'learning_rate': 1.4482629132391985e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1931/5198 [10:58:30<19:06:46, 21.06s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1932/5198 [10:58:48<18:11:33, 20.05s/it]                                                         {'loss': 0.8037, 'learning_rate': 1.4477058498198993e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1932/5198 [10:58:48<18:11:33, 20.05s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1933/5198 [10:59:05<17:24:30, 19.19s/it]                                                         {'loss': 0.8673, 'learning_rate': 1.4471486125857743e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1933/5198 [10:59:05<17:24:30, 19.19s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1934/5198 [10:59:23<17:11:30, 18.96s/it]                                                         {'loss': 0.8578, 'learning_rate': 1.446591201753162e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1934/5198 [10:59:23<17:11:30, 18.96s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1935/5198 [10:59:41<16:51:55, 18.61s/it]                                                         {'loss': 0.3497, 'learning_rate': 1.4460336175384688e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1935/5198 [10:59:41<16:51:55, 18.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1936/5198 [10:59:58<16:27:18, 18.16s/it]                                                         {'loss': 0.911, 'learning_rate': 1.4454758601581675e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1936/5198 [10:59:58<16:27:18, 18.16s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1937/5198 [11:00:16<16:20:26, 18.04s/it]                                                         {'loss': 0.8683, 'learning_rate': 1.4449179298287999e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1937/5198 [11:00:16<16:20:26, 18.04s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1938/5198 [11:00:34<16:14:43, 17.94s/it]                                                         {'loss': 0.8964, 'learning_rate': 1.4443598267669723e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1938/5198 [11:00:34<16:14:43, 17.94s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1939/5198 [11:00:50<15:57:57, 17.64s/it]                                                         {'loss': 0.8486, 'learning_rate': 1.4438015511893602e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1939/5198 [11:00:50<15:57:57, 17.64s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1940/5198 [11:01:09<16:11:55, 17.90s/it]                                                         {'loss': 0.8212, 'learning_rate': 1.4432431033127056e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1940/5198 [11:01:09<16:11:55, 17.90s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1941/5198 [11:01:27<16:21:03, 18.07s/it]                                                         {'loss': 0.8182, 'learning_rate': 1.442684483353817e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1941/5198 [11:01:27<16:21:03, 18.07s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1942/5198 [11:01:46<16:24:08, 18.14s/it]                                                         {'loss': 0.8422, 'learning_rate': 1.4421256915295697e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1942/5198 [11:01:46<16:24:08, 18.14s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1943/5198 [11:02:04<16:21:28, 18.09s/it]                                                         {'loss': 0.8576, 'learning_rate': 1.4415667280569064e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1943/5198 [11:02:04<16:21:28, 18.09s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1944/5198 [11:02:22<16:16:47, 18.01s/it]                                                         {'loss': 0.8296, 'learning_rate': 1.4410075931528356e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1944/5198 [11:02:22<16:16:47, 18.01s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1945/5198 [11:02:39<16:08:50, 17.87s/it]                                                         {'loss': 0.8431, 'learning_rate': 1.4404482870344322e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1945/5198 [11:02:39<16:08:50, 17.87s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1946/5198 [11:02:57<16:01:55, 17.75s/it]                                                         {'loss': 0.8938, 'learning_rate': 1.4398888099188396e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1946/5198 [11:02:57<16:01:55, 17.75s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1947/5198 [11:03:14<15:54:49, 17.62s/it]                                                         {'loss': 0.8399, 'learning_rate': 1.4393291620232646e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1947/5198 [11:03:14<15:54:49, 17.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1948/5198 [11:03:32<15:55:47, 17.65s/it]                                                         {'loss': 0.8089, 'learning_rate': 1.4387693435649826e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1948/5198 [11:03:32<15:55:47, 17.65s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1949/5198 [11:03:49<15:59:48, 17.73s/it]                                                         {'loss': 0.886, 'learning_rate': 1.4382093547613338e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1949/5198 [11:03:50<15:59:48, 17.73s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1950/5198 [11:04:06<15:42:44, 17.42s/it]                                                         {'loss': 0.347, 'learning_rate': 1.4376491958297263e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1950/5198 [11:04:06<15:42:44, 17.42s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1951/5198 [11:05:34<34:40:12, 38.44s/it]                                                         {'loss': 0.8767, 'learning_rate': 1.4370888669876317e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1951/5198 [11:05:34<34:40:12, 38.44s/it]WARNING: tokenization mismatch: 1 vs. 624. (ignored)
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1952/5198 [11:05:52<29:05:40, 32.27s/it]                                                         {'loss': 0.8522, 'learning_rate': 1.4365283684525895e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1952/5198 [11:05:52<29:05:40, 32.27s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1953/5198 [11:06:09<25:07:43, 27.88s/it]                                                         {'loss': 0.8648, 'learning_rate': 1.4359677004422045e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1953/5198 [11:06:09<25:07:43, 27.88s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1954/5198 [11:06:26<22:14:37, 24.68s/it]                                                         {'loss': 0.7996, 'learning_rate': 1.4354068631741476e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1954/5198 [11:06:26<22:14:37, 24.68s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1955/5198 [11:06:44<20:26:01, 22.68s/it]                                                         {'loss': 0.8678, 'learning_rate': 1.4348458568661548e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1955/5198 [11:06:44<20:26:01, 22.68s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1956/5198 [11:07:02<19:06:53, 21.23s/it]                                                         {'loss': 0.8696, 'learning_rate': 1.434284681736028e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1956/5198 [11:07:02<19:06:53, 21.23s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1957/5198 [11:07:19<17:59:38, 19.99s/it]                                                         {'loss': 0.3402, 'learning_rate': 1.4337233380016354e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1957/5198 [11:07:19<17:59:38, 19.99s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1958/5198 [11:07:38<17:32:24, 19.49s/it]                                                         {'loss': 0.9406, 'learning_rate': 1.433161825880909e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1958/5198 [11:07:38<17:32:24, 19.49s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1959/5198 [11:07:56<17:11:34, 19.11s/it]                                                         {'loss': 0.8322, 'learning_rate': 1.432600145591848e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1959/5198 [11:07:56<17:11:34, 19.11s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1960/5198 [11:08:14<17:02:24, 18.95s/it]                                                         {'loss': 0.8381, 'learning_rate': 1.4320382973525151e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1960/5198 [11:08:14<17:02:24, 18.95s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1961/5198 [11:08:33<16:52:09, 18.76s/it]                                                         {'loss': 0.8536, 'learning_rate': 1.43147628138104e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1961/5198 [11:08:33<16:52:09, 18.76s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1962/5198 [11:08:50<16:25:38, 18.28s/it]                                                         {'loss': 0.7851, 'learning_rate': 1.4309140978956161e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1962/5198 [11:08:50<16:25:38, 18.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1963/5198 [11:09:08<16:15:03, 18.08s/it]                                                         {'loss': 0.8583, 'learning_rate': 1.430351747114503e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1963/5198 [11:09:08<16:15:03, 18.08s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1964/5198 [11:09:24<15:53:15, 17.69s/it]                                                         {'loss': 0.846, 'learning_rate': 1.429789229256024e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1964/5198 [11:09:24<15:53:15, 17.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1965/5198 [11:09:42<15:45:49, 17.55s/it]                                                         {'loss': 0.8663, 'learning_rate': 1.429226544538568e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1965/5198 [11:09:42<15:45:49, 17.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1966/5198 [11:10:00<15:53:22, 17.70s/it]                                                         {'loss': 0.833, 'learning_rate': 1.4286636931805887e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1966/5198 [11:10:00<15:53:22, 17.70s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1967/5198 [11:10:16<15:39:29, 17.45s/it]                                                         {'loss': 0.3469, 'learning_rate': 1.4281006754006045e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1967/5198 [11:10:16<15:39:29, 17.45s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1968/5198 [11:10:34<15:39:14, 17.45s/it]                                                         {'loss': 0.8025, 'learning_rate': 1.427537491417198e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1968/5198 [11:10:34<15:39:14, 17.45s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1969/5198 [11:10:52<15:48:35, 17.63s/it]                                                         {'loss': 0.8266, 'learning_rate': 1.426974141449017e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1969/5198 [11:10:52<15:48:35, 17.63s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1970/5198 [11:11:10<15:50:59, 17.68s/it]                                                         {'loss': 0.8498, 'learning_rate': 1.4264106257147732e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1970/5198 [11:11:10<15:50:59, 17.68s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1971/5198 [11:11:27<15:47:10, 17.61s/it]                                                         {'loss': 0.3303, 'learning_rate': 1.4258469444332423e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1971/5198 [11:11:27<15:47:10, 17.61s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1972/5198 [11:11:45<15:52:51, 17.72s/it]                                                         {'loss': 0.8582, 'learning_rate': 1.4252830978232658e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1972/5198 [11:11:45<15:52:51, 17.72s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1973/5198 [11:12:03<15:59:04, 17.84s/it]                                                         {'loss': 0.8375, 'learning_rate': 1.4247190861037474e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1973/5198 [11:12:03<15:59:04, 17.84s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1974/5198 [11:12:20<15:45:57, 17.60s/it]                                                         {'loss': 0.8632, 'learning_rate': 1.4241549094936567e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1974/5198 [11:12:20<15:45:57, 17.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1975/5198 [11:12:38<15:45:20, 17.60s/it]                                                         {'loss': 0.7894, 'learning_rate': 1.4235905682120255e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1975/5198 [11:12:38<15:45:20, 17.60s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1976/5198 [11:14:06<34:43:47, 38.80s/it]                                                         {'loss': 0.8532, 'learning_rate': 1.4230260624779512e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1976/5198 [11:14:06<34:43:47, 38.80s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1977/5198 [11:14:24<29:05:13, 32.51s/it]                                                         {'loss': 0.8986, 'learning_rate': 1.4224613925105947e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1977/5198 [11:14:24<29:05:13, 32.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1978/5198 [11:14:42<25:12:55, 28.19s/it]                                                         {'loss': 0.7944, 'learning_rate': 1.4218965585291792e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1978/5198 [11:14:42<25:12:55, 28.19s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1979/5198 [11:15:00<22:20:01, 24.98s/it]                                                         {'loss': 0.8881, 'learning_rate': 1.4213315607529939e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1979/5198 [11:15:00<22:20:01, 24.98s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1980/5198 [11:15:18<20:30:27, 22.94s/it]                                                         {'loss': 0.8845, 'learning_rate': 1.4207663994013896e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1980/5198 [11:15:18<20:30:27, 22.94s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1981/5198 [11:15:36<19:14:47, 21.54s/it]                                                         {'loss': 0.8112, 'learning_rate': 1.4202010746937815e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1981/5198 [11:15:36<19:14:47, 21.54s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1982/5198 [11:15:54<18:14:24, 20.42s/it]                                                         {'loss': 0.8594, 'learning_rate': 1.4196355868496485e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1982/5198 [11:15:54<18:14:24, 20.42s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1983/5198 [11:16:12<17:30:41, 19.61s/it]                                                         {'loss': 0.8713, 'learning_rate': 1.4190699360885323e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1983/5198 [11:16:12<17:30:41, 19.61s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1984/5198 [11:16:29<16:57:46, 19.00s/it]                                                         {'loss': 0.822, 'learning_rate': 1.4185041226300376e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1984/5198 [11:16:29<16:57:46, 19.00s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1985/5198 [11:16:47<16:41:54, 18.71s/it]                                                         {'loss': 0.8757, 'learning_rate': 1.4179381466938332e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1985/5198 [11:16:47<16:41:54, 18.71s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1986/5198 [11:17:04<16:16:52, 18.25s/it]                                                         {'loss': 0.8715, 'learning_rate': 1.4173720084996501e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1986/5198 [11:17:04<16:16:52, 18.25s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1987/5198 [11:17:22<16:00:04, 17.94s/it]                                                         {'loss': 0.9231, 'learning_rate': 1.4168057082672828e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1987/5198 [11:17:22<16:00:04, 17.94s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1988/5198 [11:17:40<16:02:58, 18.00s/it]                                                         {'loss': 0.7867, 'learning_rate': 1.4162392462165884e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1988/5198 [11:17:40<16:02:58, 18.00s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1989/5198 [11:17:58<16:01:34, 17.98s/it]                                                         {'loss': 0.869, 'learning_rate': 1.4156726225674874e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1989/5198 [11:17:58<16:01:34, 17.98s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1990/5198 [11:18:15<15:53:47, 17.84s/it]                                                         {'loss': 0.8289, 'learning_rate': 1.415105837539962e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1990/5198 [11:18:15<15:53:47, 17.84s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1991/5198 [11:18:34<16:01:35, 17.99s/it]                                                         {'loss': 0.7684, 'learning_rate': 1.414538891354058e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1991/5198 [11:18:34<16:01:35, 17.99s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1992/5198 [11:18:51<15:47:42, 17.74s/it]                                                         {'loss': 0.3755, 'learning_rate': 1.4139717842298835e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1992/5198 [11:18:51<15:47:42, 17.74s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1993/5198 [11:19:08<15:40:57, 17.62s/it]                                                         {'loss': 0.8664, 'learning_rate': 1.4134045163876086e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1993/5198 [11:19:08<15:40:57, 17.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1994/5198 [11:19:25<15:32:25, 17.46s/it]                                                         {'loss': 0.8521, 'learning_rate': 1.4128370880474667e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1994/5198 [11:19:25<15:32:25, 17.46s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1995/5198 [11:19:42<15:23:15, 17.29s/it]                                                         {'loss': 0.8479, 'learning_rate': 1.412269499429753e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1995/5198 [11:19:42<15:23:15, 17.29s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1996/5198 [11:20:00<15:34:25, 17.51s/it]                                                         {'loss': 0.8555, 'learning_rate': 1.4117017507548244e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1996/5198 [11:20:00<15:34:25, 17.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1997/5198 [11:20:18<15:40:50, 17.64s/it]                                                         {'loss': 0.8572, 'learning_rate': 1.4111338422431013e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1997/5198 [11:20:18<15:40:50, 17.64s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1998/5198 [11:20:35<15:37:45, 17.58s/it]                                                         {'loss': 0.351, 'learning_rate': 1.4105657741150648e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1998/5198 [11:20:35<15:37:45, 17.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1999/5198 [11:20:54<15:54:25, 17.90s/it]                                                         {'loss': 0.8645, 'learning_rate': 1.4099975465912584e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1999/5198 [11:20:54<15:54:25, 17.90s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 2000/5198 [11:21:12<15:48:18, 17.79s/it]                                                         {'loss': 0.8647, 'learning_rate': 1.4094291598922877e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 2000/5198 [11:21:12<15:48:18, 17.79s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 2001/5198 [11:22:40<34:40:53, 39.05s/it]                                                         {'loss': 0.8615, 'learning_rate': 1.40886061423882e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 2001/5198 [11:22:40<34:40:53, 39.05s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2002/5198 [11:22:58<28:58:25, 32.64s/it]                                                         {'loss': 0.3162, 'learning_rate': 1.4082919098515846e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2002/5198 [11:22:58<28:58:25, 32.64s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2003/5198 [11:23:15<24:46:07, 27.91s/it]                                                         {'loss': 0.8582, 'learning_rate': 1.407723046951372e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2003/5198 [11:23:15<24:46:07, 27.91s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2004/5198 [11:23:31<21:45:40, 24.53s/it]                                                         {'loss': 0.8627, 'learning_rate': 1.4071540257590341e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2004/5198 [11:23:31<21:45:40, 24.53s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2005/5198 [11:23:48<19:32:38, 22.04s/it]                                                         {'loss': 0.3675, 'learning_rate': 1.4065848464954848e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2005/5198 [11:23:48<19:32:38, 22.04s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2006/5198 [11:24:06<18:33:14, 20.93s/it]                                                         {'loss': 0.822, 'learning_rate': 1.4060155093816988e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2006/5198 [11:24:06<18:33:14, 20.93s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2007/5198 [11:24:23<17:29:30, 19.73s/it]                                                         {'loss': 0.8382, 'learning_rate': 1.4054460146387124e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2007/5198 [11:24:23<17:29:30, 19.73s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2008/5198 [11:24:40<16:48:18, 18.96s/it]                                                         {'loss': 0.8764, 'learning_rate': 1.4048763624876233e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2008/5198 [11:24:40<16:48:18, 18.96s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2009/5198 [11:24:58<16:23:10, 18.50s/it]                                                         {'loss': 0.3585, 'learning_rate': 1.4043065531495904e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2009/5198 [11:24:58<16:23:10, 18.50s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2010/5198 [11:25:15<16:10:14, 18.26s/it]                                                         {'loss': 0.8314, 'learning_rate': 1.4037365868458325e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2010/5198 [11:25:15<16:10:14, 18.26s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2011/5198 [11:25:33<15:55:20, 17.99s/it]                                                         {'loss': 0.7533, 'learning_rate': 1.4031664637976305e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2011/5198 [11:25:33<15:55:20, 17.99s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2012/5198 [11:25:50<15:39:12, 17.69s/it]                                                         {'loss': 0.8777, 'learning_rate': 1.402596184226326e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2012/5198 [11:25:50<15:39:12, 17.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2013/5198 [11:26:08<15:45:36, 17.81s/it]                                                         {'loss': 0.846, 'learning_rate': 1.4020257483533208e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2013/5198 [11:26:08<15:45:36, 17.81s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2014/5198 [11:26:25<15:39:35, 17.71s/it]                                                         {'loss': 0.8489, 'learning_rate': 1.401455156400078e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 2014/5198 [11:26:25<15:39:35, 17.71s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2015/5198 [11:26:44<15:53:01, 17.96s/it]                                                         {'loss': 0.8278, 'learning_rate': 1.400884408588121e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2015/5198 [11:26:44<15:53:01, 17.96s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2016/5198 [11:27:01<15:49:05, 17.90s/it]                                                         {'loss': 0.842, 'learning_rate': 1.400313505139034e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2016/5198 [11:27:01<15:49:05, 17.90s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2017/5198 [11:27:20<15:56:40, 18.04s/it]                                                         {'loss': 0.8824, 'learning_rate': 1.3997424462744607e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2017/5198 [11:27:20<15:56:40, 18.04s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2018/5198 [11:27:37<15:39:30, 17.73s/it]                                                         {'loss': 0.8777, 'learning_rate': 1.3991712322161065e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2018/5198 [11:27:37<15:39:30, 17.73s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2019/5198 [11:27:55<15:53:43, 18.00s/it]                                                         {'loss': 0.8416, 'learning_rate': 1.3985998631857359e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2019/5198 [11:27:55<15:53:43, 18.00s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2020/5198 [11:28:14<16:05:05, 18.22s/it]                                                         {'loss': 0.7946, 'learning_rate': 1.398028339405174e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2020/5198 [11:28:14<16:05:05, 18.22s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2021/5198 [11:28:32<15:56:58, 18.07s/it]                                                         {'loss': 0.8733, 'learning_rate': 1.3974566610963068e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2021/5198 [11:28:32<15:56:58, 18.07s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2022/5198 [11:28:49<15:45:36, 17.86s/it]                                                         {'loss': 0.8096, 'learning_rate': 1.3968848284810785e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2022/5198 [11:28:49<15:45:36, 17.86s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2023/5198 [11:29:07<15:36:08, 17.69s/it]                                                         {'loss': 0.8213, 'learning_rate': 1.3963128417814951e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2023/5198 [11:29:07<15:36:08, 17.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2024/5198 [11:29:24<15:37:18, 17.72s/it]                                                         {'loss': 0.842, 'learning_rate': 1.3957407012196204e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2024/5198 [11:29:24<15:37:18, 17.72s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2025/5198 [11:29:43<15:48:27, 17.93s/it]                                                         {'loss': 0.7895, 'learning_rate': 1.3951684070175802e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2025/5198 [11:29:43<15:48:27, 17.93s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2026/5198 [11:31:11<34:17:34, 38.92s/it]                                                         {'loss': 0.8686, 'learning_rate': 1.3945959593975582e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2026/5198 [11:31:11<34:17:34, 38.92s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2027/5198 [11:31:28<28:36:19, 32.48s/it]                                                         {'loss': 0.8364, 'learning_rate': 1.3940233585817984e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2027/5198 [11:31:28<28:36:19, 32.48s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2028/5198 [11:31:46<24:39:56, 28.01s/it]                                                         {'loss': 0.8279, 'learning_rate': 1.3934506047926042e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2028/5198 [11:31:46<24:39:56, 28.01s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2029/5198 [11:32:03<21:43:56, 24.69s/it]                                                         {'loss': 0.829, 'learning_rate': 1.3928776982523384e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2029/5198 [11:32:03<21:43:56, 24.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2030/5198 [11:32:20<19:53:09, 22.60s/it]                                                         {'loss': 0.8416, 'learning_rate': 1.3923046391834229e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2030/5198 [11:32:20<19:53:09, 22.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2031/5198 [11:32:38<18:29:36, 21.02s/it]                                                         {'loss': 0.9032, 'learning_rate': 1.3917314278083391e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2031/5198 [11:32:38<18:29:36, 21.02s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2032/5198 [11:32:55<17:37:10, 20.04s/it]                                                         {'loss': 0.8504, 'learning_rate': 1.3911580643496272e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2032/5198 [11:32:55<17:37:10, 20.04s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2033/5198 [11:33:13<17:02:05, 19.38s/it]                                                         {'loss': 0.8827, 'learning_rate': 1.3905845490298867e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2033/5198 [11:33:13<17:02:05, 19.38s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2034/5198 [11:33:32<16:47:57, 19.11s/it]                                                         {'loss': 0.7665, 'learning_rate': 1.390010882071776e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2034/5198 [11:33:32<16:47:57, 19.11s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2035/5198 [11:33:50<16:34:43, 18.87s/it]                                                         {'loss': 0.8339, 'learning_rate': 1.3894370636980128e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2035/5198 [11:33:50<16:34:43, 18.87s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2036/5198 [11:34:07<15:56:59, 18.16s/it]                                                         {'loss': 0.9094, 'learning_rate': 1.3888630941313728e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2036/5198 [11:34:07<15:56:59, 18.16s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2037/5198 [11:34:26<16:08:33, 18.38s/it]                                                         {'loss': 0.8352, 'learning_rate': 1.3882889735946901e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2037/5198 [11:34:26<16:08:33, 18.38s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2038/5198 [11:34:43<15:58:55, 18.21s/it]                                                         {'loss': 0.8265, 'learning_rate': 1.3877147023108592e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2038/5198 [11:34:43<15:58:55, 18.21s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2039/5198 [11:35:01<15:51:44, 18.08s/it]                                                         {'loss': 0.3751, 'learning_rate': 1.3871402805028314e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2039/5198 [11:35:01<15:51:44, 18.08s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2040/5198 [11:35:19<15:46:32, 17.98s/it]                                                         {'loss': 0.8794, 'learning_rate': 1.3865657083936167e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2040/5198 [11:35:19<15:46:32, 17.98s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2041/5198 [11:35:35<15:20:09, 17.49s/it]                                                         {'loss': 0.2962, 'learning_rate': 1.3859909862062844e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2041/5198 [11:35:35<15:20:09, 17.49s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2042/5198 [11:35:53<15:20:47, 17.51s/it]                                                         {'loss': 0.8837, 'learning_rate': 1.385416114163961e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2042/5198 [11:35:53<15:20:47, 17.51s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2043/5198 [11:36:09<15:00:38, 17.13s/it]                                                         {'loss': 0.8493, 'learning_rate': 1.3848410924898321e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2043/5198 [11:36:09<15:00:38, 17.13s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2044/5198 [11:36:26<15:04:19, 17.20s/it]                                                         {'loss': 0.858, 'learning_rate': 1.3842659214071406e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2044/5198 [11:36:26<15:04:19, 17.20s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2045/5198 [11:36:45<15:22:32, 17.56s/it]                                                         {'loss': 0.8396, 'learning_rate': 1.3836906011391878e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2045/5198 [11:36:45<15:22:32, 17.56s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2046/5198 [11:37:03<15:25:40, 17.62s/it]                                                         {'loss': 0.8, 'learning_rate': 1.3831151319093323e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2046/5198 [11:37:03<15:25:40, 17.62s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2047/5198 [11:37:20<15:30:42, 17.72s/it]                                                         {'loss': 0.8614, 'learning_rate': 1.382539513940992e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2047/5198 [11:37:20<15:30:42, 17.72s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2048/5198 [11:37:38<15:20:23, 17.53s/it]                                                         {'loss': 0.8115, 'learning_rate': 1.3819637474576411e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2048/5198 [11:37:38<15:20:23, 17.53s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2049/5198 [11:37:55<15:11:33, 17.37s/it]                                                         {'loss': 0.9033, 'learning_rate': 1.381387832682812e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2049/5198 [11:37:55<15:11:33, 17.37s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2050/5198 [11:38:11<15:04:22, 17.24s/it]                                                         {'loss': 0.8352, 'learning_rate': 1.380811769840095e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2050/5198 [11:38:11<15:04:22, 17.24s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2051/5198 [11:39:37<33:02:02, 37.79s/it]                                                         {'loss': 0.8823, 'learning_rate': 1.3802355591531366e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2051/5198 [11:39:37<33:02:02, 37.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2052/5198 [11:39:55<27:52:33, 31.90s/it]                                                         {'loss': 0.8441, 'learning_rate': 1.3796592008456427e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2052/5198 [11:39:55<27:52:33, 31.90s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2053/5198 [11:40:13<24:07:44, 27.62s/it]                                                         {'loss': 0.369, 'learning_rate': 1.3790826951413747e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 2053/5198 [11:40:13<24:07:44, 27.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2054/5198 [11:40:31<21:33:50, 24.69s/it]                                                         {'loss': 0.8358, 'learning_rate': 1.3785060422641526e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2054/5198 [11:40:31<21:33:50, 24.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2055/5198 [11:40:49<19:52:45, 22.77s/it]                                                         {'loss': 0.8469, 'learning_rate': 1.3779292424378521e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2055/5198 [11:40:49<19:52:45, 22.77s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2056/5198 [11:41:07<18:31:07, 21.22s/it]                                                         {'loss': 0.8452, 'learning_rate': 1.3773522958864076e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2056/5198 [11:41:07<18:31:07, 21.22s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2057/5198 [11:41:24<17:32:04, 20.10s/it]                                                         {'loss': 0.8091, 'learning_rate': 1.3767752028338091e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2057/5198 [11:41:24<17:32:04, 20.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2058/5198 [11:41:41<16:37:08, 19.05s/it]                                                         {'loss': 0.3352, 'learning_rate': 1.376197963504104e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2058/5198 [11:41:41<16:37:08, 19.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2059/5198 [11:41:59<16:24:27, 18.82s/it]                                                         {'loss': 0.8431, 'learning_rate': 1.3756205781213965e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2059/5198 [11:41:59<16:24:27, 18.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2060/5198 [11:42:16<15:53:31, 18.23s/it]                                                         {'loss': 0.8436, 'learning_rate': 1.375043046909848e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2060/5198 [11:42:16<15:53:31, 18.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2061/5198 [11:42:34<15:50:12, 18.17s/it]                                                         {'loss': 0.8691, 'learning_rate': 1.3744653700936752e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2061/5198 [11:42:34<15:50:12, 18.17s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2062/5198 [11:42:52<15:48:03, 18.14s/it]                                                         {'loss': 0.8602, 'learning_rate': 1.3738875478971526e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2062/5198 [11:42:52<15:48:03, 18.14s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2063/5198 [11:43:10<15:47:01, 18.12s/it]                                                         {'loss': 0.8259, 'learning_rate': 1.3733095805446107e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2063/5198 [11:43:10<15:47:01, 18.12s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2064/5198 [11:43:29<15:50:33, 18.20s/it]                                                         {'loss': 0.8411, 'learning_rate': 1.372731468260436e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2064/5198 [11:43:29<15:50:33, 18.20s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2065/5198 [11:43:46<15:35:13, 17.91s/it]                                                         {'loss': 0.8315, 'learning_rate': 1.372153211269072e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2065/5198 [11:43:46<15:35:13, 17.91s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2066/5198 [11:44:04<15:33:47, 17.89s/it]                                                         {'loss': 0.8462, 'learning_rate': 1.3715748097950176e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2066/5198 [11:44:04<15:33:47, 17.89s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2067/5198 [11:44:22<15:33:25, 17.89s/it]                                                         {'loss': 0.8429, 'learning_rate': 1.3709962640628284e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2067/5198 [11:44:22<15:33:25, 17.89s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2068/5198 [11:44:39<15:22:00, 17.67s/it]                                                         {'loss': 0.8267, 'learning_rate': 1.3704175742971158e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2068/5198 [11:44:39<15:22:00, 17.67s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2069/5198 [11:44:56<15:10:46, 17.46s/it]                                                         {'loss': 0.8093, 'learning_rate': 1.369838740722547e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2069/5198 [11:44:56<15:10:46, 17.46s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2070/5198 [11:45:13<15:11:13, 17.48s/it]                                                         {'loss': 0.3445, 'learning_rate': 1.3692597635638452e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2070/5198 [11:45:13<15:11:13, 17.48s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2071/5198 [11:45:30<15:00:10, 17.27s/it]                                                         {'loss': 0.3535, 'learning_rate': 1.368680643045789e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2071/5198 [11:45:30<15:00:10, 17.27s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2072/5198 [11:45:48<15:09:39, 17.46s/it]                                                         {'loss': 0.861, 'learning_rate': 1.3681013793932132e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2072/5198 [11:45:48<15:09:39, 17.46s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2073/5198 [11:46:06<15:21:06, 17.69s/it]                                                         {'loss': 0.7891, 'learning_rate': 1.3675219728310076e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2073/5198 [11:46:06<15:21:06, 17.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2074/5198 [11:46:23<15:12:46, 17.53s/it]                                                         {'loss': 0.8598, 'learning_rate': 1.3669424235841185e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2074/5198 [11:46:23<15:12:46, 17.53s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2075/5198 [11:46:41<15:19:49, 17.67s/it]                                                         {'loss': 0.8451, 'learning_rate': 1.3663627318775459e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2075/5198 [11:46:41<15:19:49, 17.67s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2076/5198 [11:48:07<33:03:35, 38.12s/it]                                                         {'loss': 0.8183, 'learning_rate': 1.3657828979363468e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2076/5198 [11:48:07<33:03:35, 38.12s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2077/5198 [11:48:25<27:48:23, 32.07s/it]                                                         {'loss': 0.8111, 'learning_rate': 1.3652029219856324e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2077/5198 [11:48:25<27:48:23, 32.07s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2078/5198 [11:48:43<24:02:02, 27.73s/it]                                                         {'loss': 0.8504, 'learning_rate': 1.3646228042505694e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2078/5198 [11:48:43<24:02:02, 27.73s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2079/5198 [11:49:00<21:26:50, 24.76s/it]                                                         {'loss': 0.8715, 'learning_rate': 1.3640425449563793e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 2079/5198 [11:49:00<21:26:50, 24.76s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2080/5198 [11:49:18<19:34:45, 22.61s/it]                                                         {'loss': 0.8366, 'learning_rate': 1.3634621443283389e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2080/5198 [11:49:18<19:34:45, 22.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2081/5198 [11:49:35<18:13:40, 21.05s/it]                                                         {'loss': 0.781, 'learning_rate': 1.36288160259178e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2081/5198 [11:49:35<18:13:40, 21.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2082/5198 [11:49:53<17:24:04, 20.10s/it]                                                         {'loss': 0.8508, 'learning_rate': 1.3623009199720882e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2082/5198 [11:49:53<17:24:04, 20.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2083/5198 [11:50:11<16:41:24, 19.29s/it]                                                         {'loss': 0.8042, 'learning_rate': 1.3617200966947053e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2083/5198 [11:50:11<16:41:24, 19.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2084/5198 [11:50:28<16:09:54, 18.69s/it]                                                         {'loss': 0.8379, 'learning_rate': 1.3611391329851262e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2084/5198 [11:50:28<16:09:54, 18.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2085/5198 [11:50:45<15:48:51, 18.29s/it]                                                         {'loss': 0.8425, 'learning_rate': 1.3605580290689013e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2085/5198 [11:50:45<15:48:51, 18.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2086/5198 [11:51:03<15:31:35, 17.96s/it]                                                         {'loss': 0.8428, 'learning_rate': 1.3599767851716353e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2086/5198 [11:51:03<15:31:35, 17.96s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2087/5198 [11:51:21<15:31:59, 17.97s/it]                                                         {'loss': 0.8695, 'learning_rate': 1.3593954015189867e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2087/5198 [11:51:21<15:31:59, 17.97s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2088/5198 [11:51:38<15:23:54, 17.82s/it]                                                         {'loss': 0.8678, 'learning_rate': 1.3588138783366692e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2088/5198 [11:51:38<15:23:54, 17.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2089/5198 [11:51:56<15:22:44, 17.81s/it]                                                         {'loss': 0.8365, 'learning_rate': 1.3582322158504495e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2089/5198 [11:51:56<15:22:44, 17.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2090/5198 [11:52:13<15:12:00, 17.61s/it]                                                         {'loss': 0.8361, 'learning_rate': 1.3576504142861496e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2090/5198 [11:52:13<15:12:00, 17.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2091/5198 [11:52:30<15:02:35, 17.43s/it]                                                         {'loss': 0.8557, 'learning_rate': 1.3570684738696444e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2091/5198 [11:52:30<15:02:35, 17.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2092/5198 [11:52:48<15:07:14, 17.53s/it]                                                         {'loss': 0.8858, 'learning_rate': 1.3564863948268631e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2092/5198 [11:52:48<15:07:14, 17.53s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2093/5198 [11:53:05<14:59:04, 17.37s/it]                                                         {'loss': 0.8728, 'learning_rate': 1.3559041773837898e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2093/5198 [11:53:05<14:59:04, 17.37s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2094/5198 [11:53:23<15:07:36, 17.54s/it]                                                         {'loss': 0.8111, 'learning_rate': 1.3553218217664603e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2094/5198 [11:53:23<15:07:36, 17.54s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2095/5198 [11:53:40<15:07:26, 17.55s/it]                                                         {'loss': 0.8618, 'learning_rate': 1.3547393282009656e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2095/5198 [11:53:40<15:07:26, 17.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2096/5198 [11:53:59<15:23:24, 17.86s/it]                                                         {'loss': 0.8504, 'learning_rate': 1.3541566969134496e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2096/5198 [11:53:59<15:23:24, 17.86s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2097/5198 [11:54:18<15:36:24, 18.12s/it]                                                         {'loss': 0.8066, 'learning_rate': 1.3535739281301102e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2097/5198 [11:54:18<15:36:24, 18.12s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2098/5198 [11:54:35<15:24:21, 17.89s/it]                                                         {'loss': 0.8581, 'learning_rate': 1.3529910220771975e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2098/5198 [11:54:35<15:24:21, 17.89s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2099/5198 [11:54:52<15:06:38, 17.55s/it]                                                         {'loss': 0.8931, 'learning_rate': 1.3524079789810163e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2099/5198 [11:54:52<15:06:38, 17.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2100/5198 [11:55:10<15:12:55, 17.68s/it]                                                         {'loss': 0.8608, 'learning_rate': 1.3518247990679241e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2100/5198 [11:55:10<15:12:55, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2101/5198 [11:56:38<33:31:27, 38.97s/it]                                                         {'loss': 0.8101, 'learning_rate': 1.3512414825643312e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2101/5198 [11:56:38<33:31:27, 38.97s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2102/5198 [11:56:57<28:22:17, 32.99s/it]                                                         {'loss': 0.8059, 'learning_rate': 1.3506580296967011e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2102/5198 [11:56:57<28:22:17, 32.99s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2103/5198 [11:57:15<24:27:18, 28.45s/it]                                                         {'loss': 0.8095, 'learning_rate': 1.3500744406915505e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2103/5198 [11:57:15<24:27:18, 28.45s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2104/5198 [11:57:34<21:58:30, 25.57s/it]                                                         {'loss': 0.9191, 'learning_rate': 1.3494907157754485e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2104/5198 [11:57:34<21:58:30, 25.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2105/5198 [11:57:52<19:56:17, 23.21s/it]                                                         {'loss': 0.9065, 'learning_rate': 1.348906855175017e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2105/5198 [11:57:52<19:56:17, 23.21s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2106/5198 [11:58:09<18:29:35, 21.53s/it]                                                         {'loss': 0.8235, 'learning_rate': 1.3483228591169315e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2106/5198 [11:58:09<18:29:35, 21.53s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2107/5198 [11:58:27<17:22:02, 20.23s/it]                                                         {'loss': 0.8282, 'learning_rate': 1.347738727827919e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2107/5198 [11:58:27<17:22:02, 20.23s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2108/5198 [11:58:44<16:39:08, 19.40s/it]                                                         {'loss': 0.8018, 'learning_rate': 1.3471544615347591e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2108/5198 [11:58:44<16:39:08, 19.40s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2109/5198 [11:59:01<16:08:34, 18.81s/it]                                                         {'loss': 0.8809, 'learning_rate': 1.3465700604642847e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2109/5198 [11:59:01<16:08:34, 18.81s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2110/5198 [11:59:18<15:40:00, 18.26s/it]                                                         {'loss': 0.8274, 'learning_rate': 1.34598552484338e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2110/5198 [11:59:18<15:40:00, 18.26s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2111/5198 [11:59:36<15:31:18, 18.10s/it]                                                         {'loss': 0.8013, 'learning_rate': 1.3454008548989816e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2111/5198 [11:59:36<15:31:18, 18.10s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2112/5198 [11:59:54<15:19:48, 17.88s/it]                                                         {'loss': 0.8509, 'learning_rate': 1.3448160508580789e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2112/5198 [11:59:54<15:19:48, 17.88s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2113/5198 [12:00:11<15:12:50, 17.75s/it]                                                         {'loss': 0.3726, 'learning_rate': 1.3442311129477133e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2113/5198 [12:00:11<15:12:50, 17.75s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2114/5198 [12:00:29<15:10:57, 17.72s/it]                                                         {'loss': 0.8734, 'learning_rate': 1.343646041394977e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2114/5198 [12:00:29<15:10:57, 17.72s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2115/5198 [12:00:46<15:09:34, 17.70s/it]                                                         {'loss': 0.8208, 'learning_rate': 1.3430608364270156e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2115/5198 [12:00:46<15:09:34, 17.70s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2116/5198 [12:01:04<15:03:07, 17.58s/it]                                                         {'loss': 0.367, 'learning_rate': 1.3424754982710256e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2116/5198 [12:01:04<15:03:07, 17.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2117/5198 [12:01:21<14:58:39, 17.50s/it]                                                         {'loss': 0.8751, 'learning_rate': 1.3418900271542552e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2117/5198 [12:01:21<14:58:39, 17.50s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2118/5198 [12:01:39<15:00:20, 17.54s/it]                                                         {'loss': 0.8446, 'learning_rate': 1.3413044233040045e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2118/5198 [12:01:39<15:00:20, 17.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2119/5198 [12:01:57<15:21:25, 17.96s/it]                                                         {'loss': 0.7893, 'learning_rate': 1.3407186869476253e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2119/5198 [12:01:57<15:21:25, 17.96s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2120/5198 [12:02:15<15:12:57, 17.80s/it]                                                         {'loss': 0.8092, 'learning_rate': 1.3401328183125208e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2120/5198 [12:02:15<15:12:57, 17.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2121/5198 [12:02:32<15:04:15, 17.63s/it]                                                         {'loss': 0.866, 'learning_rate': 1.339546817626145e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2121/5198 [12:02:32<15:04:15, 17.63s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2122/5198 [12:02:50<15:03:39, 17.63s/it]                                                         {'loss': 0.8853, 'learning_rate': 1.3389606851160037e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2122/5198 [12:02:50<15:03:39, 17.63s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2123/5198 [12:03:08<15:08:09, 17.72s/it]                                                         {'loss': 0.8012, 'learning_rate': 1.3383744210096537e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2123/5198 [12:03:08<15:08:09, 17.72s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2124/5198 [12:03:25<15:00:24, 17.57s/it]                                                         {'loss': 0.8513, 'learning_rate': 1.3377880255347026e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2124/5198 [12:03:25<15:00:24, 17.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2125/5198 [12:03:43<15:13:44, 17.84s/it]                                                         {'loss': 0.8113, 'learning_rate': 1.3372014989188098e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2125/5198 [12:03:43<15:13:44, 17.84s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2126/5198 [12:05:12<33:15:46, 38.98s/it]                                                         {'loss': 0.3521, 'learning_rate': 1.3366148413896851e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2126/5198 [12:05:12<33:15:46, 38.98s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2127/5198 [12:05:29<27:47:09, 32.57s/it]                                                         {'loss': 0.8848, 'learning_rate': 1.3360280531750886e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2127/5198 [12:05:29<27:47:09, 32.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2128/5198 [12:05:46<23:48:33, 27.92s/it]                                                         {'loss': 0.7971, 'learning_rate': 1.3354411345028324e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2128/5198 [12:05:46<23:48:33, 27.92s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2129/5198 [12:06:03<21:01:54, 24.67s/it]                                                         {'loss': 0.8913, 'learning_rate': 1.3348540856007782e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2129/5198 [12:06:03<21:01:54, 24.67s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2130/5198 [12:06:21<19:12:39, 22.54s/it]                                                         {'loss': 0.9063, 'learning_rate': 1.3342669066968385e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2130/5198 [12:06:21<19:12:39, 22.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2131/5198 [12:06:39<18:02:45, 21.18s/it]                                                         {'loss': 0.8641, 'learning_rate': 1.3336795980189763e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2131/5198 [12:06:39<18:02:45, 21.18s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2132/5198 [12:06:58<17:27:44, 20.50s/it]                                                         {'loss': 0.8417, 'learning_rate': 1.3330921597952056e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2132/5198 [12:06:58<17:27:44, 20.50s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2133/5198 [12:07:14<16:25:18, 19.29s/it]                                                         {'loss': 0.8797, 'learning_rate': 1.3325045922535896e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2133/5198 [12:07:14<16:25:18, 19.29s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2134/5198 [12:07:32<15:59:37, 18.79s/it]                                                         {'loss': 0.8272, 'learning_rate': 1.3319168956222423e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2134/5198 [12:07:32<15:59:37, 18.79s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2135/5198 [12:07:50<15:46:58, 18.55s/it]                                                         {'loss': 0.8574, 'learning_rate': 1.331329070129328e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2135/5198 [12:07:50<15:46:58, 18.55s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2136/5198 [12:08:08<15:39:38, 18.41s/it]                                                         {'loss': 0.8417, 'learning_rate': 1.3307411160030608e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2136/5198 [12:08:08<15:39:38, 18.41s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2137/5198 [12:08:27<15:41:55, 18.46s/it]                                                         {'loss': 0.8762, 'learning_rate': 1.3301530334717046e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2137/5198 [12:08:27<15:41:55, 18.46s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2138/5198 [12:08:44<15:30:34, 18.25s/it]                                                         {'loss': 0.8906, 'learning_rate': 1.3295648227635729e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2138/5198 [12:08:44<15:30:34, 18.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2139/5198 [12:09:03<15:31:48, 18.28s/it]                                                         {'loss': 0.8466, 'learning_rate': 1.32897648410703e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2139/5198 [12:09:03<15:31:48, 18.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2140/5198 [12:09:20<15:21:28, 18.08s/it]                                                         {'loss': 0.7984, 'learning_rate': 1.328388017730489e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2140/5198 [12:09:20<15:21:28, 18.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2141/5198 [12:09:38<15:16:16, 17.98s/it]                                                         {'loss': 0.8669, 'learning_rate': 1.327799423862413e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2141/5198 [12:09:38<15:16:16, 17.98s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2142/5198 [12:09:56<15:14:00, 17.95s/it]                                                         {'loss': 0.862, 'learning_rate': 1.3272107027313142e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2142/5198 [12:09:56<15:14:00, 17.95s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2143/5198 [12:10:13<15:01:45, 17.71s/it]                                                         {'loss': 0.8659, 'learning_rate': 1.3266218545657541e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2143/5198 [12:10:13<15:01:45, 17.71s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2144/5198 [12:10:31<15:09:33, 17.87s/it]                                                         {'loss': 0.765, 'learning_rate': 1.326032879594344e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2144/5198 [12:10:31<15:09:33, 17.87s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2145/5198 [12:10:48<14:53:44, 17.56s/it]                                                         {'loss': 0.8317, 'learning_rate': 1.3254437780457448e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2145/5198 [12:10:48<14:53:44, 17.56s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2146/5198 [12:11:06<14:52:00, 17.54s/it]                                                         {'loss': 0.8258, 'learning_rate': 1.3248545501486654e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2146/5198 [12:11:06<14:52:00, 17.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2147/5198 [12:11:24<14:58:57, 17.68s/it]                                                         {'loss': 0.8621, 'learning_rate': 1.3242651961318646e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2147/5198 [12:11:24<14:58:57, 17.68s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2148/5198 [12:11:42<15:05:47, 17.82s/it]                                                         {'loss': 0.9014, 'learning_rate': 1.32367571622415e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2148/5198 [12:11:42<15:05:47, 17.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2149/5198 [12:12:00<15:08:44, 17.88s/it]                                                         {'loss': 0.8286, 'learning_rate': 1.3230861106543777e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2149/5198 [12:12:00<15:08:44, 17.88s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2150/5198 [12:12:18<15:08:57, 17.89s/it]                                                         {'loss': 0.8528, 'learning_rate': 1.3224963796514532e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2150/5198 [12:12:18<15:08:57, 17.89s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2151/5198 [12:13:46<32:58:31, 38.96s/it]                                                         {'loss': 0.8051, 'learning_rate': 1.32190652344433e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2151/5198 [12:13:46<32:58:31, 38.96s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2152/5198 [12:14:03<27:24:20, 32.39s/it]                                                         {'loss': 0.8757, 'learning_rate': 1.3213165422620111e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2152/5198 [12:14:03<27:24:20, 32.39s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2153/5198 [12:14:22<23:54:39, 28.27s/it]                                                         {'loss': 0.8338, 'learning_rate': 1.3207264363335472e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2153/5198 [12:14:22<23:54:39, 28.27s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2154/5198 [12:14:39<21:03:30, 24.90s/it]                                                         {'loss': 0.8501, 'learning_rate': 1.3201362058880375e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2154/5198 [12:14:39<21:03:30, 24.90s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2155/5198 [12:14:56<19:09:16, 22.66s/it]                                                         {'loss': 0.8077, 'learning_rate': 1.3195458511546307e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2155/5198 [12:14:56<19:09:16, 22.66s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2156/5198 [12:15:14<17:53:00, 21.16s/it]                                                         {'loss': 0.8293, 'learning_rate': 1.3189553723625217e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2156/5198 [12:15:14<17:53:00, 21.16s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2157/5198 [12:15:31<16:58:51, 20.10s/it]                                                         {'loss': 0.8781, 'learning_rate': 1.318364769740955e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2157/5198 [12:15:32<16:58:51, 20.10s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2158/5198 [12:15:49<16:16:25, 19.27s/it]                                                         {'loss': 0.8304, 'learning_rate': 1.3177740435192235e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2158/5198 [12:15:49<16:16:25, 19.27s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2159/5198 [12:16:07<15:55:41, 18.87s/it]                                                         {'loss': 0.7813, 'learning_rate': 1.3171831939266668e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2159/5198 [12:16:07<15:55:41, 18.87s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2160/5198 [12:16:24<15:35:19, 18.47s/it]                                                         {'loss': 0.8483, 'learning_rate': 1.3165922211926734e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2160/5198 [12:16:24<15:35:19, 18.47s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2161/5198 [12:16:42<15:17:50, 18.13s/it]                                                         {'loss': 0.9132, 'learning_rate': 1.3160011255466791e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2161/5198 [12:16:42<15:17:50, 18.13s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2162/5198 [12:17:00<15:15:01, 18.08s/it]                                                         {'loss': 0.8671, 'learning_rate': 1.3154099072181677e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2162/5198 [12:17:00<15:15:01, 18.08s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2163/5198 [12:17:17<15:10:28, 18.00s/it]                                                         {'loss': 0.8316, 'learning_rate': 1.3148185664366704e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2163/5198 [12:17:17<15:10:28, 18.00s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2164/5198 [12:17:35<14:57:04, 17.74s/it]                                                         {'loss': 0.8134, 'learning_rate': 1.314227103431766e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2164/5198 [12:17:35<14:57:04, 17.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2165/5198 [12:17:52<14:46:26, 17.54s/it]                                                         {'loss': 0.8267, 'learning_rate': 1.3136355184330809e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2165/5198 [12:17:52<14:46:26, 17.54s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2166/5198 [12:18:10<14:56:27, 17.74s/it]                                                         {'loss': 0.77, 'learning_rate': 1.3130438116702888e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2166/5198 [12:18:10<14:56:27, 17.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2167/5198 [12:18:28<14:55:58, 17.74s/it]                                                         {'loss': 0.8285, 'learning_rate': 1.3124519833731106e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2167/5198 [12:18:28<14:55:58, 17.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2168/5198 [12:18:45<14:48:12, 17.59s/it]                                                         {'loss': 0.8483, 'learning_rate': 1.3118600337713146e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2168/5198 [12:18:45<14:48:12, 17.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2169/5198 [12:19:02<14:44:39, 17.52s/it]                                                         {'loss': 0.8554, 'learning_rate': 1.3112679630947156e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2169/5198 [12:19:02<14:44:39, 17.52s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2170/5198 [12:19:20<14:46:03, 17.56s/it]                                                         {'loss': 0.8133, 'learning_rate': 1.310675771573176e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2170/5198 [12:19:20<14:46:03, 17.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2171/5198 [12:19:37<14:46:36, 17.57s/it]                                                         {'loss': 0.809, 'learning_rate': 1.310083459436605e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2171/5198 [12:19:37<14:46:36, 17.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2172/5198 [12:19:55<14:49:16, 17.63s/it]                                                         {'loss': 0.8321, 'learning_rate': 1.3094910269149587e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2172/5198 [12:19:55<14:49:16, 17.63s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2173/5198 [12:20:13<14:58:47, 17.83s/it]                                                         {'loss': 0.8459, 'learning_rate': 1.3088984742382395e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2173/5198 [12:20:13<14:58:47, 17.83s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2174/5198 [12:20:32<15:03:31, 17.93s/it]                                                         {'loss': 0.8538, 'learning_rate': 1.3083058016364972e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2174/5198 [12:20:32<15:03:31, 17.93s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2175/5198 [12:20:50<15:08:17, 18.03s/it]                                                         {'loss': 0.804, 'learning_rate': 1.3077130093398274e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2175/5198 [12:20:50<15:08:17, 18.03s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2176/5198 [12:22:16<32:20:12, 38.52s/it]                                                         {'loss': 0.7908, 'learning_rate': 1.3071200975783725e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2176/5198 [12:22:16<32:20:12, 38.52s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2177/5198 [12:22:34<27:03:33, 32.25s/it]                                                         {'loss': 0.8542, 'learning_rate': 1.3065270665823206e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2177/5198 [12:22:34<27:03:33, 32.25s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2178/5198 [12:22:52<23:30:46, 28.03s/it]                                                         {'loss': 0.9176, 'learning_rate': 1.3059339165819082e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2178/5198 [12:22:52<23:30:46, 28.03s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2179/5198 [12:23:10<20:51:36, 24.87s/it]                                                         {'loss': 0.8113, 'learning_rate': 1.3053406478074155e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2179/5198 [12:23:10<20:51:36, 24.87s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2180/5198 [12:23:28<19:06:52, 22.80s/it]                                                         {'loss': 0.8763, 'learning_rate': 1.3047472604891701e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2180/5198 [12:23:28<19:06:52, 22.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2181/5198 [12:23:45<17:49:14, 21.26s/it]                                                         {'loss': 0.8063, 'learning_rate': 1.3041537548575455e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2181/5198 [12:23:45<17:49:14, 21.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2182/5198 [12:24:04<17:06:07, 20.41s/it]                                                         {'loss': 0.8415, 'learning_rate': 1.303560131142961e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2182/5198 [12:24:04<17:06:07, 20.41s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2183/5198 [12:24:21<16:16:19, 19.43s/it]                                                         {'loss': 0.8541, 'learning_rate': 1.3029663895758814e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2183/5198 [12:24:21<16:16:19, 19.43s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2184/5198 [12:24:38<15:44:27, 18.80s/it]                                                         {'loss': 0.8642, 'learning_rate': 1.3023725303868183e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2184/5198 [12:24:38<15:44:27, 18.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2185/5198 [12:24:56<15:29:45, 18.51s/it]                                                         {'loss': 0.8268, 'learning_rate': 1.3017785538063277e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2185/5198 [12:24:56<15:29:45, 18.51s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2186/5198 [12:25:14<15:17:53, 18.28s/it]                                                         {'loss': 0.8802, 'learning_rate': 1.3011844600650121e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2186/5198 [12:25:14<15:17:53, 18.28s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2187/5198 [12:25:31<15:09:54, 18.13s/it]                                                         {'loss': 0.861, 'learning_rate': 1.300590249393519e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2187/5198 [12:25:31<15:09:54, 18.13s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2188/5198 [12:25:49<14:58:18, 17.91s/it]                                                         {'loss': 0.8822, 'learning_rate': 1.2999959220225416e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2188/5198 [12:25:49<14:58:18, 17.91s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2189/5198 [12:26:08<15:16:16, 18.27s/it]                                                         {'loss': 0.8306, 'learning_rate': 1.299401478182818e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2189/5198 [12:26:08<15:16:16, 18.27s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2190/5198 [12:26:25<15:02:28, 18.00s/it]                                                         {'loss': 0.829, 'learning_rate': 1.2988069181051314e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2190/5198 [12:26:25<15:02:28, 18.00s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2191/5198 [12:26:42<14:48:05, 17.72s/it]                                                         {'loss': 0.8031, 'learning_rate': 1.2982122420203114e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2191/5198 [12:26:42<14:48:05, 17.72s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2192/5198 [12:27:00<14:51:00, 17.78s/it]                                                         {'loss': 0.8337, 'learning_rate': 1.2976174501592313e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2192/5198 [12:27:00<14:51:00, 17.78s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2193/5198 [12:27:18<14:45:47, 17.69s/it]                                                         {'loss': 0.8018, 'learning_rate': 1.2970225427528098e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2193/5198 [12:27:18<14:45:47, 17.69s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2194/5198 [12:27:35<14:43:37, 17.65s/it]                                                         {'loss': 0.8213, 'learning_rate': 1.2964275200320104e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2194/5198 [12:27:35<14:43:37, 17.65s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2195/5198 [12:27:53<14:43:01, 17.64s/it]                                                         {'loss': 0.8707, 'learning_rate': 1.2958323822278413e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2195/5198 [12:27:53<14:43:01, 17.64s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2196/5198 [12:28:10<14:37:30, 17.54s/it]                                                         {'loss': 0.847, 'learning_rate': 1.2952371295713558e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2196/5198 [12:28:10<14:37:30, 17.54s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2197/5198 [12:28:28<14:44:15, 17.68s/it]                                                         {'loss': 0.8753, 'learning_rate': 1.2946417622936512e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2197/5198 [12:28:28<14:44:15, 17.68s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2198/5198 [12:28:46<14:40:08, 17.60s/it]                                                         {'loss': 0.3566, 'learning_rate': 1.2940462806258696e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2198/5198 [12:28:46<14:40:08, 17.60s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2199/5198 [12:29:03<14:40:14, 17.61s/it]                                                         {'loss': 0.8325, 'learning_rate': 1.2934506847991976e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2199/5198 [12:29:03<14:40:14, 17.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2200/5198 [12:29:21<14:45:40, 17.73s/it]                                                         {'loss': 0.3727, 'learning_rate': 1.2928549750448661e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2200/5198 [12:29:21<14:45:40, 17.73s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2201/5198 [12:30:51<32:50:55, 39.46s/it]                                                         {'loss': 0.7848, 'learning_rate': 1.2922591515941498e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2201/5198 [12:30:51<32:50:55, 39.46s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2202/5198 [12:31:09<27:15:51, 32.76s/it]                                                         {'loss': 0.8478, 'learning_rate': 1.2916632146783683e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2202/5198 [12:31:09<27:15:51, 32.76s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2203/5198 [12:31:26<23:29:42, 28.24s/it]                                                         {'loss': 0.7967, 'learning_rate': 1.2910671645288841e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2203/5198 [12:31:26<23:29:42, 28.24s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2204/5198 [12:31:44<20:58:26, 25.22s/it]                                                         {'loss': 0.8124, 'learning_rate': 1.2904710013771054e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2204/5198 [12:31:44<20:58:26, 25.22s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2205/5198 [12:32:02<18:59:49, 22.85s/it]                                                         {'loss': 0.8768, 'learning_rate': 1.2898747254544826e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2205/5198 [12:32:02<18:59:49, 22.85s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2206/5198 [12:32:19<17:40:36, 21.27s/it]                                                         {'loss': 0.8268, 'learning_rate': 1.2892783369925105e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2206/5198 [12:32:19<17:40:36, 21.27s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2207/5198 [12:32:37<16:47:20, 20.21s/it]                                                         {'loss': 0.3441, 'learning_rate': 1.2886818362227283e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2207/5198 [12:32:37<16:47:20, 20.21s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2208/5198 [12:32:55<16:13:01, 19.53s/it]                                                         {'loss': 0.8223, 'learning_rate': 1.2880852233767174e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2208/5198 [12:32:55<16:13:01, 19.53s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2209/5198 [12:33:13<15:52:17, 19.12s/it]                                                         {'loss': 0.8325, 'learning_rate': 1.2874884986861038e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2209/5198 [12:33:13<15:52:17, 19.12s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2210/5198 [12:33:31<15:28:58, 18.65s/it]                                                         {'loss': 0.8261, 'learning_rate': 1.2868916623825561e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2210/5198 [12:33:31<15:28:58, 18.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2211/5198 [12:33:47<14:58:02, 18.04s/it]                                                         {'loss': 0.3464, 'learning_rate': 1.2862947146977876e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2211/5198 [12:33:47<14:58:02, 18.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2212/5198 [12:34:04<14:32:58, 17.54s/it]                                                         {'loss': 0.8399, 'learning_rate': 1.2856976558635532e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2212/5198 [12:34:04<14:32:58, 17.54s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2213/5198 [12:34:22<14:38:11, 17.65s/it]                                                         {'loss': 0.7733, 'learning_rate': 1.2851004861116519e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2213/5198 [12:34:22<14:38:11, 17.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2214/5198 [12:34:39<14:31:28, 17.52s/it]                                                         {'loss': 0.8423, 'learning_rate': 1.2845032056739257e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2214/5198 [12:34:39<14:31:28, 17.52s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2215/5198 [12:34:57<14:37:54, 17.66s/it]                                                         {'loss': 0.8193, 'learning_rate': 1.2839058147822595e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2215/5198 [12:34:57<14:37:54, 17.66s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2216/5198 [12:35:15<14:49:04, 17.89s/it]                                                         {'loss': 0.798, 'learning_rate': 1.2833083136685803e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2216/5198 [12:35:15<14:49:04, 17.89s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2217/5198 [12:35:33<14:43:05, 17.77s/it]                                                         {'loss': 0.8077, 'learning_rate': 1.2827107025648595e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2217/5198 [12:35:33<14:43:05, 17.77s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2218/5198 [12:35:51<14:42:13, 17.76s/it]                                                         {'loss': 0.8754, 'learning_rate': 1.2821129817031099e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2218/5198 [12:35:51<14:42:13, 17.76s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2219/5198 [12:36:09<14:47:07, 17.87s/it]                                                         {'loss': 0.9062, 'learning_rate': 1.2815151513153874e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2219/5198 [12:36:09<14:47:07, 17.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2220/5198 [12:36:26<14:39:51, 17.73s/it]                                                         {'loss': 0.8445, 'learning_rate': 1.2809172116337903e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2220/5198 [12:36:26<14:39:51, 17.73s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2221/5198 [12:36:43<14:31:17, 17.56s/it]                                                         {'loss': 0.8662, 'learning_rate': 1.2803191628904594e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2221/5198 [12:36:43<14:31:17, 17.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2222/5198 [12:37:01<14:34:18, 17.63s/it]                                                         {'loss': 0.3529, 'learning_rate': 1.2797210053175779e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2222/5198 [12:37:01<14:34:18, 17.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2223/5198 [12:37:19<14:42:25, 17.80s/it]                                                         {'loss': 0.8345, 'learning_rate': 1.2791227391473706e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2223/5198 [12:37:19<14:42:25, 17.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2224/5198 [12:37:36<14:26:33, 17.48s/it]                                                         {'loss': 0.8904, 'learning_rate': 1.2785243646121059e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2224/5198 [12:37:36<14:26:33, 17.48s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2225/5198 [12:37:54<14:37:31, 17.71s/it]                                                         {'loss': 0.8243, 'learning_rate': 1.277925881944093e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2225/5198 [12:37:54<14:37:31, 17.71s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2226/5198 [12:39:19<31:16:53, 37.89s/it]                                                         {'loss': 0.8231, 'learning_rate': 1.2773272913756833e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2226/5198 [12:39:19<31:16:53, 37.89s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2227/5198 [12:39:37<26:21:54, 31.95s/it]                                                         {'loss': 0.8027, 'learning_rate': 1.2767285931392705e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2227/5198 [12:39:37<26:21:54, 31.95s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2228/5198 [12:39:55<22:49:24, 27.66s/it]                                                         {'loss': 0.8485, 'learning_rate': 1.27612978746729e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2228/5198 [12:39:55<22:49:24, 27.66s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2229/5198 [12:40:12<20:10:39, 24.47s/it]                                                         {'loss': 0.7467, 'learning_rate': 1.2755308745922182e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2229/5198 [12:40:12<20:10:39, 24.47s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2230/5198 [12:40:30<18:30:16, 22.44s/it]                                                         {'loss': 0.8055, 'learning_rate': 1.2749318547465742e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2230/5198 [12:40:30<18:30:16, 22.44s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2231/5198 [12:40:48<17:27:41, 21.19s/it]                                                         {'loss': 0.8377, 'learning_rate': 1.2743327281629181e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2231/5198 [12:40:48<17:27:41, 21.19s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2232/5198 [12:41:05<16:25:29, 19.94s/it]                                                         {'loss': 0.8399, 'learning_rate': 1.2737334950738512e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2232/5198 [12:41:05<16:25:29, 19.94s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2233/5198 [12:41:24<16:05:56, 19.55s/it]                                                         {'loss': 0.8238, 'learning_rate': 1.273134155712017e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2233/5198 [12:41:24<16:05:56, 19.55s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2234/5198 [12:41:40<15:23:07, 18.69s/it]                                                         {'loss': 0.8277, 'learning_rate': 1.272534710310099e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2234/5198 [12:41:40<15:23:07, 18.69s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2235/5198 [12:41:57<14:51:15, 18.05s/it]                                                         {'loss': 0.8625, 'learning_rate': 1.2719351591008228e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2235/5198 [12:41:57<14:51:15, 18.05s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2236/5198 [12:42:15<14:48:06, 17.99s/it]                                                         {'loss': 0.8967, 'learning_rate': 1.2713355023169547e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2236/5198 [12:42:15<14:48:06, 17.99s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2237/5198 [12:42:33<14:48:00, 17.99s/it]                                                         {'loss': 0.8611, 'learning_rate': 1.2707357401913022e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2237/5198 [12:42:33<14:48:00, 17.99s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2238/5198 [12:42:50<14:33:50, 17.71s/it]                                                         {'loss': 0.8788, 'learning_rate': 1.270135872956714e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2238/5198 [12:42:50<14:33:50, 17.71s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2239/5198 [12:43:08<14:44:53, 17.94s/it]                                                         {'loss': 0.8319, 'learning_rate': 1.2695359008460785e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2239/5198 [12:43:08<14:44:53, 17.94s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2240/5198 [12:43:26<14:40:54, 17.87s/it]                                                         {'loss': 0.8401, 'learning_rate': 1.2689358240923264e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2240/5198 [12:43:26<14:40:54, 17.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2241/5198 [12:43:43<14:35:47, 17.77s/it]                                                         {'loss': 0.8401, 'learning_rate': 1.2683356429284273e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2241/5198 [12:43:43<14:35:47, 17.77s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2242/5198 [12:44:01<14:28:28, 17.63s/it]                                                         {'loss': 0.7832, 'learning_rate': 1.2677353575873926e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2242/5198 [12:44:01<14:28:28, 17.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2243/5198 [12:44:19<14:30:19, 17.67s/it]                                                         {'loss': 0.8245, 'learning_rate': 1.2671349683022736e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2243/5198 [12:44:19<14:30:19, 17.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2244/5198 [12:44:37<14:40:12, 17.88s/it]                                                         {'loss': 0.8278, 'learning_rate': 1.2665344753061622e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2244/5198 [12:44:37<14:40:12, 17.88s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2245/5198 [12:44:55<14:37:24, 17.83s/it]                                                         {'loss': 0.8099, 'learning_rate': 1.2659338788321904e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2245/5198 [12:44:55<14:37:24, 17.83s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2246/5198 [12:45:13<14:41:42, 17.92s/it]                                                         {'loss': 0.7918, 'learning_rate': 1.2653331791135308e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2246/5198 [12:45:13<14:41:42, 17.92s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2247/5198 [12:45:30<14:39:00, 17.87s/it]                                                         {'loss': 0.8362, 'learning_rate': 1.2647323763833952e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2247/5198 [12:45:30<14:39:00, 17.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2248/5198 [12:45:48<14:34:00, 17.78s/it]                                                         {'loss': 0.8668, 'learning_rate': 1.264131470875036e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2248/5198 [12:45:48<14:34:00, 17.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2249/5198 [12:46:07<14:49:02, 18.09s/it]                                                         {'loss': 0.7868, 'learning_rate': 1.2635304628217452e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2249/5198 [12:46:07<14:49:02, 18.09s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2250/5198 [12:46:25<14:48:53, 18.09s/it]                                                         {'loss': 0.8929, 'learning_rate': 1.2629293524568555e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2250/5198 [12:46:25<14:48:53, 18.09s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2251/5198 [12:47:48<30:50:56, 37.68s/it]                                                         {'loss': 0.8212, 'learning_rate': 1.2623281400137383e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2251/5198 [12:47:48<30:50:56, 37.68s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2252/5198 [12:48:06<25:49:25, 31.56s/it]                                                         {'loss': 0.3466, 'learning_rate': 1.2617268257258051e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2252/5198 [12:48:06<25:49:25, 31.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2253/5198 [12:48:24<22:33:53, 27.58s/it]                                                         {'loss': 0.7987, 'learning_rate': 1.2611254098265063e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2253/5198 [12:48:24<22:33:53, 27.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2254/5198 [12:48:42<20:19:14, 24.85s/it]                                                         {'loss': 0.8677, 'learning_rate': 1.2605238925493326e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2254/5198 [12:48:42<20:19:14, 24.85s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2255/5198 [12:49:00<18:32:03, 22.67s/it]                                                         {'loss': 0.8351, 'learning_rate': 1.2599222741278136e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2255/5198 [12:49:00<18:32:03, 22.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2256/5198 [12:49:17<17:08:39, 20.98s/it]                                                         {'loss': 0.8345, 'learning_rate': 1.2593205547955185e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2256/5198 [12:49:17<17:08:39, 20.98s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2257/5198 [12:49:33<16:02:27, 19.64s/it]                                                         {'loss': 0.8916, 'learning_rate': 1.2587187347860554e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2257/5198 [12:49:33<16:02:27, 19.64s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2258/5198 [12:49:50<15:16:04, 18.70s/it]                                                         {'loss': 0.8179, 'learning_rate': 1.2581168143330716e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2258/5198 [12:49:50<15:16:04, 18.70s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2259/5198 [12:50:09<15:15:51, 18.70s/it]                                                         {'loss': 0.8829, 'learning_rate': 1.2575147936702531e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2259/5198 [12:50:09<15:15:51, 18.70s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2260/5198 [12:50:27<15:10:26, 18.59s/it]                                                         {'loss': 0.8372, 'learning_rate': 1.2569126730313255e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2260/5198 [12:50:27<15:10:26, 18.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2261/5198 [12:50:44<14:45:58, 18.10s/it]                                                         {'loss': 0.8699, 'learning_rate': 1.2563104526500523e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2261/5198 [12:50:44<14:45:58, 18.10s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2262/5198 [12:51:02<14:46:31, 18.12s/it]                                                         {'loss': 0.8607, 'learning_rate': 1.2557081327602361e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2262/5198 [12:51:02<14:46:31, 18.12s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2263/5198 [12:51:19<14:32:49, 17.84s/it]                                                         {'loss': 0.3386, 'learning_rate': 1.2551057135957187e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2263/5198 [12:51:19<14:32:49, 17.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2264/5198 [12:51:37<14:24:51, 17.69s/it]                                                         {'loss': 0.8538, 'learning_rate': 1.2545031953903796e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2264/5198 [12:51:37<14:24:51, 17.69s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2265/5198 [12:51:54<14:14:50, 17.49s/it]                                                         {'loss': 0.8666, 'learning_rate': 1.2539005783781374e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2265/5198 [12:51:54<14:14:50, 17.49s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2266/5198 [12:52:11<14:12:33, 17.45s/it]                                                         {'loss': 0.7917, 'learning_rate': 1.2532978627929486e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2266/5198 [12:52:11<14:12:33, 17.45s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2267/5198 [12:52:29<14:16:47, 17.54s/it]                                                         {'loss': 0.8404, 'learning_rate': 1.2526950488688083e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2267/5198 [12:52:29<14:16:47, 17.54s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2268/5198 [12:52:47<14:29:05, 17.80s/it]                                                         {'loss': 0.8939, 'learning_rate': 1.2520921368397492e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2268/5198 [12:52:47<14:29:05, 17.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2269/5198 [12:53:05<14:35:50, 17.94s/it]                                                         {'loss': 0.8126, 'learning_rate': 1.2514891269398429e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2269/5198 [12:53:05<14:35:50, 17.94s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2270/5198 [12:53:22<14:17:36, 17.57s/it]                                                         {'loss': 0.8486, 'learning_rate': 1.2508860194031986e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2270/5198 [12:53:22<14:17:36, 17.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2271/5198 [12:53:40<14:18:58, 17.61s/it]                                                         {'loss': 0.8716, 'learning_rate': 1.2502828144639629e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2271/5198 [12:53:40<14:18:58, 17.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2272/5198 [12:53:58<14:24:00, 17.72s/it]                                                         {'loss': 0.8585, 'learning_rate': 1.2496795123563218e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2272/5198 [12:53:58<14:24:00, 17.72s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2273/5198 [12:54:15<14:13:17, 17.50s/it]                                                         {'loss': 0.8903, 'learning_rate': 1.249076113314497e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2273/5198 [12:54:15<14:13:17, 17.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2274/5198 [12:54:33<14:19:01, 17.63s/it]                                                         {'loss': 0.7942, 'learning_rate': 1.248472617572749e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2274/5198 [12:54:33<14:19:01, 17.63s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2275/5198 [12:54:52<14:37:26, 18.01s/it]                                                         {'loss': 0.8286, 'learning_rate': 1.2478690253653756e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2275/5198 [12:54:52<14:37:26, 18.01s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2276/5198 [12:56:14<30:13:12, 37.23s/it]                                                         {'loss': 0.8059, 'learning_rate': 1.2472653369267122e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2276/5198 [12:56:14<30:13:12, 37.23s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2277/5198 [12:56:31<25:16:23, 31.15s/it]                                                         {'loss': 0.8196, 'learning_rate': 1.2466615524911316e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2277/5198 [12:56:31<25:16:23, 31.15s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2278/5198 [12:56:48<21:59:20, 27.11s/it]                                                         {'loss': 0.8161, 'learning_rate': 1.2460576722930432e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2278/5198 [12:56:48<21:59:20, 27.11s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2279/5198 [12:57:07<19:47:26, 24.41s/it]                                                         {'loss': 0.8747, 'learning_rate': 1.2454536965668949e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2279/5198 [12:57:07<19:47:26, 24.41s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2280/5198 [12:57:24<18:04:58, 22.31s/it]                                                         {'loss': 0.8573, 'learning_rate': 1.24484962554717e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2280/5198 [12:57:24<18:04:58, 22.31s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2281/5198 [12:57:42<17:08:08, 21.15s/it]                                                         {'loss': 0.8563, 'learning_rate': 1.24424545946839e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2281/5198 [12:57:42<17:08:08, 21.15s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2282/5198 [12:58:00<16:10:34, 19.97s/it]                                                         {'loss': 0.8987, 'learning_rate': 1.2436411985651131e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2282/5198 [12:58:00<16:10:34, 19.97s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2283/5198 [12:58:18<15:44:29, 19.44s/it]                                                         {'loss': 0.8722, 'learning_rate': 1.2430368430719342e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2283/5198 [12:58:18<15:44:29, 19.44s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2284/5198 [12:58:35<15:05:04, 18.64s/it]                                                         {'loss': 0.7992, 'learning_rate': 1.242432393223485e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2284/5198 [12:58:35<15:05:04, 18.64s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2285/5198 [12:58:52<14:40:17, 18.13s/it]                                                         {'loss': 0.8366, 'learning_rate': 1.2418278492544328e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2285/5198 [12:58:52<14:40:17, 18.13s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2286/5198 [12:59:09<14:24:10, 17.81s/it]                                                         {'loss': 0.907, 'learning_rate': 1.2412232113994841e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2286/5198 [12:59:09<14:24:10, 17.81s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2287/5198 [12:59:27<14:31:44, 17.97s/it]                                                         {'loss': 0.8317, 'learning_rate': 1.2406184798933786e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2287/5198 [12:59:27<14:31:44, 17.97s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2288/5198 [12:59:44<14:19:07, 17.71s/it]                                                         {'loss': 0.8566, 'learning_rate': 1.2400136549708945e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2288/5198 [12:59:44<14:19:07, 17.71s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2289/5198 [13:00:01<14:09:30, 17.52s/it]                                                         {'loss': 0.8366, 'learning_rate': 1.239408736866846e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2289/5198 [13:00:01<14:09:30, 17.52s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2290/5198 [13:00:19<14:14:05, 17.62s/it]                                                         {'loss': 0.8379, 'learning_rate': 1.2388037258160823e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2290/5198 [13:00:19<14:14:05, 17.62s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2291/5198 [13:00:37<14:17:54, 17.71s/it]                                                         {'loss': 0.86, 'learning_rate': 1.23819862205349e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2291/5198 [13:00:37<14:17:54, 17.71s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2292/5198 [13:00:54<14:11:25, 17.58s/it]                                                         {'loss': 0.7924, 'learning_rate': 1.2375934258139917e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2292/5198 [13:00:54<14:11:25, 17.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2293/5198 [13:01:12<14:13:39, 17.63s/it]                                                         {'loss': 0.8678, 'learning_rate': 1.2369881373325448e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2293/5198 [13:01:12<14:13:39, 17.63s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2294/5198 [13:01:29<14:06:49, 17.50s/it]                                                         {'loss': 0.8259, 'learning_rate': 1.236382756844143e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2294/5198 [13:01:29<14:06:49, 17.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2295/5198 [13:01:47<14:18:13, 17.74s/it]                                                         {'loss': 0.7761, 'learning_rate': 1.2357772845838159e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2295/5198 [13:01:47<14:18:13, 17.74s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2296/5198 [13:02:05<14:20:48, 17.80s/it]                                                         {'loss': 0.841, 'learning_rate': 1.2351717207866292e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2296/5198 [13:02:05<14:20:48, 17.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2297/5198 [13:02:22<14:01:02, 17.39s/it]                                                         {'loss': 0.8104, 'learning_rate': 1.2345660656876832e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2297/5198 [13:02:22<14:01:02, 17.39s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2298/5198 [13:02:40<14:11:01, 17.61s/it]                                                         {'loss': 0.3451, 'learning_rate': 1.233960319522114e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2298/5198 [13:02:40<14:11:01, 17.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2299/5198 [13:02:58<14:12:25, 17.64s/it]                                                         {'loss': 0.8908, 'learning_rate': 1.2333544825250938e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2299/5198 [13:02:58<14:12:25, 17.64s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2300/5198 [13:03:15<14:07:30, 17.55s/it]                                                         {'loss': 0.8458, 'learning_rate': 1.2327485549318285e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2300/5198 [13:03:15<14:07:30, 17.55s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2301/5198 [13:04:42<30:57:35, 38.47s/it]                                                         {'loss': 0.8851, 'learning_rate': 1.2321425369775601e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2301/5198 [13:04:42<30:57:35, 38.47s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2302/5198 [13:05:00<26:02:37, 32.37s/it]                                                         {'loss': 0.889, 'learning_rate': 1.2315364288975665e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2302/5198 [13:05:00<26:02:37, 32.37s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2303/5198 [13:05:18<22:27:31, 27.93s/it]                                                         {'loss': 0.901, 'learning_rate': 1.2309302309271587e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2303/5198 [13:05:18<22:27:31, 27.93s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2304/5198 [13:05:36<20:05:20, 24.99s/it]                                                         {'loss': 0.812, 'learning_rate': 1.2303239433016842e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2304/5198 [13:05:36<20:05:20, 24.99s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2305/5198 [13:05:54<18:25:13, 22.92s/it]                                                         {'loss': 0.8617, 'learning_rate': 1.2297175662565248e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2305/5198 [13:05:54<18:25:13, 22.92s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2306/5198 [13:06:12<17:08:26, 21.34s/it]                                                         {'loss': 0.8534, 'learning_rate': 1.229111100027097e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2306/5198 [13:06:12<17:08:26, 21.34s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2307/5198 [13:06:29<16:08:09, 20.09s/it]                                                         {'loss': 0.854, 'learning_rate': 1.228504544848851e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2307/5198 [13:06:29<16:08:09, 20.09s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2308/5198 [13:06:46<15:21:42, 19.14s/it]                                                         {'loss': 0.8079, 'learning_rate': 1.2278979009572736e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2308/5198 [13:06:46<15:21:42, 19.14s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2309/5198 [13:07:04<15:02:19, 18.74s/it]                                                         {'loss': 0.8247, 'learning_rate': 1.2272911685878841e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2309/5198 [13:07:04<15:02:19, 18.74s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2310/5198 [13:07:22<14:51:05, 18.51s/it]                                                         {'loss': 0.817, 'learning_rate': 1.2266843479762372e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2310/5198 [13:07:22<14:51:05, 18.51s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2311/5198 [13:07:40<14:50:07, 18.50s/it]                                                         {'loss': 0.7805, 'learning_rate': 1.2260774393579209e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2311/5198 [13:07:40<14:50:07, 18.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2312/5198 [13:07:57<14:28:38, 18.06s/it]                                                         {'loss': 0.8205, 'learning_rate': 1.2254704429685593e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2312/5198 [13:07:57<14:28:38, 18.06s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2313/5198 [13:08:15<14:31:38, 18.13s/it]                                                         {'loss': 0.8606, 'learning_rate': 1.2248633590438084e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2313/5198 [13:08:15<14:31:38, 18.13s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2314/5198 [13:08:33<14:26:41, 18.03s/it]                                                         {'loss': 0.8366, 'learning_rate': 1.2242561878193589e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2314/5198 [13:08:33<14:26:41, 18.03s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2315/5198 [13:08:51<14:18:45, 17.87s/it]                                                         {'loss': 0.3608, 'learning_rate': 1.2236489295309362e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2315/5198 [13:08:51<14:18:45, 17.87s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2316/5198 [13:09:09<14:21:54, 17.94s/it]                                                         {'loss': 0.8529, 'learning_rate': 1.2230415844142984e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2316/5198 [13:09:09<14:21:54, 17.94s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2317/5198 [13:09:26<14:09:16, 17.69s/it]                                                         {'loss': 0.8553, 'learning_rate': 1.2224341527052378e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2317/5198 [13:09:26<14:09:16, 17.69s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2318/5198 [13:09:43<13:54:28, 17.39s/it]                                                         {'loss': 0.872, 'learning_rate': 1.2218266346395811e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2318/5198 [13:09:43<13:54:28, 17.39s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2319/5198 [13:10:00<13:55:11, 17.41s/it]                                                         {'loss': 0.8469, 'learning_rate': 1.221219030453187e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2319/5198 [13:10:00<13:55:11, 17.41s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2320/5198 [13:10:17<13:51:44, 17.34s/it]                                                         {'loss': 0.8554, 'learning_rate': 1.220611340381948e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2320/5198 [13:10:17<13:51:44, 17.34s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2321/5198 [13:10:34<13:45:34, 17.22s/it]                                                         {'loss': 0.8499, 'learning_rate': 1.2200035646617912e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2321/5198 [13:10:34<13:45:34, 17.22s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2322/5198 [13:10:52<14:00:30, 17.53s/it]                                                         {'loss': 0.8127, 'learning_rate': 1.2193957035286757e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2322/5198 [13:10:53<14:00:30, 17.53s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2323/5198 [13:11:09<13:51:45, 17.36s/it]                                                         {'loss': 0.8271, 'learning_rate': 1.2187877572185937e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2323/5198 [13:11:09<13:51:45, 17.36s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2324/5198 [13:11:27<13:48:41, 17.30s/it]                                                         {'loss': 0.9088, 'learning_rate': 1.2181797259675713e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2324/5198 [13:11:27<13:48:41, 17.30s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2325/5198 [13:11:44<13:54:32, 17.43s/it]                                                         {'loss': 0.8613, 'learning_rate': 1.2175716100116677e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2325/5198 [13:11:44<13:54:32, 17.43s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2326/5198 [13:13:08<29:49:59, 37.40s/it]                                                         {'loss': 0.8737, 'learning_rate': 1.2169634095869736e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2326/5198 [13:13:08<29:49:59, 37.40s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2327/5198 [13:13:25<24:56:48, 31.28s/it]                                                         {'loss': 0.7846, 'learning_rate': 1.2163551249296132e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2327/5198 [13:13:25<24:56:48, 31.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2328/5198 [13:13:42<21:24:31, 26.85s/it]                                                         {'loss': 0.7857, 'learning_rate': 1.2157467562757443e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2328/5198 [13:13:42<21:24:31, 26.85s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2329/5198 [13:14:00<19:26:06, 24.39s/it]                                                         {'loss': 0.8209, 'learning_rate': 1.2151383038615563e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2329/5198 [13:14:00<19:26:06, 24.39s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2330/5198 [13:14:19<17:57:19, 22.54s/it]                                                         {'loss': 0.817, 'learning_rate': 1.214529767923271e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2330/5198 [13:14:19<17:57:19, 22.54s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2331/5198 [13:14:37<16:53:07, 21.20s/it]                                                         {'loss': 0.8743, 'learning_rate': 1.2139211486971436e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2331/5198 [13:14:37<16:53:07, 21.20s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2332/5198 [13:14:56<16:25:56, 20.64s/it]                                                         {'loss': 0.7954, 'learning_rate': 1.213312446419461e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2332/5198 [13:14:56<16:25:56, 20.64s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2333/5198 [13:15:14<15:49:17, 19.88s/it]                                                         {'loss': 0.8181, 'learning_rate': 1.2127036613265418e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2333/5198 [13:15:14<15:49:17, 19.88s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2334/5198 [13:15:32<15:24:11, 19.36s/it]                                                         {'loss': 0.8233, 'learning_rate': 1.2120947936547375e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2334/5198 [13:15:32<15:24:11, 19.36s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2335/5198 [13:15:49<14:49:55, 18.65s/it]                                                         {'loss': 0.8128, 'learning_rate': 1.2114858436404322e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2335/5198 [13:15:49<14:49:55, 18.65s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2336/5198 [13:16:07<14:29:01, 18.22s/it]                                                         {'loss': 0.3093, 'learning_rate': 1.2108768115200405e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2336/5198 [13:16:07<14:29:01, 18.22s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2337/5198 [13:16:24<14:22:59, 18.10s/it]                                                         {'loss': 0.799, 'learning_rate': 1.2102676975300095e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2337/5198 [13:16:24<14:22:59, 18.10s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2338/5198 [13:16:42<14:17:03, 17.98s/it]                                                         {'loss': 0.7467, 'learning_rate': 1.209658501906819e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2338/5198 [13:16:42<14:17:03, 17.98s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2339/5198 [13:16:59<14:00:45, 17.64s/it]                                                         {'loss': 0.8333, 'learning_rate': 1.2090492248869795e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2339/5198 [13:16:59<14:00:45, 17.64s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2340/5198 [13:17:16<13:50:26, 17.43s/it]                                                         {'loss': 0.7914, 'learning_rate': 1.2084398667070325e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2340/5198 [13:17:16<13:50:26, 17.43s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2341/5198 [13:17:33<13:42:30, 17.27s/it]                                                         {'loss': 0.8816, 'learning_rate': 1.2078304276035527e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2341/5198 [13:17:33<13:42:30, 17.27s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2342/5198 [13:17:50<13:43:46, 17.31s/it]                                                         {'loss': 0.3527, 'learning_rate': 1.2072209078131451e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2342/5198 [13:17:50<13:43:46, 17.31s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2343/5198 [13:18:07<13:41:08, 17.26s/it]                                                         {'loss': 0.8598, 'learning_rate': 1.2066113075724461e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2343/5198 [13:18:07<13:41:08, 17.26s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2344/5198 [13:18:25<13:46:45, 17.38s/it]                                                         {'loss': 0.88, 'learning_rate': 1.206001627118124e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2344/5198 [13:18:25<13:46:45, 17.38s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2345/5198 [13:18:43<13:58:14, 17.63s/it]                                                         {'loss': 0.8429, 'learning_rate': 1.2053918666868776e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2345/5198 [13:18:43<13:58:14, 17.63s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2346/5198 [13:19:01<13:55:07, 17.57s/it]                                                         {'loss': 0.8628, 'learning_rate': 1.2047820265154362e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2346/5198 [13:19:01<13:55:07, 17.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2347/5198 [13:19:18<13:57:18, 17.62s/it]                                                         {'loss': 0.8736, 'learning_rate': 1.2041721068405614e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2347/5198 [13:19:18<13:57:18, 17.62s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2348/5198 [13:19:35<13:49:30, 17.46s/it]                                                         {'loss': 0.7882, 'learning_rate': 1.203562107899045e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2348/5198 [13:19:35<13:49:30, 17.46s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2349/5198 [13:19:53<13:54:18, 17.57s/it]                                                         {'loss': 0.8748, 'learning_rate': 1.2029520299277095e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2349/5198 [13:19:53<13:54:18, 17.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2350/5198 [13:20:10<13:43:10, 17.34s/it]                                                         {'loss': 0.8059, 'learning_rate': 1.2023418731634078e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2350/5198 [13:20:10<13:43:10, 17.34s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2351/5198 [13:21:40<30:58:00, 39.16s/it]                                                         {'loss': 0.8389, 'learning_rate': 1.2017316378430244e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2351/5198 [13:21:40<30:58:00, 39.16s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2352/5198 [13:21:58<25:58:47, 32.86s/it]                                                         {'loss': 0.8193, 'learning_rate': 1.2011213242034733e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2352/5198 [13:21:58<25:58:47, 32.86s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2353/5198 [13:22:16<22:16:15, 28.18s/it]                                                         {'loss': 0.8316, 'learning_rate': 1.2005109324816992e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2353/5198 [13:22:16<22:16:15, 28.18s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2354/5198 [13:22:33<19:42:39, 24.95s/it]                                                         {'loss': 0.8006, 'learning_rate': 1.1999004629146775e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2354/5198 [13:22:33<19:42:39, 24.95s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2355/5198 [13:22:51<17:59:54, 22.79s/it]                                                         {'loss': 0.862, 'learning_rate': 1.1992899157394133e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2355/5198 [13:22:51<17:59:54, 22.79s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2356/5198 [13:23:09<16:54:30, 21.42s/it]                                                         {'loss': 0.8332, 'learning_rate': 1.1986792911929418e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2356/5198 [13:23:09<16:54:30, 21.42s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2357/5198 [13:23:27<16:02:57, 20.34s/it]                                                         {'loss': 0.8488, 'learning_rate': 1.198068589512329e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2357/5198 [13:23:27<16:02:57, 20.34s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2358/5198 [13:23:45<15:34:24, 19.74s/it]                                                         {'loss': 0.7725, 'learning_rate': 1.1974578109346702e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2358/5198 [13:23:45<15:34:24, 19.74s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2359/5198 [13:24:03<15:04:23, 19.11s/it]                                                         {'loss': 0.8479, 'learning_rate': 1.1968469556970905e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2359/5198 [13:24:03<15:04:23, 19.11s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2360/5198 [13:24:22<15:00:14, 19.03s/it]                                                         {'loss': 0.8429, 'learning_rate': 1.1962360240367445e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2360/5198 [13:24:22<15:00:14, 19.03s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2361/5198 [13:24:39<14:34:04, 18.49s/it]                                                         {'loss': 0.8856, 'learning_rate': 1.1956250161908179e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2361/5198 [13:24:39<14:34:04, 18.49s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2362/5198 [13:24:55<14:06:00, 17.90s/it]                                                         {'loss': 0.8874, 'learning_rate': 1.195013932396524e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2362/5198 [13:24:56<14:06:00, 17.90s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2363/5198 [13:25:14<14:10:45, 18.01s/it]                                                         {'loss': 0.3689, 'learning_rate': 1.1944027728911072e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2363/5198 [13:25:14<14:10:45, 18.01s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2364/5198 [13:25:32<14:12:01, 18.04s/it]                                                         {'loss': 0.7961, 'learning_rate': 1.1937915379118406e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2364/5198 [13:25:32<14:12:01, 18.04s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2365/5198 [13:25:49<14:01:52, 17.83s/it]                                                         {'loss': 0.8585, 'learning_rate': 1.1931802276960265e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2365/5198 [13:25:49<14:01:52, 17.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2366/5198 [13:26:06<13:54:59, 17.69s/it]                                                         {'loss': 0.8489, 'learning_rate': 1.1925688424809965e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2366/5198 [13:26:06<13:54:59, 17.69s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2367/5198 [13:26:25<13:59:36, 17.79s/it]                                                         {'loss': 0.7732, 'learning_rate': 1.1919573825041115e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2367/5198 [13:26:25<13:59:36, 17.79s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2368/5198 [13:26:43<14:05:32, 17.93s/it]                                                         {'loss': 0.8067, 'learning_rate': 1.1913458480027614e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2368/5198 [13:26:43<14:05:32, 17.93s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2369/5198 [13:27:01<14:09:28, 18.02s/it]                                                         {'loss': 0.828, 'learning_rate': 1.1907342392143646e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2369/5198 [13:27:01<14:09:28, 18.02s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2370/5198 [13:27:19<14:10:02, 18.03s/it]                                                         {'loss': 0.8268, 'learning_rate': 1.1901225563763694e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2370/5198 [13:27:19<14:10:02, 18.03s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2371/5198 [13:27:37<14:06:21, 17.96s/it]                                                         {'loss': 0.7968, 'learning_rate': 1.1895107997262516e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2371/5198 [13:27:37<14:06:21, 17.96s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2372/5198 [13:27:54<13:58:10, 17.80s/it]                                                         {'loss': 0.7958, 'learning_rate': 1.1888989695015166e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2372/5198 [13:27:54<13:58:10, 17.80s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2373/5198 [13:28:12<13:52:03, 17.67s/it]                                                         {'loss': 0.7902, 'learning_rate': 1.1882870659396968e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2373/5198 [13:28:12<13:52:03, 17.67s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2374/5198 [13:28:30<13:57:20, 17.79s/it]                                                         {'loss': 0.8617, 'learning_rate': 1.1876750892783558e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2374/5198 [13:28:30<13:57:20, 17.79s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2375/5198 [13:28:48<13:57:59, 17.81s/it]                                                         {'loss': 0.8037, 'learning_rate': 1.1870630397550831e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2375/5198 [13:28:48<13:57:59, 17.81s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2376/5198 [13:30:19<31:11:45, 39.80s/it]                                                         {'loss': 0.8085, 'learning_rate': 1.1864509176074974e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2376/5198 [13:30:19<31:11:45, 39.80s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2377/5198 [13:30:36<25:56:54, 33.11s/it]                                                         {'loss': 0.8201, 'learning_rate': 1.185838723073246e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2377/5198 [13:30:36<25:56:54, 33.11s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2378/5198 [13:30:53<22:10:29, 28.31s/it]                                                         {'loss': 0.3399, 'learning_rate': 1.1852264563900038e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2378/5198 [13:30:53<22:10:29, 28.31s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2379/5198 [13:31:11<19:43:15, 25.18s/it]                                                         {'loss': 0.8591, 'learning_rate': 1.1846141177954733e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2379/5198 [13:31:11<19:43:15, 25.18s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2380/5198 [13:31:30<18:09:54, 23.21s/it]                                                         {'loss': 0.833, 'learning_rate': 1.1840017075273861e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2380/5198 [13:31:30<18:09:54, 23.21s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2381/5198 [13:31:47<16:50:32, 21.52s/it]                                                         {'loss': 0.8303, 'learning_rate': 1.1833892258235008e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2381/5198 [13:31:47<16:50:32, 21.52s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2382/5198 [13:32:05<15:59:23, 20.44s/it]                                                         {'loss': 0.8552, 'learning_rate': 1.1827766729216035e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2382/5198 [13:32:05<15:59:23, 20.44s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2383/5198 [13:32:23<15:16:07, 19.53s/it]                                                         {'loss': 0.3318, 'learning_rate': 1.1821640490595086e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2383/5198 [13:32:23<15:16:07, 19.53s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2384/5198 [13:32:40<14:42:27, 18.82s/it]                                                         {'loss': 0.8169, 'learning_rate': 1.181551354475058e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2384/5198 [13:32:40<14:42:27, 18.82s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2385/5198 [13:32:57<14:17:46, 18.30s/it]                                                         {'loss': 0.8424, 'learning_rate': 1.1809385894061206e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2385/5198 [13:32:57<14:17:46, 18.30s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2386/5198 [13:33:15<14:08:52, 18.11s/it]                                                         {'loss': 0.8172, 'learning_rate': 1.1803257540905926e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2386/5198 [13:33:15<14:08:52, 18.11s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2387/5198 [13:33:32<14:01:20, 17.96s/it]                                                         {'loss': 0.7776, 'learning_rate': 1.1797128487663982e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2387/5198 [13:33:32<14:01:20, 17.96s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2388/5198 [13:33:50<14:04:55, 18.04s/it]                                                         {'loss': 0.8322, 'learning_rate': 1.1790998736714882e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2388/5198 [13:33:50<14:04:55, 18.04s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2389/5198 [13:34:08<13:54:36, 17.83s/it]                                                         {'loss': 0.798, 'learning_rate': 1.1784868290438404e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2389/5198 [13:34:08<13:54:36, 17.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2390/5198 [13:34:25<13:44:05, 17.61s/it]                                                         {'loss': 0.8399, 'learning_rate': 1.1778737151214606e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2390/5198 [13:34:25<13:44:05, 17.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2391/5198 [13:34:43<13:50:35, 17.75s/it]                                                         {'loss': 0.7928, 'learning_rate': 1.17726053214238e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2391/5198 [13:34:43<13:50:35, 17.75s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2392/5198 [13:35:00<13:40:59, 17.56s/it]                                                         {'loss': 0.8489, 'learning_rate': 1.1766472803446577e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2392/5198 [13:35:00<13:40:59, 17.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2393/5198 [13:35:18<13:44:13, 17.63s/it]                                                         {'loss': 0.8021, 'learning_rate': 1.1760339599663788e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2393/5198 [13:35:18<13:44:13, 17.63s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2394/5198 [13:35:36<13:51:08, 17.78s/it]                                                         {'loss': 0.8073, 'learning_rate': 1.1754205712456556e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2394/5198 [13:35:36<13:51:08, 17.78s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2395/5198 [13:35:54<13:47:50, 17.72s/it]                                                         {'loss': 0.8049, 'learning_rate': 1.1748071144206266e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2395/5198 [13:35:54<13:47:50, 17.72s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2396/5198 [13:36:11<13:47:14, 17.71s/it]                                                         {'loss': 0.3233, 'learning_rate': 1.1741935897294572e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2396/5198 [13:36:11<13:47:14, 17.71s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2397/5198 [13:36:29<13:46:24, 17.70s/it]                                                         {'loss': 0.8621, 'learning_rate': 1.1735799974103388e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2397/5198 [13:36:29<13:46:24, 17.70s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2398/5198 [13:36:47<13:44:57, 17.68s/it]                                                         {'loss': 0.8064, 'learning_rate': 1.1729663377014888e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2398/5198 [13:36:47<13:44:57, 17.68s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2399/5198 [13:37:04<13:40:15, 17.58s/it]                                                         {'loss': 0.8536, 'learning_rate': 1.172352610841151e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2399/5198 [13:37:04<13:40:15, 17.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2400/5198 [13:37:21<13:34:56, 17.48s/it]                                                         {'loss': 0.8427, 'learning_rate': 1.1717388170675954e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2400/5198 [13:37:21<13:34:56, 17.48s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2401/5198 [13:38:50<30:16:54, 38.98s/it]                                                         {'loss': 0.7892, 'learning_rate': 1.1711249566191179e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2401/5198 [13:38:50<30:16:54, 38.98s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2402/5198 [13:39:08<25:22:02, 32.66s/it]                                                         {'loss': 0.8759, 'learning_rate': 1.17051102973404e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2402/5198 [13:39:08<25:22:02, 32.66s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2403/5198 [13:39:25<21:44:44, 28.01s/it]                                                         {'loss': 0.7872, 'learning_rate': 1.1698970366507096e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2403/5198 [13:39:25<21:44:44, 28.01s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2404/5198 [13:39:43<19:21:08, 24.94s/it]                                                         {'loss': 0.8168, 'learning_rate': 1.1692829776074999e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2404/5198 [13:39:43<19:21:08, 24.94s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2405/5198 [13:40:01<17:44:22, 22.87s/it]                                                         {'loss': 0.8441, 'learning_rate': 1.1686688528428099e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2405/5198 [13:40:01<17:44:22, 22.87s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2406/5198 [13:40:19<16:29:03, 21.25s/it]                                                         {'loss': 0.8261, 'learning_rate': 1.1680546625950635e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2406/5198 [13:40:19<16:29:03, 21.25s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2407/5198 [13:40:36<15:29:26, 19.98s/it]                                                         {'loss': 0.8498, 'learning_rate': 1.167440407102711e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2407/5198 [13:40:36<15:29:26, 19.98s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2408/5198 [13:40:53<14:49:13, 19.12s/it]                                                         {'loss': 0.8079, 'learning_rate': 1.1668260866042271e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2408/5198 [13:40:53<14:49:13, 19.12s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2409/5198 [13:41:11<14:31:42, 18.75s/it]                                                         {'loss': 0.8942, 'learning_rate': 1.1662117013381126e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2409/5198 [13:41:11<14:31:42, 18.75s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2410/5198 [13:41:29<14:19:04, 18.49s/it]                                                         {'loss': 0.7915, 'learning_rate': 1.1655972515428928e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2410/5198 [13:41:29<14:19:04, 18.49s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2411/5198 [13:41:46<14:08:00, 18.26s/it]                                                         {'loss': 0.3438, 'learning_rate': 1.1649827374571182e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2411/5198 [13:41:46<14:08:00, 18.26s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2412/5198 [13:42:05<14:08:01, 18.26s/it]                                                         {'loss': 0.8433, 'learning_rate': 1.1643681593193642e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2412/5198 [13:42:05<14:08:01, 18.26s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2413/5198 [13:42:23<14:12:43, 18.37s/it]                                                         {'loss': 0.8532, 'learning_rate': 1.1637535173682318e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2413/5198 [13:42:23<14:12:43, 18.37s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2414/5198 [13:42:41<14:00:51, 18.12s/it]                                                         {'loss': 0.7776, 'learning_rate': 1.1631388118423457e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2414/5198 [13:42:41<14:00:51, 18.12s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2415/5198 [13:42:58<13:52:32, 17.95s/it]                                                         {'loss': 0.8435, 'learning_rate': 1.1625240429803553e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2415/5198 [13:42:58<13:52:32, 17.95s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2416/5198 [13:43:16<13:43:58, 17.77s/it]                                                         {'loss': 0.88, 'learning_rate': 1.1619092110209361e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2416/5198 [13:43:16<13:43:58, 17.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2417/5198 [13:43:33<13:43:35, 17.77s/it]                                                         {'loss': 0.8274, 'learning_rate': 1.1612943162027863e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2417/5198 [13:43:33<13:43:35, 17.77s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2418/5198 [13:43:50<13:33:29, 17.56s/it]                                                         {'loss': 0.8117, 'learning_rate': 1.1606793587646295e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2418/5198 [13:43:50<13:33:29, 17.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2419/5198 [13:44:08<13:35:45, 17.61s/it]                                                         {'loss': 0.848, 'learning_rate': 1.160064338945213e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2419/5198 [13:44:08<13:35:45, 17.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2420/5198 [13:44:25<13:26:33, 17.42s/it]                                                         {'loss': 0.8509, 'learning_rate': 1.1594492569833093e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2420/5198 [13:44:25<13:26:33, 17.42s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2421/5198 [13:44:44<13:41:56, 17.76s/it]                                                         {'loss': 0.8434, 'learning_rate': 1.1588341131177137e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2421/5198 [13:44:44<13:41:56, 17.76s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2422/5198 [13:45:02<13:49:35, 17.93s/it]                                                         {'loss': 0.8143, 'learning_rate': 1.1582189075872467e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2422/5198 [13:45:02<13:49:35, 17.93s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2423/5198 [13:45:21<14:06:15, 18.30s/it]                                                         {'loss': 0.799, 'learning_rate': 1.1576036406307523e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2423/5198 [13:45:21<14:06:15, 18.30s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2424/5198 [13:45:39<13:59:19, 18.15s/it]                                                         {'loss': 0.8475, 'learning_rate': 1.156988312487098e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2424/5198 [13:45:39<13:59:19, 18.15s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2425/5198 [13:45:57<13:57:30, 18.12s/it]                                                         {'loss': 0.8388, 'learning_rate': 1.1563729233951757e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2425/5198 [13:45:57<13:57:30, 18.12s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2426/5198 [13:47:25<30:02:55, 39.02s/it]                                                         {'loss': 0.8054, 'learning_rate': 1.1557574735939003e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2426/5198 [13:47:25<30:02:55, 39.02s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2427/5198 [13:47:43<25:13:20, 32.77s/it]                                                         {'loss': 0.8655, 'learning_rate': 1.1551419633222107e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2427/5198 [13:47:43<25:13:20, 32.77s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2428/5198 [13:48:01<21:50:25, 28.38s/it]                                                         {'loss': 0.8633, 'learning_rate': 1.1545263928190692e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2428/5198 [13:48:01<21:50:25, 28.38s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2429/5198 [13:48:19<19:24:29, 25.23s/it]                                                         {'loss': 0.3456, 'learning_rate': 1.1539107623234618e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2429/5198 [13:48:19<19:24:29, 25.23s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2430/5198 [13:48:36<17:27:24, 22.70s/it]                                                         {'loss': 0.8411, 'learning_rate': 1.153295072074397e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2430/5198 [13:48:36<17:27:24, 22.70s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2431/5198 [13:48:54<16:24:12, 21.34s/it]                                                         {'loss': 0.8431, 'learning_rate': 1.1526793223109072e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2431/5198 [13:48:54<16:24:12, 21.34s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2432/5198 [13:49:11<15:25:21, 20.07s/it]                                                         {'loss': 0.8768, 'learning_rate': 1.1520635132720475e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2432/5198 [13:49:11<15:25:21, 20.07s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2433/5198 [13:49:28<14:38:56, 19.07s/it]                                                         {'loss': 0.8127, 'learning_rate': 1.1514476451968961e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2433/5198 [13:49:28<14:38:56, 19.07s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2434/5198 [13:49:46<14:22:24, 18.72s/it]                                                         {'loss': 0.8799, 'learning_rate': 1.1508317183245545e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2434/5198 [13:49:46<14:22:24, 18.72s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2435/5198 [13:50:03<14:08:30, 18.43s/it]                                                         {'loss': 0.8163, 'learning_rate': 1.1502157328941466e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2435/5198 [13:50:03<14:08:30, 18.43s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2436/5198 [13:50:20<13:46:40, 17.96s/it]                                                         {'loss': 0.8339, 'learning_rate': 1.149599689144819e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2436/5198 [13:50:20<13:46:40, 17.96s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2437/5198 [13:50:37<13:29:16, 17.59s/it]                                                         {'loss': 0.8182, 'learning_rate': 1.1489835873157414e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2437/5198 [13:50:37<13:29:16, 17.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2438/5198 [13:50:54<13:22:14, 17.44s/it]                                                         {'loss': 0.8083, 'learning_rate': 1.1483674276461053e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2438/5198 [13:50:54<13:22:14, 17.44s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2439/5198 [13:51:12<13:23:01, 17.46s/it]                                                         {'loss': 0.8229, 'learning_rate': 1.1477512103751254e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2439/5198 [13:51:12<13:23:01, 17.46s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2440/5198 [13:51:29<13:21:05, 17.43s/it]                                                         {'loss': 0.3478, 'learning_rate': 1.1471349357420384e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2440/5198 [13:51:29<13:21:05, 17.43s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2441/5198 [13:51:46<13:20:26, 17.42s/it]                                                         {'loss': 0.8148, 'learning_rate': 1.1465186039861033e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2441/5198 [13:51:46<13:20:26, 17.42s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2442/5198 [13:52:04<13:20:29, 17.43s/it]                                                         {'loss': 0.8021, 'learning_rate': 1.1459022153466016e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2442/5198 [13:52:04<13:20:29, 17.43s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2443/5198 [13:52:21<13:21:56, 17.47s/it]                                                         {'loss': 0.7753, 'learning_rate': 1.1452857700628362e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2443/5198 [13:52:21<13:21:56, 17.47s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2444/5198 [13:52:39<13:25:31, 17.55s/it]                                                         {'loss': 0.86, 'learning_rate': 1.1446692683741326e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2444/5198 [13:52:39<13:25:31, 17.55s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2445/5198 [13:52:57<13:28:28, 17.62s/it]                                                         {'loss': 0.8593, 'learning_rate': 1.1440527105198377e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2445/5198 [13:52:57<13:28:28, 17.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2446/5198 [13:53:15<13:31:49, 17.70s/it]                                                         {'loss': 0.8046, 'learning_rate': 1.143436096739321e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2446/5198 [13:53:15<13:31:49, 17.70s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2447/5198 [13:53:33<13:31:03, 17.69s/it]                                                         {'loss': 0.3858, 'learning_rate': 1.1428194272719729e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2447/5198 [13:53:33<13:31:03, 17.69s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2448/5198 [13:53:50<13:24:06, 17.54s/it]                                                         {'loss': 0.856, 'learning_rate': 1.1422027023572052e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2448/5198 [13:53:50<13:24:06, 17.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2449/5198 [13:54:07<13:14:57, 17.35s/it]                                                         {'loss': 0.8548, 'learning_rate': 1.1415859222344525e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2449/5198 [13:54:07<13:14:57, 17.35s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2450/5198 [13:54:24<13:20:23, 17.48s/it]                                                         {'loss': 0.8528, 'learning_rate': 1.14096908714317e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2450/5198 [13:54:24<13:20:23, 17.48s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2451/5198 [13:55:55<30:01:49, 39.36s/it]                                                         {'loss': 0.7782, 'learning_rate': 1.1403521973228342e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2451/5198 [13:55:55<30:01:49, 39.36s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2452/5198 [13:56:12<24:56:30, 32.70s/it]                                                         {'loss': 0.8868, 'learning_rate': 1.1397352530129428e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2452/5198 [13:56:12<24:56:30, 32.70s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2453/5198 [13:56:30<21:33:33, 28.27s/it]                                                         {'loss': 0.783, 'learning_rate': 1.139118254453015e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2453/5198 [13:56:30<21:33:33, 28.27s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2454/5198 [13:56:47<19:05:16, 25.04s/it]                                                         {'loss': 0.7998, 'learning_rate': 1.1385012018825907e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2454/5198 [13:56:47<19:05:16, 25.04s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2455/5198 [13:57:04<17:13:54, 22.62s/it]                                                         {'loss': 0.8264, 'learning_rate': 1.1378840955412313e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2455/5198 [13:57:04<17:13:54, 22.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2456/5198 [13:57:22<16:04:24, 21.10s/it]                                                         {'loss': 0.8821, 'learning_rate': 1.1372669356685185e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2456/5198 [13:57:22<16:04:24, 21.10s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2457/5198 [13:57:40<15:22:45, 20.20s/it]                                                         {'loss': 0.8, 'learning_rate': 1.1366497225040549e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2457/5198 [13:57:40<15:22:45, 20.20s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2458/5198 [13:57:58<14:48:53, 19.46s/it]                                                         {'loss': 0.8686, 'learning_rate': 1.1360324562874643e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2458/5198 [13:57:58<14:48:53, 19.46s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2459/5198 [13:58:15<14:15:59, 18.75s/it]                                                         {'loss': 0.3349, 'learning_rate': 1.1354151372583901e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2459/5198 [13:58:15<14:15:59, 18.75s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2460/5198 [13:58:32<13:56:16, 18.33s/it]                                                         {'loss': 0.8587, 'learning_rate': 1.1347977656564974e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2460/5198 [13:58:32<13:56:16, 18.33s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2461/5198 [13:58:50<13:48:28, 18.16s/it]                                                         {'loss': 0.8105, 'learning_rate': 1.1341803417214705e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2461/5198 [13:58:50<13:48:28, 18.16s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2462/5198 [13:59:07<13:32:16, 17.81s/it]                                                         {'loss': 0.7913, 'learning_rate': 1.1335628656930153e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2462/5198 [13:59:07<13:32:16, 17.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2463/5198 [13:59:24<13:25:59, 17.68s/it]                                                         {'loss': 0.7694, 'learning_rate': 1.132945337810857e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2463/5198 [13:59:24<13:25:59, 17.68s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2464/5198 [13:59:42<13:24:59, 17.67s/it]                                                         {'loss': 0.8336, 'learning_rate': 1.132327758314741e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2464/5198 [13:59:42<13:24:59, 17.67s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2465/5198 [14:00:00<13:34:14, 17.88s/it]                                                         {'loss': 0.8051, 'learning_rate': 1.131710127444433e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2465/5198 [14:00:00<13:34:14, 17.88s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2466/5198 [14:00:18<13:27:33, 17.74s/it]                                                         {'loss': 0.8717, 'learning_rate': 1.1310924454397187e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2466/5198 [14:00:18<13:27:33, 17.74s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2467/5198 [14:00:35<13:14:44, 17.46s/it]                                                         {'loss': 0.8144, 'learning_rate': 1.1304747125404031e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2467/5198 [14:00:35<13:14:44, 17.46s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2468/5198 [14:00:52<13:20:00, 17.58s/it]                                                         {'loss': 0.8014, 'learning_rate': 1.129856928986312e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2468/5198 [14:00:52<13:20:00, 17.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2469/5198 [14:01:11<13:29:04, 17.79s/it]                                                         {'loss': 0.8323, 'learning_rate': 1.12923909501729e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2469/5198 [14:01:11<13:29:04, 17.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2470/5198 [14:01:28<13:22:49, 17.66s/it]                                                         {'loss': 0.8578, 'learning_rate': 1.1286212108732015e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2470/5198 [14:01:28<13:22:49, 17.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2471/5198 [14:01:45<13:17:30, 17.55s/it]                                                         {'loss': 0.8143, 'learning_rate': 1.1280032767939302e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2471/5198 [14:01:45<13:17:30, 17.55s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2472/5198 [14:02:03<13:20:45, 17.63s/it]                                                         {'loss': 0.8864, 'learning_rate': 1.1273852930193798e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2472/5198 [14:02:03<13:20:45, 17.63s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2473/5198 [14:02:20<13:12:52, 17.46s/it]                                                         {'loss': 0.8592, 'learning_rate': 1.1267672597894725e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2473/5198 [14:02:20<13:12:52, 17.46s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2474/5198 [14:02:38<13:20:06, 17.62s/it]                                                         {'loss': 0.7895, 'learning_rate': 1.12614917734415e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2474/5198 [14:02:38<13:20:06, 17.62s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2475/5198 [14:02:56<13:21:41, 17.66s/it]                                                         {'loss': 0.8455, 'learning_rate': 1.1255310459233737e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2475/5198 [14:02:56<13:21:41, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2476/5198 [14:04:26<29:40:16, 39.24s/it]                                                         {'loss': 0.8285, 'learning_rate': 1.1249128657671233e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2476/5198 [14:04:26<29:40:16, 39.24s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2477/5198 [14:04:44<24:49:36, 32.85s/it]                                                         {'loss': 0.8098, 'learning_rate': 1.1242946371153974e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2477/5198 [14:04:44<24:49:36, 32.85s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2478/5198 [14:05:00<21:11:07, 28.04s/it]                                                         {'loss': 0.8223, 'learning_rate': 1.1236763602082136e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2478/5198 [14:05:00<21:11:07, 28.04s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2479/5198 [14:05:17<18:40:12, 24.72s/it]                                                         {'loss': 0.79, 'learning_rate': 1.1230580352856088e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2479/5198 [14:05:17<18:40:12, 24.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2480/5198 [14:05:34<16:55:02, 22.41s/it]                                                         {'loss': 0.7916, 'learning_rate': 1.1224396625876375e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2480/5198 [14:05:34<16:55:02, 22.41s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2481/5198 [14:05:51<15:36:02, 20.67s/it]                                                         {'loss': 0.8557, 'learning_rate': 1.1218212423543734e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2481/5198 [14:05:51<15:36:02, 20.67s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2482/5198 [14:06:09<15:03:20, 19.96s/it]                                                         {'loss': 0.7674, 'learning_rate': 1.1212027748259086e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2482/5198 [14:06:09<15:03:20, 19.96s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2483/5198 [14:06:28<14:40:07, 19.45s/it]                                                         {'loss': 0.7365, 'learning_rate': 1.1205842602423537e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2483/5198 [14:06:28<14:40:07, 19.45s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2484/5198 [14:06:44<14:02:47, 18.63s/it]                                                         {'loss': 0.758, 'learning_rate': 1.1199656988438373e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2484/5198 [14:06:44<14:02:47, 18.63s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2485/5198 [14:07:02<13:56:36, 18.50s/it]                                                         {'loss': 0.9007, 'learning_rate': 1.1193470908705055e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2485/5198 [14:07:02<13:56:36, 18.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2486/5198 [14:07:20<13:46:06, 18.28s/it]                                                         {'loss': 0.7834, 'learning_rate': 1.1187284365625241e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2486/5198 [14:07:20<13:46:06, 18.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2487/5198 [14:07:37<13:31:01, 17.95s/it]                                                         {'loss': 0.8595, 'learning_rate': 1.1181097361600754e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2487/5198 [14:07:37<13:31:01, 17.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2488/5198 [14:07:55<13:29:43, 17.93s/it]                                                         {'loss': 0.8775, 'learning_rate': 1.1174909899033608e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2488/5198 [14:07:55<13:29:43, 17.93s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2489/5198 [14:08:14<13:37:59, 18.12s/it]                                                         {'loss': 0.8132, 'learning_rate': 1.1168721980325987e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2489/5198 [14:08:14<13:37:59, 18.12s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2490/5198 [14:08:32<13:36:05, 18.08s/it]                                                         {'loss': 0.8305, 'learning_rate': 1.1162533607880251e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2490/5198 [14:08:32<13:36:05, 18.08s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2491/5198 [14:08:50<13:31:21, 17.98s/it]                                                         {'loss': 0.8416, 'learning_rate': 1.1156344784098942e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2491/5198 [14:08:50<13:31:21, 17.98s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2492/5198 [14:09:07<13:27:56, 17.91s/it]                                                         {'loss': 0.8068, 'learning_rate': 1.1150155511384772e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2492/5198 [14:09:07<13:27:56, 17.91s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2493/5198 [14:09:25<13:28:43, 17.94s/it]                                                         {'loss': 0.8251, 'learning_rate': 1.1143965792140631e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2493/5198 [14:09:25<13:28:43, 17.94s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2494/5198 [14:09:42<13:15:45, 17.66s/it]                                                         {'loss': 0.8326, 'learning_rate': 1.1137775628769584e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2494/5198 [14:09:42<13:15:45, 17.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2495/5198 [14:09:59<13:02:38, 17.37s/it]                                                         {'loss': 0.8668, 'learning_rate': 1.1131585023674863e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2495/5198 [14:09:59<13:02:38, 17.37s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2496/5198 [14:10:17<13:09:44, 17.54s/it]                                                         {'loss': 0.8289, 'learning_rate': 1.1125393979259874e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2496/5198 [14:10:17<13:09:44, 17.54s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2497/5198 [14:10:35<13:17:39, 17.72s/it]                                                         {'loss': 0.798, 'learning_rate': 1.1119202497928192e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2497/5198 [14:10:35<13:17:39, 17.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2498/5198 [14:10:53<13:22:13, 17.83s/it]                                                         {'loss': 0.8817, 'learning_rate': 1.1113010582083568e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2498/5198 [14:10:53<13:22:13, 17.83s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2499/5198 [14:11:11<13:24:03, 17.87s/it]                                                         {'loss': 0.7936, 'learning_rate': 1.1106818234129913e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2499/5198 [14:11:11<13:24:03, 17.87s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2500/5198 [14:11:30<13:34:06, 18.10s/it]                                                         {'loss': 0.8469, 'learning_rate': 1.1100625456471307e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2500/5198 [14:11:30<13:34:06, 18.10s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2501/5198 [14:13:01<29:56:56, 39.98s/it]                                                         {'loss': 0.8063, 'learning_rate': 1.1094432251512006e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2501/5198 [14:13:01<29:56:56, 39.98s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2502/5198 [14:13:18<24:46:19, 33.08s/it]                                                         {'loss': 0.8397, 'learning_rate': 1.1088238621656422e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2502/5198 [14:13:18<24:46:19, 33.08s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2503/5198 [14:13:36<21:25:34, 28.62s/it]                                                         {'loss': 0.7452, 'learning_rate': 1.1082044569309138e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2503/5198 [14:13:36<21:25:34, 28.62s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2504/5198 [14:13:54<18:57:23, 25.33s/it]                                                         {'loss': 0.8229, 'learning_rate': 1.1075850096874894e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2504/5198 [14:13:54<18:57:23, 25.33s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2505/5198 [14:14:10<17:02:23, 22.78s/it]                                                         {'loss': 0.8247, 'learning_rate': 1.1069655206758603e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2505/5198 [14:14:10<17:02:23, 22.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2506/5198 [14:14:28<15:48:03, 21.13s/it]                                                         {'loss': 0.352, 'learning_rate': 1.1063459901365325e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2506/5198 [14:14:28<15:48:03, 21.13s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2507/5198 [14:14:45<14:53:46, 19.93s/it]                                                         {'loss': 0.8345, 'learning_rate': 1.1057264183100303e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2507/5198 [14:14:45<14:53:46, 19.93s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2508/5198 [14:15:02<14:21:04, 19.21s/it]                                                         {'loss': 0.7801, 'learning_rate': 1.1051068054368921e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2508/5198 [14:15:02<14:21:04, 19.21s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2509/5198 [14:15:20<14:04:18, 18.84s/it]                                                         {'loss': 0.8891, 'learning_rate': 1.104487151757673e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2509/5198 [14:15:20<14:04:18, 18.84s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2510/5198 [14:15:39<13:58:03, 18.71s/it]                                                         {'loss': 0.8425, 'learning_rate': 1.1038674575129442e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2510/5198 [14:15:39<13:58:03, 18.71s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2511/5198 [14:15:56<13:36:09, 18.22s/it]                                                         {'loss': 0.8959, 'learning_rate': 1.1032477229432921e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2511/5198 [14:15:56<13:36:09, 18.22s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2512/5198 [14:16:14<13:28:53, 18.07s/it]                                                         {'loss': 0.8707, 'learning_rate': 1.1026279482893187e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2512/5198 [14:16:14<13:28:53, 18.07s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2513/5198 [14:16:31<13:15:28, 17.78s/it]                                                         {'loss': 0.8535, 'learning_rate': 1.1020081337916425e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2513/5198 [14:16:31<13:15:28, 17.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2514/5198 [14:16:48<13:12:49, 17.72s/it]                                                         {'loss': 0.8333, 'learning_rate': 1.1013882796908963e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2514/5198 [14:16:48<13:12:49, 17.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2515/5198 [14:17:06<13:13:37, 17.75s/it]                                                         {'loss': 0.8618, 'learning_rate': 1.1007683862277292e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2515/5198 [14:17:06<13:13:37, 17.75s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2516/5198 [14:17:23<13:06:14, 17.59s/it]                                                         {'loss': 0.8161, 'learning_rate': 1.1001484536428052e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2516/5198 [14:17:23<13:06:14, 17.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2517/5198 [14:17:42<13:15:44, 17.81s/it]                                                         {'loss': 0.7731, 'learning_rate': 1.0995284821768029e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2517/5198 [14:17:42<13:15:44, 17.81s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2518/5198 [14:17:59<13:12:47, 17.75s/it]                                                         {'loss': 0.8424, 'learning_rate': 1.098908472070417e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2518/5198 [14:17:59<13:12:47, 17.75s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2519/5198 [14:18:17<13:09:14, 17.68s/it]                                                         {'loss': 0.8866, 'learning_rate': 1.0982884235643567e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2519/5198 [14:18:17<13:09:14, 17.68s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2520/5198 [14:18:35<13:11:41, 17.74s/it]                                                         {'loss': 0.8453, 'learning_rate': 1.0976683368993464e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2520/5198 [14:18:35<13:11:41, 17.74s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2521/5198 [14:18:53<13:17:35, 17.88s/it]                                                         {'loss': 0.713, 'learning_rate': 1.0970482123161249e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2521/5198 [14:18:53<13:17:35, 17.88s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2522/5198 [14:19:11<13:24:21, 18.03s/it]                                                         {'loss': 0.8434, 'learning_rate': 1.0964280500554459e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2522/5198 [14:19:11<13:24:21, 18.03s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2523/5198 [14:19:28<13:02:52, 17.56s/it]                                                         {'loss': 0.739, 'learning_rate': 1.0958078503580776e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2523/5198 [14:19:28<13:02:52, 17.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2524/5198 [14:19:45<12:57:10, 17.44s/it]                                                         {'loss': 0.8284, 'learning_rate': 1.0951876134648032e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2524/5198 [14:19:45<12:57:10, 17.44s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2525/5198 [14:20:02<12:52:45, 17.35s/it]                                                         {'loss': 0.8073, 'learning_rate': 1.0945673396164198e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2525/5198 [14:20:02<12:52:45, 17.35s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2526/5198 [14:21:33<29:18:48, 39.49s/it]                                                         {'loss': 0.8643, 'learning_rate': 1.0939470290537389e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2526/5198 [14:21:33<29:18:48, 39.49s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2527/5198 [14:21:51<24:23:14, 32.87s/it]                                                         {'loss': 0.8268, 'learning_rate': 1.0933266820175868e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2527/5198 [14:21:51<24:23:14, 32.87s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2528/5198 [14:22:08<21:02:18, 28.37s/it]                                                         {'loss': 0.8852, 'learning_rate': 1.0927062987488035e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2528/5198 [14:22:08<21:02:18, 28.37s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2529/5198 [14:22:25<18:25:36, 24.85s/it]                                                         {'loss': 0.8684, 'learning_rate': 1.0920858794882429e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2529/5198 [14:22:25<18:25:36, 24.85s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2530/5198 [14:22:42<16:45:28, 22.61s/it]                                                         {'loss': 0.8203, 'learning_rate': 1.0914654244767736e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2530/5198 [14:22:42<16:45:28, 22.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2531/5198 [14:23:02<16:05:13, 21.71s/it]                                                         {'loss': 0.8595, 'learning_rate': 1.0908449339552769e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2531/5198 [14:23:02<16:05:13, 21.71s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2532/5198 [14:23:19<15:06:17, 20.40s/it]                                                         {'loss': 0.8411, 'learning_rate': 1.0902244081646489e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2532/5198 [14:23:19<15:06:17, 20.40s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2533/5198 [14:23:37<14:30:24, 19.60s/it]                                                         {'loss': 0.8465, 'learning_rate': 1.0896038473457993e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2533/5198 [14:23:37<14:30:24, 19.60s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2534/5198 [14:23:54<13:56:08, 18.83s/it]                                                         {'loss': 0.8672, 'learning_rate': 1.0889832517396511e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2534/5198 [14:23:54<13:56:08, 18.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2535/5198 [14:24:12<13:37:03, 18.41s/it]                                                         {'loss': 0.8673, 'learning_rate': 1.0883626215871408e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2535/5198 [14:24:12<13:37:03, 18.41s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2536/5198 [14:24:29<13:29:05, 18.24s/it]                                                         {'loss': 0.8029, 'learning_rate': 1.0877419571292183e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2536/5198 [14:24:29<13:29:05, 18.24s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2537/5198 [14:24:47<13:25:56, 18.17s/it]                                                         {'loss': 0.8271, 'learning_rate': 1.0871212586068469e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2537/5198 [14:24:47<13:25:56, 18.17s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2538/5198 [14:25:05<13:22:07, 18.09s/it]                                                         {'loss': 0.8632, 'learning_rate': 1.0865005262610033e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2538/5198 [14:25:05<13:22:07, 18.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2539/5198 [14:25:24<13:26:34, 18.20s/it]                                                         {'loss': 0.806, 'learning_rate': 1.085879760332677e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2539/5198 [14:25:24<13:26:34, 18.20s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2540/5198 [14:25:41<13:13:11, 17.91s/it]                                                         {'loss': 0.7941, 'learning_rate': 1.085258961062871e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2540/5198 [14:25:41<13:13:11, 17.91s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2541/5198 [14:25:59<13:13:07, 17.91s/it]                                                         {'loss': 0.8752, 'learning_rate': 1.0846381286926007e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2541/5198 [14:25:59<13:13:07, 17.91s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2542/5198 [14:26:17<13:15:14, 17.96s/it]                                                         {'loss': 0.8281, 'learning_rate': 1.0840172634628948e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2542/5198 [14:26:17<13:15:14, 17.96s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2543/5198 [14:26:36<13:24:22, 18.18s/it]                                                         {'loss': 0.8379, 'learning_rate': 1.0833963656147944e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2543/5198 [14:26:36<13:24:22, 18.18s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2544/5198 [14:26:53<13:13:49, 17.95s/it]                                                         {'loss': 0.8675, 'learning_rate': 1.082775435389353e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2544/5198 [14:26:53<13:13:49, 17.95s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2545/5198 [14:27:11<13:09:39, 17.86s/it]                                                         {'loss': 0.8405, 'learning_rate': 1.0821544730276379e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2545/5198 [14:27:11<13:09:39, 17.86s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2546/5198 [14:27:28<12:59:43, 17.64s/it]                                                         {'loss': 0.8436, 'learning_rate': 1.0815334787707277e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2546/5198 [14:27:28<12:59:43, 17.64s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2547/5198 [14:27:46<13:07:26, 17.82s/it]                                                         {'loss': 0.8336, 'learning_rate': 1.0809124528597138e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2547/5198 [14:27:46<13:07:26, 17.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2548/5198 [14:28:03<12:52:17, 17.49s/it]                                                         {'loss': 0.8416, 'learning_rate': 1.0802913955356998e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2548/5198 [14:28:03<12:52:17, 17.49s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2549/5198 [14:28:20<12:53:50, 17.53s/it]                                                         {'loss': 0.7831, 'learning_rate': 1.0796703070398016e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2549/5198 [14:28:20<12:53:50, 17.53s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2550/5198 [14:28:38<12:59:23, 17.66s/it]                                                         {'loss': 0.8362, 'learning_rate': 1.079049187613147e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2550/5198 [14:28:38<12:59:23, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2551/5198 [14:30:09<29:08:54, 39.64s/it]                                                         {'loss': 0.8411, 'learning_rate': 1.0784280374968761e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2551/5198 [14:30:09<29:08:54, 39.64s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2552/5198 [14:30:28<24:24:21, 33.21s/it]                                                         {'loss': 0.8606, 'learning_rate': 1.0778068569321403e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2552/5198 [14:30:28<24:24:21, 33.21s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2553/5198 [14:30:46<21:02:55, 28.65s/it]                                                         {'loss': 0.7521, 'learning_rate': 1.077185646160104e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2553/5198 [14:30:46<21:02:55, 28.65s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2554/5198 [14:31:02<18:26:48, 25.12s/it]                                                         {'loss': 0.8558, 'learning_rate': 1.0765644054219422e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2554/5198 [14:31:02<18:26:48, 25.12s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2555/5198 [14:31:20<16:48:06, 22.89s/it]                                                         {'loss': 0.8485, 'learning_rate': 1.0759431349588421e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2555/5198 [14:31:20<16:48:06, 22.89s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2556/5198 [14:31:37<15:27:22, 21.06s/it]                                                         {'loss': 0.8351, 'learning_rate': 1.0753218350120023e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2556/5198 [14:31:37<15:27:22, 21.06s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2557/5198 [14:31:55<14:45:22, 20.11s/it]                                                         {'loss': 0.3398, 'learning_rate': 1.0747005058226325e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2557/5198 [14:31:55<14:45:22, 20.11s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2558/5198 [14:32:12<14:05:43, 19.22s/it]                                                         {'loss': 0.8492, 'learning_rate': 1.0740791476319543e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2558/5198 [14:32:12<14:05:43, 19.22s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2559/5198 [14:32:30<13:45:53, 18.78s/it]                                                         {'loss': 0.8066, 'learning_rate': 1.0734577606812007e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2559/5198 [14:32:30<13:45:53, 18.78s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2560/5198 [14:32:47<13:21:43, 18.23s/it]                                                         {'loss': 0.8455, 'learning_rate': 1.0728363452116149e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2560/5198 [14:32:47<13:21:43, 18.23s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2561/5198 [14:33:05<13:20:55, 18.22s/it]                                                         {'loss': 0.8057, 'learning_rate': 1.0722149014644523e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2561/5198 [14:33:05<13:20:55, 18.22s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2562/5198 [14:33:23<13:14:48, 18.09s/it]                                                         {'loss': 0.8326, 'learning_rate': 1.0715934296809782e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2562/5198 [14:33:23<13:14:48, 18.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2563/5198 [14:33:41<13:14:15, 18.09s/it]                                                         {'loss': 0.7596, 'learning_rate': 1.0709719301024698e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2563/5198 [14:33:41<13:14:15, 18.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2564/5198 [14:33:58<13:01:27, 17.80s/it]                                                         {'loss': 0.824, 'learning_rate': 1.0703504029702148e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2564/5198 [14:33:58<13:01:27, 17.80s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2565/5198 [14:34:16<13:10:14, 18.01s/it]                                                         {'loss': 0.7881, 'learning_rate': 1.0697288485255107e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2565/5198 [14:34:16<13:10:14, 18.01s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2566/5198 [14:34:35<13:12:29, 18.07s/it]                                                         {'loss': 0.8296, 'learning_rate': 1.0691072670096669e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2566/5198 [14:34:35<13:12:29, 18.07s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2567/5198 [14:34:52<13:04:25, 17.89s/it]                                                         {'loss': 0.8899, 'learning_rate': 1.0684856586640026e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2567/5198 [14:34:52<13:04:25, 17.89s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2568/5198 [14:35:10<13:02:23, 17.85s/it]                                                         {'loss': 0.8274, 'learning_rate': 1.0678640237298476e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2568/5198 [14:35:10<13:02:23, 17.85s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2569/5198 [14:35:28<13:01:34, 17.84s/it]                                                         {'loss': 0.8831, 'learning_rate': 1.0672423624485423e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2569/5198 [14:35:28<13:01:34, 17.84s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2570/5198 [14:35:46<13:06:05, 17.95s/it]                                                         {'loss': 0.8307, 'learning_rate': 1.0666206750614363e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2570/5198 [14:35:46<13:06:05, 17.95s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2571/5198 [14:36:04<13:13:38, 18.13s/it]                                                         {'loss': 0.8305, 'learning_rate': 1.0659989618098904e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2571/5198 [14:36:04<13:13:38, 18.13s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2572/5198 [14:36:22<13:04:13, 17.92s/it]                                                         {'loss': 0.829, 'learning_rate': 1.065377222935275e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2572/5198 [14:36:22<13:04:13, 17.92s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2573/5198 [14:36:40<13:07:05, 17.99s/it]                                                         {'loss': 0.8161, 'learning_rate': 1.0647554586789708e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2573/5198 [14:36:40<13:07:05, 17.99s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2574/5198 [14:36:58<13:06:57, 17.99s/it]                                                         {'loss': 0.7873, 'learning_rate': 1.064133669282368e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2574/5198 [14:36:58<13:06:57, 17.99s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2575/5198 [14:37:16<13:02:40, 17.90s/it]                                                         {'loss': 0.7956, 'learning_rate': 1.0635118549868668e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2575/5198 [14:37:16<13:02:40, 17.90s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2576/5198 [14:38:46<28:48:09, 39.55s/it]                                                         {'loss': 0.7803, 'learning_rate': 1.0628900160338764e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2576/5198 [14:38:46<28:48:09, 39.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2577/5198 [14:39:03<23:59:02, 32.94s/it]                                                         {'loss': 0.8869, 'learning_rate': 1.0622681526648167e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2577/5198 [14:39:03<23:59:02, 32.94s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2578/5198 [14:39:21<20:40:54, 28.42s/it]                                                         {'loss': 0.8202, 'learning_rate': 1.0616462651211156e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2578/5198 [14:39:21<20:40:54, 28.42s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2579/5198 [14:39:40<18:31:03, 25.45s/it]                                                         {'loss': 0.8152, 'learning_rate': 1.0610243536442125e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2579/5198 [14:39:40<18:31:03, 25.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2580/5198 [14:39:57<16:39:47, 22.91s/it]                                                         {'loss': 0.7684, 'learning_rate': 1.0604024184755539e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2580/5198 [14:39:57<16:39:47, 22.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2581/5198 [14:40:15<15:40:15, 21.56s/it]                                                         {'loss': 0.7976, 'learning_rate': 1.0597804598565969e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2581/5198 [14:40:15<15:40:15, 21.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2582/5198 [14:40:32<14:38:15, 20.14s/it]                                                         {'loss': 0.8171, 'learning_rate': 1.0591584780288069e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2582/5198 [14:40:32<14:38:15, 20.14s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2583/5198 [14:40:49<13:59:12, 19.26s/it]                                                         {'loss': 0.7865, 'learning_rate': 1.0585364732336587e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2583/5198 [14:40:49<13:59:12, 19.26s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2584/5198 [14:41:06<13:22:34, 18.42s/it]                                                         {'loss': 0.8523, 'learning_rate': 1.0579144457126365e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2584/5198 [14:41:06<13:22:34, 18.42s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2585/5198 [14:41:24<13:17:54, 18.32s/it]                                                         {'loss': 0.8506, 'learning_rate': 1.057292395707232e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2585/5198 [14:41:24<13:17:54, 18.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2586/5198 [14:41:41<13:11:46, 18.19s/it]                                                         {'loss': 0.8216, 'learning_rate': 1.0566703234589471e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2586/5198 [14:41:41<13:11:46, 18.19s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2587/5198 [14:41:59<13:05:39, 18.05s/it]                                                         {'loss': 0.8059, 'learning_rate': 1.0560482292092912e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2587/5198 [14:41:59<13:05:39, 18.05s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2588/5198 [14:42:17<12:58:55, 17.91s/it]                                                         {'loss': 0.8359, 'learning_rate': 1.0554261131997833e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2588/5198 [14:42:17<12:58:55, 17.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2589/5198 [14:42:35<12:56:53, 17.87s/it]                                                         {'loss': 0.7835, 'learning_rate': 1.0548039756719497e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2589/5198 [14:42:35<12:56:53, 17.87s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2590/5198 [14:42:52<12:52:39, 17.78s/it]                                                         {'loss': 0.7775, 'learning_rate': 1.054181816867326e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2590/5198 [14:42:52<12:52:39, 17.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2591/5198 [14:43:11<13:00:48, 17.97s/it]                                                         {'loss': 0.8164, 'learning_rate': 1.053559637027455e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2591/5198 [14:43:11<13:00:48, 17.97s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2592/5198 [14:43:27<12:46:11, 17.64s/it]                                                         {'loss': 0.8919, 'learning_rate': 1.0529374363938888e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2592/5198 [14:43:27<12:46:11, 17.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2593/5198 [14:43:46<12:55:46, 17.87s/it]                                                         {'loss': 0.7881, 'learning_rate': 1.0523152152081875e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2593/5198 [14:43:46<12:55:46, 17.87s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2594/5198 [14:44:04<12:59:23, 17.96s/it]                                                         {'loss': 0.8559, 'learning_rate': 1.051692973711918e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2594/5198 [14:44:04<12:59:23, 17.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2595/5198 [14:44:22<12:57:44, 17.93s/it]                                                         {'loss': 0.8692, 'learning_rate': 1.0510707121466568e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2595/5198 [14:44:22<12:57:44, 17.93s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2596/5198 [14:44:39<12:46:03, 17.66s/it]                                                         {'loss': 0.8716, 'learning_rate': 1.0504484307539864e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2596/5198 [14:44:39<12:46:03, 17.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2597/5198 [14:44:57<12:45:28, 17.66s/it]                                                         {'loss': 0.8146, 'learning_rate': 1.0498261297754984e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2597/5198 [14:44:57<12:45:28, 17.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2598/5198 [14:45:14<12:48:56, 17.74s/it]                                                         {'loss': 0.8081, 'learning_rate': 1.0492038094527907e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2598/5198 [14:45:14<12:48:56, 17.74s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2599/5198 [14:45:31<12:36:17, 17.46s/it]                                                         {'loss': 0.7793, 'learning_rate': 1.0485814700274706e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2599/5198 [14:45:31<12:36:17, 17.46s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2600/5198 [14:45:48<12:26:05, 17.23s/it]                                                         {'loss': 0.8307, 'learning_rate': 1.047959111741151e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2600/5198 [14:45:48<12:26:05, 17.23s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2601/5198 [14:47:15<27:29:21, 38.11s/it]                                                         {'loss': 0.8468, 'learning_rate': 1.0473367348354529e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2601/5198 [14:47:15<27:29:21, 38.11s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2602/5198 [14:47:33<23:05:27, 32.02s/it]                                                         {'loss': 0.342, 'learning_rate': 1.0467143395520044e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2602/5198 [14:47:33<23:05:27, 32.02s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2603/5198 [14:47:49<19:42:22, 27.34s/it]                                                         {'loss': 0.8891, 'learning_rate': 1.046091926132441e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2603/5198 [14:47:49<19:42:22, 27.34s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2604/5198 [14:48:06<17:24:55, 24.17s/it]                                                         {'loss': 0.812, 'learning_rate': 1.0454694948184045e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2604/5198 [14:48:06<17:24:55, 24.17s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2605/5198 [14:48:24<16:08:03, 22.40s/it]                                                         {'loss': 0.8015, 'learning_rate': 1.044847045851545e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2605/5198 [14:48:24<16:08:03, 22.40s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2606/5198 [14:48:41<14:55:16, 20.72s/it]                                                         {'loss': 0.8605, 'learning_rate': 1.044224579473518e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2606/5198 [14:48:41<14:55:16, 20.72s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2607/5198 [14:48:58<14:05:59, 19.59s/it]                                                         {'loss': 0.8429, 'learning_rate': 1.0436020959259862e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2607/5198 [14:48:58<14:05:59, 19.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2608/5198 [14:49:15<13:38:25, 18.96s/it]                                                         {'loss': 0.8268, 'learning_rate': 1.0429795954506203e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2608/5198 [14:49:15<13:38:25, 18.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2609/5198 [14:49:33<13:24:40, 18.65s/it]                                                         {'loss': 0.9052, 'learning_rate': 1.0423570782890951e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2609/5198 [14:49:33<13:24:40, 18.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2610/5198 [14:49:52<13:19:37, 18.54s/it]                                                         {'loss': 0.7359, 'learning_rate': 1.0417345446830938e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2610/5198 [14:49:52<13:19:37, 18.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2611/5198 [14:50:09<13:08:11, 18.28s/it]                                                         {'loss': 0.7983, 'learning_rate': 1.0411119948743052e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2611/5198 [14:50:09<13:08:11, 18.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2612/5198 [14:50:26<12:45:36, 17.76s/it]                                                         {'loss': 0.8102, 'learning_rate': 1.0404894291044247e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2612/5198 [14:50:26<12:45:36, 17.76s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2613/5198 [14:50:43<12:41:40, 17.68s/it]                                                         {'loss': 0.779, 'learning_rate': 1.0398668476151538e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2613/5198 [14:50:43<12:41:40, 17.68s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2614/5198 [14:51:01<12:36:08, 17.56s/it]                                                         {'loss': 0.8253, 'learning_rate': 1.0392442506482e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2614/5198 [14:51:01<12:36:08, 17.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2615/5198 [14:51:18<12:33:40, 17.51s/it]                                                         {'loss': 0.3359, 'learning_rate': 1.038621638445277e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2615/5198 [14:51:18<12:33:40, 17.51s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2616/5198 [14:51:35<12:26:46, 17.35s/it]                                                         {'loss': 0.863, 'learning_rate': 1.037999011248104e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2616/5198 [14:51:35<12:26:46, 17.35s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2617/5198 [14:51:53<12:39:28, 17.66s/it]                                                         {'loss': 0.8035, 'learning_rate': 1.0373763692984062e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2617/5198 [14:51:53<12:39:28, 17.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2618/5198 [14:52:11<12:37:49, 17.62s/it]                                                         {'loss': 0.8826, 'learning_rate': 1.0367537128379154e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2618/5198 [14:52:11<12:37:49, 17.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2619/5198 [14:52:27<12:24:30, 17.32s/it]                                                         {'loss': 0.8943, 'learning_rate': 1.0361310421083677e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2619/5198 [14:52:27<12:24:30, 17.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2620/5198 [14:52:45<12:24:00, 17.32s/it]                                                         {'loss': 0.7803, 'learning_rate': 1.0355083573515052e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2620/5198 [14:52:45<12:24:00, 17.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2621/5198 [14:53:03<12:30:46, 17.48s/it]                                                         {'loss': 0.7619, 'learning_rate': 1.0348856588090764e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2621/5198 [14:53:03<12:30:46, 17.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2622/5198 [14:53:20<12:29:25, 17.46s/it]                                                         {'loss': 0.8032, 'learning_rate': 1.0342629467228331e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2622/5198 [14:53:20<12:29:25, 17.46s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2623/5198 [14:53:39<12:42:59, 17.78s/it]                                                         {'loss': 0.861, 'learning_rate': 1.0336402213345345e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2623/5198 [14:53:39<12:42:59, 17.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2624/5198 [14:53:56<12:33:36, 17.57s/it]                                                         {'loss': 0.8297, 'learning_rate': 1.0330174828859434e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2624/5198 [14:53:56<12:33:36, 17.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2625/5198 [14:54:14<12:45:40, 17.85s/it]                                                         {'loss': 0.8182, 'learning_rate': 1.0323947316188288e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2625/5198 [14:54:14<12:45:40, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2626/5198 [14:55:44<28:12:08, 39.47s/it]                                                         {'loss': 0.8258, 'learning_rate': 1.031771967774964e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2626/5198 [14:55:44<28:12:08, 39.47s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2627/5198 [14:56:01<23:26:34, 32.83s/it]                                                         {'loss': 0.3506, 'learning_rate': 1.0311491915961271e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2627/5198 [14:56:01<23:26:34, 32.83s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2628/5198 [14:56:20<20:20:02, 28.48s/it]                                                         {'loss': 0.7943, 'learning_rate': 1.030526403324102e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2628/5198 [14:56:20<20:20:02, 28.48s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2629/5198 [14:56:37<17:53:54, 25.08s/it]                                                         {'loss': 0.8025, 'learning_rate': 1.0299036032006759e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2629/5198 [14:56:37<17:53:54, 25.08s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2630/5198 [14:56:54<16:16:36, 22.82s/it]                                                         {'loss': 0.3114, 'learning_rate': 1.0292807914676412e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2630/5198 [14:56:54<16:16:36, 22.82s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2631/5198 [14:57:13<15:21:55, 21.55s/it]                                                         {'loss': 0.8003, 'learning_rate': 1.0286579683667952e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2631/5198 [14:57:13<15:21:55, 21.55s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2632/5198 [14:57:30<14:27:23, 20.28s/it]                                                         {'loss': 0.8626, 'learning_rate': 1.0280351341399392e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2632/5198 [14:57:30<14:27:23, 20.28s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2633/5198 [14:57:48<13:52:41, 19.48s/it]                                                         {'loss': 0.8146, 'learning_rate': 1.027412289028879e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2633/5198 [14:57:48<13:52:41, 19.48s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2634/5198 [14:58:07<13:41:46, 19.23s/it]                                                         {'loss': 0.7867, 'learning_rate': 1.0267894332754243e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2634/5198 [14:58:07<13:41:46, 19.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2635/5198 [14:58:25<13:25:27, 18.86s/it]                                                         {'loss': 0.8102, 'learning_rate': 1.0261665671213891e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2635/5198 [14:58:25<13:25:27, 18.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2636/5198 [14:58:42<13:09:35, 18.49s/it]                                                         {'loss': 0.7855, 'learning_rate': 1.0255436908085919e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2636/5198 [14:58:42<13:09:35, 18.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2637/5198 [14:59:00<12:56:55, 18.20s/it]                                                         {'loss': 0.8682, 'learning_rate': 1.024920804578854e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2637/5198 [14:59:00<12:56:55, 18.20s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2638/5198 [14:59:18<13:02:13, 18.33s/it]                                                         {'loss': 0.8307, 'learning_rate': 1.0242979086740019e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2638/5198 [14:59:18<13:02:13, 18.33s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2639/5198 [14:59:36<12:53:47, 18.14s/it]                                                         {'loss': 0.8267, 'learning_rate': 1.023675003335865e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2639/5198 [14:59:36<12:53:47, 18.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2640/5198 [14:59:54<12:51:07, 18.09s/it]                                                         {'loss': 0.7809, 'learning_rate': 1.0230520888062765e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2640/5198 [14:59:54<12:51:07, 18.09s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2641/5198 [15:00:11<12:41:17, 17.86s/it]                                                         {'loss': 0.8995, 'learning_rate': 1.0224291653270739e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2641/5198 [15:00:11<12:41:17, 17.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2642/5198 [15:00:29<12:34:55, 17.72s/it]                                                         {'loss': 0.8306, 'learning_rate': 1.0218062331400969e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2642/5198 [15:00:29<12:34:55, 17.72s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2643/5198 [15:00:47<12:40:02, 17.85s/it]                                                         {'loss': 0.8819, 'learning_rate': 1.0211832924871889e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2643/5198 [15:00:47<12:40:02, 17.85s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2644/5198 [15:01:04<12:31:53, 17.66s/it]                                                         {'loss': 0.8123, 'learning_rate': 1.0205603436101978e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2644/5198 [15:01:04<12:31:53, 17.66s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2645/5198 [15:01:22<12:29:51, 17.62s/it]                                                         {'loss': 0.8095, 'learning_rate': 1.0199373867509734e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2645/5198 [15:01:22<12:29:51, 17.62s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2646/5198 [15:01:40<12:37:33, 17.81s/it]                                                         {'loss': 0.7701, 'learning_rate': 1.019314422151369e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2646/5198 [15:01:40<12:37:33, 17.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2647/5198 [15:01:57<12:33:46, 17.73s/it]                                                         {'loss': 0.8433, 'learning_rate': 1.0186914500532408e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2647/5198 [15:01:57<12:33:46, 17.73s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2648/5198 [15:02:15<12:25:41, 17.55s/it]                                                         {'loss': 0.7922, 'learning_rate': 1.0180684706984483e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2648/5198 [15:02:15<12:25:41, 17.55s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2649/5198 [15:02:32<12:27:05, 17.59s/it]                                                         {'loss': 0.8098, 'learning_rate': 1.0174454843288533e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2649/5198 [15:02:32<12:27:05, 17.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2650/5198 [15:02:50<12:25:42, 17.56s/it]                                                         {'loss': 0.7945, 'learning_rate': 1.0168224911863205e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2650/5198 [15:02:50<12:25:42, 17.56s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2651/5198 [15:04:19<27:40:14, 39.11s/it]                                                         {'loss': 0.8011, 'learning_rate': 1.0161994915127173e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2651/5198 [15:04:19<27:40:14, 39.11s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2652/5198 [15:04:36<23:00:33, 32.53s/it]                                                         {'loss': 0.8136, 'learning_rate': 1.015576485549914e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2652/5198 [15:04:36<23:00:33, 32.53s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2653/5198 [15:04:54<19:53:39, 28.14s/it]                                                         {'loss': 0.844, 'learning_rate': 1.0149534735397823e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2653/5198 [15:04:54<19:53:39, 28.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2654/5198 [15:05:12<17:40:43, 25.02s/it]                                                         {'loss': 0.8142, 'learning_rate': 1.0143304557241979e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2654/5198 [15:05:12<17:40:43, 25.02s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2655/5198 [15:05:30<16:06:11, 22.80s/it]                                                         {'loss': 0.3202, 'learning_rate': 1.0137074323450372e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2655/5198 [15:05:30<16:06:11, 22.80s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2656/5198 [15:05:48<15:16:22, 21.63s/it]                                                         {'loss': 0.7848, 'learning_rate': 1.0130844036441787e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2656/5198 [15:05:48<15:16:22, 21.63s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2657/5198 [15:06:05<14:17:11, 20.24s/it]                                                         {'loss': 0.8445, 'learning_rate': 1.0124613698635043e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2657/5198 [15:06:05<14:17:11, 20.24s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2658/5198 [15:06:24<13:50:05, 19.61s/it]                                                         {'loss': 0.8128, 'learning_rate': 1.0118383312448973e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2658/5198 [15:06:24<13:50:05, 19.61s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2659/5198 [15:06:41<13:22:16, 18.96s/it]                                                         {'loss': 0.3333, 'learning_rate': 1.0112152880302426e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2659/5198 [15:06:41<13:22:16, 18.96s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2660/5198 [15:06:59<13:13:03, 18.75s/it]                                                         {'loss': 0.8445, 'learning_rate': 1.0105922404614265e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2660/5198 [15:06:59<13:13:03, 18.75s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2661/5198 [15:07:17<12:59:23, 18.43s/it]                                                         {'loss': 0.7976, 'learning_rate': 1.0099691887803385e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2661/5198 [15:07:17<12:59:23, 18.43s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2662/5198 [15:07:35<12:48:08, 18.17s/it]                                                         {'loss': 0.3675, 'learning_rate': 1.0093461332288678e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2662/5198 [15:07:35<12:48:08, 18.17s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2663/5198 [15:07:53<12:50:05, 18.23s/it]                                                         {'loss': 0.8051, 'learning_rate': 1.0087230740489065e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2663/5198 [15:07:53<12:50:05, 18.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2664/5198 [15:08:11<12:53:37, 18.32s/it]                                                         {'loss': 0.7767, 'learning_rate': 1.0081000114823473e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2664/5198 [15:08:11<12:53:37, 18.32s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2665/5198 [15:08:29<12:40:04, 18.00s/it]                                                         {'loss': 0.8458, 'learning_rate': 1.007476945771085e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2665/5198 [15:08:29<12:40:04, 18.00s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2666/5198 [15:08:46<12:25:30, 17.67s/it]                                                         {'loss': 0.8063, 'learning_rate': 1.006853877157015e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2666/5198 [15:08:46<12:25:30, 17.67s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2667/5198 [15:09:03<12:18:59, 17.52s/it]                                                         {'loss': 0.8774, 'learning_rate': 1.0062308058820337e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2667/5198 [15:09:03<12:18:59, 17.52s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2668/5198 [15:09:20<12:14:33, 17.42s/it]                                                         {'loss': 0.7885, 'learning_rate': 1.0056077321880393e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2668/5198 [15:09:20<12:14:33, 17.42s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2669/5198 [15:09:38<12:20:26, 17.57s/it]                                                         {'loss': 0.8367, 'learning_rate': 1.0049846563169297e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2669/5198 [15:09:38<12:20:26, 17.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2670/5198 [15:09:55<12:16:55, 17.49s/it]                                                         {'loss': 0.8085, 'learning_rate': 1.0043615785106051e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2670/5198 [15:09:55<12:16:55, 17.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2671/5198 [15:10:13<12:20:35, 17.58s/it]                                                         {'loss': 0.8364, 'learning_rate': 1.0037384990109658e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2671/5198 [15:10:13<12:20:35, 17.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2672/5198 [15:10:31<12:19:54, 17.58s/it]                                                         {'loss': 0.3494, 'learning_rate': 1.0031154180599123e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2672/5198 [15:10:31<12:19:54, 17.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2673/5198 [15:10:48<12:24:23, 17.69s/it]                                                         {'loss': 0.8257, 'learning_rate': 1.0024923358993458e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2673/5198 [15:10:48<12:24:23, 17.69s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2674/5198 [15:11:06<12:20:52, 17.61s/it]                                                         {'loss': 0.8269, 'learning_rate': 1.0018692527711695e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2674/5198 [15:11:06<12:20:52, 17.61s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2675/5198 [15:11:24<12:23:26, 17.68s/it]                                                         {'loss': 0.8336, 'learning_rate': 1.0012461689172846e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2675/5198 [15:11:24<12:23:26, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2676/5198 [15:12:53<27:28:53, 39.23s/it]                                                         {'loss': 0.8702, 'learning_rate': 1.0006230845795937e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2676/5198 [15:12:53<27:28:53, 39.23s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2677/5198 [15:13:11<22:54:22, 32.71s/it]                                                         {'loss': 0.8086, 'learning_rate': 1e-05, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2677/5198 [15:13:11<22:54:22, 32.71s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2678/5198 [15:13:28<19:38:49, 28.07s/it]                                                         {'loss': 0.8357, 'learning_rate': 9.993769154204063e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2678/5198 [15:13:28<19:38:49, 28.07s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2679/5198 [15:13:45<17:16:19, 24.68s/it]                                                         {'loss': 0.7632, 'learning_rate': 9.987538310827159e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2679/5198 [15:13:45<17:16:19, 24.68s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2680/5198 [15:14:03<15:54:07, 22.74s/it]                                                         {'loss': 0.812, 'learning_rate': 9.981307472288308e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2680/5198 [15:14:03<15:54:07, 22.74s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2681/5198 [15:14:20<14:46:50, 21.14s/it]                                                         {'loss': 0.8902, 'learning_rate': 9.975076641006542e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2681/5198 [15:14:20<14:46:50, 21.14s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2682/5198 [15:14:38<14:03:42, 20.12s/it]                                                         {'loss': 0.7693, 'learning_rate': 9.968845819400883e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2682/5198 [15:14:38<14:03:42, 20.12s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2683/5198 [15:14:55<13:16:43, 19.01s/it]                                                         {'loss': 0.9176, 'learning_rate': 9.962615009890346e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2683/5198 [15:14:55<13:16:43, 19.01s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2684/5198 [15:15:13<13:11:27, 18.89s/it]                                                         {'loss': 0.8329, 'learning_rate': 9.956384214893949e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2684/5198 [15:15:13<13:11:27, 18.89s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2685/5198 [15:15:30<12:49:07, 18.36s/it]                                                         {'loss': 0.8303, 'learning_rate': 9.950153436830707e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2685/5198 [15:15:30<12:49:07, 18.36s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2686/5198 [15:15:48<12:44:57, 18.27s/it]                                                         {'loss': 0.8678, 'learning_rate': 9.94392267811961e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2686/5198 [15:15:48<12:44:57, 18.27s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2687/5198 [15:16:06<12:31:51, 17.97s/it]                                                         {'loss': 0.7867, 'learning_rate': 9.937691941179665e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2687/5198 [15:16:06<12:31:51, 17.97s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2688/5198 [15:16:23<12:29:02, 17.91s/it]                                                         {'loss': 0.8327, 'learning_rate': 9.931461228429856e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2688/5198 [15:16:23<12:29:02, 17.91s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2689/5198 [15:16:41<12:21:46, 17.74s/it]                                                         {'loss': 0.8089, 'learning_rate': 9.925230542289151e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2689/5198 [15:16:41<12:21:46, 17.74s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2690/5198 [15:16:58<12:17:10, 17.64s/it]                                                         {'loss': 0.866, 'learning_rate': 9.91899988517653e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2690/5198 [15:16:58<12:17:10, 17.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2691/5198 [15:17:15<12:13:49, 17.56s/it]                                                         {'loss': 0.8192, 'learning_rate': 9.912769259510938e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2691/5198 [15:17:15<12:13:49, 17.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2692/5198 [15:17:34<12:21:00, 17.74s/it]                                                         {'loss': 0.8382, 'learning_rate': 9.906538667711324e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2692/5198 [15:17:34<12:21:00, 17.74s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2693/5198 [15:17:52<12:28:34, 17.93s/it]                                                         {'loss': 0.8307, 'learning_rate': 9.90030811219662e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2693/5198 [15:17:52<12:28:34, 17.93s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2694/5198 [15:18:09<12:21:46, 17.77s/it]                                                         {'loss': 0.7854, 'learning_rate': 9.894077595385736e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2694/5198 [15:18:09<12:21:46, 17.77s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2695/5198 [15:18:27<12:14:53, 17.62s/it]                                                         {'loss': 0.8365, 'learning_rate': 9.887847119697577e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2695/5198 [15:18:27<12:14:53, 17.62s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2696/5198 [15:18:44<12:16:34, 17.66s/it]                                                         {'loss': 0.7569, 'learning_rate': 9.881616687551032e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2696/5198 [15:18:44<12:16:34, 17.66s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2697/5198 [15:19:02<12:13:19, 17.59s/it]                                                         {'loss': 0.7383, 'learning_rate': 9.875386301364958e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2697/5198 [15:19:02<12:13:19, 17.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2698/5198 [15:19:20<12:21:47, 17.80s/it]                                                         {'loss': 0.8562, 'learning_rate': 9.869155963558215e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2698/5198 [15:19:20<12:21:47, 17.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2699/5198 [15:19:38<12:25:14, 17.89s/it]                                                         {'loss': 0.8369, 'learning_rate': 9.862925676549635e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2699/5198 [15:19:38<12:25:14, 17.89s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2700/5198 [15:19:56<12:19:09, 17.75s/it]                                                         {'loss': 0.8667, 'learning_rate': 9.856695442758023e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2700/5198 [15:19:56<12:19:09, 17.75s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2701/5198 [15:21:24<27:03:10, 39.00s/it]                                                         {'loss': 0.8593, 'learning_rate': 9.850465264602175e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2701/5198 [15:21:24<27:03:10, 39.00s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2702/5198 [15:21:43<22:46:16, 32.84s/it]                                                         {'loss': 0.8306, 'learning_rate': 9.844235144500865e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2702/5198 [15:21:43<22:46:16, 32.84s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2703/5198 [15:22:01<19:49:35, 28.61s/it]                                                         {'loss': 0.8034, 'learning_rate': 9.83800508487283e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2703/5198 [15:22:01<19:49:35, 28.61s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2704/5198 [15:22:19<17:33:17, 25.34s/it]                                                         {'loss': 0.824, 'learning_rate': 9.831775088136797e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2704/5198 [15:22:19<17:33:17, 25.34s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2705/5198 [15:22:37<15:58:03, 23.06s/it]                                                         {'loss': 0.809, 'learning_rate': 9.82554515671147e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2705/5198 [15:22:37<15:58:03, 23.06s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2706/5198 [15:22:55<14:58:16, 21.63s/it]                                                         {'loss': 0.8565, 'learning_rate': 9.819315293015519e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2706/5198 [15:22:55<14:58:16, 21.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2707/5198 [15:23:13<14:04:40, 20.35s/it]                                                         {'loss': 0.8428, 'learning_rate': 9.813085499467594e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2707/5198 [15:23:13<14:04:40, 20.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2708/5198 [15:23:31<13:35:13, 19.64s/it]                                                         {'loss': 0.7807, 'learning_rate': 9.806855778486314e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2708/5198 [15:23:31<13:35:13, 19.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2709/5198 [15:23:48<13:07:09, 18.98s/it]                                                         {'loss': 0.8185, 'learning_rate': 9.800626132490268e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2709/5198 [15:23:48<13:07:09, 18.98s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2710/5198 [15:24:06<12:58:32, 18.78s/it]                                                         {'loss': 0.8232, 'learning_rate': 9.794396563898022e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2710/5198 [15:24:06<12:58:32, 18.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2711/5198 [15:24:23<12:33:39, 18.18s/it]                                                         {'loss': 0.8321, 'learning_rate': 9.788167075128113e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2711/5198 [15:24:23<12:33:39, 18.18s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2712/5198 [15:24:40<12:15:17, 17.75s/it]                                                         {'loss': 0.827, 'learning_rate': 9.781937668599035e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2712/5198 [15:24:40<12:15:17, 17.75s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2713/5198 [15:24:58<12:18:55, 17.84s/it]                                                         {'loss': 0.8003, 'learning_rate': 9.775708346729263e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2713/5198 [15:24:58<12:18:55, 17.84s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2714/5198 [15:25:15<12:12:16, 17.69s/it]                                                         {'loss': 0.8111, 'learning_rate': 9.769479111937238e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2714/5198 [15:25:15<12:12:16, 17.69s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2715/5198 [15:25:33<12:16:04, 17.79s/it]                                                         {'loss': 0.8082, 'learning_rate': 9.763249966641352e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2715/5198 [15:25:33<12:16:04, 17.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2716/5198 [15:25:51<12:09:42, 17.64s/it]                                                         {'loss': 0.787, 'learning_rate': 9.757020913259986e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2716/5198 [15:25:51<12:09:42, 17.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2717/5198 [15:26:09<12:13:57, 17.75s/it]                                                         {'loss': 0.8417, 'learning_rate': 9.750791954211464e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2717/5198 [15:26:09<12:13:57, 17.75s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2718/5198 [15:26:26<12:11:32, 17.70s/it]                                                         {'loss': 0.8091, 'learning_rate': 9.744563091914085e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2718/5198 [15:26:26<12:11:32, 17.70s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2719/5198 [15:26:45<12:22:41, 17.98s/it]                                                         {'loss': 0.8342, 'learning_rate': 9.738334328786114e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2719/5198 [15:26:45<12:22:41, 17.98s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2720/5198 [15:27:01<12:05:46, 17.57s/it]                                                         {'loss': 0.7745, 'learning_rate': 9.732105667245759e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2720/5198 [15:27:01<12:05:46, 17.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2721/5198 [15:27:18<11:57:24, 17.38s/it]                                                         {'loss': 0.8581, 'learning_rate': 9.725877109711212e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2721/5198 [15:27:18<11:57:24, 17.38s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2722/5198 [15:27:37<12:07:53, 17.64s/it]                                                         {'loss': 0.8054, 'learning_rate': 9.719648658600611e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2722/5198 [15:27:37<12:07:53, 17.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2723/5198 [15:27:54<12:04:36, 17.57s/it]                                                         {'loss': 0.8022, 'learning_rate': 9.71342031633205e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2723/5198 [15:27:54<12:04:36, 17.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2724/5198 [15:28:12<12:08:42, 17.67s/it]                                                         {'loss': 0.7783, 'learning_rate': 9.70719208532359e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2724/5198 [15:28:12<12:08:42, 17.67s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2725/5198 [15:28:29<12:06:24, 17.62s/it]                                                         {'loss': 0.793, 'learning_rate': 9.700963967993246e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2725/5198 [15:28:29<12:06:24, 17.62s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2726/5198 [15:29:59<26:57:59, 39.27s/it]                                                         {'loss': 0.8469, 'learning_rate': 9.694735966758982e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2726/5198 [15:29:59<26:57:59, 39.27s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2727/5198 [15:30:16<22:17:04, 32.47s/it]                                                         {'loss': 0.8293, 'learning_rate': 9.688508084038729e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2727/5198 [15:30:16<22:17:04, 32.47s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2728/5198 [15:30:33<19:08:24, 27.90s/it]                                                         {'loss': 0.795, 'learning_rate': 9.682280322250365e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2728/5198 [15:30:33<19:08:24, 27.90s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2729/5198 [15:30:51<17:01:15, 24.82s/it]                                                         {'loss': 0.8803, 'learning_rate': 9.676052683811715e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2729/5198 [15:30:51<17:01:15, 24.82s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2730/5198 [15:31:08<15:31:09, 22.64s/it]                                                         {'loss': 0.8013, 'learning_rate': 9.669825171140568e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2730/5198 [15:31:08<15:31:09, 22.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2731/5198 [15:31:26<14:33:48, 21.25s/it]                                                         {'loss': 0.8634, 'learning_rate': 9.66359778665466e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2731/5198 [15:31:26<14:33:48, 21.25s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2732/5198 [15:31:44<13:50:24, 20.20s/it]                                                         {'loss': 0.8068, 'learning_rate': 9.657370532771672e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2732/5198 [15:31:44<13:50:24, 20.20s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2733/5198 [15:32:02<13:19:01, 19.45s/it]                                                         {'loss': 0.849, 'learning_rate': 9.651143411909241e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2733/5198 [15:32:02<13:19:01, 19.45s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2734/5198 [15:32:19<12:54:24, 18.86s/it]                                                         {'loss': 0.807, 'learning_rate': 9.64491642648495e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2734/5198 [15:32:19<12:54:24, 18.86s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2735/5198 [15:32:38<12:51:40, 18.80s/it]                                                         {'loss': 0.8038, 'learning_rate': 9.638689578916326e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2735/5198 [15:32:38<12:51:40, 18.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2736/5198 [15:32:55<12:37:57, 18.47s/it]                                                         {'loss': 0.8026, 'learning_rate': 9.632462871620847e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2736/5198 [15:32:55<12:37:57, 18.47s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2737/5198 [15:33:13<12:23:43, 18.13s/it]                                                         {'loss': 0.7721, 'learning_rate': 9.62623630701594e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2737/5198 [15:33:13<12:23:43, 18.13s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2738/5198 [15:33:30<12:17:31, 17.99s/it]                                                         {'loss': 0.7904, 'learning_rate': 9.620009887518963e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2738/5198 [15:33:30<12:17:31, 17.99s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2739/5198 [15:33:49<12:18:20, 18.02s/it]                                                         {'loss': 0.8146, 'learning_rate': 9.613783615547233e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2739/5198 [15:33:49<12:18:20, 18.02s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2740/5198 [15:34:06<12:17:11, 17.99s/it]                                                         {'loss': 0.8259, 'learning_rate': 9.607557493518006e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2740/5198 [15:34:07<12:17:11, 17.99s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2741/5198 [15:34:25<12:18:12, 18.03s/it]                                                         {'loss': 0.7926, 'learning_rate': 9.601331523848464e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2741/5198 [15:34:25<12:18:12, 18.03s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2742/5198 [15:34:42<12:07:16, 17.77s/it]                                                         {'loss': 0.3884, 'learning_rate': 9.595105708955758e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2742/5198 [15:34:42<12:07:16, 17.77s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2743/5198 [15:35:00<12:08:43, 17.81s/it]                                                         {'loss': 0.8316, 'learning_rate': 9.588880051256951e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2743/5198 [15:35:00<12:08:43, 17.81s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2744/5198 [15:35:17<11:57:31, 17.54s/it]                                                         {'loss': 0.8807, 'learning_rate': 9.582654553169064e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2744/5198 [15:35:17<11:57:31, 17.54s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2745/5198 [15:35:34<11:59:20, 17.59s/it]                                                         {'loss': 0.7976, 'learning_rate': 9.576429217109054e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2745/5198 [15:35:34<11:59:20, 17.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2746/5198 [15:35:52<11:59:28, 17.61s/it]                                                         {'loss': 0.8086, 'learning_rate': 9.5702040454938e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2746/5198 [15:35:52<11:59:28, 17.61s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2747/5198 [15:36:10<12:00:06, 17.63s/it]                                                         {'loss': 0.842, 'learning_rate': 9.563979040740138e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2747/5198 [15:36:10<12:00:06, 17.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2748/5198 [15:36:27<11:59:47, 17.63s/it]                                                         {'loss': 0.844, 'learning_rate': 9.557754205264826e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2748/5198 [15:36:27<11:59:47, 17.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2749/5198 [15:36:44<11:54:38, 17.51s/it]                                                         {'loss': 0.8513, 'learning_rate': 9.551529541484554e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2749/5198 [15:36:44<11:54:38, 17.51s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2750/5198 [15:37:02<11:56:58, 17.57s/it]                                                         {'loss': 0.7612, 'learning_rate': 9.545305051815957e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2750/5198 [15:37:02<11:56:58, 17.57s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2751/5198 [15:38:31<26:28:46, 38.96s/it]                                                         {'loss': 0.8351, 'learning_rate': 9.539080738675597e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2751/5198 [15:38:31<26:28:46, 38.96s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2752/5198 [15:38:49<22:07:54, 32.57s/it]                                                         {'loss': 0.8063, 'learning_rate': 9.53285660447996e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2752/5198 [15:38:49<22:07:54, 32.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2753/5198 [15:39:06<19:05:50, 28.12s/it]                                                         {'loss': 0.7875, 'learning_rate': 9.526632651645476e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2753/5198 [15:39:06<19:05:50, 28.12s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2754/5198 [15:39:24<17:00:20, 25.05s/it]                                                         {'loss': 0.8643, 'learning_rate': 9.520408882588497e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2754/5198 [15:39:24<17:00:20, 25.05s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2755/5198 [15:39:42<15:33:37, 22.93s/it]                                                         {'loss': 0.8702, 'learning_rate': 9.514185299725299e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2755/5198 [15:39:42<15:33:37, 22.93s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2756/5198 [15:39:59<14:19:40, 21.12s/it]                                                         {'loss': 0.8634, 'learning_rate': 9.507961905472093e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2756/5198 [15:39:59<14:19:40, 21.12s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2757/5198 [15:40:17<13:35:12, 20.04s/it]                                                         {'loss': 0.7864, 'learning_rate': 9.501738702245023e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2757/5198 [15:40:17<13:35:12, 20.04s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2758/5198 [15:40:34<12:55:55, 19.08s/it]                                                         {'loss': 0.7949, 'learning_rate': 9.495515692460138e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2758/5198 [15:40:34<12:55:55, 19.08s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2759/5198 [15:40:50<12:28:17, 18.41s/it]                                                         {'loss': 0.8232, 'learning_rate': 9.489292878533436e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2759/5198 [15:40:50<12:28:17, 18.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2760/5198 [15:41:08<12:20:55, 18.23s/it]                                                         {'loss': 0.79, 'learning_rate': 9.483070262880823e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2760/5198 [15:41:08<12:20:55, 18.23s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2761/5198 [15:41:26<12:13:59, 18.07s/it]                                                         {'loss': 0.3471, 'learning_rate': 9.476847847918126e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2761/5198 [15:41:26<12:13:59, 18.07s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2762/5198 [15:41:44<12:07:57, 17.93s/it]                                                         {'loss': 0.8433, 'learning_rate': 9.47062563606111e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2762/5198 [15:41:44<12:07:57, 17.93s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2763/5198 [15:42:00<11:55:38, 17.63s/it]                                                         {'loss': 0.8647, 'learning_rate': 9.464403629725454e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2763/5198 [15:42:00<11:55:38, 17.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2764/5198 [15:42:18<11:48:08, 17.46s/it]                                                         {'loss': 0.742, 'learning_rate': 9.458181831326744e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2764/5198 [15:42:18<11:48:08, 17.46s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2765/5198 [15:42:36<11:55:31, 17.65s/it]                                                         {'loss': 0.8344, 'learning_rate': 9.451960243280506e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2765/5198 [15:42:36<11:55:31, 17.65s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2766/5198 [15:42:53<11:54:52, 17.64s/it]                                                         {'loss': 0.797, 'learning_rate': 9.44573886800217e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2766/5198 [15:42:53<11:54:52, 17.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2767/5198 [15:43:10<11:48:17, 17.48s/it]                                                         {'loss': 0.8715, 'learning_rate': 9.43951770790709e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2767/5198 [15:43:10<11:48:17, 17.48s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2768/5198 [15:43:29<12:00:39, 17.79s/it]                                                         {'loss': 0.8171, 'learning_rate': 9.433296765410534e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2768/5198 [15:43:29<12:00:39, 17.79s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2769/5198 [15:43:45<11:45:57, 17.44s/it]                                                         {'loss': 0.8386, 'learning_rate': 9.427076042927683e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2769/5198 [15:43:45<11:45:57, 17.44s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2770/5198 [15:44:04<11:56:05, 17.70s/it]                                                         {'loss': 0.8179, 'learning_rate': 9.420855542873638e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2770/5198 [15:44:04<11:56:05, 17.70s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2771/5198 [15:44:21<11:52:42, 17.62s/it]                                                         {'loss': 0.825, 'learning_rate': 9.414635267663416e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2771/5198 [15:44:21<11:52:42, 17.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2772/5198 [15:44:39<11:48:40, 17.53s/it]                                                         {'loss': 0.8048, 'learning_rate': 9.408415219711934e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2772/5198 [15:44:39<11:48:40, 17.53s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2773/5198 [15:44:56<11:42:55, 17.39s/it]                                                         {'loss': 0.8635, 'learning_rate': 9.402195401434036e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2773/5198 [15:44:56<11:42:55, 17.39s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2774/5198 [15:45:13<11:41:47, 17.37s/it]                                                         {'loss': 0.8801, 'learning_rate': 9.395975815244468e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2774/5198 [15:45:13<11:41:47, 17.37s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2775/5198 [15:45:31<11:46:47, 17.50s/it]                                                         {'loss': 0.347, 'learning_rate': 9.389756463557878e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2775/5198 [15:45:31<11:46:47, 17.50s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2776/5198 [15:46:57<25:40:15, 38.16s/it]                                                         {'loss': 0.7846, 'learning_rate': 9.383537348788844e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2776/5198 [15:46:57<25:40:15, 38.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2777/5198 [15:47:15<21:31:26, 32.01s/it]                                                         {'loss': 0.7801, 'learning_rate': 9.377318473351838e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2777/5198 [15:47:15<21:31:26, 32.01s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2778/5198 [15:47:32<18:35:39, 27.66s/it]                                                         {'loss': 0.842, 'learning_rate': 9.371099839661238e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2778/5198 [15:47:32<18:35:39, 27.66s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2779/5198 [15:47:50<16:33:23, 24.64s/it]                                                         {'loss': 0.8305, 'learning_rate': 9.364881450131335e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2779/5198 [15:47:50<16:33:23, 24.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2780/5198 [15:48:08<15:10:41, 22.60s/it]                                                         {'loss': 0.8495, 'learning_rate': 9.358663307176323e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2780/5198 [15:48:08<15:10:41, 22.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2781/5198 [15:48:25<14:09:12, 21.08s/it]                                                         {'loss': 0.8225, 'learning_rate': 9.352445413210294e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2781/5198 [15:48:25<14:09:12, 21.08s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2782/5198 [15:48:43<13:25:09, 20.00s/it]                                                         {'loss': 0.8361, 'learning_rate': 9.346227770647251e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2782/5198 [15:48:43<13:25:09, 20.00s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2783/5198 [15:49:00<12:47:55, 19.08s/it]                                                         {'loss': 0.7989, 'learning_rate': 9.3400103819011e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2783/5198 [15:49:00<12:47:55, 19.08s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2784/5198 [15:49:17<12:27:53, 18.59s/it]                                                         {'loss': 0.8308, 'learning_rate': 9.33379324938564e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2784/5198 [15:49:17<12:27:53, 18.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2785/5198 [15:49:35<12:24:04, 18.50s/it]                                                         {'loss': 0.8257, 'learning_rate': 9.327576375514582e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2785/5198 [15:49:35<12:24:04, 18.50s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2786/5198 [15:49:54<12:23:15, 18.49s/it]                                                         {'loss': 0.7849, 'learning_rate': 9.321359762701527e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2786/5198 [15:49:54<12:23:15, 18.49s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2787/5198 [15:50:11<12:07:30, 18.10s/it]                                                         {'loss': 0.8122, 'learning_rate': 9.315143413359975e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2787/5198 [15:50:11<12:07:30, 18.10s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2788/5198 [15:50:29<12:07:58, 18.12s/it]                                                         {'loss': 0.8474, 'learning_rate': 9.308927329903333e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2788/5198 [15:50:29<12:07:58, 18.12s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2789/5198 [15:50:47<11:58:37, 17.90s/it]                                                         {'loss': 0.847, 'learning_rate': 9.302711514744897e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2789/5198 [15:50:47<11:58:37, 17.90s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2790/5198 [15:51:05<12:09:55, 18.19s/it]                                                         {'loss': 0.8072, 'learning_rate': 9.296495970297855e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2790/5198 [15:51:05<12:09:55, 18.19s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2791/5198 [15:51:23<12:05:03, 18.07s/it]                                                         {'loss': 0.8593, 'learning_rate': 9.290280698975307e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2791/5198 [15:51:23<12:05:03, 18.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2792/5198 [15:51:40<11:52:53, 17.78s/it]                                                         {'loss': 0.7999, 'learning_rate': 9.284065703190221e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2792/5198 [15:51:40<11:52:53, 17.78s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2793/5198 [15:51:58<11:45:45, 17.61s/it]                                                         {'loss': 0.8302, 'learning_rate': 9.27785098535548e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2793/5198 [15:51:58<11:45:45, 17.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2794/5198 [15:52:15<11:39:00, 17.45s/it]                                                         {'loss': 0.8433, 'learning_rate': 9.271636547883856e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2794/5198 [15:52:15<11:39:00, 17.45s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2795/5198 [15:52:32<11:41:32, 17.52s/it]                                                         {'loss': 0.7882, 'learning_rate': 9.265422393187998e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2795/5198 [15:52:32<11:41:32, 17.52s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2796/5198 [15:52:51<11:50:21, 17.74s/it]                                                         {'loss': 0.8235, 'learning_rate': 9.259208523680457e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2796/5198 [15:52:51<11:50:21, 17.74s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2797/5198 [15:53:08<11:49:52, 17.74s/it]                                                         {'loss': 0.8483, 'learning_rate': 9.252994941773679e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2797/5198 [15:53:08<11:49:52, 17.74s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2798/5198 [15:53:26<11:54:05, 17.85s/it]                                                         {'loss': 0.8267, 'learning_rate': 9.24678164987998e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2798/5198 [15:53:26<11:54:05, 17.85s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2799/5198 [15:53:45<11:58:04, 17.96s/it]                                                         {'loss': 0.8151, 'learning_rate': 9.24056865041158e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2799/5198 [15:53:45<11:58:04, 17.96s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2800/5198 [15:54:02<11:45:37, 17.66s/it]                                                         {'loss': 0.7746, 'learning_rate': 9.234355945780581e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2800/5198 [15:54:02<11:45:37, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2801/5198 [15:55:29<25:45:40, 38.69s/it]                                                         {'loss': 0.8461, 'learning_rate': 9.228143538398963e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2801/5198 [15:55:29<25:45:40, 38.69s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2802/5198 [15:55:47<21:32:20, 32.36s/it]                                                         {'loss': 0.8238, 'learning_rate': 9.221931430678598e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2802/5198 [15:55:47<21:32:20, 32.36s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2803/5198 [15:56:05<18:37:01, 27.98s/it]                                                         {'loss': 0.8218, 'learning_rate': 9.215719625031245e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2803/5198 [15:56:05<18:37:01, 27.98s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2804/5198 [15:56:22<16:31:33, 24.85s/it]                                                         {'loss': 0.8383, 'learning_rate': 9.209508123868534e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2804/5198 [15:56:22<16:31:33, 24.85s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2805/5198 [15:56:40<15:04:27, 22.68s/it]                                                         {'loss': 0.8009, 'learning_rate': 9.203296929601986e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2805/5198 [15:56:40<15:04:27, 22.68s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2806/5198 [15:56:58<14:07:46, 21.27s/it]                                                         {'loss': 0.3487, 'learning_rate': 9.197086044643004e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2806/5198 [15:56:58<14:07:46, 21.27s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2807/5198 [15:57:16<13:28:57, 20.30s/it]                                                         {'loss': 0.7801, 'learning_rate': 9.190875471402865e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2807/5198 [15:57:16<13:28:57, 20.30s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2808/5198 [15:57:34<13:01:36, 19.62s/it]                                                         {'loss': 0.7841, 'learning_rate': 9.184665212292723e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2808/5198 [15:57:34<13:01:36, 19.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2809/5198 [15:57:51<12:25:13, 18.72s/it]                                                         {'loss': 0.819, 'learning_rate': 9.178455269723623e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2809/5198 [15:57:51<12:25:13, 18.72s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2810/5198 [15:58:09<12:20:12, 18.60s/it]                                                         {'loss': 0.8455, 'learning_rate': 9.172245646106471e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2810/5198 [15:58:09<12:20:12, 18.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2811/5198 [15:58:26<12:01:00, 18.12s/it]                                                         {'loss': 0.8047, 'learning_rate': 9.166036343852061e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2811/5198 [15:58:26<12:01:00, 18.12s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2812/5198 [15:58:44<11:55:57, 18.00s/it]                                                         {'loss': 0.8022, 'learning_rate': 9.159827365371055e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2812/5198 [15:58:44<11:55:57, 18.00s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2813/5198 [15:59:01<11:52:34, 17.93s/it]                                                         {'loss': 0.7552, 'learning_rate': 9.153618713073995e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2813/5198 [15:59:01<11:52:34, 17.93s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2814/5198 [15:59:19<11:52:55, 17.94s/it]                                                         {'loss': 0.7948, 'learning_rate': 9.14741038937129e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2814/5198 [15:59:19<11:52:55, 17.94s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2815/5198 [15:59:36<11:37:50, 17.57s/it]                                                         {'loss': 0.8784, 'learning_rate': 9.141202396673232e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2815/5198 [15:59:36<11:37:50, 17.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2816/5198 [15:59:54<11:36:36, 17.55s/it]                                                         {'loss': 0.8215, 'learning_rate': 9.13499473738997e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2816/5198 [15:59:54<11:36:36, 17.55s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2817/5198 [16:00:10<11:29:44, 17.38s/it]                                                         {'loss': 0.8364, 'learning_rate': 9.128787413931536e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2817/5198 [16:00:11<11:29:44, 17.38s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2818/5198 [16:00:29<11:40:59, 17.67s/it]                                                         {'loss': 0.7867, 'learning_rate': 9.122580428707822e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2818/5198 [16:00:29<11:40:59, 17.67s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2819/5198 [16:00:46<11:36:45, 17.57s/it]                                                         {'loss': 0.8019, 'learning_rate': 9.116373784128597e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2819/5198 [16:00:46<11:36:45, 17.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2820/5198 [16:01:04<11:36:54, 17.58s/it]                                                         {'loss': 0.8478, 'learning_rate': 9.110167482603494e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2820/5198 [16:01:04<11:36:54, 17.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2821/5198 [16:01:21<11:27:58, 17.37s/it]                                                         {'loss': 0.8313, 'learning_rate': 9.10396152654201e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2821/5198 [16:01:21<11:27:58, 17.37s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2822/5198 [16:01:38<11:23:13, 17.25s/it]                                                         {'loss': 0.8921, 'learning_rate': 9.097755918353513e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2822/5198 [16:01:38<11:23:13, 17.25s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2823/5198 [16:01:54<11:17:22, 17.11s/it]                                                         {'loss': 0.874, 'learning_rate': 9.091550660447236e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2823/5198 [16:01:54<11:17:22, 17.11s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2824/5198 [16:02:11<11:15:52, 17.08s/it]                                                         {'loss': 0.876, 'learning_rate': 9.08534575523227e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2824/5198 [16:02:11<11:15:52, 17.08s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2825/5198 [16:02:29<11:19:26, 17.18s/it]                                                         {'loss': 0.33, 'learning_rate': 9.079141205117573e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2825/5198 [16:02:29<11:19:26, 17.18s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2826/5198 [16:04:01<26:05:10, 39.59s/it]                                                         {'loss': 0.7818, 'learning_rate': 9.072937012511968e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2826/5198 [16:04:01<26:05:10, 39.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2827/5198 [16:04:19<21:52:21, 33.21s/it]                                                         {'loss': 0.7586, 'learning_rate': 9.066733179824134e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2827/5198 [16:04:19<21:52:21, 33.21s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2828/5198 [16:04:36<18:42:27, 28.42s/it]                                                         {'loss': 0.8361, 'learning_rate': 9.060529709462613e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2828/5198 [16:04:36<18:42:27, 28.42s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2829/5198 [16:04:54<16:37:10, 25.26s/it]                                                         {'loss': 0.7532, 'learning_rate': 9.054326603835807e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2829/5198 [16:04:54<16:37:10, 25.26s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2830/5198 [16:05:11<14:58:08, 22.76s/it]                                                         {'loss': 0.7955, 'learning_rate': 9.048123865351971e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2830/5198 [16:05:11<14:58:08, 22.76s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2831/5198 [16:05:30<14:06:42, 21.46s/it]                                                         {'loss': 0.798, 'learning_rate': 9.041921496419225e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2831/5198 [16:05:30<14:06:42, 21.46s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2832/5198 [16:05:47<13:15:54, 20.18s/it]                                                         {'loss': 0.8094, 'learning_rate': 9.035719499445545e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2832/5198 [16:05:47<13:15:54, 20.18s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2833/5198 [16:06:04<12:38:02, 19.23s/it]                                                         {'loss': 0.8705, 'learning_rate': 9.029517876838755e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2833/5198 [16:06:04<12:38:02, 19.23s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2834/5198 [16:06:22<12:27:22, 18.97s/it]                                                         {'loss': 0.8053, 'learning_rate': 9.023316631006536e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2834/5198 [16:06:22<12:27:22, 18.97s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2835/5198 [16:06:41<12:22:17, 18.85s/it]                                                         {'loss': 0.8106, 'learning_rate': 9.017115764356436e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2835/5198 [16:06:41<12:22:17, 18.85s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2836/5198 [16:06:59<12:14:18, 18.65s/it]                                                         {'loss': 0.8271, 'learning_rate': 9.010915279295833e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2836/5198 [16:06:59<12:14:18, 18.65s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2837/5198 [16:07:16<12:00:51, 18.32s/it]                                                         {'loss': 0.8313, 'learning_rate': 9.004715178231975e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2837/5198 [16:07:16<12:00:51, 18.32s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2838/5198 [16:07:34<11:54:26, 18.16s/it]                                                         {'loss': 0.8234, 'learning_rate': 8.998515463571953e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2838/5198 [16:07:34<11:54:26, 18.16s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2839/5198 [16:07:52<11:51:51, 18.11s/it]                                                         {'loss': 0.785, 'learning_rate': 8.992316137722711e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2839/5198 [16:07:52<11:51:51, 18.11s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2840/5198 [16:08:11<11:54:56, 18.19s/it]                                                         {'loss': 0.845, 'learning_rate': 8.986117203091042e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2840/5198 [16:08:11<11:54:56, 18.19s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2841/5198 [16:08:28<11:43:41, 17.91s/it]                                                         {'loss': 0.8793, 'learning_rate': 8.97991866208358e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2841/5198 [16:08:28<11:43:41, 17.91s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2842/5198 [16:08:45<11:32:20, 17.63s/it]                                                         {'loss': 0.8431, 'learning_rate': 8.973720517106814e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2842/5198 [16:08:45<11:32:20, 17.63s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2843/5198 [16:09:03<11:37:54, 17.78s/it]                                                         {'loss': 0.824, 'learning_rate': 8.967522770567086e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2843/5198 [16:09:03<11:37:54, 17.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2844/5198 [16:09:22<11:50:00, 18.10s/it]                                                         {'loss': 0.8168, 'learning_rate': 8.961325424870561e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2844/5198 [16:09:22<11:50:00, 18.10s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2845/5198 [16:09:39<11:42:37, 17.92s/it]                                                         {'loss': 0.8105, 'learning_rate': 8.955128482423271e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2845/5198 [16:09:39<11:42:37, 17.92s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2846/5198 [16:09:57<11:40:16, 17.86s/it]                                                         {'loss': 0.8144, 'learning_rate': 8.948931945631082e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2846/5198 [16:09:57<11:40:16, 17.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2847/5198 [16:10:15<11:45:38, 18.01s/it]                                                         {'loss': 0.8053, 'learning_rate': 8.9427358168997e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2847/5198 [16:10:15<11:45:38, 18.01s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2848/5198 [16:10:33<11:36:27, 17.78s/it]                                                         {'loss': 0.8139, 'learning_rate': 8.936540098634675e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2848/5198 [16:10:33<11:36:27, 17.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2849/5198 [16:10:51<11:41:10, 17.91s/it]                                                         {'loss': 0.8059, 'learning_rate': 8.930344793241404e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2849/5198 [16:10:51<11:41:10, 17.91s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2850/5198 [16:11:08<11:30:03, 17.63s/it]                                                         {'loss': 0.824, 'learning_rate': 8.924149903125108e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2850/5198 [16:11:08<11:30:03, 17.63s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2851/5198 [16:12:35<25:08:07, 38.55s/it]                                                         {'loss': 0.8089, 'learning_rate': 8.917955430690865e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2851/5198 [16:12:35<25:08:07, 38.55s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2852/5198 [16:12:52<20:49:57, 31.97s/it]                                                         {'loss': 0.8322, 'learning_rate': 8.91176137834358e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2852/5198 [16:12:52<20:49:57, 31.97s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2853/5198 [16:13:09<17:56:19, 27.54s/it]                                                         {'loss': 0.8199, 'learning_rate': 8.905567748487997e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2853/5198 [16:13:09<17:56:19, 27.54s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2854/5198 [16:13:27<16:04:07, 24.68s/it]                                                         {'loss': 0.7759, 'learning_rate': 8.899374543528695e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2854/5198 [16:13:27<16:04:07, 24.68s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2855/5198 [16:13:46<14:54:24, 22.90s/it]                                                         {'loss': 0.8207, 'learning_rate': 8.893181765870094e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2855/5198 [16:13:46<14:54:24, 22.90s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2856/5198 [16:14:04<14:05:00, 21.65s/it]                                                         {'loss': 0.8157, 'learning_rate': 8.886989417916435e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2856/5198 [16:14:04<14:05:00, 21.65s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2857/5198 [16:14:23<13:24:33, 20.62s/it]                                                         {'loss': 0.8284, 'learning_rate': 8.88079750207181e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2857/5198 [16:14:23<13:24:33, 20.62s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2858/5198 [16:14:40<12:40:16, 19.49s/it]                                                         {'loss': 0.785, 'learning_rate': 8.87460602074013e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2858/5198 [16:14:40<12:40:16, 19.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2859/5198 [16:14:57<12:12:43, 18.80s/it]                                                         {'loss': 0.3615, 'learning_rate': 8.86841497632514e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2859/5198 [16:14:57<12:12:43, 18.80s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2860/5198 [16:15:14<11:58:02, 18.43s/it]                                                         {'loss': 0.8821, 'learning_rate': 8.862224371230418e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2860/5198 [16:15:14<11:58:02, 18.43s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2861/5198 [16:15:32<11:45:39, 18.12s/it]                                                         {'loss': 0.7776, 'learning_rate': 8.85603420785937e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2861/5198 [16:15:32<11:45:39, 18.12s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2862/5198 [16:15:49<11:41:05, 18.01s/it]                                                         {'loss': 0.8336, 'learning_rate': 8.84984448861523e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2862/5198 [16:15:49<11:41:05, 18.01s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2863/5198 [16:16:07<11:35:08, 17.86s/it]                                                         {'loss': 0.7965, 'learning_rate': 8.84365521590106e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2863/5198 [16:16:07<11:35:08, 17.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2864/5198 [16:16:25<11:34:49, 17.86s/it]                                                         {'loss': 0.8224, 'learning_rate': 8.837466392119752e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2864/5198 [16:16:25<11:34:49, 17.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2865/5198 [16:16:43<11:33:50, 17.84s/it]                                                         {'loss': 0.767, 'learning_rate': 8.831278019674017e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2865/5198 [16:16:43<11:33:50, 17.84s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2866/5198 [16:17:00<11:25:54, 17.65s/it]                                                         {'loss': 0.8601, 'learning_rate': 8.825090100966396e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2866/5198 [16:17:00<11:25:54, 17.65s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2867/5198 [16:17:18<11:30:37, 17.78s/it]                                                         {'loss': 0.8455, 'learning_rate': 8.818902638399247e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2867/5198 [16:17:18<11:30:37, 17.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2868/5198 [16:17:36<11:28:32, 17.73s/it]                                                         {'loss': 0.8274, 'learning_rate': 8.81271563437476e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2868/5198 [16:17:36<11:28:32, 17.73s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2869/5198 [16:17:52<11:17:11, 17.45s/it]                                                         {'loss': 0.8114, 'learning_rate': 8.806529091294948e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2869/5198 [16:17:52<11:17:11, 17.45s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2870/5198 [16:18:11<11:26:07, 17.68s/it]                                                         {'loss': 0.8089, 'learning_rate': 8.800343011561633e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2870/5198 [16:18:11<11:26:07, 17.68s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2871/5198 [16:18:29<11:32:36, 17.86s/it]                                                         {'loss': 0.804, 'learning_rate': 8.794157397576464e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2871/5198 [16:18:29<11:32:36, 17.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2872/5198 [16:18:47<11:31:06, 17.83s/it]                                                         {'loss': 0.8662, 'learning_rate': 8.787972251740916e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2872/5198 [16:18:47<11:31:06, 17.83s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2873/5198 [16:19:04<11:21:00, 17.57s/it]                                                         {'loss': 0.7758, 'learning_rate': 8.781787576456269e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2873/5198 [16:19:04<11:21:00, 17.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2874/5198 [16:19:21<11:17:37, 17.49s/it]                                                         {'loss': 0.8266, 'learning_rate': 8.775603374123627e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2874/5198 [16:19:21<11:17:37, 17.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2875/5198 [16:19:38<11:13:57, 17.41s/it]                                                         {'loss': 0.8532, 'learning_rate': 8.769419647143917e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2875/5198 [16:19:38<11:13:57, 17.41s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2876/5198 [16:21:05<24:40:16, 38.25s/it]                                                         {'loss': 0.8063, 'learning_rate': 8.763236397917865e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2876/5198 [16:21:05<24:40:16, 38.25s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2877/5198 [16:21:22<20:32:31, 31.86s/it]                                                         {'loss': 0.8063, 'learning_rate': 8.757053628846028e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2877/5198 [16:21:22<20:32:31, 31.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2878/5198 [16:21:39<17:44:18, 27.53s/it]                                                         {'loss': 0.8113, 'learning_rate': 8.75087134232877e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2878/5198 [16:21:39<17:44:18, 27.53s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2879/5198 [16:21:56<15:42:01, 24.37s/it]                                                         {'loss': 0.8009, 'learning_rate': 8.744689540766265e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2879/5198 [16:21:56<15:42:01, 24.37s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2880/5198 [16:22:15<14:30:30, 22.53s/it]                                                         {'loss': 0.8479, 'learning_rate': 8.738508226558499e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2880/5198 [16:22:15<14:30:30, 22.53s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2881/5198 [16:22:33<13:47:52, 21.44s/it]                                                         {'loss': 0.8069, 'learning_rate': 8.73232740210528e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2881/5198 [16:22:33<13:47:52, 21.44s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2882/5198 [16:22:51<13:05:17, 20.34s/it]                                                         {'loss': 0.8002, 'learning_rate': 8.726147069806206e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2882/5198 [16:22:51<13:05:17, 20.34s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2883/5198 [16:23:09<12:29:38, 19.43s/it]                                                         {'loss': 0.8453, 'learning_rate': 8.719967232060698e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2883/5198 [16:23:09<12:29:38, 19.43s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2884/5198 [16:23:26<12:04:40, 18.79s/it]                                                         {'loss': 0.8677, 'learning_rate': 8.713787891267988e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2884/5198 [16:23:26<12:04:40, 18.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2885/5198 [16:23:44<12:00:09, 18.68s/it]                                                         {'loss': 0.7544, 'learning_rate': 8.707609049827102e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2885/5198 [16:23:44<12:00:09, 18.68s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2886/5198 [16:24:02<11:51:49, 18.47s/it]                                                         {'loss': 0.8113, 'learning_rate': 8.70143071013688e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2886/5198 [16:24:02<11:51:49, 18.47s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2887/5198 [16:24:19<11:32:57, 17.99s/it]                                                         {'loss': 0.8484, 'learning_rate': 8.695252874595972e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2887/5198 [16:24:19<11:32:57, 17.99s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2888/5198 [16:24:37<11:32:43, 17.99s/it]                                                         {'loss': 0.8374, 'learning_rate': 8.689075545602816e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2888/5198 [16:24:37<11:32:43, 17.99s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2889/5198 [16:24:54<11:18:44, 17.64s/it]                                                         {'loss': 0.8203, 'learning_rate': 8.68289872555567e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2889/5198 [16:24:54<11:18:44, 17.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2890/5198 [16:25:12<11:19:53, 17.68s/it]                                                         {'loss': 0.8213, 'learning_rate': 8.676722416852594e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2890/5198 [16:25:12<11:19:53, 17.68s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2891/5198 [16:25:29<11:19:29, 17.67s/it]                                                         {'loss': 0.8301, 'learning_rate': 8.670546621891434e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2891/5198 [16:25:29<11:19:29, 17.67s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2892/5198 [16:25:46<11:11:08, 17.46s/it]                                                         {'loss': 0.8341, 'learning_rate': 8.66437134306985e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2892/5198 [16:25:46<11:11:08, 17.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2893/5198 [16:26:04<11:12:03, 17.49s/it]                                                         {'loss': 0.8765, 'learning_rate': 8.658196582785297e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2893/5198 [16:26:04<11:12:03, 17.49s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2894/5198 [16:26:21<11:09:01, 17.42s/it]                                                         {'loss': 0.7524, 'learning_rate': 8.652022343435027e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2894/5198 [16:26:21<11:09:01, 17.42s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2895/5198 [16:26:39<11:10:15, 17.46s/it]                                                         {'loss': 0.86, 'learning_rate': 8.645848627416102e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2895/5198 [16:26:39<11:10:15, 17.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2896/5198 [16:26:57<11:16:28, 17.63s/it]                                                         {'loss': 0.8165, 'learning_rate': 8.63967543712536e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2896/5198 [16:26:57<11:16:28, 17.63s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2897/5198 [16:27:14<11:15:57, 17.63s/it]                                                         {'loss': 0.8076, 'learning_rate': 8.633502774959453e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2897/5198 [16:27:14<11:15:57, 17.63s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2898/5198 [16:27:33<11:30:07, 18.00s/it]                                                         {'loss': 0.7816, 'learning_rate': 8.627330643314818e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2898/5198 [16:27:33<11:30:07, 18.00s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2899/5198 [16:27:51<11:21:51, 17.80s/it]                                                         {'loss': 0.8312, 'learning_rate': 8.62115904458769e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2899/5198 [16:27:51<11:21:51, 17.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2900/5198 [16:28:08<11:23:30, 17.85s/it]                                                         {'loss': 0.8258, 'learning_rate': 8.614987981174093e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2900/5198 [16:28:08<11:23:30, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2901/5198 [16:29:38<25:10:42, 39.46s/it]                                                         {'loss': 0.8164, 'learning_rate': 8.608817455469854e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2901/5198 [16:29:38<25:10:42, 39.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2902/5198 [16:29:56<20:54:25, 32.78s/it]                                                         {'loss': 0.7635, 'learning_rate': 8.602647469870573e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2902/5198 [16:29:56<20:54:25, 32.78s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2903/5198 [16:30:14<18:09:11, 28.48s/it]                                                         {'loss': 0.8483, 'learning_rate': 8.596478026771658e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2903/5198 [16:30:14<18:09:11, 28.48s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2904/5198 [16:30:31<16:01:27, 25.15s/it]                                                         {'loss': 0.327, 'learning_rate': 8.590309128568303e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2904/5198 [16:30:31<16:01:27, 25.15s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2905/5198 [16:30:50<14:44:41, 23.15s/it]                                                         {'loss': 0.8146, 'learning_rate': 8.584140777655476e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2905/5198 [16:30:50<14:44:41, 23.15s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2906/5198 [16:31:07<13:33:38, 21.30s/it]                                                         {'loss': 0.8142, 'learning_rate': 8.57797297642795e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2906/5198 [16:31:07<13:33:38, 21.30s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2907/5198 [16:31:25<12:52:01, 20.22s/it]                                                         {'loss': 0.8588, 'learning_rate': 8.571805727280278e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2907/5198 [16:31:25<12:52:01, 20.22s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2908/5198 [16:31:41<12:13:40, 19.22s/it]                                                         {'loss': 0.8478, 'learning_rate': 8.565639032606794e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2908/5198 [16:31:41<12:13:40, 19.22s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2909/5198 [16:31:59<11:54:07, 18.72s/it]                                                         {'loss': 0.8981, 'learning_rate': 8.559472894801623e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2909/5198 [16:31:59<11:54:07, 18.72s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2910/5198 [16:32:16<11:34:07, 18.20s/it]                                                         {'loss': 0.8046, 'learning_rate': 8.553307316258678e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2910/5198 [16:32:16<11:34:07, 18.20s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2911/5198 [16:32:34<11:28:53, 18.07s/it]                                                         {'loss': 0.8741, 'learning_rate': 8.547142299371642e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2911/5198 [16:32:34<11:28:53, 18.07s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2912/5198 [16:32:50<11:10:09, 17.59s/it]                                                         {'loss': 0.8626, 'learning_rate': 8.540977846533986e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2912/5198 [16:32:50<11:10:09, 17.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2913/5198 [16:33:09<11:19:08, 17.83s/it]                                                         {'loss': 0.811, 'learning_rate': 8.534813960138968e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2913/5198 [16:33:09<11:19:08, 17.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2914/5198 [16:33:27<11:27:33, 18.06s/it]                                                         {'loss': 0.7797, 'learning_rate': 8.528650642579618e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2914/5198 [16:33:27<11:27:33, 18.06s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2915/5198 [16:33:45<11:18:40, 17.84s/it]                                                         {'loss': 0.8352, 'learning_rate': 8.52248789624875e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2915/5198 [16:33:45<11:18:40, 17.84s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2916/5198 [16:34:02<11:15:44, 17.77s/it]                                                         {'loss': 0.7967, 'learning_rate': 8.516325723538949e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2916/5198 [16:34:02<11:15:44, 17.77s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2917/5198 [16:34:19<11:07:11, 17.55s/it]                                                         {'loss': 0.8167, 'learning_rate': 8.510164126842591e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2917/5198 [16:34:19<11:07:11, 17.55s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2918/5198 [16:34:36<10:58:11, 17.32s/it]                                                         {'loss': 0.8003, 'learning_rate': 8.504003108551814e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2918/5198 [16:34:36<10:58:11, 17.32s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2919/5198 [16:34:53<10:54:58, 17.24s/it]                                                         {'loss': 0.8328, 'learning_rate': 8.497842671058539e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2919/5198 [16:34:53<10:54:58, 17.24s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2920/5198 [16:35:09<10:44:33, 16.98s/it]                                                         {'loss': 0.8371, 'learning_rate': 8.491682816754456e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2920/5198 [16:35:09<10:44:33, 16.98s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2921/5198 [16:35:29<11:11:57, 17.71s/it]                                                         {'loss': 0.7747, 'learning_rate': 8.485523548031044e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2921/5198 [16:35:29<11:11:57, 17.71s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2922/5198 [16:35:46<11:03:23, 17.49s/it]                                                         {'loss': 0.814, 'learning_rate': 8.479364867279529e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2922/5198 [16:35:46<11:03:23, 17.49s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2923/5198 [16:36:03<11:00:20, 17.42s/it]                                                         {'loss': 0.817, 'learning_rate': 8.47320677689093e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2923/5198 [16:36:03<11:00:20, 17.42s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2924/5198 [16:36:20<11:00:43, 17.43s/it]                                                         {'loss': 0.8699, 'learning_rate': 8.467049279256034e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2924/5198 [16:36:20<11:00:43, 17.43s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2925/5198 [16:36:39<11:12:44, 17.76s/it]                                                         {'loss': 0.8593, 'learning_rate': 8.460892376765387e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2925/5198 [16:36:39<11:12:44, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2926/5198 [16:38:08<24:39:14, 39.06s/it]                                                         {'loss': 0.8385, 'learning_rate': 8.45473607180931e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2926/5198 [16:38:08<24:39:14, 39.06s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2927/5198 [16:38:25<20:30:03, 32.50s/it]                                                         {'loss': 0.8338, 'learning_rate': 8.448580366777898e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2927/5198 [16:38:25<20:30:03, 32.50s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2928/5198 [16:38:42<17:36:06, 27.91s/it]                                                         {'loss': 0.7819, 'learning_rate': 8.442425264061e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2928/5198 [16:38:42<17:36:06, 27.91s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2929/5198 [16:38:59<15:26:42, 24.51s/it]                                                         {'loss': 0.8335, 'learning_rate': 8.436270766048245e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2929/5198 [16:38:59<15:26:42, 24.51s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2930/5198 [16:39:17<14:10:22, 22.50s/it]                                                         {'loss': 0.8111, 'learning_rate': 8.430116875129023e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2930/5198 [16:39:17<14:10:22, 22.50s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2931/5198 [16:39:35<13:19:14, 21.15s/it]                                                         {'loss': 0.8184, 'learning_rate': 8.42396359369248e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2931/5198 [16:39:35<13:19:14, 21.15s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2932/5198 [16:39:52<12:31:57, 19.91s/it]                                                         {'loss': 0.8629, 'learning_rate': 8.417810924127533e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2932/5198 [16:39:52<12:31:57, 19.91s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2933/5198 [16:40:10<12:16:42, 19.52s/it]                                                         {'loss': 0.8008, 'learning_rate': 8.411658868822866e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2933/5198 [16:40:10<12:16:42, 19.52s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2934/5198 [16:40:28<11:56:39, 18.99s/it]                                                         {'loss': 0.8162, 'learning_rate': 8.40550743016691e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2934/5198 [16:40:28<11:56:39, 18.99s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2935/5198 [16:40:47<11:52:04, 18.88s/it]                                                         {'loss': 0.8398, 'learning_rate': 8.39935661054787e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2935/5198 [16:40:47<11:52:04, 18.88s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2936/5198 [16:41:05<11:43:00, 18.65s/it]                                                         {'loss': 0.874, 'learning_rate': 8.393206412353709e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2936/5198 [16:41:05<11:43:00, 18.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2937/5198 [16:41:23<11:41:26, 18.61s/it]                                                         {'loss': 0.8212, 'learning_rate': 8.38705683797214e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2937/5198 [16:41:23<11:41:26, 18.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2938/5198 [16:41:40<11:22:29, 18.12s/it]                                                         {'loss': 0.8571, 'learning_rate': 8.38090788979064e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2938/5198 [16:41:40<11:22:29, 18.12s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2939/5198 [16:41:57<11:10:13, 17.80s/it]                                                         {'loss': 0.8665, 'learning_rate': 8.374759570196448e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2939/5198 [16:41:57<11:10:13, 17.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2940/5198 [16:42:16<11:16:43, 17.98s/it]                                                         {'loss': 0.8212, 'learning_rate': 8.368611881576547e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2940/5198 [16:42:16<11:16:43, 17.98s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2941/5198 [16:42:32<11:02:25, 17.61s/it]                                                         {'loss': 0.8144, 'learning_rate': 8.362464826317687e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2941/5198 [16:42:32<11:02:25, 17.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2942/5198 [16:42:51<11:08:24, 17.78s/it]                                                         {'loss': 0.9108, 'learning_rate': 8.35631840680636e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2942/5198 [16:42:51<11:08:24, 17.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2943/5198 [16:43:08<11:09:25, 17.81s/it]                                                         {'loss': 0.7684, 'learning_rate': 8.35017262542882e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2943/5198 [16:43:08<11:09:25, 17.81s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2944/5198 [16:43:27<11:12:30, 17.90s/it]                                                         {'loss': 0.8557, 'learning_rate': 8.344027484571075e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2944/5198 [16:43:27<11:12:30, 17.90s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2945/5198 [16:43:44<11:06:17, 17.74s/it]                                                         {'loss': 0.3353, 'learning_rate': 8.337882986618877e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2945/5198 [16:43:44<11:06:17, 17.74s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2946/5198 [16:44:01<10:58:08, 17.53s/it]                                                         {'loss': 0.8558, 'learning_rate': 8.331739133957729e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2946/5198 [16:44:01<10:58:08, 17.53s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2947/5198 [16:44:18<10:49:54, 17.32s/it]                                                         {'loss': 0.8259, 'learning_rate': 8.325595928972894e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2947/5198 [16:44:18<10:49:54, 17.32s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2948/5198 [16:44:35<10:45:04, 17.20s/it]                                                         {'loss': 0.8363, 'learning_rate': 8.319453374049367e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2948/5198 [16:44:35<10:45:04, 17.20s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2949/5198 [16:44:53<10:55:46, 17.50s/it]                                                         {'loss': 0.8128, 'learning_rate': 8.313311471571903e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2949/5198 [16:44:53<10:55:46, 17.50s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2950/5198 [16:45:12<11:10:03, 17.88s/it]                                                         {'loss': 0.8525, 'learning_rate': 8.307170223925003e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2950/5198 [16:45:12<11:10:03, 17.88s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2951/5198 [16:46:39<24:05:32, 38.60s/it]                                                         {'loss': 0.3405, 'learning_rate': 8.301029633492907e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2951/5198 [16:46:39<24:05:32, 38.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2952/5198 [16:46:55<19:56:51, 31.97s/it]                                                         {'loss': 0.7772, 'learning_rate': 8.294889702659602e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2952/5198 [16:46:55<19:56:51, 31.97s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2953/5198 [16:47:13<17:21:39, 27.84s/it]                                                         {'loss': 0.8187, 'learning_rate': 8.288750433808828e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2953/5198 [16:47:13<17:21:39, 27.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2954/5198 [16:47:30<15:17:19, 24.53s/it]                                                         {'loss': 0.8375, 'learning_rate': 8.282611829324049e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2954/5198 [16:47:30<15:17:19, 24.53s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2955/5198 [16:47:49<14:08:55, 22.71s/it]                                                         {'loss': 0.7996, 'learning_rate': 8.276473891588492e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2955/5198 [16:47:49<14:08:55, 22.71s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2956/5198 [16:48:07<13:15:34, 21.29s/it]                                                         {'loss': 0.8668, 'learning_rate': 8.270336622985116e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2956/5198 [16:48:07<13:15:34, 21.29s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2957/5198 [16:48:24<12:36:25, 20.25s/it]                                                         {'loss': 0.8473, 'learning_rate': 8.264200025896616e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2957/5198 [16:48:24<12:36:25, 20.25s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2958/5198 [16:48:42<12:07:25, 19.48s/it]                                                         {'loss': 0.8481, 'learning_rate': 8.258064102705428e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2958/5198 [16:48:42<12:07:25, 19.48s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2959/5198 [16:49:00<11:52:48, 19.10s/it]                                                         {'loss': 0.8214, 'learning_rate': 8.251928855793736e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2959/5198 [16:49:00<11:52:48, 19.10s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2960/5198 [16:49:18<11:32:03, 18.55s/it]                                                         {'loss': 0.821, 'learning_rate': 8.245794287543447e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2960/5198 [16:49:18<11:32:03, 18.55s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2961/5198 [16:49:35<11:19:45, 18.23s/it]                                                         {'loss': 0.7923, 'learning_rate': 8.239660400336213e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2961/5198 [16:49:35<11:19:45, 18.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2962/5198 [16:49:53<11:14:56, 18.11s/it]                                                         {'loss': 0.8712, 'learning_rate': 8.233527196553428e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2962/5198 [16:49:53<11:14:56, 18.11s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2963/5198 [16:50:09<10:55:18, 17.59s/it]                                                         {'loss': 0.8092, 'learning_rate': 8.227394678576204e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2963/5198 [16:50:09<10:55:18, 17.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2964/5198 [16:50:28<11:02:53, 17.80s/it]                                                         {'loss': 0.808, 'learning_rate': 8.221262848785395e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2964/5198 [16:50:28<11:02:53, 17.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2965/5198 [16:50:46<11:06:57, 17.92s/it]                                                         {'loss': 0.8415, 'learning_rate': 8.215131709561597e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2965/5198 [16:50:46<11:06:57, 17.92s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2966/5198 [16:51:04<11:08:34, 17.97s/it]                                                         {'loss': 0.7068, 'learning_rate': 8.20900126328512e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2966/5198 [16:51:04<11:08:34, 17.97s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2967/5198 [16:51:23<11:17:41, 18.23s/it]                                                         {'loss': 0.7563, 'learning_rate': 8.202871512336023e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2967/5198 [16:51:23<11:17:41, 18.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2968/5198 [16:51:40<11:10:45, 18.05s/it]                                                         {'loss': 0.7902, 'learning_rate': 8.196742459094079e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2968/5198 [16:51:40<11:10:45, 18.05s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2969/5198 [16:51:57<10:59:03, 17.74s/it]                                                         {'loss': 0.8034, 'learning_rate': 8.190614105938796e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2969/5198 [16:51:57<10:59:03, 17.74s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2970/5198 [16:52:15<10:59:52, 17.77s/it]                                                         {'loss': 0.8241, 'learning_rate': 8.184486455249424e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2970/5198 [16:52:15<10:59:52, 17.77s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2971/5198 [16:52:32<10:53:03, 17.59s/it]                                                         {'loss': 0.8233, 'learning_rate': 8.178359509404916e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2971/5198 [16:52:32<10:53:03, 17.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2972/5198 [16:52:50<10:55:40, 17.67s/it]                                                         {'loss': 0.7851, 'learning_rate': 8.172233270783966e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2972/5198 [16:52:50<10:55:40, 17.67s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2973/5198 [16:53:07<10:46:58, 17.45s/it]                                                         {'loss': 0.7924, 'learning_rate': 8.166107741764997e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2973/5198 [16:53:07<10:46:58, 17.45s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2974/5198 [16:53:25<10:54:12, 17.65s/it]                                                         {'loss': 0.7858, 'learning_rate': 8.15998292472614e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2974/5198 [16:53:25<10:54:12, 17.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2975/5198 [16:53:44<11:01:16, 17.85s/it]                                                         {'loss': 0.8139, 'learning_rate': 8.153858822045267e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2975/5198 [16:53:44<11:01:16, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2976/5198 [16:55:10<23:38:36, 38.31s/it]                                                         {'loss': 0.8594, 'learning_rate': 8.147735436099967e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2976/5198 [16:55:10<23:38:36, 38.31s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2977/5198 [16:55:27<19:48:22, 32.10s/it]                                                         {'loss': 0.794, 'learning_rate': 8.141612769267543e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2977/5198 [16:55:27<19:48:22, 32.10s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2978/5198 [16:55:44<17:00:50, 27.59s/it]                                                         {'loss': 0.7877, 'learning_rate': 8.135490823925027e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2978/5198 [16:55:44<17:00:50, 27.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2979/5198 [16:56:02<15:12:26, 24.67s/it]                                                         {'loss': 0.7987, 'learning_rate': 8.129369602449176e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2979/5198 [16:56:02<15:12:26, 24.67s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2980/5198 [16:56:20<13:57:46, 22.66s/it]                                                         {'loss': 0.7833, 'learning_rate': 8.123249107216446e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2980/5198 [16:56:20<13:57:46, 22.66s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2981/5198 [16:56:37<12:56:25, 21.01s/it]                                                         {'loss': 0.7911, 'learning_rate': 8.117129340603032e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2981/5198 [16:56:37<12:56:25, 21.01s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2982/5198 [16:56:54<12:08:26, 19.72s/it]                                                         {'loss': 0.7929, 'learning_rate': 8.111010304984841e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2982/5198 [16:56:54<12:08:26, 19.72s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2983/5198 [16:57:12<11:49:50, 19.23s/it]                                                         {'loss': 0.8064, 'learning_rate': 8.104892002737488e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2983/5198 [16:57:12<11:49:50, 19.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2984/5198 [16:57:30<11:31:23, 18.74s/it]                                                         {'loss': 0.7768, 'learning_rate': 8.098774436236308e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2984/5198 [16:57:30<11:31:23, 18.74s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2985/5198 [16:57:47<11:15:56, 18.33s/it]                                                         {'loss': 0.7644, 'learning_rate': 8.092657607856356e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2985/5198 [16:57:47<11:15:56, 18.33s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2986/5198 [16:58:04<11:03:49, 18.01s/it]                                                         {'loss': 0.8311, 'learning_rate': 8.086541519972388e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2986/5198 [16:58:04<11:03:49, 18.01s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2987/5198 [16:58:22<11:01:05, 17.94s/it]                                                         {'loss': 0.7848, 'learning_rate': 8.080426174958886e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2987/5198 [16:58:22<11:01:05, 17.94s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2988/5198 [16:58:40<10:59:21, 17.90s/it]                                                         {'loss': 0.3559, 'learning_rate': 8.074311575190039e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2988/5198 [16:58:40<10:59:21, 17.90s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2989/5198 [16:58:58<11:01:02, 17.95s/it]                                                         {'loss': 0.8076, 'learning_rate': 8.068197723039738e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2989/5198 [16:58:58<11:01:02, 17.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2990/5198 [16:59:17<11:09:11, 18.18s/it]                                                         {'loss': 0.7985, 'learning_rate': 8.062084620881598e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2990/5198 [16:59:17<11:09:11, 18.18s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2991/5198 [16:59:34<11:02:54, 18.02s/it]                                                         {'loss': 0.8057, 'learning_rate': 8.055972271088933e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2991/5198 [16:59:34<11:02:54, 18.02s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2992/5198 [16:59:53<11:06:49, 18.14s/it]                                                         {'loss': 0.7957, 'learning_rate': 8.049860676034762e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2992/5198 [16:59:53<11:06:49, 18.14s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2993/5198 [17:00:11<11:03:00, 18.04s/it]                                                         {'loss': 0.8596, 'learning_rate': 8.043749838091828e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2993/5198 [17:00:11<11:03:00, 18.04s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2994/5198 [17:00:29<11:07:24, 18.17s/it]                                                         {'loss': 0.7786, 'learning_rate': 8.037639759632558e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2994/5198 [17:00:29<11:07:24, 18.17s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2995/5198 [17:00:47<11:02:07, 18.03s/it]                                                         {'loss': 0.7643, 'learning_rate': 8.031530443029099e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2995/5198 [17:00:47<11:02:07, 18.03s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2996/5198 [17:01:04<10:54:34, 17.84s/it]                                                         {'loss': 0.853, 'learning_rate': 8.025421890653303e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2996/5198 [17:01:04<10:54:34, 17.84s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2997/5198 [17:01:21<10:45:03, 17.58s/it]                                                         {'loss': 0.7584, 'learning_rate': 8.019314104876712e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2997/5198 [17:01:21<10:45:03, 17.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2998/5198 [17:01:39<10:50:11, 17.73s/it]                                                         {'loss': 0.8406, 'learning_rate': 8.013207088070582e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2998/5198 [17:01:39<10:50:11, 17.73s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2999/5198 [17:01:58<10:57:04, 17.93s/it]                                                         {'loss': 0.8442, 'learning_rate': 8.007100842605872e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2999/5198 [17:01:58<10:57:04, 17.93s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3000/5198 [17:02:15<10:53:47, 17.85s/it]                                                         {'loss': 0.8313, 'learning_rate': 8.000995370853227e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3000/5198 [17:02:15<10:53:47, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3001/5198 [17:03:44<23:49:06, 39.03s/it]                                                         {'loss': 0.8187, 'learning_rate': 7.994890675183008e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3001/5198 [17:03:44<23:49:06, 39.03s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3002/5198 [17:04:02<19:55:30, 32.66s/it]                                                         {'loss': 0.8135, 'learning_rate': 7.98878675796527e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3002/5198 [17:04:02<19:55:30, 32.66s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3003/5198 [17:04:20<17:15:21, 28.30s/it]                                                         {'loss': 0.8171, 'learning_rate': 7.98268362156976e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3003/5198 [17:04:20<17:15:21, 28.30s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3004/5198 [17:04:37<15:16:15, 25.06s/it]                                                         {'loss': 0.7864, 'learning_rate': 7.976581268365924e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3004/5198 [17:04:37<15:16:15, 25.06s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3005/5198 [17:04:54<13:41:07, 22.47s/it]                                                         {'loss': 0.8512, 'learning_rate': 7.97047970072291e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3005/5198 [17:04:54<13:41:07, 22.47s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3006/5198 [17:05:11<12:46:06, 20.97s/it]                                                         {'loss': 0.8079, 'learning_rate': 7.964378921009552e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3006/5198 [17:05:11<12:46:06, 20.97s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3007/5198 [17:05:28<12:02:26, 19.78s/it]                                                         {'loss': 0.3486, 'learning_rate': 7.958278931594385e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3007/5198 [17:05:28<12:02:26, 19.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3008/5198 [17:05:46<11:39:33, 19.17s/it]                                                         {'loss': 0.7622, 'learning_rate': 7.952179734845642e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3008/5198 [17:05:46<11:39:33, 19.17s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3009/5198 [17:06:04<11:25:07, 18.78s/it]                                                         {'loss': 0.8442, 'learning_rate': 7.946081333131227e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3009/5198 [17:06:04<11:25:07, 18.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3010/5198 [17:06:22<11:15:46, 18.53s/it]                                                         {'loss': 0.8203, 'learning_rate': 7.93998372881876e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3010/5198 [17:06:22<11:15:46, 18.53s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3011/5198 [17:06:40<11:09:32, 18.37s/it]                                                         {'loss': 0.7346, 'learning_rate': 7.93388692427554e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3011/5198 [17:06:40<11:09:32, 18.37s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3012/5198 [17:06:57<11:02:02, 18.17s/it]                                                         {'loss': 0.7788, 'learning_rate': 7.92779092186855e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3012/5198 [17:06:57<11:02:02, 18.17s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3013/5198 [17:07:15<11:01:52, 18.18s/it]                                                         {'loss': 0.8673, 'learning_rate': 7.921695723964473e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3013/5198 [17:07:15<11:01:52, 18.18s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3014/5198 [17:07:34<11:02:40, 18.21s/it]                                                         {'loss': 0.7571, 'learning_rate': 7.915601332929678e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3014/5198 [17:07:34<11:02:40, 18.21s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3015/5198 [17:07:51<10:54:09, 17.98s/it]                                                         {'loss': 0.8257, 'learning_rate': 7.90950775113021e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3015/5198 [17:07:51<10:54:09, 17.98s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3016/5198 [17:08:09<10:48:01, 17.82s/it]                                                         {'loss': 0.7672, 'learning_rate': 7.903414980931813e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3016/5198 [17:08:09<10:48:01, 17.82s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3017/5198 [17:08:27<10:51:31, 17.92s/it]                                                         {'loss': 0.7912, 'learning_rate': 7.897323024699907e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3017/5198 [17:08:27<10:51:31, 17.92s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3018/5198 [17:08:44<10:46:09, 17.78s/it]                                                         {'loss': 0.8046, 'learning_rate': 7.8912318847996e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3018/5198 [17:08:44<10:46:09, 17.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3019/5198 [17:09:01<10:36:04, 17.51s/it]                                                         {'loss': 0.7928, 'learning_rate': 7.885141563595685e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3019/5198 [17:09:01<10:36:04, 17.51s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3020/5198 [17:09:19<10:36:30, 17.53s/it]                                                         {'loss': 0.7799, 'learning_rate': 7.879052063452626e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3020/5198 [17:09:19<10:36:30, 17.53s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3021/5198 [17:09:36<10:34:54, 17.50s/it]                                                         {'loss': 0.8224, 'learning_rate': 7.872963386734584e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3021/5198 [17:09:36<10:34:54, 17.50s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3022/5198 [17:09:54<10:41:01, 17.68s/it]                                                         {'loss': 0.81, 'learning_rate': 7.866875535805394e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3022/5198 [17:09:54<10:41:01, 17.68s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3023/5198 [17:10:11<10:34:14, 17.50s/it]                                                         {'loss': 0.8315, 'learning_rate': 7.860788513028566e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3023/5198 [17:10:11<10:34:14, 17.50s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3024/5198 [17:10:28<10:29:27, 17.37s/it]                                                         {'loss': 0.8287, 'learning_rate': 7.85470232076729e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3024/5198 [17:10:28<10:29:27, 17.37s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3025/5198 [17:10:45<10:25:04, 17.26s/it]                                                         {'loss': 0.872, 'learning_rate': 7.848616961384442e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3025/5198 [17:10:45<10:25:04, 17.26s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3026/5198 [17:12:09<22:30:23, 37.30s/it]                                                         {'loss': 0.7915, 'learning_rate': 7.842532437242559e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3026/5198 [17:12:09<22:30:23, 37.30s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3027/5198 [17:12:27<18:53:11, 31.32s/it]                                                         {'loss': 0.7956, 'learning_rate': 7.83644875070387e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3027/5198 [17:12:27<18:53:11, 31.32s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3028/5198 [17:12:45<16:33:42, 27.48s/it]                                                         {'loss': 0.8402, 'learning_rate': 7.83036590413027e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3028/5198 [17:12:45<16:33:42, 27.48s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3029/5198 [17:13:04<14:55:28, 24.77s/it]                                                         {'loss': 0.7762, 'learning_rate': 7.824283899883327e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3029/5198 [17:13:04<14:55:28, 24.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3030/5198 [17:13:22<13:39:01, 22.67s/it]                                                         {'loss': 0.8194, 'learning_rate': 7.818202740324287e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3030/5198 [17:13:22<13:39:01, 22.67s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3031/5198 [17:13:39<12:40:53, 21.07s/it]                                                         {'loss': 0.8192, 'learning_rate': 7.812122427814068e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3031/5198 [17:13:39<12:40:53, 21.07s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3032/5198 [17:13:57<12:08:52, 20.19s/it]                                                         {'loss': 0.793, 'learning_rate': 7.806042964713248e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3032/5198 [17:13:57<12:08:52, 20.19s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3033/5198 [17:14:15<11:43:06, 19.49s/it]                                                         {'loss': 0.7959, 'learning_rate': 7.79996435338209e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3033/5198 [17:14:15<11:43:06, 19.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3034/5198 [17:14:33<11:25:21, 19.00s/it]                                                         {'loss': 0.8009, 'learning_rate': 7.793886596180521e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3034/5198 [17:14:33<11:25:21, 19.00s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3035/5198 [17:14:50<11:08:14, 18.54s/it]                                                         {'loss': 0.3381, 'learning_rate': 7.787809695468134e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3035/5198 [17:14:50<11:08:14, 18.54s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3036/5198 [17:15:08<11:02:30, 18.39s/it]                                                         {'loss': 0.8455, 'learning_rate': 7.78173365360419e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3036/5198 [17:15:08<11:02:30, 18.39s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3037/5198 [17:15:26<10:57:56, 18.27s/it]                                                         {'loss': 0.8179, 'learning_rate': 7.775658472947623e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3037/5198 [17:15:26<10:57:56, 18.27s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3038/5198 [17:15:44<10:50:48, 18.08s/it]                                                         {'loss': 0.8272, 'learning_rate': 7.769584155857019e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3038/5198 [17:15:44<10:50:48, 18.08s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3039/5198 [17:16:01<10:45:15, 17.93s/it]                                                         {'loss': 0.8042, 'learning_rate': 7.763510704690645e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3039/5198 [17:16:01<10:45:15, 17.93s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3040/5198 [17:16:18<10:34:46, 17.65s/it]                                                         {'loss': 0.8164, 'learning_rate': 7.757438121806414e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3040/5198 [17:16:18<10:34:46, 17.65s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3041/5198 [17:16:37<10:40:22, 17.81s/it]                                                         {'loss': 0.7876, 'learning_rate': 7.75136640956192e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3041/5198 [17:16:37<10:40:22, 17.81s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3042/5198 [17:16:54<10:37:09, 17.73s/it]                                                         {'loss': 0.8365, 'learning_rate': 7.745295570314412e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3042/5198 [17:16:54<10:37:09, 17.73s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3043/5198 [17:17:12<10:34:05, 17.65s/it]                                                         {'loss': 0.8161, 'learning_rate': 7.739225606420793e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3043/5198 [17:17:12<10:34:05, 17.65s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3044/5198 [17:17:28<10:25:09, 17.41s/it]                                                         {'loss': 0.8462, 'learning_rate': 7.733156520237633e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3044/5198 [17:17:29<10:25:09, 17.41s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3045/5198 [17:17:47<10:37:05, 17.75s/it]                                                         {'loss': 0.7925, 'learning_rate': 7.727088314121165e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3045/5198 [17:17:47<10:37:05, 17.75s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3046/5198 [17:18:05<10:39:59, 17.84s/it]                                                         {'loss': 0.8225, 'learning_rate': 7.721020990427268e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3046/5198 [17:18:05<10:39:59, 17.84s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3047/5198 [17:18:22<10:31:07, 17.60s/it]                                                         {'loss': 0.8116, 'learning_rate': 7.714954551511489e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3047/5198 [17:18:22<10:31:07, 17.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3048/5198 [17:18:39<10:24:38, 17.43s/it]                                                         {'loss': 0.805, 'learning_rate': 7.708888999729036e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3048/5198 [17:18:39<10:24:38, 17.43s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3049/5198 [17:18:57<10:24:48, 17.44s/it]                                                         {'loss': 0.3406, 'learning_rate': 7.702824337434756e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3049/5198 [17:18:57<10:24:48, 17.44s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3050/5198 [17:19:14<10:24:25, 17.44s/it]                                                         {'loss': 0.8223, 'learning_rate': 7.69676056698316e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3050/5198 [17:19:14<10:24:25, 17.44s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3051/5198 [17:20:41<22:54:01, 38.40s/it]                                                         {'loss': 0.8013, 'learning_rate': 7.690697690728417e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3051/5198 [17:20:41<22:54:01, 38.40s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3052/5198 [17:20:58<19:02:41, 31.95s/it]                                                         {'loss': 0.7757, 'learning_rate': 7.68463571102434e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3052/5198 [17:20:58<19:02:41, 31.95s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3053/5198 [17:21:16<16:31:43, 27.74s/it]                                                         {'loss': 0.7873, 'learning_rate': 7.678574630224399e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3053/5198 [17:21:16<16:31:43, 27.74s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3054/5198 [17:21:34<14:48:48, 24.87s/it]                                                         {'loss': 0.8118, 'learning_rate': 7.672514450681721e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3054/5198 [17:21:34<14:48:48, 24.87s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3055/5198 [17:21:52<13:31:07, 22.71s/it]                                                         {'loss': 0.8069, 'learning_rate': 7.666455174749066e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3055/5198 [17:21:52<13:31:07, 22.71s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3056/5198 [17:22:09<12:26:37, 20.91s/it]                                                         {'loss': 0.7997, 'learning_rate': 7.66039680477886e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3056/5198 [17:22:09<12:26:37, 20.91s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3057/5198 [17:22:28<12:06:05, 20.35s/it]                                                         {'loss': 0.7859, 'learning_rate': 7.654339343123173e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3057/5198 [17:22:28<12:06:05, 20.35s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3058/5198 [17:22:46<11:37:38, 19.56s/it]                                                         {'loss': 0.7796, 'learning_rate': 7.648282792133711e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3058/5198 [17:22:46<11:37:38, 19.56s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3059/5198 [17:23:03<11:13:12, 18.88s/it]                                                         {'loss': 0.838, 'learning_rate': 7.642227154161841e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3059/5198 [17:23:03<11:13:12, 18.88s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3060/5198 [17:23:21<11:00:05, 18.52s/it]                                                         {'loss': 0.879, 'learning_rate': 7.636172431558575e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3060/5198 [17:23:21<11:00:05, 18.52s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3061/5198 [17:23:38<10:43:57, 18.08s/it]                                                         {'loss': 0.3415, 'learning_rate': 7.630118626674557e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3061/5198 [17:23:38<10:43:57, 18.08s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3062/5198 [17:23:54<10:26:42, 17.60s/it]                                                         {'loss': 0.3448, 'learning_rate': 7.6240657418600846e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3062/5198 [17:23:54<10:26:42, 17.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3063/5198 [17:24:12<10:31:25, 17.74s/it]                                                         {'loss': 0.809, 'learning_rate': 7.618013779465101e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3063/5198 [17:24:12<10:31:25, 17.74s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3064/5198 [17:24:30<10:33:20, 17.81s/it]                                                         {'loss': 0.8398, 'learning_rate': 7.611962741839178e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3064/5198 [17:24:30<10:33:20, 17.81s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3065/5198 [17:24:48<10:31:43, 17.77s/it]                                                         {'loss': 0.8088, 'learning_rate': 7.6059126313315466e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3065/5198 [17:24:48<10:31:43, 17.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3066/5198 [17:25:06<10:37:08, 17.93s/it]                                                         {'loss': 0.832, 'learning_rate': 7.599863450291056e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3066/5198 [17:25:06<10:37:08, 17.93s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3067/5198 [17:25:25<10:44:30, 18.15s/it]                                                         {'loss': 0.8314, 'learning_rate': 7.593815201066215e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3067/5198 [17:25:25<10:44:30, 18.15s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3068/5198 [17:25:41<10:28:56, 17.72s/it]                                                         {'loss': 0.8345, 'learning_rate': 7.587767886005164e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3068/5198 [17:25:41<10:28:56, 17.72s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3069/5198 [17:25:59<10:28:56, 17.73s/it]                                                         {'loss': 0.8376, 'learning_rate': 7.581721507455672e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3069/5198 [17:25:59<10:28:56, 17.73s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3070/5198 [17:26:17<10:27:55, 17.70s/it]                                                         {'loss': 0.8202, 'learning_rate': 7.575676067765154e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3070/5198 [17:26:17<10:27:55, 17.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3071/5198 [17:26:34<10:17:42, 17.42s/it]                                                         {'loss': 0.8173, 'learning_rate': 7.569631569280662e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3071/5198 [17:26:34<10:17:42, 17.42s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3072/5198 [17:26:52<10:22:38, 17.57s/it]                                                         {'loss': 0.8431, 'learning_rate': 7.563588014348871e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3072/5198 [17:26:52<10:22:38, 17.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3073/5198 [17:27:10<10:29:29, 17.77s/it]                                                         {'loss': 0.8352, 'learning_rate': 7.5575454053161e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3073/5198 [17:27:10<10:29:29, 17.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3074/5198 [17:27:27<10:27:55, 17.74s/it]                                                         {'loss': 0.7564, 'learning_rate': 7.551503744528304e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3074/5198 [17:27:27<10:27:55, 17.74s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3075/5198 [17:27:45<10:28:13, 17.76s/it]                                                         {'loss': 0.8375, 'learning_rate': 7.545463034331054e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3075/5198 [17:27:45<10:28:13, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3076/5198 [17:29:12<22:42:55, 38.54s/it]                                                         {'loss': 0.3538, 'learning_rate': 7.539423277069568e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3076/5198 [17:29:12<22:42:55, 38.54s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3077/5198 [17:29:29<18:48:18, 31.92s/it]                                                         {'loss': 0.7856, 'learning_rate': 7.53338447508869e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3077/5198 [17:29:29<18:48:18, 31.92s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3078/5198 [17:29:45<16:02:59, 27.25s/it]                                                         {'loss': 0.7762, 'learning_rate': 7.52734663073288e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3078/5198 [17:29:45<16:02:59, 27.25s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3079/5198 [17:30:02<14:17:27, 24.28s/it]                                                         {'loss': 0.8615, 'learning_rate': 7.521309746346246e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3079/5198 [17:30:02<14:17:27, 24.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3080/5198 [17:30:20<13:06:11, 22.27s/it]                                                         {'loss': 0.833, 'learning_rate': 7.515273824272516e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3080/5198 [17:30:20<13:06:11, 22.27s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3081/5198 [17:30:38<12:16:59, 20.89s/it]                                                         {'loss': 0.3563, 'learning_rate': 7.509238866855033e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3081/5198 [17:30:38<12:16:59, 20.89s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3082/5198 [17:30:56<11:48:01, 20.08s/it]                                                         {'loss': 0.8183, 'learning_rate': 7.503204876436785e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3082/5198 [17:30:56<11:48:01, 20.08s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3083/5198 [17:31:13<11:13:37, 19.11s/it]                                                         {'loss': 0.8263, 'learning_rate': 7.497171855360372e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3083/5198 [17:31:13<11:13:37, 19.11s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3084/5198 [17:31:30<10:55:06, 18.59s/it]                                                         {'loss': 0.7843, 'learning_rate': 7.491139805968018e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3084/5198 [17:31:30<10:55:06, 18.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3085/5198 [17:31:48<10:51:10, 18.49s/it]                                                         {'loss': 0.7959, 'learning_rate': 7.485108730601571e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3085/5198 [17:31:48<10:51:10, 18.49s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3086/5198 [17:32:06<10:39:25, 18.17s/it]                                                         {'loss': 0.8239, 'learning_rate': 7.4790786316025125e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3086/5198 [17:32:06<10:39:25, 18.17s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3087/5198 [17:32:23<10:29:47, 17.90s/it]                                                         {'loss': 0.8195, 'learning_rate': 7.473049511311921e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3087/5198 [17:32:23<10:29:47, 17.90s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3088/5198 [17:32:41<10:25:00, 17.77s/it]                                                         {'loss': 0.3653, 'learning_rate': 7.467021372070515e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3088/5198 [17:32:41<10:25:00, 17.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3089/5198 [17:32:58<10:20:14, 17.65s/it]                                                         {'loss': 0.323, 'learning_rate': 7.46099421621863e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3089/5198 [17:32:58<10:20:14, 17.65s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3090/5198 [17:33:15<10:19:15, 17.63s/it]                                                         {'loss': 0.8607, 'learning_rate': 7.4549680460962044e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3090/5198 [17:33:15<10:19:15, 17.63s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3091/5198 [17:33:33<10:19:33, 17.64s/it]                                                         {'loss': 0.3665, 'learning_rate': 7.448942864042819e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3091/5198 [17:33:33<10:19:33, 17.64s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3092/5198 [17:33:50<10:08:33, 17.34s/it]                                                         {'loss': 0.8288, 'learning_rate': 7.4429186723976425e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3092/5198 [17:33:50<10:08:33, 17.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3093/5198 [17:34:07<10:08:53, 17.36s/it]                                                         {'loss': 0.8262, 'learning_rate': 7.43689547349948e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3093/5198 [17:34:07<10:08:53, 17.36s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3094/5198 [17:34:25<10:15:38, 17.56s/it]                                                         {'loss': 0.7985, 'learning_rate': 7.43087326968675e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3094/5198 [17:34:25<10:15:38, 17.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3095/5198 [17:34:43<10:16:48, 17.60s/it]                                                         {'loss': 0.8157, 'learning_rate': 7.42485206329747e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3095/5198 [17:34:43<10:16:48, 17.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3096/5198 [17:35:01<10:21:18, 17.73s/it]                                                         {'loss': 0.8157, 'learning_rate': 7.418831856669286e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3096/5198 [17:35:01<10:21:18, 17.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3097/5198 [17:35:19<10:19:34, 17.69s/it]                                                         {'loss': 0.8317, 'learning_rate': 7.41281265213945e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3097/5198 [17:35:19<10:19:34, 17.69s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3098/5198 [17:35:35<10:09:06, 17.40s/it]                                                         {'loss': 0.8445, 'learning_rate': 7.406794452044816e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3098/5198 [17:35:35<10:09:06, 17.40s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3099/5198 [17:35:53<10:15:57, 17.61s/it]                                                         {'loss': 0.8266, 'learning_rate': 7.400777258721865e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3099/5198 [17:35:53<10:15:57, 17.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3100/5198 [17:36:11<10:15:22, 17.60s/it]                                                         {'loss': 0.8765, 'learning_rate': 7.394761074506679e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3100/5198 [17:36:11<10:15:22, 17.60s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3101/5198 [17:37:38<22:24:39, 38.47s/it]                                                         {'loss': 0.7885, 'learning_rate': 7.3887459017349405e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3101/5198 [17:37:38<22:24:39, 38.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3102/5198 [17:37:56<18:45:11, 32.21s/it]                                                         {'loss': 0.7977, 'learning_rate': 7.382731742741953e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3102/5198 [17:37:56<18:45:11, 32.21s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3103/5198 [17:38:13<16:10:51, 27.80s/it]                                                         {'loss': 0.8139, 'learning_rate': 7.376718599862621e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3103/5198 [17:38:13<16:10:51, 27.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3104/5198 [17:38:30<14:19:29, 24.63s/it]                                                         {'loss': 0.8612, 'learning_rate': 7.370706475431446e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3104/5198 [17:38:30<14:19:29, 24.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3105/5198 [17:38:48<13:05:45, 22.53s/it]                                                         {'loss': 0.8112, 'learning_rate': 7.364695371782547e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3105/5198 [17:38:48<13:05:45, 22.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3106/5198 [17:39:05<12:05:24, 20.81s/it]                                                         {'loss': 0.8718, 'learning_rate': 7.358685291249644e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3106/5198 [17:39:05<12:05:24, 20.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3107/5198 [17:39:23<11:36:31, 19.99s/it]                                                         {'loss': 0.7838, 'learning_rate': 7.352676236166051e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3107/5198 [17:39:23<11:36:31, 19.99s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3108/5198 [17:39:40<11:04:15, 19.07s/it]                                                         {'loss': 0.89, 'learning_rate': 7.346668208864695e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3108/5198 [17:39:40<11:04:15, 19.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3109/5198 [17:39:57<10:48:21, 18.62s/it]                                                         {'loss': 0.7578, 'learning_rate': 7.3406612116781e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3109/5198 [17:39:57<10:48:21, 18.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3110/5198 [17:40:14<10:27:36, 18.03s/it]                                                         {'loss': 0.7964, 'learning_rate': 7.33465524693838e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3110/5198 [17:40:14<10:27:36, 18.03s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3111/5198 [17:40:31<10:18:51, 17.79s/it]                                                         {'loss': 0.8178, 'learning_rate': 7.328650316977265e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3111/5198 [17:40:31<10:18:51, 17.79s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3112/5198 [17:40:49<10:19:20, 17.81s/it]                                                         {'loss': 0.8125, 'learning_rate': 7.322646424126079e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3112/5198 [17:40:49<10:19:20, 17.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3113/5198 [17:41:07<10:15:23, 17.71s/it]                                                         {'loss': 0.8084, 'learning_rate': 7.316643570715729e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3113/5198 [17:41:07<10:15:23, 17.71s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3114/5198 [17:41:24<10:14:02, 17.68s/it]                                                         {'loss': 0.7996, 'learning_rate': 7.310641759076742e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3114/5198 [17:41:24<10:14:02, 17.68s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3115/5198 [17:41:42<10:10:41, 17.59s/it]                                                         {'loss': 0.8169, 'learning_rate': 7.304640991539216e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3115/5198 [17:41:42<10:10:41, 17.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3116/5198 [17:41:59<10:12:15, 17.64s/it]                                                         {'loss': 0.8248, 'learning_rate': 7.2986412704328625e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3116/5198 [17:41:59<10:12:15, 17.64s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3117/5198 [17:42:18<10:18:23, 17.83s/it]                                                         {'loss': 0.8199, 'learning_rate': 7.292642598086982e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3117/5198 [17:42:18<10:18:23, 17.83s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3118/5198 [17:42:35<10:17:21, 17.81s/it]                                                         {'loss': 0.8048, 'learning_rate': 7.286644976830457e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3118/5198 [17:42:35<10:17:21, 17.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3119/5198 [17:42:53<10:13:45, 17.71s/it]                                                         {'loss': 0.7686, 'learning_rate': 7.280648408991775e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3119/5198 [17:42:53<10:13:45, 17.71s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3120/5198 [17:43:11<10:18:00, 17.84s/it]                                                         {'loss': 0.8691, 'learning_rate': 7.274652896899015e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3120/5198 [17:43:11<10:18:00, 17.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3121/5198 [17:43:29<10:16:20, 17.80s/it]                                                         {'loss': 0.7927, 'learning_rate': 7.268658442879834e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3121/5198 [17:43:29<10:16:20, 17.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3122/5198 [17:43:47<10:15:53, 17.80s/it]                                                         {'loss': 0.8609, 'learning_rate': 7.262665049261489e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3122/5198 [17:43:47<10:15:53, 17.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3123/5198 [17:44:04<10:09:41, 17.63s/it]                                                         {'loss': 0.8191, 'learning_rate': 7.256672718370824e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3123/5198 [17:44:04<10:09:41, 17.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3124/5198 [17:44:22<10:14:26, 17.78s/it]                                                         {'loss': 0.8477, 'learning_rate': 7.250681452534261e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3124/5198 [17:44:22<10:14:26, 17.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3125/5198 [17:44:40<10:13:29, 17.76s/it]                                                         {'loss': 0.7731, 'learning_rate': 7.2446912540778196e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3125/5198 [17:44:40<10:13:29, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3126/5198 [17:46:06<22:04:07, 38.34s/it]                                                         {'loss': 0.8226, 'learning_rate': 7.238702125327106e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3126/5198 [17:46:06<22:04:07, 38.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3127/5198 [17:46:24<18:31:05, 32.19s/it]                                                         {'loss': 0.3324, 'learning_rate': 7.232714068607296e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3127/5198 [17:46:24<18:31:05, 32.19s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3128/5198 [17:46:41<15:59:58, 27.83s/it]                                                         {'loss': 0.8015, 'learning_rate': 7.226727086243168e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3128/5198 [17:46:41<15:59:58, 27.83s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3129/5198 [17:46:59<14:09:09, 24.62s/it]                                                         {'loss': 0.8384, 'learning_rate': 7.220741180559074e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3129/5198 [17:46:59<14:09:09, 24.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3130/5198 [17:47:17<13:00:46, 22.65s/it]                                                         {'loss': 0.7693, 'learning_rate': 7.214756353878942e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3130/5198 [17:47:17<13:00:46, 22.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3131/5198 [17:47:35<12:11:03, 21.22s/it]                                                         {'loss': 0.8141, 'learning_rate': 7.208772608526293e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3131/5198 [17:47:35<12:11:03, 21.22s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3132/5198 [17:47:52<11:31:20, 20.08s/it]                                                         {'loss': 0.8353, 'learning_rate': 7.202789946824227e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3132/5198 [17:47:52<11:31:20, 20.08s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3133/5198 [17:48:09<11:04:23, 19.30s/it]                                                         {'loss': 0.7977, 'learning_rate': 7.1968083710954075e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3133/5198 [17:48:09<11:04:23, 19.30s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3134/5198 [17:48:28<10:52:15, 18.96s/it]                                                         {'loss': 0.8279, 'learning_rate': 7.1908278836621e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3134/5198 [17:48:28<10:52:15, 18.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3135/5198 [17:48:45<10:39:54, 18.61s/it]                                                         {'loss': 0.875, 'learning_rate': 7.184848486846128e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3135/5198 [17:48:45<10:39:54, 18.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3136/5198 [17:49:03<10:30:42, 18.35s/it]                                                         {'loss': 0.8322, 'learning_rate': 7.178870182968904e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3136/5198 [17:49:03<10:30:42, 18.35s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3137/5198 [17:49:20<10:13:38, 17.86s/it]                                                         {'loss': 0.8419, 'learning_rate': 7.1728929743514065e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3137/5198 [17:49:20<10:13:38, 17.86s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3138/5198 [17:49:38<10:16:15, 17.95s/it]                                                         {'loss': 0.8428, 'learning_rate': 7.166916863314199e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3138/5198 [17:49:38<10:16:15, 17.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3139/5198 [17:49:55<10:09:19, 17.76s/it]                                                         {'loss': 0.7689, 'learning_rate': 7.1609418521774095e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3139/5198 [17:49:55<10:09:19, 17.76s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3140/5198 [17:50:12<10:00:03, 17.49s/it]                                                         {'loss': 0.7803, 'learning_rate': 7.154967943260748e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3140/5198 [17:50:12<10:00:03, 17.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3141/5198 [17:50:30<9:59:34, 17.49s/it]                                                         {'loss': 0.7846, 'learning_rate': 7.148995138883483e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3141/5198 [17:50:30<9:59:34, 17.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3142/5198 [17:50:47<9:56:50, 17.42s/it]                                                        {'loss': 0.7889, 'learning_rate': 7.143023441364471e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3142/5198 [17:50:47<9:56:50, 17.42s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3143/5198 [17:51:05<9:59:24, 17.50s/it]                                                        {'loss': 0.8361, 'learning_rate': 7.13705285302213e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3143/5198 [17:51:05<9:59:24, 17.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3144/5198 [17:51:23<10:05:55, 17.70s/it]                                                         {'loss': 0.8183, 'learning_rate': 7.131083376174441e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3144/5198 [17:51:23<10:05:55, 17.70s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3145/5198 [17:51:41<10:07:40, 17.76s/it]                                                         {'loss': 0.8006, 'learning_rate': 7.125115013138966e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3145/5198 [17:51:41<10:07:40, 17.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3146/5198 [17:51:59<10:08:04, 17.78s/it]                                                         {'loss': 0.8209, 'learning_rate': 7.119147766232832e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3146/5198 [17:51:59<10:08:04, 17.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3147/5198 [17:52:16<10:06:28, 17.74s/it]                                                         {'loss': 0.8559, 'learning_rate': 7.113181637772721e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3147/5198 [17:52:16<10:06:28, 17.74s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3148/5198 [17:52:34<10:11:46, 17.91s/it]                                                         {'loss': 0.7245, 'learning_rate': 7.107216630074895e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3148/5198 [17:52:34<10:11:46, 17.91s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3149/5198 [17:52:52<10:05:01, 17.72s/it]                                                         {'loss': 0.8676, 'learning_rate': 7.1012527454551795e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3149/5198 [17:52:52<10:05:01, 17.72s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3150/5198 [17:53:09<10:00:33, 17.59s/it]                                                         {'loss': 0.8078, 'learning_rate': 7.09528998622895e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3150/5198 [17:53:09<10:00:33, 17.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3151/5198 [17:54:35<21:44:35, 38.24s/it]                                                         {'loss': 0.8488, 'learning_rate': 7.089328354711159e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3151/5198 [17:54:35<21:44:35, 38.24s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3152/5198 [17:54:53<18:07:32, 31.89s/it]                                                         {'loss': 0.8404, 'learning_rate': 7.083367853216323e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3152/5198 [17:54:53<18:07:32, 31.89s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3153/5198 [17:55:10<15:40:55, 27.61s/it]                                                         {'loss': 0.8364, 'learning_rate': 7.077408484058505e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3153/5198 [17:55:10<15:40:55, 27.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3154/5198 [17:55:27<13:53:41, 24.47s/it]                                                         {'loss': 0.8077, 'learning_rate': 7.071450249551342e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3154/5198 [17:55:27<13:53:41, 24.47s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3155/5198 [17:55:45<12:42:39, 22.40s/it]                                                         {'loss': 0.799, 'learning_rate': 7.065493152008026e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3155/5198 [17:55:45<12:42:39, 22.40s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3156/5198 [17:56:03<11:54:57, 21.01s/it]                                                         {'loss': 0.8264, 'learning_rate': 7.059537193741306e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3156/5198 [17:56:03<11:54:57, 21.01s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3157/5198 [17:56:20<11:15:40, 19.86s/it]                                                         {'loss': 0.8173, 'learning_rate': 7.053582377063489e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3157/5198 [17:56:20<11:15:40, 19.86s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3158/5198 [17:56:38<10:53:30, 19.22s/it]                                                         {'loss': 0.8257, 'learning_rate': 7.047628704286446e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3158/5198 [17:56:38<10:53:30, 19.22s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3159/5198 [17:56:56<10:40:42, 18.85s/it]                                                         {'loss': 0.8611, 'learning_rate': 7.041676177721588e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3159/5198 [17:56:56<10:40:42, 18.85s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3160/5198 [17:57:13<10:27:42, 18.48s/it]                                                         {'loss': 0.7802, 'learning_rate': 7.035724799679898e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3160/5198 [17:57:13<10:27:42, 18.48s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3161/5198 [17:57:31<10:21:02, 18.29s/it]                                                         {'loss': 0.79, 'learning_rate': 7.029774572471904e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3161/5198 [17:57:31<10:21:02, 18.29s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3162/5198 [17:57:49<10:13:17, 18.07s/it]                                                         {'loss': 0.8514, 'learning_rate': 7.023825498407689e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3162/5198 [17:57:49<10:13:17, 18.07s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3163/5198 [17:58:06<10:04:35, 17.83s/it]                                                         {'loss': 0.7805, 'learning_rate': 7.0178775797968855e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3163/5198 [17:58:06<10:04:35, 17.83s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3164/5198 [17:58:24<10:02:37, 17.78s/it]                                                         {'loss': 0.341, 'learning_rate': 7.011930818948688e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3164/5198 [17:58:24<10:02:37, 17.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3165/5198 [17:58:42<10:13:42, 18.11s/it]                                                         {'loss': 0.8055, 'learning_rate': 7.005985218171825e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3165/5198 [17:58:42<10:13:42, 18.11s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3166/5198 [17:59:01<10:16:06, 18.19s/it]                                                         {'loss': 0.8506, 'learning_rate': 7.000040779774591e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3166/5198 [17:59:01<10:16:06, 18.19s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3167/5198 [17:59:19<10:17:27, 18.24s/it]                                                         {'loss': 0.834, 'learning_rate': 6.994097506064812e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3167/5198 [17:59:19<10:17:27, 18.24s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3168/5198 [17:59:36<10:05:23, 17.89s/it]                                                         {'loss': 0.3493, 'learning_rate': 6.9881553993498805e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3168/5198 [17:59:36<10:05:23, 17.89s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3169/5198 [17:59:54<10:05:08, 17.89s/it]                                                         {'loss': 0.8112, 'learning_rate': 6.9822144619367275e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3169/5198 [17:59:54<10:05:08, 17.89s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3170/5198 [18:00:11<9:56:13, 17.64s/it]                                                         {'loss': 0.8152, 'learning_rate': 6.97627469613182e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3170/5198 [18:00:11<9:56:13, 17.64s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3171/5198 [18:00:29<9:54:20, 17.59s/it]                                                        {'loss': 0.7845, 'learning_rate': 6.970336104241186e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3171/5198 [18:00:29<9:54:20, 17.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3172/5198 [18:00:46<9:52:04, 17.53s/it]                                                        {'loss': 0.8514, 'learning_rate': 6.9643986885703955e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3172/5198 [18:00:46<9:52:04, 17.53s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3173/5198 [18:01:03<9:46:09, 17.37s/it]                                                        {'loss': 0.3411, 'learning_rate': 6.958462451424547e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3173/5198 [18:01:03<9:46:09, 17.37s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3174/5198 [18:01:21<9:51:40, 17.54s/it]                                                        {'loss': 0.861, 'learning_rate': 6.952527395108302e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3174/5198 [18:01:21<9:51:40, 17.54s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3175/5198 [18:01:39<9:55:04, 17.65s/it]                                                        {'loss': 0.7436, 'learning_rate': 6.9465935219258504e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3175/5198 [18:01:39<9:55:04, 17.65s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3176/5198 [18:03:03<21:07:38, 37.62s/it]                                                         {'loss': 0.8411, 'learning_rate': 6.9406608341809215e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3176/5198 [18:03:03<21:07:38, 37.62s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3177/5198 [18:03:22<17:54:23, 31.90s/it]                                                         {'loss': 0.813, 'learning_rate': 6.934729334176793e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3177/5198 [18:03:22<17:54:23, 31.90s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3178/5198 [18:03:39<15:30:01, 27.62s/it]                                                         {'loss': 0.7346, 'learning_rate': 6.928799024216282e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3178/5198 [18:03:39<15:30:01, 27.62s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3179/5198 [18:03:57<13:47:33, 24.59s/it]                                                         {'loss': 0.7924, 'learning_rate': 6.92286990660173e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3179/5198 [18:03:57<13:47:33, 24.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3180/5198 [18:04:15<12:39:21, 22.58s/it]                                                         {'loss': 0.7483, 'learning_rate': 6.91694198363503e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3180/5198 [18:04:15<12:39:21, 22.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3181/5198 [18:04:32<11:47:15, 21.04s/it]                                                         {'loss': 0.8106, 'learning_rate': 6.911015257617606e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3181/5198 [18:04:32<11:47:15, 21.04s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3182/5198 [18:04:50<11:10:11, 19.95s/it]                                                         {'loss': 0.7761, 'learning_rate': 6.905089730850416e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3182/5198 [18:04:50<11:10:11, 19.95s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3183/5198 [18:05:07<10:47:32, 19.28s/it]                                                         {'loss': 0.8134, 'learning_rate': 6.8991654056339505e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3183/5198 [18:05:07<10:47:32, 19.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3184/5198 [18:05:24<10:23:08, 18.56s/it]                                                         {'loss': 0.84, 'learning_rate': 6.893242284268244e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3184/5198 [18:05:24<10:23:08, 18.56s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3185/5198 [18:05:42<10:16:57, 18.39s/it]                                                         {'loss': 0.8448, 'learning_rate': 6.887320369052848e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3185/5198 [18:05:42<10:16:57, 18.39s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3186/5198 [18:06:00<10:13:14, 18.29s/it]                                                         {'loss': 0.8242, 'learning_rate': 6.8813996622868584e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3186/5198 [18:06:00<10:13:14, 18.29s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3187/5198 [18:06:18<10:09:21, 18.18s/it]                                                         {'loss': 0.7835, 'learning_rate': 6.8754801662688964e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3187/5198 [18:06:18<10:09:21, 18.18s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3188/5198 [18:06:36<10:10:06, 18.21s/it]                                                         {'loss': 0.8077, 'learning_rate': 6.869561883297116e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3188/5198 [18:06:36<10:10:06, 18.21s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3189/5198 [18:06:54<10:01:15, 17.96s/it]                                                         {'loss': 0.8333, 'learning_rate': 6.863644815669197e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3189/5198 [18:06:54<10:01:15, 17.96s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3190/5198 [18:07:10<9:47:41, 17.56s/it]                                                         {'loss': 0.8558, 'learning_rate': 6.857728965682344e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3190/5198 [18:07:10<9:47:41, 17.56s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3191/5198 [18:07:29<9:54:07, 17.76s/it]                                                        {'loss': 0.7961, 'learning_rate': 6.851814335633298e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3191/5198 [18:07:29<9:54:07, 17.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3192/5198 [18:07:47<9:55:48, 17.82s/it]                                                        {'loss': 0.806, 'learning_rate': 6.8459009278183275e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3192/5198 [18:07:47<9:55:48, 17.82s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3193/5198 [18:08:04<9:48:24, 17.61s/it]                                                        {'loss': 0.3237, 'learning_rate': 6.839988744533211e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3193/5198 [18:08:04<9:48:24, 17.61s/it]WARNING: tokenization mismatch: 1 vs. 1419. (ignored)
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3194/5198 [18:08:22<9:52:40, 17.74s/it]                                                        {'loss': 0.8285, 'learning_rate': 6.834077788073268e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3194/5198 [18:08:22<9:52:40, 17.74s/it]WARNING: tokenization mismatch: 1 vs. 737. (ignored)
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3195/5198 [18:08:39<9:51:04, 17.71s/it]                                                        {'loss': 0.8653, 'learning_rate': 6.8281680607333364e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3195/5198 [18:08:39<9:51:04, 17.71s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3196/5198 [18:08:56<9:42:08, 17.45s/it]                                                        {'loss': 0.8379, 'learning_rate': 6.822259564807768e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3196/5198 [18:08:56<9:42:08, 17.45s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3197/5198 [18:09:15<9:52:55, 17.78s/it]                                                        {'loss': 0.8204, 'learning_rate': 6.81635230259045e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3197/5198 [18:09:15<9:52:55, 17.78s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3198/5198 [18:09:33<9:59:39, 17.99s/it]                                                        {'loss': 0.8436, 'learning_rate': 6.810446276374789e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3198/5198 [18:09:33<9:59:39, 17.99s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3199/5198 [18:09:50<9:49:21, 17.69s/it]                                                        {'loss': 0.3587, 'learning_rate': 6.8045414884536975e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3199/5198 [18:09:50<9:49:21, 17.69s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3200/5198 [18:10:08<9:45:47, 17.59s/it]                                                        {'loss': 0.8223, 'learning_rate': 6.7986379411196255e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3200/5198 [18:10:08<9:45:47, 17.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3201/5198 [18:11:36<21:36:18, 38.95s/it]                                                         {'loss': 0.812, 'learning_rate': 6.7927356366645315e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3201/5198 [18:11:36<21:36:18, 38.95s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3202/5198 [18:11:55<18:08:07, 32.71s/it]                                                         {'loss': 0.8314, 'learning_rate': 6.786834577379893e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3202/5198 [18:11:55<18:08:07, 32.71s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3203/5198 [18:12:12<15:40:30, 28.29s/it]                                                         {'loss': 0.8278, 'learning_rate': 6.780934765556702e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3203/5198 [18:12:12<15:40:30, 28.29s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3204/5198 [18:12:31<13:58:25, 25.23s/it]                                                         {'loss': 0.8054, 'learning_rate': 6.775036203485472e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3204/5198 [18:12:31<13:58:25, 25.23s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3205/5198 [18:12:48<12:39:07, 22.85s/it]                                                         {'loss': 0.8186, 'learning_rate': 6.769138893456225e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3205/5198 [18:12:48<12:39:07, 22.85s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3206/5198 [18:13:06<11:47:08, 21.30s/it]                                                         {'loss': 0.7934, 'learning_rate': 6.763242837758504e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3206/5198 [18:13:06<11:47:08, 21.30s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3207/5198 [18:13:23<11:13:00, 20.28s/it]                                                         {'loss': 0.8378, 'learning_rate': 6.757348038681357e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3207/5198 [18:13:23<11:13:00, 20.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3208/5198 [18:13:42<10:51:20, 19.64s/it]                                                         {'loss': 0.7371, 'learning_rate': 6.751454498513349e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3208/5198 [18:13:42<10:51:20, 19.64s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3209/5198 [18:14:00<10:38:43, 19.27s/it]                                                         {'loss': 0.7936, 'learning_rate': 6.745562219542554e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3209/5198 [18:14:00<10:38:43, 19.27s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3210/5198 [18:14:18<10:27:37, 18.94s/it]                                                         {'loss': 0.77, 'learning_rate': 6.7396712040565625e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3210/5198 [18:14:18<10:27:37, 18.94s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3211/5198 [18:14:36<10:19:35, 18.71s/it]                                                         {'loss': 0.795, 'learning_rate': 6.733781454342463e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3211/5198 [18:14:36<10:19:35, 18.71s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3212/5198 [18:14:54<10:12:04, 18.49s/it]                                                         {'loss': 0.7906, 'learning_rate': 6.727892972686861e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3212/5198 [18:14:54<10:12:04, 18.49s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3213/5198 [18:15:11<9:51:34, 17.88s/it]                                                         {'loss': 0.8024, 'learning_rate': 6.722005761375873e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3213/5198 [18:15:11<9:51:34, 17.88s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3214/5198 [18:15:28<9:49:21, 17.82s/it]                                                        {'loss': 0.8542, 'learning_rate': 6.716119822695111e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3214/5198 [18:15:28<9:49:21, 17.82s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3215/5198 [18:15:45<9:39:37, 17.54s/it]                                                        {'loss': 0.8313, 'learning_rate': 6.710235158929703e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3215/5198 [18:15:45<9:39:37, 17.54s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3216/5198 [18:16:02<9:34:41, 17.40s/it]                                                        {'loss': 0.8316, 'learning_rate': 6.704351772364274e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3216/5198 [18:16:02<9:34:41, 17.40s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3217/5198 [18:16:20<9:32:52, 17.35s/it]                                                        {'loss': 0.7635, 'learning_rate': 6.698469665282958e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3217/5198 [18:16:20<9:32:52, 17.35s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3218/5198 [18:16:38<9:41:20, 17.62s/it]                                                        {'loss': 0.8096, 'learning_rate': 6.692588839969397e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3218/5198 [18:16:38<9:41:20, 17.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3219/5198 [18:16:57<9:53:49, 18.00s/it]                                                        {'loss': 0.7892, 'learning_rate': 6.6867092987067214e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3219/5198 [18:16:57<9:53:49, 18.00s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3220/5198 [18:17:14<9:49:00, 17.87s/it]                                                        {'loss': 0.8019, 'learning_rate': 6.680831043777579e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3220/5198 [18:17:14<9:49:00, 17.87s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3221/5198 [18:17:32<9:46:02, 17.79s/it]                                                        {'loss': 0.8008, 'learning_rate': 6.674954077464108e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3221/5198 [18:17:32<9:46:02, 17.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3222/5198 [18:17:50<9:44:50, 17.76s/it]                                                        {'loss': 0.7459, 'learning_rate': 6.6690784020479484e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3222/5198 [18:17:50<9:44:50, 17.76s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3223/5198 [18:18:07<9:38:43, 17.58s/it]                                                        {'loss': 0.7835, 'learning_rate': 6.6632040198102364e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3223/5198 [18:18:07<9:38:43, 17.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3224/5198 [18:18:25<9:39:46, 17.62s/it]                                                        {'loss': 0.8162, 'learning_rate': 6.657330933031619e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3224/5198 [18:18:25<9:39:46, 17.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3225/5198 [18:18:42<9:35:56, 17.51s/it]                                                        {'loss': 0.3346, 'learning_rate': 6.651459143992221e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3225/5198 [18:18:42<9:35:56, 17.51s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3226/5198 [18:20:08<20:50:47, 38.06s/it]                                                         {'loss': 0.8177, 'learning_rate': 6.645588654971677e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3226/5198 [18:20:08<20:50:47, 38.06s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3227/5198 [18:20:25<17:24:26, 31.79s/it]                                                         {'loss': 0.831, 'learning_rate': 6.639719468249115e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3227/5198 [18:20:25<17:24:26, 31.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3228/5198 [18:20:43<15:05:40, 27.58s/it]                                                         {'loss': 0.8254, 'learning_rate': 6.633851586103153e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3228/5198 [18:20:43<15:05:40, 27.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3229/5198 [18:21:01<13:34:12, 24.81s/it]                                                         {'loss': 0.8051, 'learning_rate': 6.627985010811903e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3229/5198 [18:21:01<13:34:12, 24.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3230/5198 [18:21:18<12:18:58, 22.53s/it]                                                         {'loss': 0.787, 'learning_rate': 6.622119744652977e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3230/5198 [18:21:18<12:18:58, 22.53s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3231/5198 [18:21:36<11:30:21, 21.06s/it]                                                         {'loss': 0.8176, 'learning_rate': 6.616255789903467e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3231/5198 [18:21:36<11:30:21, 21.06s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3232/5198 [18:21:53<10:53:20, 19.94s/it]                                                         {'loss': 0.8008, 'learning_rate': 6.610393148839964e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3232/5198 [18:21:53<10:53:20, 19.94s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3233/5198 [18:22:10<10:23:18, 19.03s/it]                                                         {'loss': 0.8703, 'learning_rate': 6.6045318237385526e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3233/5198 [18:22:10<10:23:18, 19.03s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3234/5198 [18:22:27<10:02:04, 18.39s/it]                                                         {'loss': 0.8782, 'learning_rate': 6.598671816874794e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3234/5198 [18:22:27<10:02:04, 18.39s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3235/5198 [18:22:44<9:49:18, 18.01s/it]                                                         {'loss': 0.8213, 'learning_rate': 6.5928131305237465e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3235/5198 [18:22:44<9:49:18, 18.01s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3236/5198 [18:23:01<9:41:44, 17.79s/it]                                                        {'loss': 0.8461, 'learning_rate': 6.586955766959958e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3236/5198 [18:23:01<9:41:44, 17.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3237/5198 [18:23:19<9:36:52, 17.65s/it]                                                        {'loss': 0.8398, 'learning_rate': 6.581099728457451e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3237/5198 [18:23:19<9:36:52, 17.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3238/5198 [18:23:37<9:38:18, 17.70s/it]                                                        {'loss': 0.7847, 'learning_rate': 6.5752450172897466e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3238/5198 [18:23:37<9:38:18, 17.70s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3239/5198 [18:23:54<9:34:36, 17.60s/it]                                                        {'loss': 0.8073, 'learning_rate': 6.569391635729847e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3239/5198 [18:23:54<9:34:36, 17.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3240/5198 [18:24:12<9:37:40, 17.70s/it]                                                        {'loss': 0.8228, 'learning_rate': 6.563539586050233e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3240/5198 [18:24:12<9:37:40, 17.70s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3241/5198 [18:24:30<9:38:29, 17.74s/it]                                                        {'loss': 0.8262, 'learning_rate': 6.557688870522871e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3241/5198 [18:24:30<9:38:29, 17.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3242/5198 [18:24:47<9:35:29, 17.65s/it]                                                        {'loss': 0.8117, 'learning_rate': 6.551839491419213e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3242/5198 [18:24:47<9:35:29, 17.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3243/5198 [18:25:05<9:37:55, 17.74s/it]                                                        {'loss': 0.8084, 'learning_rate': 6.545991451010185e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3243/5198 [18:25:05<9:37:55, 17.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3244/5198 [18:25:22<9:29:55, 17.50s/it]                                                        {'loss': 0.8276, 'learning_rate': 6.5401447515662065e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3244/5198 [18:25:22<9:29:55, 17.50s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3245/5198 [18:25:39<9:27:37, 17.44s/it]                                                        {'loss': 0.853, 'learning_rate': 6.5342993953571556e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3245/5198 [18:25:39<9:27:37, 17.44s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3246/5198 [18:25:57<9:32:13, 17.59s/it]                                                        {'loss': 0.7737, 'learning_rate': 6.52845538465241e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3246/5198 [18:25:57<9:32:13, 17.59s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3247/5198 [18:26:16<9:42:00, 17.90s/it]                                                        {'loss': 0.7808, 'learning_rate': 6.522612721720813e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3247/5198 [18:26:16<9:42:00, 17.90s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3248/5198 [18:26:33<9:37:27, 17.77s/it]                                                        {'loss': 0.8427, 'learning_rate': 6.5167714088306865e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3248/5198 [18:26:33<9:37:27, 17.77s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3249/5198 [18:26:51<9:37:30, 17.78s/it]                                                        {'loss': 0.7987, 'learning_rate': 6.51093144824983e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3249/5198 [18:26:51<9:37:30, 17.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3250/5198 [18:27:10<9:44:24, 18.00s/it]                                                        {'loss': 0.7523, 'learning_rate': 6.505092842245519e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3250/5198 [18:27:10<9:44:24, 18.00s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3251/5198 [18:28:42<21:44:53, 40.21s/it]                                                         {'loss': 0.8131, 'learning_rate': 6.499255593084498e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3251/5198 [18:28:42<21:44:53, 40.21s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3252/5198 [18:28:59<17:58:04, 33.24s/it]                                                         {'loss': 0.8524, 'learning_rate': 6.493419703032991e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3252/5198 [18:28:59<17:58:04, 33.24s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3253/5198 [18:29:16<15:24:52, 28.53s/it]                                                         {'loss': 0.3141, 'learning_rate': 6.487585174356691e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3253/5198 [18:29:16<15:24:52, 28.53s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3254/5198 [18:29:33<13:33:02, 25.09s/it]                                                         {'loss': 0.8527, 'learning_rate': 6.481752009320761e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3254/5198 [18:29:33<13:33:02, 25.09s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3255/5198 [18:29:52<12:28:33, 23.12s/it]                                                         {'loss': 0.7787, 'learning_rate': 6.4759202101898366e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3255/5198 [18:29:52<12:28:33, 23.12s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3256/5198 [18:30:09<11:27:26, 21.24s/it]                                                         {'loss': 0.8222, 'learning_rate': 6.4700897792280285e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3256/5198 [18:30:09<11:27:26, 21.24s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3257/5198 [18:30:27<11:01:32, 20.45s/it]                                                         {'loss': 0.777, 'learning_rate': 6.464260718698902e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3257/5198 [18:30:27<11:01:32, 20.45s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3258/5198 [18:30:45<10:39:13, 19.77s/it]                                                         {'loss': 0.8092, 'learning_rate': 6.458433030865503e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3258/5198 [18:30:45<10:39:13, 19.77s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3259/5198 [18:31:03<10:19:37, 19.17s/it]                                                         {'loss': 0.8356, 'learning_rate': 6.452606717990346e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3259/5198 [18:31:03<10:19:37, 19.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3260/5198 [18:31:21<10:00:56, 18.61s/it]                                                         {'loss': 0.8256, 'learning_rate': 6.4467817823354005e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3260/5198 [18:31:21<10:00:56, 18.61s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3261/5198 [18:31:39<10:00:04, 18.59s/it]                                                         {'loss': 0.8368, 'learning_rate': 6.440958226162104e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3261/5198 [18:31:39<10:00:04, 18.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3262/5198 [18:31:57<9:57:00, 18.50s/it]                                                         {'loss': 0.8086, 'learning_rate': 6.43513605173137e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3262/5198 [18:31:57<9:57:00, 18.50s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3263/5198 [18:32:16<9:55:34, 18.47s/it]                                                        {'loss': 0.8706, 'learning_rate': 6.4293152613035594e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3263/5198 [18:32:16<9:55:34, 18.47s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3264/5198 [18:32:35<10:00:56, 18.64s/it]                                                         {'loss': 0.7184, 'learning_rate': 6.4234958571385095e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3264/5198 [18:32:35<10:00:56, 18.64s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3265/5198 [18:32:52<9:44:42, 18.15s/it]                                                         {'loss': 0.8757, 'learning_rate': 6.4176778414955075e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3265/5198 [18:32:52<9:44:42, 18.15s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3266/5198 [18:33:09<9:37:00, 17.92s/it]                                                        {'loss': 0.7961, 'learning_rate': 6.4118612166333124e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3266/5198 [18:33:09<9:37:00, 17.92s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3267/5198 [18:33:28<9:42:31, 18.10s/it]                                                        {'loss': 0.7832, 'learning_rate': 6.4060459848101354e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3267/5198 [18:33:28<9:42:31, 18.10s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3268/5198 [18:33:46<9:39:35, 18.02s/it]                                                        {'loss': 0.3694, 'learning_rate': 6.400232148283651e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3268/5198 [18:33:46<9:39:35, 18.02s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3269/5198 [18:34:03<9:36:48, 17.94s/it]                                                        {'loss': 0.811, 'learning_rate': 6.3944197093109885e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3269/5198 [18:34:03<9:36:48, 17.94s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3270/5198 [18:34:21<9:37:26, 17.97s/it]                                                        {'loss': 0.7798, 'learning_rate': 6.388608670148741e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3270/5198 [18:34:21<9:37:26, 17.97s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3271/5198 [18:34:39<9:35:49, 17.93s/it]                                                        {'loss': 0.8122, 'learning_rate': 6.38279903305295e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3271/5198 [18:34:39<9:35:49, 17.93s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3272/5198 [18:34:57<9:36:48, 17.97s/it]                                                        {'loss': 0.8409, 'learning_rate': 6.376990800279119e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3272/5198 [18:34:57<9:36:48, 17.97s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3273/5198 [18:35:15<9:33:29, 17.87s/it]                                                        {'loss': 0.7832, 'learning_rate': 6.3711839740822035e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3273/5198 [18:35:15<9:33:29, 17.87s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3274/5198 [18:35:34<9:44:24, 18.22s/it]                                                        {'loss': 0.7963, 'learning_rate': 6.3653785567166125e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3274/5198 [18:35:34<9:44:24, 18.22s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3275/5198 [18:35:52<9:43:53, 18.22s/it]                                                        {'loss': 0.8088, 'learning_rate': 6.359574550436209e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3275/5198 [18:35:52<9:43:53, 18.22s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3276/5198 [18:37:21<21:03:01, 39.43s/it]                                                         {'loss': 0.8304, 'learning_rate': 6.3537719574943105e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3276/5198 [18:37:21<21:03:01, 39.43s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3277/5198 [18:37:38<17:30:52, 32.82s/it]                                                         {'loss': 0.8266, 'learning_rate': 6.347970780143678e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3277/5198 [18:37:38<17:30:52, 32.82s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3278/5198 [18:37:57<15:10:19, 28.45s/it]                                                         {'loss': 0.7852, 'learning_rate': 6.342171020636533e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3278/5198 [18:37:57<15:10:19, 28.45s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3279/5198 [18:38:15<13:28:04, 25.27s/it]                                                         {'loss': 0.8926, 'learning_rate': 6.336372681224543e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3279/5198 [18:38:15<13:28:04, 25.27s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3280/5198 [18:38:33<12:20:57, 23.18s/it]                                                         {'loss': 0.7595, 'learning_rate': 6.330575764158819e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3280/5198 [18:38:33<12:20:57, 23.18s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3281/5198 [18:38:50<11:21:37, 21.33s/it]                                                         {'loss': 0.8355, 'learning_rate': 6.324780271689923e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3281/5198 [18:38:50<11:21:37, 21.33s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3282/5198 [18:39:08<10:46:26, 20.24s/it]                                                         {'loss': 0.8649, 'learning_rate': 6.318986206067872e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3282/5198 [18:39:08<10:46:26, 20.24s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3283/5198 [18:39:25<10:15:52, 19.30s/it]                                                         {'loss': 0.7873, 'learning_rate': 6.313193569542113e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3283/5198 [18:39:25<10:15:52, 19.30s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3284/5198 [18:39:42<9:57:56, 18.74s/it]                                                         {'loss': 0.8346, 'learning_rate': 6.30740236436155e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3284/5198 [18:39:42<9:57:56, 18.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3285/5198 [18:39:59<9:39:59, 18.19s/it]                                                        {'loss': 0.8508, 'learning_rate': 6.301612592774533e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3285/5198 [18:39:59<9:39:59, 18.19s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3286/5198 [18:40:17<9:39:08, 18.17s/it]                                                        {'loss': 0.7798, 'learning_rate': 6.295824257028844e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3286/5198 [18:40:17<9:39:08, 18.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3287/5198 [18:40:34<9:28:35, 17.85s/it]                                                        {'loss': 0.7645, 'learning_rate': 6.290037359371717e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3287/5198 [18:40:34<9:28:35, 17.85s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3288/5198 [18:40:52<9:26:15, 17.79s/it]                                                        {'loss': 0.8058, 'learning_rate': 6.284251902049827e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3288/5198 [18:40:52<9:26:15, 17.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3289/5198 [18:41:09<9:20:42, 17.62s/it]                                                        {'loss': 0.8207, 'learning_rate': 6.278467887309283e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3289/5198 [18:41:09<9:20:42, 17.62s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3290/5198 [18:41:26<9:15:41, 17.47s/it]                                                        {'loss': 0.8198, 'learning_rate': 6.272685317395644e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3290/5198 [18:41:26<9:15:41, 17.47s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3291/5198 [18:41:44<9:14:59, 17.46s/it]                                                        {'loss': 0.8413, 'learning_rate': 6.266904194553896e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3291/5198 [18:41:44<9:14:59, 17.46s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3292/5198 [18:42:01<9:14:50, 17.47s/it]                                                        {'loss': 0.7378, 'learning_rate': 6.261124521028477e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3292/5198 [18:42:01<9:14:50, 17.47s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3293/5198 [18:42:18<9:09:25, 17.30s/it]                                                        {'loss': 0.7989, 'learning_rate': 6.255346299063252e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3293/5198 [18:42:18<9:09:25, 17.30s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3294/5198 [18:42:37<9:22:57, 17.74s/it]                                                        {'loss': 0.8191, 'learning_rate': 6.249569530901525e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3294/5198 [18:42:37<9:22:57, 17.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3295/5198 [18:42:54<9:19:53, 17.65s/it]                                                        {'loss': 0.8291, 'learning_rate': 6.243794218786034e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3295/5198 [18:42:54<9:19:53, 17.65s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3296/5198 [18:43:13<9:33:18, 18.09s/it]                                                        {'loss': 0.8211, 'learning_rate': 6.238020364958964e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3296/5198 [18:43:13<9:33:18, 18.09s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3297/5198 [18:43:32<9:35:30, 18.16s/it]                                                        {'loss': 0.7827, 'learning_rate': 6.232247971661912e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3297/5198 [18:43:32<9:35:30, 18.16s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3298/5198 [18:43:48<9:21:21, 17.73s/it]                                                        {'loss': 0.841, 'learning_rate': 6.2264770411359256e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3298/5198 [18:43:48<9:21:21, 17.73s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3299/5198 [18:44:06<9:19:58, 17.69s/it]                                                        {'loss': 0.8393, 'learning_rate': 6.22070757562148e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3299/5198 [18:44:06<9:19:58, 17.69s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3300/5198 [18:44:23<9:17:00, 17.61s/it]                                                        {'loss': 0.8413, 'learning_rate': 6.214939577358479e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3300/5198 [18:44:24<9:17:00, 17.61s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3301/5198 [18:45:53<20:42:20, 39.29s/it]                                                         {'loss': 0.7948, 'learning_rate': 6.209173048586253e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3301/5198 [18:45:53<20:42:20, 39.29s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3302/5198 [18:46:12<17:21:54, 32.97s/it]                                                         {'loss': 0.8279, 'learning_rate': 6.203407991543577e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3302/5198 [18:46:12<17:21:54, 32.97s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3303/5198 [18:46:29<14:58:08, 28.44s/it]                                                         {'loss': 0.8157, 'learning_rate': 6.197644408468635e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3303/5198 [18:46:29<14:58:08, 28.44s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3304/5198 [18:46:48<13:20:55, 25.37s/it]                                                         {'loss': 0.7963, 'learning_rate': 6.191882301599052e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3304/5198 [18:46:48<13:20:55, 25.37s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3305/5198 [18:47:04<11:57:50, 22.75s/it]                                                         {'loss': 0.7976, 'learning_rate': 6.186121673171882e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3305/5198 [18:47:04<11:57:50, 22.75s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3306/5198 [18:47:21<11:03:02, 21.03s/it]                                                         {'loss': 0.7767, 'learning_rate': 6.180362525423591e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3306/5198 [18:47:21<11:03:02, 21.03s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3307/5198 [18:47:38<10:23:42, 19.79s/it]                                                         {'loss': 0.7956, 'learning_rate': 6.174604860590081e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3307/5198 [18:47:38<10:23:42, 19.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3308/5198 [18:48:00<10:37:44, 20.25s/it]                                                         {'loss': 0.8305, 'learning_rate': 6.168848680906678e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3308/5198 [18:48:00<10:37:44, 20.25s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3309/5198 [18:48:18<10:23:10, 19.79s/it]                                                         {'loss': 0.8216, 'learning_rate': 6.163093988608127e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3309/5198 [18:48:18<10:23:10, 19.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3310/5198 [18:48:36<10:05:51, 19.25s/it]                                                         {'loss': 0.8359, 'learning_rate': 6.157340785928595e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3310/5198 [18:48:36<10:05:51, 19.25s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3311/5198 [18:48:54<9:51:23, 18.80s/it]                                                         {'loss': 0.8487, 'learning_rate': 6.151589075101681e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3311/5198 [18:48:54<9:51:23, 18.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3312/5198 [18:49:12<9:46:20, 18.65s/it]                                                        {'loss': 0.8094, 'learning_rate': 6.145838858360391e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3312/5198 [18:49:12<9:46:20, 18.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3313/5198 [18:49:29<9:28:19, 18.09s/it]                                                        {'loss': 0.8305, 'learning_rate': 6.140090137937158e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3313/5198 [18:49:29<9:28:19, 18.09s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3314/5198 [18:49:47<9:25:51, 18.02s/it]                                                        {'loss': 0.3417, 'learning_rate': 6.134342916063838e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3314/5198 [18:49:47<9:25:51, 18.02s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3315/5198 [18:50:04<9:12:40, 17.61s/it]                                                        {'loss': 0.8409, 'learning_rate': 6.128597194971691e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3315/5198 [18:50:04<9:12:40, 17.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3316/5198 [18:50:22<9:15:27, 17.71s/it]                                                        {'loss': 0.7949, 'learning_rate': 6.122852976891413e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3316/5198 [18:50:22<9:15:27, 17.71s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3317/5198 [18:50:39<9:09:45, 17.54s/it]                                                        {'loss': 0.7998, 'learning_rate': 6.117110264053101e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3317/5198 [18:50:39<9:09:45, 17.54s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3318/5198 [18:50:57<9:18:26, 17.82s/it]                                                        {'loss': 0.8127, 'learning_rate': 6.111369058686276e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3318/5198 [18:50:57<9:18:26, 17.82s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3319/5198 [18:51:16<9:25:22, 18.05s/it]                                                        {'loss': 0.7897, 'learning_rate': 6.105629363019875e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3319/5198 [18:51:16<9:25:22, 18.05s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3320/5198 [18:51:34<9:25:40, 18.07s/it]                                                        {'loss': 0.7767, 'learning_rate': 6.099891179282242e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3320/5198 [18:51:34<9:25:40, 18.07s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3321/5198 [18:51:51<9:20:43, 17.92s/it]                                                        {'loss': 0.8838, 'learning_rate': 6.094154509701133e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3321/5198 [18:51:51<9:20:43, 17.92s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3322/5198 [18:52:10<9:26:41, 18.12s/it]                                                        {'loss': 0.7394, 'learning_rate': 6.088419356503732e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3322/5198 [18:52:10<9:26:41, 18.12s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3323/5198 [18:52:28<9:25:17, 18.09s/it]                                                        {'loss': 0.7914, 'learning_rate': 6.082685721916612e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3323/5198 [18:52:28<9:25:17, 18.09s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3324/5198 [18:52:46<9:27:58, 18.18s/it]                                                        {'loss': 0.8274, 'learning_rate': 6.076953608165772e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3324/5198 [18:52:46<9:27:58, 18.18s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3325/5198 [18:53:04<9:24:46, 18.09s/it]                                                        {'loss': 0.8324, 'learning_rate': 6.07122301747662e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3325/5198 [18:53:04<9:24:46, 18.09s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3326/5198 [18:54:37<21:04:33, 40.53s/it]                                                         {'loss': 0.7705, 'learning_rate': 6.065493952073961e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3326/5198 [18:54:37<21:04:33, 40.53s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3327/5198 [18:54:55<17:29:59, 33.67s/it]                                                         {'loss': 0.825, 'learning_rate': 6.0597664141820176e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3327/5198 [18:54:55<17:29:59, 33.67s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3328/5198 [18:55:12<14:57:17, 28.79s/it]                                                         {'loss': 0.8816, 'learning_rate': 6.054040406024422e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3328/5198 [18:55:12<14:57:17, 28.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3329/5198 [18:55:30<13:15:06, 25.53s/it]                                                         {'loss': 0.7717, 'learning_rate': 6.0483159298242e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3329/5198 [18:55:30<13:15:06, 25.53s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3330/5198 [18:55:48<12:06:46, 23.34s/it]                                                         {'loss': 0.8688, 'learning_rate': 6.042592987803796e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3330/5198 [18:55:48<12:06:46, 23.34s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3331/5198 [18:56:06<11:09:49, 21.53s/it]                                                         {'loss': 0.7422, 'learning_rate': 6.036871582185054e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3331/5198 [18:56:06<11:09:49, 21.53s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3332/5198 [18:56:23<10:28:03, 20.19s/it]                                                         {'loss': 0.8061, 'learning_rate': 6.031151715189217e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3332/5198 [18:56:23<10:28:03, 20.19s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3333/5198 [18:56:42<10:14:51, 19.78s/it]                                                         {'loss': 0.7861, 'learning_rate': 6.025433389036935e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3333/5198 [18:56:42<10:14:51, 19.78s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3334/5198 [18:56:59<9:47:15, 18.90s/it]                                                         {'loss': 0.8001, 'learning_rate': 6.019716605948261e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3334/5198 [18:56:59<9:47:15, 18.90s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3335/5198 [18:57:17<9:38:39, 18.64s/it]                                                        {'loss': 0.7878, 'learning_rate': 6.014001368142643e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3335/5198 [18:57:17<9:38:39, 18.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3336/5198 [18:57:34<9:27:09, 18.28s/it]                                                        {'loss': 0.7499, 'learning_rate': 6.008287677838937e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3336/5198 [18:57:34<9:27:09, 18.28s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3337/5198 [18:57:52<9:25:32, 18.23s/it]                                                        {'loss': 0.8081, 'learning_rate': 6.002575537255395e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3337/5198 [18:57:52<9:25:32, 18.23s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3338/5198 [18:58:09<9:11:38, 17.80s/it]                                                        {'loss': 0.7486, 'learning_rate': 5.996864948609662e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3338/5198 [18:58:09<9:11:38, 17.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3339/5198 [18:58:26<9:09:05, 17.72s/it]                                                        {'loss': 0.3415, 'learning_rate': 5.9911559141187924e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3339/5198 [18:58:26<9:09:05, 17.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3340/5198 [18:58:43<9:01:56, 17.50s/it]                                                        {'loss': 0.7807, 'learning_rate': 5.9854484359992235e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3340/5198 [18:58:43<9:01:56, 17.50s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3341/5198 [18:59:02<9:11:33, 17.82s/it]                                                        {'loss': 0.7959, 'learning_rate': 5.979742516466793e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3341/5198 [18:59:02<9:11:33, 17.82s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3342/5198 [18:59:20<9:13:17, 17.89s/it]                                                        {'loss': 0.8415, 'learning_rate': 5.974038157736746e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3342/5198 [18:59:20<9:13:17, 17.89s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3343/5198 [18:59:38<9:15:47, 17.98s/it]                                                        {'loss': 0.7924, 'learning_rate': 5.968335362023697e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3343/5198 [18:59:38<9:15:47, 17.98s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3344/5198 [18:59:56<9:12:48, 17.89s/it]                                                        {'loss': 0.7945, 'learning_rate': 5.962634131541676e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3344/5198 [18:59:56<9:12:48, 17.89s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3345/5198 [19:00:15<9:22:04, 18.20s/it]                                                        {'loss': 0.7828, 'learning_rate': 5.956934468504101e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3345/5198 [19:00:15<9:22:04, 18.20s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3346/5198 [19:00:33<9:18:24, 18.09s/it]                                                        {'loss': 0.3528, 'learning_rate': 5.951236375123768e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3346/5198 [19:00:33<9:18:24, 18.09s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3347/5198 [19:00:51<9:23:35, 18.27s/it]                                                        {'loss': 0.7367, 'learning_rate': 5.945539853612876e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3347/5198 [19:00:51<9:23:35, 18.27s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3348/5198 [19:01:09<9:19:55, 18.16s/it]                                                        {'loss': 0.8168, 'learning_rate': 5.939844906183016e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3348/5198 [19:01:09<9:19:55, 18.16s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3349/5198 [19:01:27<9:17:44, 18.10s/it]                                                        {'loss': 0.8633, 'learning_rate': 5.934151535045156e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3349/5198 [19:01:27<9:17:44, 18.10s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3350/5198 [19:01:44<9:09:00, 17.82s/it]                                                        {'loss': 0.7932, 'learning_rate': 5.92845974240966e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3350/5198 [19:01:44<9:09:00, 17.82s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3351/5198 [19:03:14<20:08:12, 39.25s/it]                                                         {'loss': 0.7687, 'learning_rate': 5.922769530486283e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3351/5198 [19:03:14<20:08:12, 39.25s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3352/5198 [19:03:31<16:49:08, 32.80s/it]                                                         {'loss': 0.8216, 'learning_rate': 5.917080901484156e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3352/5198 [19:03:31<16:49:08, 32.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3353/5198 [19:03:49<14:27:11, 28.20s/it]                                                         {'loss': 0.7902, 'learning_rate': 5.9113938576118e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3353/5198 [19:03:49<14:27:11, 28.20s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3354/5198 [19:04:07<12:51:33, 25.10s/it]                                                         {'loss': 0.8079, 'learning_rate': 5.905708401077128e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3354/5198 [19:04:07<12:51:33, 25.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3355/5198 [19:04:24<11:37:27, 22.71s/it]                                                         {'loss': 0.8354, 'learning_rate': 5.900024534087421e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3355/5198 [19:04:24<11:37:27, 22.71s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3356/5198 [19:04:41<10:42:58, 20.94s/it]                                                         {'loss': 0.7794, 'learning_rate': 5.894342258849355e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3356/5198 [19:04:41<10:42:58, 20.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3357/5198 [19:04:58<10:10:47, 19.91s/it]                                                         {'loss': 0.8177, 'learning_rate': 5.88866157756899e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3357/5198 [19:04:58<10:10:47, 19.91s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3358/5198 [19:05:15<9:45:41, 19.10s/it]                                                         {'loss': 0.3338, 'learning_rate': 5.882982492451757e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3358/5198 [19:05:15<9:45:41, 19.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3359/5198 [19:05:33<9:27:50, 18.53s/it]                                                        {'loss': 0.8044, 'learning_rate': 5.877305005702471e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3359/5198 [19:05:33<9:27:50, 18.53s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3360/5198 [19:05:49<9:12:02, 18.02s/it]                                                        {'loss': 0.755, 'learning_rate': 5.871629119525335e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3360/5198 [19:05:49<9:12:02, 18.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3361/5198 [19:06:07<9:04:45, 17.79s/it]                                                        {'loss': 0.8347, 'learning_rate': 5.865954836123915e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3361/5198 [19:06:07<9:04:45, 17.79s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3362/5198 [19:06:25<9:05:46, 17.84s/it]                                                        {'loss': 0.8278, 'learning_rate': 5.860282157701167e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3362/5198 [19:06:25<9:05:46, 17.84s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3363/5198 [19:06:43<9:10:07, 17.99s/it]                                                        {'loss': 0.8194, 'learning_rate': 5.854611086459423e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3363/5198 [19:06:43<9:10:07, 17.99s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3364/5198 [19:07:00<9:03:04, 17.77s/it]                                                        {'loss': 0.8117, 'learning_rate': 5.8489416246003814e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3364/5198 [19:07:00<9:03:04, 17.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3365/5198 [19:07:18<9:02:51, 17.77s/it]                                                        {'loss': 0.3546, 'learning_rate': 5.8432737743251315e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3365/5198 [19:07:18<9:02:51, 17.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3366/5198 [19:07:35<8:52:22, 17.44s/it]                                                        {'loss': 0.8372, 'learning_rate': 5.8376075378341194e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3366/5198 [19:07:35<8:52:22, 17.44s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3367/5198 [19:07:52<8:54:03, 17.50s/it]                                                        {'loss': 0.813, 'learning_rate': 5.831942917327172e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3367/5198 [19:07:52<8:54:03, 17.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3368/5198 [19:08:10<8:53:04, 17.48s/it]                                                        {'loss': 0.8441, 'learning_rate': 5.826279915003503e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3368/5198 [19:08:10<8:53:04, 17.48s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3369/5198 [19:08:28<8:55:59, 17.58s/it]                                                        {'loss': 0.8235, 'learning_rate': 5.8206185330616725e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3369/5198 [19:08:28<8:55:59, 17.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3370/5198 [19:08:45<8:57:30, 17.64s/it]                                                        {'loss': 0.3644, 'learning_rate': 5.814958773699625e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3370/5198 [19:08:45<8:57:30, 17.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3371/5198 [19:09:04<9:04:07, 17.87s/it]                                                        {'loss': 0.8062, 'learning_rate': 5.809300639114683e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3371/5198 [19:09:04<9:04:07, 17.87s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3372/5198 [19:09:21<9:00:16, 17.75s/it]                                                        {'loss': 0.8273, 'learning_rate': 5.803644131503516e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3372/5198 [19:09:21<9:00:16, 17.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3373/5198 [19:09:38<8:55:15, 17.60s/it]                                                        {'loss': 0.8588, 'learning_rate': 5.797989253062186e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3373/5198 [19:09:38<8:55:15, 17.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3374/5198 [19:09:55<8:45:29, 17.29s/it]                                                        {'loss': 0.8627, 'learning_rate': 5.792336005986105e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3374/5198 [19:09:55<8:45:29, 17.29s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3375/5198 [19:10:13<8:54:31, 17.59s/it]                                                        {'loss': 0.8126, 'learning_rate': 5.786684392470064e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3375/5198 [19:10:13<8:54:31, 17.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3376/5198 [19:11:43<19:49:39, 39.18s/it]                                                         {'loss': 0.8349, 'learning_rate': 5.781034414708208e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3376/5198 [19:11:43<19:49:39, 39.18s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3377/5198 [19:12:00<16:29:38, 32.61s/it]                                                         {'loss': 0.3637, 'learning_rate': 5.775386074894058e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3377/5198 [19:12:00<16:29:38, 32.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3378/5198 [19:12:17<14:10:09, 28.03s/it]                                                         {'loss': 0.8061, 'learning_rate': 5.769739375220489e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3378/5198 [19:12:17<14:10:09, 28.03s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3379/5198 [19:12:36<12:46:26, 25.28s/it]                                                         {'loss': 0.8141, 'learning_rate': 5.7640943178797445e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3379/5198 [19:12:36<12:46:26, 25.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3380/5198 [19:12:54<11:36:54, 23.00s/it]                                                         {'loss': 0.7471, 'learning_rate': 5.7584509050634395e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3380/5198 [19:12:54<11:36:54, 23.00s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3381/5198 [19:13:11<10:43:12, 21.24s/it]                                                         {'loss': 0.8371, 'learning_rate': 5.752809138962525e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3381/5198 [19:13:11<10:43:12, 21.24s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3382/5198 [19:13:30<10:20:08, 20.49s/it]                                                         {'loss': 0.8007, 'learning_rate': 5.747169021767342e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3382/5198 [19:13:30<10:20:08, 20.49s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3383/5198 [19:13:48<9:56:45, 19.73s/it]                                                         {'loss': 0.8184, 'learning_rate': 5.7415305556675805e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3383/5198 [19:13:48<9:56:45, 19.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3384/5198 [19:14:05<9:37:07, 19.09s/it]                                                        {'loss': 0.8681, 'learning_rate': 5.73589374285227e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3384/5198 [19:14:05<9:37:07, 19.09s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3385/5198 [19:14:22<9:12:54, 18.30s/it]                                                        {'loss': 0.8243, 'learning_rate': 5.730258585509832e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3385/5198 [19:14:22<9:12:54, 18.30s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3386/5198 [19:14:39<9:01:04, 17.92s/it]                                                        {'loss': 0.8284, 'learning_rate': 5.724625085828022e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3386/5198 [19:14:39<9:01:04, 17.92s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3387/5198 [19:14:57<9:02:08, 17.96s/it]                                                        {'loss': 0.7988, 'learning_rate': 5.718993245993958e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3387/5198 [19:14:57<9:02:08, 17.96s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3388/5198 [19:15:14<8:52:43, 17.66s/it]                                                        {'loss': 0.859, 'learning_rate': 5.713363068194115e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3388/5198 [19:15:14<8:52:43, 17.66s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3389/5198 [19:15:30<8:42:41, 17.34s/it]                                                        {'loss': 0.8624, 'learning_rate': 5.7077345546143235e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3389/5198 [19:15:31<8:42:41, 17.34s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3390/5198 [19:15:49<8:54:30, 17.74s/it]                                                        {'loss': 0.7483, 'learning_rate': 5.702107707439766e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3390/5198 [19:15:49<8:54:30, 17.74s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3391/5198 [19:16:07<8:57:25, 17.84s/it]                                                        {'loss': 0.7501, 'learning_rate': 5.6964825288549745e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3391/5198 [19:16:07<8:57:25, 17.84s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3392/5198 [19:16:25<9:00:31, 17.96s/it]                                                        {'loss': 0.7864, 'learning_rate': 5.690859021043842e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3392/5198 [19:16:25<9:00:31, 17.96s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3393/5198 [19:16:42<8:51:26, 17.67s/it]                                                        {'loss': 0.7566, 'learning_rate': 5.685237186189601e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3393/5198 [19:16:42<8:51:26, 17.67s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3394/5198 [19:17:01<9:00:45, 17.99s/it]                                                        {'loss': 0.8058, 'learning_rate': 5.679617026474853e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3394/5198 [19:17:01<9:00:45, 17.99s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3395/5198 [19:17:20<9:08:04, 18.24s/it]                                                        {'loss': 0.7942, 'learning_rate': 5.673998544081527e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3395/5198 [19:17:20<9:08:04, 18.24s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3396/5198 [19:17:38<9:04:13, 18.12s/it]                                                        {'loss': 0.3434, 'learning_rate': 5.6683817411909114e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3396/5198 [19:17:38<9:04:13, 18.12s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3397/5198 [19:17:56<9:02:19, 18.07s/it]                                                        {'loss': 0.8251, 'learning_rate': 5.662766619983653e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3397/5198 [19:17:56<9:02:19, 18.07s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3398/5198 [19:18:13<8:55:04, 17.84s/it]                                                        {'loss': 0.3486, 'learning_rate': 5.65715318263972e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3398/5198 [19:18:13<8:55:04, 17.84s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3399/5198 [19:18:31<8:57:55, 17.94s/it]                                                        {'loss': 0.8863, 'learning_rate': 5.651541431338454e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3399/5198 [19:18:31<8:57:55, 17.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3400/5198 [19:18:49<8:55:22, 17.87s/it]                                                        {'loss': 0.7666, 'learning_rate': 5.645931368258527e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3400/5198 [19:18:49<8:55:22, 17.87s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3401/5198 [19:20:21<19:59:12, 40.04s/it]                                                         {'loss': 0.8126, 'learning_rate': 5.640322995577958e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3401/5198 [19:20:21<19:59:12, 40.04s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3402/5198 [19:20:39<16:43:55, 33.54s/it]                                                         {'loss': 0.8223, 'learning_rate': 5.634716315474109e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3402/5198 [19:20:39<16:43:55, 33.54s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3403/5198 [19:20:58<14:32:52, 29.18s/it]                                                         {'loss': 0.7994, 'learning_rate': 5.629111330123689e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3403/5198 [19:20:58<14:32:52, 29.18s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3404/5198 [19:21:17<12:57:18, 26.00s/it]                                                         {'loss': 0.8461, 'learning_rate': 5.623508041702743e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3404/5198 [19:21:17<12:57:18, 26.00s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3405/5198 [19:21:34<11:40:56, 23.46s/it]                                                         {'loss': 0.3546, 'learning_rate': 5.617906452386659e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3405/5198 [19:21:34<11:40:56, 23.46s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3406/5198 [19:21:52<10:46:15, 21.64s/it]                                                         {'loss': 0.3165, 'learning_rate': 5.612306564350179e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3406/5198 [19:21:52<10:46:15, 21.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3407/5198 [19:22:10<10:14:17, 20.58s/it]                                                         {'loss': 0.816, 'learning_rate': 5.6067083797673535e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3407/5198 [19:22:10<10:14:17, 20.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3408/5198 [19:22:27<9:47:17, 19.69s/it]                                                         {'loss': 0.3121, 'learning_rate': 5.601111900811607e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3408/5198 [19:22:27<9:47:17, 19.69s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3409/5198 [19:22:45<9:30:50, 19.15s/it]                                                        {'loss': 0.7565, 'learning_rate': 5.595517129655681e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3409/5198 [19:22:45<9:30:50, 19.15s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3410/5198 [19:23:04<9:27:01, 19.03s/it]                                                        {'loss': 0.7864, 'learning_rate': 5.589924068471648e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3410/5198 [19:23:04<9:27:01, 19.03s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3411/5198 [19:23:22<9:20:37, 18.82s/it]                                                        {'loss': 0.8303, 'learning_rate': 5.58433271943094e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3411/5198 [19:23:22<9:20:37, 18.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3412/5198 [19:23:40<9:13:22, 18.59s/it]                                                        {'loss': 0.7599, 'learning_rate': 5.578743084704306e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3412/5198 [19:23:40<9:13:22, 18.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3413/5198 [19:23:58<9:00:08, 18.16s/it]                                                        {'loss': 0.8129, 'learning_rate': 5.573155166461833e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3413/5198 [19:23:58<9:00:08, 18.16s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3414/5198 [19:24:16<9:02:11, 18.23s/it]                                                        {'loss': 0.7585, 'learning_rate': 5.567568966872947e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3414/5198 [19:24:16<9:02:11, 18.23s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3415/5198 [19:24:33<8:55:37, 18.02s/it]                                                        {'loss': 0.8432, 'learning_rate': 5.5619844881064e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3415/5198 [19:24:33<8:55:37, 18.02s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3416/5198 [19:24:51<8:48:23, 17.79s/it]                                                        {'loss': 0.8333, 'learning_rate': 5.556401732330281e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3416/5198 [19:24:51<8:48:23, 17.79s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3417/5198 [19:25:08<8:46:52, 17.75s/it]                                                        {'loss': 0.6955, 'learning_rate': 5.550820701712007e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3417/5198 [19:25:08<8:46:52, 17.75s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3418/5198 [19:25:26<8:41:31, 17.58s/it]                                                        {'loss': 0.7713, 'learning_rate': 5.545241398418326e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3418/5198 [19:25:26<8:41:31, 17.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3419/5198 [19:25:44<8:45:10, 17.71s/it]                                                        {'loss': 0.7685, 'learning_rate': 5.539663824615312e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3419/5198 [19:25:44<8:45:10, 17.71s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3420/5198 [19:26:01<8:37:55, 17.48s/it]                                                        {'loss': 0.7267, 'learning_rate': 5.534087982468384e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3420/5198 [19:26:01<8:37:55, 17.48s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3421/5198 [19:26:19<8:42:15, 17.63s/it]                                                        {'loss': 0.7781, 'learning_rate': 5.5285138741422615e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3421/5198 [19:26:19<8:42:15, 17.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3422/5198 [19:26:35<8:32:47, 17.32s/it]                                                        {'loss': 0.3315, 'learning_rate': 5.522941501801008e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3422/5198 [19:26:35<8:32:47, 17.32s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3423/5198 [19:26:52<8:24:50, 17.07s/it]                                                        {'loss': 0.8527, 'learning_rate': 5.517370867608021e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3423/5198 [19:26:52<8:24:50, 17.07s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3424/5198 [19:27:09<8:29:49, 17.24s/it]                                                        {'loss': 0.7738, 'learning_rate': 5.511801973725997e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3424/5198 [19:27:09<8:29:49, 17.24s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3425/5198 [19:27:27<8:34:04, 17.40s/it]                                                        {'loss': 0.7114, 'learning_rate': 5.506234822316983e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3425/5198 [19:27:27<8:34:04, 17.40s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3426/5198 [19:28:57<19:18:00, 39.21s/it]                                                         {'loss': 0.7866, 'learning_rate': 5.500669415542336e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3426/5198 [19:28:57<19:18:00, 39.21s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3427/5198 [19:29:14<15:58:25, 32.47s/it]                                                         {'loss': 0.7311, 'learning_rate': 5.495105755562738e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3427/5198 [19:29:14<15:58:25, 32.47s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3428/5198 [19:29:31<13:41:31, 27.85s/it]                                                         {'loss': 0.8028, 'learning_rate': 5.4895438445381945e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3428/5198 [19:29:31<13:41:31, 27.85s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3429/5198 [19:29:48<12:09:32, 24.74s/it]                                                         {'loss': 0.3585, 'learning_rate': 5.48398368462803e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3429/5198 [19:29:48<12:09:32, 24.74s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3430/5198 [19:30:06<11:03:02, 22.50s/it]                                                         {'loss': 0.8085, 'learning_rate': 5.4784252779908905e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3430/5198 [19:30:06<11:03:02, 22.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3431/5198 [19:30:23<10:18:01, 20.99s/it]                                                         {'loss': 0.7854, 'learning_rate': 5.4728686267847354e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3431/5198 [19:30:23<10:18:01, 20.99s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3432/5198 [19:30:41<9:46:56, 19.94s/it]                                                         {'loss': 0.8058, 'learning_rate': 5.467313733166863e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3432/5198 [19:30:41<9:46:56, 19.94s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3433/5198 [19:30:59<9:31:26, 19.43s/it]                                                        {'loss': 0.8485, 'learning_rate': 5.461760599293855e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3433/5198 [19:30:59<9:31:26, 19.43s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3434/5198 [19:31:16<9:12:41, 18.80s/it]                                                        {'loss': 0.8535, 'learning_rate': 5.456209227321643e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3434/5198 [19:31:16<9:12:41, 18.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3435/5198 [19:31:33<8:58:11, 18.32s/it]                                                        {'loss': 0.8635, 'learning_rate': 5.450659619405458e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3435/5198 [19:31:33<8:58:11, 18.32s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3436/5198 [19:31:51<8:47:55, 17.98s/it]                                                        {'loss': 0.8678, 'learning_rate': 5.445111777699842e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3436/5198 [19:31:51<8:47:55, 17.98s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3437/5198 [19:32:08<8:42:42, 17.81s/it]                                                        {'loss': 0.8597, 'learning_rate': 5.439565704358667e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3437/5198 [19:32:08<8:42:42, 17.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3438/5198 [19:32:25<8:37:04, 17.63s/it]                                                        {'loss': 0.7805, 'learning_rate': 5.434021401535105e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3438/5198 [19:32:25<8:37:04, 17.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3439/5198 [19:32:43<8:36:48, 17.63s/it]                                                        {'loss': 0.8066, 'learning_rate': 5.428478871381646e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3439/5198 [19:32:43<8:36:48, 17.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3440/5198 [19:32:59<8:26:29, 17.29s/it]                                                        {'loss': 0.8521, 'learning_rate': 5.422938116050092e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3440/5198 [19:32:59<8:26:29, 17.29s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3441/5198 [19:33:18<8:36:04, 17.62s/it]                                                        {'loss': 0.8059, 'learning_rate': 5.417399137691552e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3441/5198 [19:33:18<8:36:04, 17.62s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3442/5198 [19:33:35<8:36:10, 17.64s/it]                                                        {'loss': 0.8061, 'learning_rate': 5.411861938456453e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3442/5198 [19:33:35<8:36:10, 17.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3443/5198 [19:33:53<8:33:52, 17.57s/it]                                                        {'loss': 0.8988, 'learning_rate': 5.406326520494522e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3443/5198 [19:33:53<8:33:52, 17.57s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3444/5198 [19:34:11<8:41:25, 17.84s/it]                                                        {'loss': 0.7962, 'learning_rate': 5.400792885954802e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3444/5198 [19:34:11<8:41:25, 17.84s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3445/5198 [19:34:28<8:35:47, 17.65s/it]                                                        {'loss': 0.7713, 'learning_rate': 5.395261036985635e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3445/5198 [19:34:28<8:35:47, 17.65s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3446/5198 [19:34:46<8:31:41, 17.52s/it]                                                        {'loss': 0.7279, 'learning_rate': 5.389730975734686e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3446/5198 [19:34:46<8:31:41, 17.52s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3447/5198 [19:35:04<8:35:12, 17.65s/it]                                                        {'loss': 0.8208, 'learning_rate': 5.384202704348902e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3447/5198 [19:35:04<8:35:12, 17.65s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3448/5198 [19:35:21<8:35:34, 17.68s/it]                                                        {'loss': 0.7558, 'learning_rate': 5.378676224974557e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3448/5198 [19:35:21<8:35:34, 17.68s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3449/5198 [19:35:39<8:35:14, 17.68s/it]                                                        {'loss': 0.8205, 'learning_rate': 5.373151539757224e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3449/5198 [19:35:39<8:35:14, 17.68s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3450/5198 [19:35:57<8:38:27, 17.80s/it]                                                        {'loss': 0.7629, 'learning_rate': 5.367628650841761e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3450/5198 [19:35:57<8:38:27, 17.80s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3451/5198 [19:37:27<19:03:43, 39.28s/it]                                                         {'loss': 0.8395, 'learning_rate': 5.362107560372358e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3451/5198 [19:37:27<19:03:43, 39.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3452/5198 [19:37:44<15:52:07, 32.72s/it]                                                         {'loss': 0.7303, 'learning_rate': 5.356588270492487e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3452/5198 [19:37:44<15:52:07, 32.72s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3453/5198 [19:38:02<13:44:45, 28.36s/it]                                                         {'loss': 0.7874, 'learning_rate': 5.351070783344926e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3453/5198 [19:38:02<13:44:45, 28.36s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3454/5198 [19:38:19<12:04:33, 24.93s/it]                                                         {'loss': 0.7988, 'learning_rate': 5.3455551010717545e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3454/5198 [19:38:19<12:04:33, 24.93s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3455/5198 [19:38:36<10:55:35, 22.57s/it]                                                         {'loss': 0.841, 'learning_rate': 5.34004122581435e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3455/5198 [19:38:36<10:55:35, 22.57s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3456/5198 [19:38:55<10:24:59, 21.53s/it]                                                         {'loss': 0.7702, 'learning_rate': 5.334529159713389e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3456/5198 [19:38:55<10:24:59, 21.53s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3457/5198 [19:39:13<9:54:42, 20.50s/it]                                                         {'loss': 0.8116, 'learning_rate': 5.329018904908841e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3457/5198 [19:39:13<9:54:42, 20.50s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3458/5198 [19:39:31<9:29:56, 19.65s/it]                                                        {'loss': 0.7946, 'learning_rate': 5.323510463539989e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3458/5198 [19:39:31<9:29:56, 19.65s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3459/5198 [19:39:49<9:13:31, 19.10s/it]                                                        {'loss': 0.7924, 'learning_rate': 5.318003837745382e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3459/5198 [19:39:49<9:13:31, 19.10s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3460/5198 [19:40:06<8:59:59, 18.64s/it]                                                        {'loss': 0.8025, 'learning_rate': 5.3124990296628974e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3460/5198 [19:40:07<8:59:59, 18.64s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3461/5198 [19:40:25<9:00:21, 18.67s/it]                                                        {'loss': 0.7358, 'learning_rate': 5.306996041429688e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3461/5198 [19:40:25<9:00:21, 18.67s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3462/5198 [19:40:42<8:46:27, 18.20s/it]                                                        {'loss': 0.8172, 'learning_rate': 5.301494875182192e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3462/5198 [19:40:42<8:46:27, 18.20s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3463/5198 [19:41:00<8:41:38, 18.04s/it]                                                        {'loss': 0.8253, 'learning_rate': 5.295995533056162e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3463/5198 [19:41:00<8:41:38, 18.04s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3464/5198 [19:41:17<8:37:44, 17.91s/it]                                                        {'loss': 0.84, 'learning_rate': 5.290498017186631e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3464/5198 [19:41:17<8:37:44, 17.91s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3465/5198 [19:41:35<8:36:52, 17.90s/it]                                                        {'loss': 0.8191, 'learning_rate': 5.2850023297079235e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3465/5198 [19:41:35<8:36:52, 17.90s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3466/5198 [19:41:54<8:39:36, 18.00s/it]                                                        {'loss': 0.815, 'learning_rate': 5.279508472753654e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3466/5198 [19:41:54<8:39:36, 18.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3467/5198 [19:42:11<8:35:27, 17.87s/it]                                                        {'loss': 0.3097, 'learning_rate': 5.274016448456725e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3467/5198 [19:42:11<8:35:27, 17.87s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3468/5198 [19:42:30<8:40:11, 18.04s/it]                                                        {'loss': 0.8279, 'learning_rate': 5.2685262589493314e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3468/5198 [19:42:30<8:40:11, 18.04s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3469/5198 [19:42:48<8:39:58, 18.04s/it]                                                        {'loss': 0.8039, 'learning_rate': 5.263037906362953e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3469/5198 [19:42:48<8:39:58, 18.04s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3470/5198 [19:43:04<8:27:42, 17.63s/it]                                                        {'loss': 0.8424, 'learning_rate': 5.257551392828359e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3470/5198 [19:43:04<8:27:42, 17.63s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3471/5198 [19:43:23<8:34:43, 17.88s/it]                                                        {'loss': 0.8249, 'learning_rate': 5.252066720475597e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3471/5198 [19:43:23<8:34:43, 17.88s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3472/5198 [19:43:41<8:37:46, 18.00s/it]                                                        {'loss': 0.767, 'learning_rate': 5.246583891434018e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3472/5198 [19:43:41<8:37:46, 18.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3473/5198 [19:43:59<8:35:23, 17.93s/it]                                                        {'loss': 0.7943, 'learning_rate': 5.241102907832232e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3473/5198 [19:43:59<8:35:23, 17.93s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3474/5198 [19:44:15<8:23:52, 17.54s/it]                                                        {'loss': 0.3387, 'learning_rate': 5.235623771798151e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3474/5198 [19:44:15<8:23:52, 17.54s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3475/5198 [19:44:34<8:28:15, 17.70s/it]                                                        {'loss': 0.7638, 'learning_rate': 5.23014648545897e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3475/5198 [19:44:34<8:28:15, 17.70s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3476/5198 [19:46:05<19:02:32, 39.81s/it]                                                         {'loss': 0.8083, 'learning_rate': 5.224671050941146e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3476/5198 [19:46:05<19:02:32, 39.81s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3477/5198 [19:46:22<15:50:15, 33.13s/it]                                                         {'loss': 0.7575, 'learning_rate': 5.2191974703704425e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3477/5198 [19:46:22<15:50:15, 33.13s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3478/5198 [19:46:41<13:43:56, 28.74s/it]                                                         {'loss': 0.7892, 'learning_rate': 5.213725745871889e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3478/5198 [19:46:41<13:43:56, 28.74s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3479/5198 [19:46:59<12:08:33, 25.43s/it]                                                         {'loss': 0.7857, 'learning_rate': 5.208255879569799e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3479/5198 [19:46:59<12:08:33, 25.43s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3480/5198 [19:47:15<10:52:58, 22.80s/it]                                                         {'loss': 0.8061, 'learning_rate': 5.20278787358776e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3480/5198 [19:47:15<10:52:58, 22.80s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3481/5198 [19:47:34<10:13:06, 21.43s/it]                                                         {'loss': 0.7421, 'learning_rate': 5.197321730048641e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3481/5198 [19:47:34<10:13:06, 21.43s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3482/5198 [19:47:50<9:28:40, 19.88s/it]                                                         {'loss': 0.8396, 'learning_rate': 5.1918574510745865e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3482/5198 [19:47:50<9:28:40, 19.88s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3483/5198 [19:48:07<9:03:10, 19.00s/it]                                                        {'loss': 0.788, 'learning_rate': 5.186395038787017e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3483/5198 [19:48:07<9:03:10, 19.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3484/5198 [19:48:25<8:52:56, 18.66s/it]                                                        {'loss': 0.8103, 'learning_rate': 5.180934495306638e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3484/5198 [19:48:25<8:52:56, 18.66s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3485/5198 [19:48:42<8:43:21, 18.33s/it]                                                        {'loss': 0.8426, 'learning_rate': 5.175475822753404e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3485/5198 [19:48:42<8:43:21, 18.33s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3486/5198 [19:49:00<8:39:21, 18.20s/it]                                                        {'loss': 0.8475, 'learning_rate': 5.170019023246574e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3486/5198 [19:49:00<8:39:21, 18.20s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3487/5198 [19:49:17<8:29:55, 17.88s/it]                                                        {'loss': 0.7571, 'learning_rate': 5.16456409890466e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3487/5198 [19:49:17<8:29:55, 17.88s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3488/5198 [19:49:35<8:31:41, 17.95s/it]                                                        {'loss': 0.7791, 'learning_rate': 5.159111051845451e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3488/5198 [19:49:35<8:31:41, 17.95s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3489/5198 [19:49:53<8:32:36, 18.00s/it]                                                        {'loss': 0.7777, 'learning_rate': 5.153659884186013e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3489/5198 [19:49:54<8:32:36, 18.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3490/5198 [19:50:12<8:34:29, 18.07s/it]                                                        {'loss': 0.8016, 'learning_rate': 5.148210598042665e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3490/5198 [19:50:12<8:34:29, 18.07s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3491/5198 [19:50:30<8:33:52, 18.06s/it]                                                        {'loss': 0.7494, 'learning_rate': 5.142763195531017e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3491/5198 [19:50:30<8:33:52, 18.06s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3492/5198 [19:50:46<8:20:16, 17.59s/it]                                                        {'loss': 0.8641, 'learning_rate': 5.137317678765939e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3492/5198 [19:50:46<8:20:16, 17.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3493/5198 [19:51:04<8:20:48, 17.62s/it]                                                        {'loss': 0.8349, 'learning_rate': 5.131874049861563e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3493/5198 [19:51:04<8:20:48, 17.62s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3494/5198 [19:51:21<8:14:14, 17.40s/it]                                                        {'loss': 0.806, 'learning_rate': 5.126432310931295e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3494/5198 [19:51:21<8:14:14, 17.40s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3495/5198 [19:51:39<8:20:29, 17.63s/it]                                                        {'loss': 0.7571, 'learning_rate': 5.120992464087807e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3495/5198 [19:51:39<8:20:29, 17.63s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3496/5198 [19:51:56<8:18:57, 17.59s/it]                                                        {'loss': 0.8102, 'learning_rate': 5.115554511443033e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3496/5198 [19:51:56<8:18:57, 17.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3497/5198 [19:52:14<8:20:19, 17.65s/it]                                                        {'loss': 0.8019, 'learning_rate': 5.1101184551081705e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3497/5198 [19:52:14<8:20:19, 17.65s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3498/5198 [19:52:33<8:25:36, 17.84s/it]                                                        {'loss': 0.8143, 'learning_rate': 5.104684297193694e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3498/5198 [19:52:33<8:25:36, 17.84s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3499/5198 [19:52:51<8:28:24, 17.95s/it]                                                        {'loss': 0.8372, 'learning_rate': 5.099252039809317e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3499/5198 [19:52:51<8:28:24, 17.95s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3500/5198 [19:53:09<8:26:10, 17.89s/it]                                                        {'loss': 0.8308, 'learning_rate': 5.09382168506404e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3500/5198 [19:53:09<8:26:10, 17.89s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3501/5198 [19:54:39<18:40:23, 39.61s/it]                                                         {'loss': 0.8331, 'learning_rate': 5.088393235066114e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3501/5198 [19:54:39<18:40:23, 39.61s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3502/5198 [19:54:57<15:34:42, 33.07s/it]                                                         {'loss': 0.7546, 'learning_rate': 5.082966691923037e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3502/5198 [19:54:57<15:34:42, 33.07s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3503/5198 [19:55:15<13:27:15, 28.58s/it]                                                         {'loss': 0.8193, 'learning_rate': 5.077542057741592e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3503/5198 [19:55:15<13:27:15, 28.58s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3504/5198 [19:55:33<12:01:52, 25.57s/it]                                                         {'loss': 0.8137, 'learning_rate': 5.0721193346278066e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3504/5198 [19:55:33<12:01:52, 25.57s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3505/5198 [19:55:51<10:55:39, 23.24s/it]                                                         {'loss': 0.8593, 'learning_rate': 5.066698524686966e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3505/5198 [19:55:51<10:55:39, 23.24s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3506/5198 [19:56:08<10:00:26, 21.29s/it]                                                         {'loss': 0.7977, 'learning_rate': 5.061279630023618e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3506/5198 [19:56:08<10:00:26, 21.29s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3507/5198 [19:56:25<9:29:34, 20.21s/it]                                                         {'loss': 0.7967, 'learning_rate': 5.055862652741562e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3507/5198 [19:56:25<9:29:34, 20.21s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3508/5198 [19:56:44<9:16:29, 19.76s/it]                                                        {'loss': 0.8135, 'learning_rate': 5.050447594943856e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3508/5198 [19:56:44<9:16:29, 19.76s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3509/5198 [19:57:03<9:03:53, 19.32s/it]                                                        {'loss': 0.7756, 'learning_rate': 5.045034458732808e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3509/5198 [19:57:03<9:03:53, 19.32s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3510/5198 [19:57:20<8:48:52, 18.80s/it]                                                        {'loss': 0.8116, 'learning_rate': 5.0396232462099945e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3510/5198 [19:57:20<8:48:52, 18.80s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3511/5198 [19:57:37<8:34:26, 18.30s/it]                                                        {'loss': 0.7969, 'learning_rate': 5.034213959476222e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3511/5198 [19:57:37<8:34:26, 18.30s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3512/5198 [19:57:55<8:28:39, 18.10s/it]                                                        {'loss': 0.3558, 'learning_rate': 5.028806600631569e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3512/5198 [19:57:55<8:28:39, 18.10s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3513/5198 [19:58:13<8:27:24, 18.07s/it]                                                        {'loss': 0.7973, 'learning_rate': 5.023401171775357e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3513/5198 [19:58:13<8:27:24, 18.07s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3514/5198 [19:58:30<8:19:45, 17.81s/it]                                                        {'loss': 0.7968, 'learning_rate': 5.017997675006161e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3514/5198 [19:58:30<8:19:45, 17.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3515/5198 [19:58:48<8:23:47, 17.96s/it]                                                        {'loss': 0.8737, 'learning_rate': 5.012596112421806e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3515/5198 [19:58:48<8:23:47, 17.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3516/5198 [19:59:06<8:23:35, 17.96s/it]                                                        {'loss': 0.7519, 'learning_rate': 5.007196486119355e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3516/5198 [19:59:06<8:23:35, 17.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3517/5198 [19:59:24<8:24:04, 17.99s/it]                                                        {'loss': 0.8245, 'learning_rate': 5.001798798195136e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3517/5198 [19:59:24<8:24:04, 17.99s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3518/5198 [19:59:42<8:23:14, 17.97s/it]                                                        {'loss': 0.7991, 'learning_rate': 4.996403050744719e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3518/5198 [19:59:42<8:23:14, 17.97s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3519/5198 [20:00:01<8:26:00, 18.08s/it]                                                        {'loss': 0.7786, 'learning_rate': 4.991009245862917e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3519/5198 [20:00:01<8:26:00, 18.08s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3520/5198 [20:00:18<8:16:18, 17.75s/it]                                                        {'loss': 0.7768, 'learning_rate': 4.985617385643789e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3520/5198 [20:00:18<8:16:18, 17.75s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3521/5198 [20:00:35<8:10:20, 17.54s/it]                                                        {'loss': 0.7398, 'learning_rate': 4.980227472180643e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3521/5198 [20:00:35<8:10:20, 17.54s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3522/5198 [20:00:51<7:59:44, 17.17s/it]                                                        {'loss': 0.3038, 'learning_rate': 4.974839507566027e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3522/5198 [20:00:51<7:59:44, 17.17s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3523/5198 [20:01:08<8:00:19, 17.21s/it]                                                        {'loss': 0.8909, 'learning_rate': 4.969453493891733e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3523/5198 [20:01:08<8:00:19, 17.21s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3524/5198 [20:01:26<8:03:45, 17.34s/it]                                                        {'loss': 0.8652, 'learning_rate': 4.9640694332488075e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3524/5198 [20:01:26<8:03:45, 17.34s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3525/5198 [20:01:44<8:09:52, 17.57s/it]                                                        {'loss': 0.78, 'learning_rate': 4.958687327727511e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3525/5198 [20:01:44<8:09:52, 17.57s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3526/5198 [20:03:27<20:02:03, 43.14s/it]                                                         {'loss': 0.7898, 'learning_rate': 4.953307179417376e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3526/5198 [20:03:27<20:02:03, 43.14s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3527/5198 [20:03:44<16:27:50, 35.47s/it]                                                         {'loss': 0.7783, 'learning_rate': 4.947928990407156e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3527/5198 [20:03:44<16:27:50, 35.47s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3528/5198 [20:04:02<14:00:24, 30.19s/it]                                                         {'loss': 0.8131, 'learning_rate': 4.94255276278485e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3528/5198 [20:04:03<14:00:24, 30.19s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3529/5198 [20:04:21<12:23:25, 26.73s/it]                                                         {'loss': 0.7371, 'learning_rate': 4.937178498637696e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3529/5198 [20:04:21<12:23:25, 26.73s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3530/5198 [20:04:39<11:09:22, 24.08s/it]                                                         {'loss': 0.7357, 'learning_rate': 4.931806200052165e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3530/5198 [20:04:39<11:09:22, 24.08s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3531/5198 [20:04:58<10:25:21, 22.51s/it]                                                         {'loss': 0.7961, 'learning_rate': 4.926435869113971e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3531/5198 [20:04:58<10:25:21, 22.51s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3532/5198 [20:05:15<9:38:46, 20.84s/it]                                                         {'loss': 0.7799, 'learning_rate': 4.92106750790806e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3532/5198 [20:05:15<9:38:46, 20.84s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3533/5198 [20:05:32<9:13:28, 19.95s/it]                                                        {'loss': 0.7955, 'learning_rate': 4.915701118518616e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3533/5198 [20:05:32<9:13:28, 19.95s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3534/5198 [20:05:50<8:54:11, 19.26s/it]                                                        {'loss': 0.3338, 'learning_rate': 4.910336703029055e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3534/5198 [20:05:50<8:54:11, 19.26s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3535/5198 [20:06:09<8:50:47, 19.15s/it]                                                        {'loss': 0.7976, 'learning_rate': 4.904974263522025e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3535/5198 [20:06:09<8:50:47, 19.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3536/5198 [20:06:26<8:31:57, 18.48s/it]                                                        {'loss': 0.7297, 'learning_rate': 4.899613802079419e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3536/5198 [20:06:26<8:31:57, 18.48s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3537/5198 [20:06:44<8:24:38, 18.23s/it]                                                        {'loss': 0.7965, 'learning_rate': 4.8942553207823395e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3537/5198 [20:06:44<8:24:38, 18.23s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3538/5198 [20:07:02<8:22:33, 18.16s/it]                                                        {'loss': 0.8235, 'learning_rate': 4.888898821711144e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3538/5198 [20:07:02<8:22:33, 18.16s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3539/5198 [20:07:19<8:18:43, 18.04s/it]                                                        {'loss': 0.7654, 'learning_rate': 4.883544306945407e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3539/5198 [20:07:19<8:18:43, 18.04s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3540/5198 [20:07:37<8:18:46, 18.05s/it]                                                        {'loss': 0.8121, 'learning_rate': 4.878191778563934e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3540/5198 [20:07:37<8:18:46, 18.05s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3541/5198 [20:07:55<8:16:03, 17.96s/it]                                                        {'loss': 0.811, 'learning_rate': 4.872841238644766e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3541/5198 [20:07:55<8:16:03, 17.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3542/5198 [20:08:13<8:13:07, 17.87s/it]                                                        {'loss': 0.7258, 'learning_rate': 4.867492689265154e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3542/5198 [20:08:13<8:13:07, 17.87s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3543/5198 [20:08:29<8:00:58, 17.44s/it]                                                        {'loss': 0.2959, 'learning_rate': 4.8621461325016015e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3543/5198 [20:08:29<8:00:58, 17.44s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3544/5198 [20:08:47<8:00:49, 17.44s/it]                                                        {'loss': 0.8102, 'learning_rate': 4.856801570429822e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3544/5198 [20:08:47<8:00:49, 17.44s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3545/5198 [20:09:05<8:04:00, 17.57s/it]                                                        {'loss': 0.8047, 'learning_rate': 4.851459005124759e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3545/5198 [20:09:05<8:04:00, 17.57s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3546/5198 [20:09:23<8:12:12, 17.88s/it]                                                        {'loss': 0.7674, 'learning_rate': 4.846118438660578e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3546/5198 [20:09:23<8:12:12, 17.88s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3547/5198 [20:09:41<8:08:47, 17.76s/it]                                                        {'loss': 0.3546, 'learning_rate': 4.840779873110675e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3547/5198 [20:09:41<8:08:47, 17.76s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3548/5198 [20:09:58<8:06:17, 17.68s/it]                                                        {'loss': 0.7774, 'learning_rate': 4.83544331054766e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3548/5198 [20:09:58<8:06:17, 17.68s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3549/5198 [20:10:16<8:08:56, 17.79s/it]                                                        {'loss': 0.7794, 'learning_rate': 4.83010875304337e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3549/5198 [20:10:16<8:08:56, 17.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3550/5198 [20:10:34<8:05:48, 17.69s/it]                                                        {'loss': 0.8088, 'learning_rate': 4.824776202668875e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3550/5198 [20:10:34<8:05:48, 17.69s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3551/5198 [20:12:04<17:59:47, 39.34s/it]                                                         {'loss': 0.7245, 'learning_rate': 4.819445661494437e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3551/5198 [20:12:04<17:59:47, 39.34s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3552/5198 [20:12:20<14:52:39, 32.54s/it]                                                         {'loss': 0.8397, 'learning_rate': 4.8141171315895694e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3552/5198 [20:12:20<14:52:39, 32.54s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3553/5198 [20:12:39<12:57:05, 28.34s/it]                                                         {'loss': 0.7335, 'learning_rate': 4.808790615022987e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3553/5198 [20:12:39<12:57:05, 28.34s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3554/5198 [20:12:57<11:34:50, 25.36s/it]                                                         {'loss': 0.8396, 'learning_rate': 4.803466113862626e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3554/5198 [20:12:57<11:34:50, 25.36s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3555/5198 [20:13:13<10:20:12, 22.65s/it]                                                         {'loss': 0.8314, 'learning_rate': 4.798143630175642e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3555/5198 [20:13:13<10:20:12, 22.65s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3556/5198 [20:13:30<9:30:48, 20.86s/it]                                                         {'loss': 0.838, 'learning_rate': 4.792823166028405e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3556/5198 [20:13:30<9:30:48, 20.86s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3557/5198 [20:13:47<9:01:39, 19.80s/it]                                                        {'loss': 0.788, 'learning_rate': 4.787504723486505e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3557/5198 [20:13:47<9:01:39, 19.80s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3558/5198 [20:14:06<8:46:45, 19.27s/it]                                                        {'loss': 0.7822, 'learning_rate': 4.7821883046147414e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3558/5198 [20:14:06<8:46:45, 19.27s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3559/5198 [20:14:22<8:27:12, 18.57s/it]                                                        {'loss': 0.7887, 'learning_rate': 4.776873911477133e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3559/5198 [20:14:22<8:27:12, 18.57s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3560/5198 [20:14:41<8:23:15, 18.43s/it]                                                        {'loss': 0.8303, 'learning_rate': 4.771561546136908e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3560/5198 [20:14:41<8:23:15, 18.43s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3561/5198 [20:14:58<8:17:26, 18.23s/it]                                                        {'loss': 0.773, 'learning_rate': 4.766251210656509e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3561/5198 [20:14:58<8:17:26, 18.23s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3562/5198 [20:15:15<8:05:31, 17.81s/it]                                                        {'loss': 0.7987, 'learning_rate': 4.760942907097601e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3562/5198 [20:15:15<8:05:31, 17.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3563/5198 [20:15:33<8:08:56, 17.94s/it]                                                        {'loss': 0.7661, 'learning_rate': 4.755636637521035e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3563/5198 [20:15:33<8:08:56, 17.94s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3564/5198 [20:15:50<8:01:37, 17.69s/it]                                                        {'loss': 0.8276, 'learning_rate': 4.750332403986902e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3564/5198 [20:15:50<8:01:37, 17.69s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3565/5198 [20:16:08<8:01:02, 17.67s/it]                                                        {'loss': 0.767, 'learning_rate': 4.7450302085544735e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3565/5198 [20:16:08<8:01:02, 17.67s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3566/5198 [20:16:27<8:07:20, 17.92s/it]                                                        {'loss': 0.7823, 'learning_rate': 4.739730053282255e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3566/5198 [20:16:27<8:07:20, 17.92s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3567/5198 [20:16:44<8:03:51, 17.80s/it]                                                        {'loss': 0.8355, 'learning_rate': 4.734431940227951e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3567/5198 [20:16:44<8:03:51, 17.80s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3568/5198 [20:17:02<8:04:30, 17.83s/it]                                                        {'loss': 0.7788, 'learning_rate': 4.7291358714484594e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3568/5198 [20:17:02<8:04:30, 17.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3569/5198 [20:17:20<8:02:22, 17.77s/it]                                                        {'loss': 0.822, 'learning_rate': 4.723841848999907e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3569/5198 [20:17:20<8:02:22, 17.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3570/5198 [20:17:38<8:07:24, 17.96s/it]                                                        {'loss': 0.8477, 'learning_rate': 4.718549874937612e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3570/5198 [20:17:38<8:07:24, 17.96s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3571/5198 [20:17:56<8:02:50, 17.81s/it]                                                        {'loss': 0.7925, 'learning_rate': 4.713259951316103e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3571/5198 [20:17:56<8:02:50, 17.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3572/5198 [20:18:13<8:00:21, 17.73s/it]                                                        {'loss': 0.3207, 'learning_rate': 4.707972080189106e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3572/5198 [20:18:13<8:00:21, 17.73s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3573/5198 [20:18:31<7:58:55, 17.68s/it]                                                        {'loss': 0.7823, 'learning_rate': 4.702686263609559e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3573/5198 [20:18:31<7:58:55, 17.68s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3574/5198 [20:18:48<7:58:00, 17.66s/it]                                                        {'loss': 0.8267, 'learning_rate': 4.697402503629596e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3574/5198 [20:18:48<7:58:00, 17.66s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3575/5198 [20:19:06<8:02:14, 17.83s/it]                                                        {'loss': 0.7468, 'learning_rate': 4.69212080230055e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3575/5198 [20:19:06<8:02:14, 17.83s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3576/5198 [20:20:36<17:43:04, 39.32s/it]                                                         {'loss': 0.8039, 'learning_rate': 4.686841161672974e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3576/5198 [20:20:36<17:43:04, 39.32s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3577/5198 [20:20:53<14:39:49, 32.57s/it]                                                         {'loss': 0.8001, 'learning_rate': 4.681563583796587e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3577/5198 [20:20:53<14:39:49, 32.57s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3578/5198 [20:21:11<12:41:33, 28.21s/it]                                                         {'loss': 0.7465, 'learning_rate': 4.67628807072034e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3578/5198 [20:21:11<12:41:33, 28.21s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3579/5198 [20:21:29<11:16:10, 25.06s/it]                                                         {'loss': 0.7687, 'learning_rate': 4.6710146244923645e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3579/5198 [20:21:29<11:16:10, 25.06s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3580/5198 [20:21:46<10:15:57, 22.84s/it]                                                         {'loss': 0.7846, 'learning_rate': 4.665743247159995e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3580/5198 [20:21:46<10:15:57, 22.84s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3581/5198 [20:22:04<9:33:52, 21.29s/it]                                                         {'loss': 0.874, 'learning_rate': 4.660473940769761e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3581/5198 [20:22:04<9:33:52, 21.29s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3582/5198 [20:22:21<8:58:24, 19.99s/it]                                                        {'loss': 0.752, 'learning_rate': 4.655206707367388e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3582/5198 [20:22:21<8:58:24, 19.99s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3583/5198 [20:22:39<8:40:14, 19.33s/it]                                                        {'loss': 0.7874, 'learning_rate': 4.649941548997797e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3583/5198 [20:22:39<8:40:14, 19.33s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3584/5198 [20:22:56<8:24:27, 18.75s/it]                                                        {'loss': 0.8051, 'learning_rate': 4.644678467705101e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3584/5198 [20:22:56<8:24:27, 18.75s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3585/5198 [20:23:14<8:14:31, 18.40s/it]                                                        {'loss': 0.3272, 'learning_rate': 4.639417465532622e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3585/5198 [20:23:14<8:14:31, 18.40s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3586/5198 [20:23:32<8:11:41, 18.30s/it]                                                        {'loss': 0.7509, 'learning_rate': 4.634158544522849e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3586/5198 [20:23:32<8:11:41, 18.30s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3587/5198 [20:23:48<7:57:38, 17.79s/it]                                                        {'loss': 0.7926, 'learning_rate': 4.628901706717476e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3587/5198 [20:23:48<7:57:38, 17.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3588/5198 [20:24:06<7:57:03, 17.78s/it]                                                        {'loss': 0.3279, 'learning_rate': 4.623646954157399e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3588/5198 [20:24:06<7:57:03, 17.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3589/5198 [20:24:23<7:52:48, 17.63s/it]                                                        {'loss': 0.8154, 'learning_rate': 4.618394288882681e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3589/5198 [20:24:23<7:52:48, 17.63s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3590/5198 [20:24:41<7:50:43, 17.56s/it]                                                        {'loss': 0.826, 'learning_rate': 4.613143712932603e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3590/5198 [20:24:41<7:50:43, 17.56s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3591/5198 [20:24:59<7:55:18, 17.75s/it]                                                        {'loss': 0.8486, 'learning_rate': 4.607895228345603e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3591/5198 [20:24:59<7:55:18, 17.75s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3592/5198 [20:25:18<8:03:25, 18.06s/it]                                                        {'loss': 0.8115, 'learning_rate': 4.602648837159333e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3592/5198 [20:25:18<8:03:25, 18.06s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3593/5198 [20:25:35<7:56:52, 17.83s/it]                                                        {'loss': 0.7737, 'learning_rate': 4.597404541410622e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3593/5198 [20:25:35<7:56:52, 17.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3594/5198 [20:25:52<7:51:21, 17.63s/it]                                                        {'loss': 0.8265, 'learning_rate': 4.592162343135483e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3594/5198 [20:25:52<7:51:21, 17.63s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3595/5198 [20:26:10<7:53:34, 17.73s/it]                                                        {'loss': 0.7972, 'learning_rate': 4.586922244369122e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3595/5198 [20:26:10<7:53:34, 17.73s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3596/5198 [20:26:28<7:52:54, 17.71s/it]                                                        {'loss': 0.7135, 'learning_rate': 4.5816842471459224e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3596/5198 [20:26:28<7:52:54, 17.71s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3597/5198 [20:26:45<7:46:53, 17.50s/it]                                                        {'loss': 0.812, 'learning_rate': 4.576448353499457e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3597/5198 [20:26:45<7:46:53, 17.50s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3598/5198 [20:27:02<7:48:08, 17.56s/it]                                                        {'loss': 0.8086, 'learning_rate': 4.571214565462477e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3598/5198 [20:27:02<7:48:08, 17.56s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3599/5198 [20:27:19<7:41:03, 17.30s/it]                                                        {'loss': 0.7737, 'learning_rate': 4.565982885066923e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3599/5198 [20:27:19<7:41:03, 17.30s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3600/5198 [20:27:37<7:42:27, 17.36s/it]                                                        {'loss': 0.7747, 'learning_rate': 4.560753314343912e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3600/5198 [20:27:37<7:42:27, 17.36s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3601/5198 [20:29:07<17:24:35, 39.25s/it]                                                         {'loss': 0.3619, 'learning_rate': 4.555525855323738e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3601/5198 [20:29:07<17:24:35, 39.25s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3602/5198 [20:29:24<14:27:22, 32.61s/it]                                                         {'loss': 0.8591, 'learning_rate': 4.5503005100358945e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3602/5198 [20:29:24<14:27:22, 32.61s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3603/5198 [20:29:41<12:19:01, 27.80s/it]                                                         {'loss': 0.845, 'learning_rate': 4.545077280509022e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3603/5198 [20:29:41<12:19:01, 27.80s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3604/5198 [20:29:58<10:54:30, 24.64s/it]                                                         {'loss': 0.8092, 'learning_rate': 4.539856168770974e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3604/5198 [20:29:58<10:54:30, 24.64s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3605/5198 [20:30:15<9:50:04, 22.23s/it]                                                         {'loss': 0.7743, 'learning_rate': 4.534637176848758e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3605/5198 [20:30:15<9:50:04, 22.23s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3606/5198 [20:30:32<9:11:00, 20.77s/it]                                                        {'loss': 0.7873, 'learning_rate': 4.52942030676857e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3606/5198 [20:30:32<9:11:00, 20.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3607/5198 [20:30:49<8:44:41, 19.79s/it]                                                        {'loss': 0.7917, 'learning_rate': 4.524205560555774e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3607/5198 [20:30:49<8:44:41, 19.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3608/5198 [20:31:07<8:26:40, 19.12s/it]                                                        {'loss': 0.7699, 'learning_rate': 4.5189929402349175e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3608/5198 [20:31:07<8:26:40, 19.12s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3609/5198 [20:31:25<8:15:33, 18.71s/it]                                                        {'loss': 0.7825, 'learning_rate': 4.513782447829717e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3609/5198 [20:31:25<8:15:33, 18.71s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3610/5198 [20:31:42<8:05:17, 18.34s/it]                                                        {'loss': 0.8253, 'learning_rate': 4.508574085363065e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3610/5198 [20:31:42<8:05:17, 18.34s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3611/5198 [20:32:00<8:00:41, 18.17s/it]                                                        {'loss': 0.8257, 'learning_rate': 4.503367854857035e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3611/5198 [20:32:00<8:00:41, 18.17s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3612/5198 [20:32:17<7:54:09, 17.94s/it]                                                        {'loss': 0.8127, 'learning_rate': 4.498163758332853e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3612/5198 [20:32:17<7:54:09, 17.94s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3613/5198 [20:32:35<7:48:47, 17.75s/it]                                                        {'loss': 0.7942, 'learning_rate': 4.492961797810932e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3613/5198 [20:32:35<7:48:47, 17.75s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3614/5198 [20:32:53<7:53:04, 17.92s/it]                                                        {'loss': 0.8089, 'learning_rate': 4.4877619753108605e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3614/5198 [20:32:53<7:53:04, 17.92s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3615/5198 [20:33:11<7:50:57, 17.85s/it]                                                        {'loss': 0.8399, 'learning_rate': 4.4825642928513746e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3615/5198 [20:33:11<7:50:57, 17.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3616/5198 [20:33:28<7:49:13, 17.80s/it]                                                        {'loss': 0.7989, 'learning_rate': 4.477368752450409e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3616/5198 [20:33:28<7:49:13, 17.80s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3617/5198 [20:33:46<7:51:47, 17.90s/it]                                                        {'loss': 0.8162, 'learning_rate': 4.472175356125036e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3617/5198 [20:33:46<7:51:47, 17.90s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3618/5198 [20:34:05<7:54:18, 18.01s/it]                                                        {'loss': 0.7636, 'learning_rate': 4.466984105891521e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3618/5198 [20:34:05<7:54:18, 18.01s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3619/5198 [20:34:21<7:39:26, 17.46s/it]                                                        {'loss': 0.3502, 'learning_rate': 4.461795003765285e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3619/5198 [20:34:21<7:39:26, 17.46s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3620/5198 [20:34:40<7:50:16, 17.88s/it]                                                        {'loss': 0.8247, 'learning_rate': 4.456608051760914e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3620/5198 [20:34:40<7:50:16, 17.88s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3621/5198 [20:34:58<7:51:33, 17.94s/it]                                                        {'loss': 0.843, 'learning_rate': 4.45142325189216e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3621/5198 [20:34:58<7:51:33, 17.94s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3622/5198 [20:35:15<7:48:46, 17.85s/it]                                                        {'loss': 0.7884, 'learning_rate': 4.446240606171945e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3622/5198 [20:35:15<7:48:46, 17.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3623/5198 [20:35:33<7:47:02, 17.79s/it]                                                        {'loss': 0.8137, 'learning_rate': 4.4410601166123475e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3623/5198 [20:35:33<7:47:02, 17.79s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3624/5198 [20:35:50<7:43:09, 17.66s/it]                                                        {'loss': 0.857, 'learning_rate': 4.4358817852246124e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3624/5198 [20:35:50<7:43:09, 17.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3625/5198 [20:36:08<7:43:43, 17.69s/it]                                                        {'loss': 0.754, 'learning_rate': 4.430705614019147e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3625/5198 [20:36:08<7:43:43, 17.69s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3626/5198 [20:37:40<17:24:10, 39.85s/it]                                                         {'loss': 0.7476, 'learning_rate': 4.425531605005519e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3626/5198 [20:37:40<17:24:10, 39.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3627/5198 [20:37:57<14:26:56, 33.11s/it]                                                         {'loss': 0.3126, 'learning_rate': 4.420359760192452e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3627/5198 [20:37:57<14:26:56, 33.11s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3628/5198 [20:38:15<12:28:53, 28.62s/it]                                                         {'loss': 0.8317, 'learning_rate': 4.4151900815878455e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3628/5198 [20:38:15<12:28:53, 28.62s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3629/5198 [20:38:34<11:08:14, 25.55s/it]                                                         {'loss': 0.7026, 'learning_rate': 4.410022571198734e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3629/5198 [20:38:34<11:08:14, 25.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3630/5198 [20:38:52<10:07:30, 23.25s/it]                                                         {'loss': 0.7844, 'learning_rate': 4.404857231031332e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3630/5198 [20:38:52<10:07:30, 23.25s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3631/5198 [20:39:10<9:31:20, 21.88s/it]                                                         {'loss': 0.8376, 'learning_rate': 4.399694063090999e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3631/5198 [20:39:10<9:31:20, 21.88s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3632/5198 [20:39:28<8:57:52, 20.61s/it]                                                        {'loss': 0.7814, 'learning_rate': 4.394533069382255e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3632/5198 [20:39:28<8:57:52, 20.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3633/5198 [20:39:46<8:34:39, 19.73s/it]                                                        {'loss': 0.3597, 'learning_rate': 4.3893742519087754e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3633/5198 [20:39:46<8:34:39, 19.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3634/5198 [20:40:03<8:16:30, 19.05s/it]                                                        {'loss': 0.8534, 'learning_rate': 4.3842176126733914e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3634/5198 [20:40:03<8:16:30, 19.05s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3635/5198 [20:40:20<8:02:47, 18.53s/it]                                                        {'loss': 0.8223, 'learning_rate': 4.379063153678087e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3635/5198 [20:40:20<8:02:47, 18.53s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3636/5198 [20:40:38<7:55:20, 18.26s/it]                                                        {'loss': 0.7611, 'learning_rate': 4.373910876923997e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3636/5198 [20:40:38<7:55:20, 18.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3637/5198 [20:40:56<7:52:50, 18.17s/it]                                                        {'loss': 0.8494, 'learning_rate': 4.368760784411423e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3637/5198 [20:40:56<7:52:50, 18.17s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3638/5198 [20:41:15<7:55:17, 18.28s/it]                                                        {'loss': 0.8088, 'learning_rate': 4.363612878139799e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3638/5198 [20:41:15<7:55:17, 18.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3639/5198 [20:41:33<7:56:57, 18.36s/it]                                                        {'loss': 0.7784, 'learning_rate': 4.3584671601077224e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3639/5198 [20:41:33<7:56:57, 18.36s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3640/5198 [20:41:51<7:55:02, 18.29s/it]                                                        {'loss': 0.7354, 'learning_rate': 4.353323632312938e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3640/5198 [20:41:51<7:55:02, 18.29s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3641/5198 [20:42:09<7:49:22, 18.09s/it]                                                        {'loss': 0.7666, 'learning_rate': 4.348182296752336e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3641/5198 [20:42:09<7:49:22, 18.09s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3642/5198 [20:42:27<7:46:41, 18.00s/it]                                                        {'loss': 0.7863, 'learning_rate': 4.343043155421971e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3642/5198 [20:42:27<7:46:41, 18.00s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3643/5198 [20:42:44<7:40:20, 17.76s/it]                                                        {'loss': 0.8166, 'learning_rate': 4.3379062103170214e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3643/5198 [20:42:44<7:40:20, 17.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3644/5198 [20:43:01<7:39:25, 17.74s/it]                                                        {'loss': 0.8043, 'learning_rate': 4.332771463431837e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3644/5198 [20:43:01<7:39:25, 17.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3645/5198 [20:43:19<7:37:46, 17.69s/it]                                                        {'loss': 0.7812, 'learning_rate': 4.327638916759898e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3645/5198 [20:43:19<7:37:46, 17.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3646/5198 [20:43:36<7:32:34, 17.50s/it]                                                        {'loss': 0.8241, 'learning_rate': 4.322508572293836e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3646/5198 [20:43:36<7:32:34, 17.50s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3647/5198 [20:43:54<7:36:59, 17.68s/it]                                                        {'loss': 0.8015, 'learning_rate': 4.317380432025428e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3647/5198 [20:43:54<7:36:59, 17.68s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3648/5198 [20:44:12<7:37:51, 17.72s/it]                                                        {'loss': 0.806, 'learning_rate': 4.312254497945595e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3648/5198 [20:44:12<7:37:51, 17.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3649/5198 [20:44:30<7:39:41, 17.81s/it]                                                        {'loss': 0.7664, 'learning_rate': 4.3071307720444015e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3649/5198 [20:44:30<7:39:41, 17.81s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3650/5198 [20:44:47<7:34:25, 17.61s/it]                                                        {'loss': 0.8633, 'learning_rate': 4.3020092563110485e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3650/5198 [20:44:47<7:34:25, 17.61s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3651/5198 [20:46:20<17:16:23, 40.20s/it]                                                         {'loss': 0.7454, 'learning_rate': 4.2968899527338984e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3651/5198 [20:46:20<17:16:23, 40.20s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3652/5198 [20:46:38<14:24:08, 33.54s/it]                                                         {'loss': 0.775, 'learning_rate': 4.291772863300428e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3652/5198 [20:46:38<14:24:08, 33.54s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3653/5198 [20:46:54<12:10:10, 28.36s/it]                                                         {'loss': 0.7777, 'learning_rate': 4.2866579899972686e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3653/5198 [20:46:54<12:10:10, 28.36s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3654/5198 [20:47:12<10:45:00, 25.06s/it]                                                         {'loss': 0.3396, 'learning_rate': 4.281545334810201e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3654/5198 [20:47:12<10:45:00, 25.06s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3655/5198 [20:47:29<9:42:38, 22.66s/it]                                                         {'loss': 0.8276, 'learning_rate': 4.276434899724119e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3655/5198 [20:47:29<9:42:38, 22.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3656/5198 [20:47:46<9:03:14, 21.14s/it]                                                        {'loss': 0.777, 'learning_rate': 4.27132668672308e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3656/5198 [20:47:46<9:03:14, 21.14s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3657/5198 [20:48:04<8:32:13, 19.94s/it]                                                        {'loss': 0.7888, 'learning_rate': 4.266220697790266e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3657/5198 [20:48:04<8:32:13, 19.94s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3658/5198 [20:48:21<8:15:59, 19.32s/it]                                                        {'loss': 0.8077, 'learning_rate': 4.2611169349079985e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3658/5198 [20:48:21<8:15:59, 19.32s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3659/5198 [20:48:38<7:54:08, 18.49s/it]                                                        {'loss': 0.7753, 'learning_rate': 4.25601540005773e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3659/5198 [20:48:38<7:54:08, 18.49s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3660/5198 [20:48:56<7:50:17, 18.35s/it]                                                        {'loss': 0.3462, 'learning_rate': 4.250916095220056e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3660/5198 [20:48:56<7:50:17, 18.35s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3661/5198 [20:49:14<7:50:42, 18.38s/it]                                                        {'loss': 0.7927, 'learning_rate': 4.2458190223747e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3661/5198 [20:49:14<7:50:42, 18.38s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3662/5198 [20:49:31<7:40:08, 17.97s/it]                                                        {'loss': 0.7896, 'learning_rate': 4.240724183500518e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3662/5198 [20:49:31<7:40:08, 17.97s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3663/5198 [20:49:48<7:29:46, 17.58s/it]                                                        {'loss': 0.7953, 'learning_rate': 4.2356315805755135e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3663/5198 [20:49:48<7:29:46, 17.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3664/5198 [20:50:06<7:31:16, 17.65s/it]                                                        {'loss': 0.7769, 'learning_rate': 4.230541215576798e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3664/5198 [20:50:06<7:31:16, 17.65s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3665/5198 [20:50:24<7:35:55, 17.84s/it]                                                        {'loss': 0.7736, 'learning_rate': 4.225453090480631e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3665/5198 [20:50:24<7:35:55, 17.84s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3666/5198 [20:50:42<7:38:57, 17.97s/it]                                                        {'loss': 0.7428, 'learning_rate': 4.220367207262398e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3666/5198 [20:50:43<7:38:57, 17.97s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3667/5198 [20:51:01<7:39:41, 18.02s/it]                                                        {'loss': 0.8522, 'learning_rate': 4.21528356789661e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3667/5198 [20:51:01<7:39:41, 18.02s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3668/5198 [20:51:19<7:40:33, 18.06s/it]                                                        {'loss': 0.8114, 'learning_rate': 4.210202174356922e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3668/5198 [20:51:19<7:40:33, 18.06s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3669/5198 [20:51:37<7:42:46, 18.16s/it]                                                        {'loss': 0.7729, 'learning_rate': 4.20512302861609e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3669/5198 [20:51:37<7:42:46, 18.16s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3670/5198 [20:51:55<7:41:52, 18.14s/it]                                                        {'loss': 0.7775, 'learning_rate': 4.2000461326460274e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3670/5198 [20:51:55<7:41:52, 18.14s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3671/5198 [20:52:14<7:46:00, 18.31s/it]                                                        {'loss': 0.7572, 'learning_rate': 4.194971488417753e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3671/5198 [20:52:14<7:46:00, 18.31s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3672/5198 [20:52:33<7:52:36, 18.58s/it]                                                        {'loss': 0.6902, 'learning_rate': 4.189899097901421e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3672/5198 [20:52:33<7:52:36, 18.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3673/5198 [20:52:50<7:42:34, 18.20s/it]                                                        {'loss': 0.7642, 'learning_rate': 4.184828963066305e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3673/5198 [20:52:50<7:42:34, 18.20s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3674/5198 [20:53:07<7:32:07, 17.80s/it]                                                        {'loss': 0.3409, 'learning_rate': 4.179761085880809e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3674/5198 [20:53:07<7:32:07, 17.80s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3675/5198 [20:53:24<7:26:28, 17.59s/it]                                                        {'loss': 0.7849, 'learning_rate': 4.174695468312456e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3675/5198 [20:53:24<7:26:28, 17.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3676/5198 [20:54:53<16:26:27, 38.89s/it]                                                         {'loss': 0.767, 'learning_rate': 4.16963211232789e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3676/5198 [20:54:53<16:26:27, 38.89s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3677/5198 [20:55:11<13:47:21, 32.64s/it]                                                         {'loss': 0.744, 'learning_rate': 4.16457101989289e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3677/5198 [20:55:11<13:47:21, 32.64s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3678/5198 [20:55:29<11:54:52, 28.22s/it]                                                         {'loss': 0.8369, 'learning_rate': 4.159512192972337e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3678/5198 [20:55:29<11:54:52, 28.22s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3679/5198 [20:55:46<10:30:16, 24.90s/it]                                                         {'loss': 0.8043, 'learning_rate': 4.15445563353024e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3679/5198 [20:55:46<10:30:16, 24.90s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3680/5198 [20:56:04<9:35:31, 22.75s/it]                                                         {'loss': 0.7348, 'learning_rate': 4.149401343529742e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3680/5198 [20:56:04<9:35:31, 22.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3681/5198 [20:56:22<8:59:14, 21.33s/it]                                                        {'loss': 0.7448, 'learning_rate': 4.144349324933077e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3681/5198 [20:56:22<8:59:14, 21.33s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3682/5198 [20:56:40<8:36:27, 20.44s/it]                                                        {'loss': 0.754, 'learning_rate': 4.139299579701623e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3682/5198 [20:56:40<8:36:27, 20.44s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3683/5198 [20:56:58<8:17:11, 19.69s/it]                                                        {'loss': 0.8513, 'learning_rate': 4.134252109795863e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3683/5198 [20:56:58<8:17:11, 19.69s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3684/5198 [20:57:16<8:03:44, 19.17s/it]                                                        {'loss': 0.8007, 'learning_rate': 4.129206917175397e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3684/5198 [20:57:16<8:03:44, 19.17s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3685/5198 [20:57:33<7:47:05, 18.52s/it]                                                        {'loss': 0.751, 'learning_rate': 4.124164003798944e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3685/5198 [20:57:33<7:47:05, 18.52s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3686/5198 [20:57:51<7:41:35, 18.32s/it]                                                        {'loss': 0.7772, 'learning_rate': 4.119123371624335e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3686/5198 [20:57:51<7:41:35, 18.32s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3687/5198 [20:58:07<7:26:34, 17.73s/it]                                                        {'loss': 0.8146, 'learning_rate': 4.114085022608517e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3687/5198 [20:58:07<7:26:34, 17.73s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3688/5198 [20:58:24<7:20:23, 17.50s/it]                                                        {'loss': 0.7305, 'learning_rate': 4.109048958707552e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3688/5198 [20:58:24<7:20:23, 17.50s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3689/5198 [20:58:41<7:14:51, 17.29s/it]                                                        {'loss': 0.7571, 'learning_rate': 4.104015181876613e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3689/5198 [20:58:41<7:14:51, 17.29s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3690/5198 [20:58:59<7:18:04, 17.43s/it]                                                        {'loss': 0.8091, 'learning_rate': 4.09898369406998e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3690/5198 [20:58:59<7:18:04, 17.43s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3691/5198 [20:59:16<7:15:34, 17.34s/it]                                                        {'loss': 0.7949, 'learning_rate': 4.0939544972410636e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3691/5198 [20:59:16<7:15:34, 17.34s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3692/5198 [20:59:33<7:11:53, 17.21s/it]                                                        {'loss': 0.8317, 'learning_rate': 4.0889275933423576e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3692/5198 [20:59:33<7:11:53, 17.21s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3693/5198 [20:59:51<7:14:42, 17.33s/it]                                                        {'loss': 0.8333, 'learning_rate': 4.0839029843254815e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3693/5198 [20:59:51<7:14:42, 17.33s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3694/5198 [21:00:09<7:21:55, 17.63s/it]                                                        {'loss': 0.7232, 'learning_rate': 4.078880672141171e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3694/5198 [21:00:09<7:21:55, 17.63s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3695/5198 [21:00:27<7:26:13, 17.81s/it]                                                        {'loss': 0.8202, 'learning_rate': 4.073860658739246e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3695/5198 [21:00:27<7:26:13, 17.81s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3696/5198 [21:00:45<7:23:38, 17.72s/it]                                                        {'loss': 0.8414, 'learning_rate': 4.068842946068661e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3696/5198 [21:00:45<7:23:38, 17.72s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3697/5198 [21:01:02<7:18:34, 17.53s/it]                                                        {'loss': 0.7333, 'learning_rate': 4.063827536077459e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3697/5198 [21:01:02<7:18:34, 17.53s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3698/5198 [21:01:19<7:16:49, 17.47s/it]                                                        {'loss': 0.8314, 'learning_rate': 4.058814430712796e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3698/5198 [21:01:19<7:16:49, 17.47s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3699/5198 [21:01:38<7:26:34, 17.87s/it]                                                        {'loss': 0.7816, 'learning_rate': 4.0538036319209325e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3699/5198 [21:01:38<7:26:34, 17.87s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3700/5198 [21:01:56<7:26:04, 17.87s/it]                                                        {'loss': 0.7704, 'learning_rate': 4.0487951416472324e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3700/5198 [21:01:56<7:26:04, 17.87s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3701/5198 [21:03:22<15:59:59, 38.48s/it]                                                         {'loss': 0.8037, 'learning_rate': 4.043788961836164e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3701/5198 [21:03:22<15:59:59, 38.48s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3702/5198 [21:03:40<13:24:52, 32.28s/it]                                                         {'loss': 0.8242, 'learning_rate': 4.038785094431295e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3702/5198 [21:03:40<13:24:52, 32.28s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3703/5198 [21:03:58<11:35:52, 27.93s/it]                                                         {'loss': 0.8144, 'learning_rate': 4.0337835413753116e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3703/5198 [21:03:58<11:35:52, 27.93s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3704/5198 [21:04:15<10:13:38, 24.64s/it]                                                         {'loss': 0.8297, 'learning_rate': 4.0287843046099765e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3704/5198 [21:04:15<10:13:38, 24.64s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3705/5198 [21:04:33<9:22:31, 22.61s/it]                                                         {'loss': 0.7822, 'learning_rate': 4.0237873860761645e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3705/5198 [21:04:33<9:22:31, 22.61s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3706/5198 [21:04:50<8:44:18, 21.08s/it]                                                        {'loss': 0.7951, 'learning_rate': 4.018792787713865e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3706/5198 [21:04:50<8:44:18, 21.08s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3707/5198 [21:05:08<8:18:21, 20.05s/it]                                                        {'loss': 0.7906, 'learning_rate': 4.013800511462135e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3707/5198 [21:05:08<8:18:21, 20.05s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3708/5198 [21:05:26<8:02:08, 19.42s/it]                                                        {'loss': 0.7755, 'learning_rate': 4.008810559259162e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3708/5198 [21:05:26<8:02:08, 19.42s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3709/5198 [21:05:44<7:51:27, 19.00s/it]                                                        {'loss': 0.7719, 'learning_rate': 4.003822933042213e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3709/5198 [21:05:44<7:51:27, 19.00s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3710/5198 [21:06:03<7:49:18, 18.92s/it]                                                        {'loss': 0.7847, 'learning_rate': 3.998837634747655e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3710/5198 [21:06:03<7:49:18, 18.92s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3711/5198 [21:06:19<7:33:37, 18.30s/it]                                                        {'loss': 0.8511, 'learning_rate': 3.993854666310955e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3711/5198 [21:06:19<7:33:37, 18.30s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3712/5198 [21:06:37<7:29:01, 18.13s/it]                                                        {'loss': 0.8071, 'learning_rate': 3.98887402966667e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3712/5198 [21:06:37<7:29:01, 18.13s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3713/5198 [21:06:55<7:25:11, 17.99s/it]                                                        {'loss': 0.7879, 'learning_rate': 3.983895726748455e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3713/5198 [21:06:55<7:25:11, 17.99s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3714/5198 [21:07:12<7:21:56, 17.87s/it]                                                        {'loss': 0.8403, 'learning_rate': 3.97891975948906e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3714/5198 [21:07:12<7:21:56, 17.87s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3715/5198 [21:07:30<7:22:24, 17.90s/it]                                                        {'loss': 0.8281, 'learning_rate': 3.973946129820326e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3715/5198 [21:07:30<7:22:24, 17.90s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3716/5198 [21:07:48<7:20:16, 17.82s/it]                                                        {'loss': 0.8599, 'learning_rate': 3.968974839673186e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3716/5198 [21:07:48<7:20:16, 17.82s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3717/5198 [21:08:05<7:16:58, 17.70s/it]                                                        {'loss': 0.7901, 'learning_rate': 3.964005890977672e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3717/5198 [21:08:05<7:16:58, 17.70s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3718/5198 [21:08:23<7:12:01, 17.51s/it]                                                        {'loss': 0.7873, 'learning_rate': 3.9590392856628946e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3718/5198 [21:08:23<7:12:01, 17.51s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3719/5198 [21:08:40<7:08:50, 17.40s/it]                                                        {'loss': 0.8585, 'learning_rate': 3.954075025657058e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3719/5198 [21:08:40<7:08:50, 17.40s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3720/5198 [21:08:58<7:19:18, 17.83s/it]                                                        {'loss': 0.7947, 'learning_rate': 3.949113112887471e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3720/5198 [21:08:59<7:19:18, 17.83s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3721/5198 [21:09:16<7:15:25, 17.69s/it]                                                        {'loss': 0.8075, 'learning_rate': 3.944153549280506e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3721/5198 [21:09:16<7:15:25, 17.69s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3722/5198 [21:09:34<7:16:58, 17.76s/it]                                                        {'loss': 0.8419, 'learning_rate': 3.939196336761645e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3722/5198 [21:09:34<7:16:58, 17.76s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3723/5198 [21:09:51<7:11:44, 17.56s/it]                                                        {'loss': 0.844, 'learning_rate': 3.934241477255445e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3723/5198 [21:09:51<7:11:44, 17.56s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3724/5198 [21:10:07<7:03:41, 17.25s/it]                                                        {'loss': 0.8265, 'learning_rate': 3.929288972685555e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3724/5198 [21:10:07<7:03:41, 17.25s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3725/5198 [21:10:26<7:12:56, 17.64s/it]                                                        {'loss': 0.8258, 'learning_rate': 3.924338824974705e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3725/5198 [21:10:26<7:12:56, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3726/5198 [21:11:53<15:42:41, 38.42s/it]                                                         {'loss': 0.7525, 'learning_rate': 3.919391036044715e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3726/5198 [21:11:53<15:42:41, 38.42s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3727/5198 [21:12:10<13:07:16, 32.11s/it]                                                         {'loss': 0.7781, 'learning_rate': 3.914445607816486e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3727/5198 [21:12:10<13:07:16, 32.11s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3728/5198 [21:12:28<11:22:09, 27.84s/it]                                                         {'loss': 0.7866, 'learning_rate': 3.909502542210001e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3728/5198 [21:12:28<11:22:09, 27.84s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3729/5198 [21:12:46<10:09:08, 24.88s/it]                                                         {'loss': 0.7696, 'learning_rate': 3.904561841144338e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3729/5198 [21:12:46<10:09:08, 24.88s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3730/5198 [21:13:04<9:17:31, 22.79s/it]                                                         {'loss': 0.7546, 'learning_rate': 3.899623506537635e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3730/5198 [21:13:04<9:17:31, 22.79s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3731/5198 [21:13:22<8:41:21, 21.32s/it]                                                        {'loss': 0.349, 'learning_rate': 3.894687540307127e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3731/5198 [21:13:22<8:41:21, 21.32s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3732/5198 [21:13:40<8:18:15, 20.39s/it]                                                        {'loss': 0.8234, 'learning_rate': 3.8897539443691355e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3732/5198 [21:13:40<8:18:15, 20.39s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3733/5198 [21:13:58<8:01:16, 19.71s/it]                                                        {'loss': 0.8015, 'learning_rate': 3.884822720639036e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3733/5198 [21:13:58<8:01:16, 19.71s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3734/5198 [21:14:16<7:47:21, 19.15s/it]                                                        {'loss': 0.8378, 'learning_rate': 3.879893871031314e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3734/5198 [21:14:16<7:47:21, 19.15s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3735/5198 [21:14:33<7:34:00, 18.62s/it]                                                        {'loss': 0.8267, 'learning_rate': 3.874967397459511e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3735/5198 [21:14:33<7:34:00, 18.62s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3736/5198 [21:14:51<7:24:26, 18.24s/it]                                                        {'loss': 0.7833, 'learning_rate': 3.870043301836256e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3736/5198 [21:14:51<7:24:26, 18.24s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3737/5198 [21:15:08<7:13:39, 17.81s/it]                                                        {'loss': 0.7375, 'learning_rate': 3.86512158607325e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3737/5198 [21:15:08<7:13:39, 17.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3738/5198 [21:15:26<7:17:40, 17.99s/it]                                                        {'loss': 0.7871, 'learning_rate': 3.860202252081276e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3738/5198 [21:15:26<7:17:40, 17.99s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3739/5198 [21:15:43<7:12:55, 17.80s/it]                                                        {'loss': 0.7724, 'learning_rate': 3.855285301770188e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3739/5198 [21:15:43<7:12:55, 17.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3740/5198 [21:16:01<7:14:01, 17.86s/it]                                                        {'loss': 0.8718, 'learning_rate': 3.850370737048913e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3740/5198 [21:16:01<7:14:01, 17.86s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3741/5198 [21:16:19<7:13:35, 17.86s/it]                                                        {'loss': 0.7714, 'learning_rate': 3.8454585598254565e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3741/5198 [21:16:19<7:13:35, 17.86s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3742/5198 [21:16:37<7:14:16, 17.90s/it]                                                        {'loss': 0.8235, 'learning_rate': 3.840548772006891e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3742/5198 [21:16:37<7:14:16, 17.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3743/5198 [21:16:56<7:18:10, 18.07s/it]                                                        {'loss': 0.8403, 'learning_rate': 3.835641375499375e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3743/5198 [21:16:56<7:18:10, 18.07s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3744/5198 [21:17:13<7:15:24, 17.97s/it]                                                        {'loss': 0.813, 'learning_rate': 3.830736372208118e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3744/5198 [21:17:13<7:15:24, 17.97s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3745/5198 [21:17:32<7:16:27, 18.02s/it]                                                        {'loss': 0.7599, 'learning_rate': 3.8258337640374125e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3745/5198 [21:17:32<7:16:27, 18.02s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3746/5198 [21:17:49<7:10:48, 17.80s/it]                                                        {'loss': 0.3541, 'learning_rate': 3.820933552890629e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3746/5198 [21:17:49<7:10:48, 17.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3747/5198 [21:18:07<7:10:01, 17.78s/it]                                                        {'loss': 0.8043, 'learning_rate': 3.816035740670185e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3747/5198 [21:18:07<7:10:01, 17.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3748/5198 [21:18:24<7:10:26, 17.81s/it]                                                        {'loss': 0.8057, 'learning_rate': 3.811140329277591e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3748/5198 [21:18:24<7:10:26, 17.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3749/5198 [21:18:42<7:09:53, 17.80s/it]                                                        {'loss': 0.7926, 'learning_rate': 3.8062473206134088e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3749/5198 [21:18:42<7:09:53, 17.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3750/5198 [21:19:01<7:13:14, 17.95s/it]                                                        {'loss': 0.8132, 'learning_rate': 3.8013567165772735e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3750/5198 [21:19:01<7:13:14, 17.95s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3751/5198 [21:20:22<14:54:43, 37.10s/it]                                                         {'loss': 0.7945, 'learning_rate': 3.7964685190678874e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3751/5198 [21:20:22<14:54:43, 37.10s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3752/5198 [21:20:40<12:35:30, 31.35s/it]                                                         {'loss': 0.8078, 'learning_rate': 3.7915827299830154e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3752/5198 [21:20:40<12:35:30, 31.35s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3753/5198 [21:20:59<11:01:48, 27.48s/it]                                                         {'loss': 0.815, 'learning_rate': 3.7866993512194895e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3753/5198 [21:20:59<11:01:48, 27.48s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3754/5198 [21:21:16<9:45:55, 24.35s/it]                                                         {'loss': 0.7514, 'learning_rate': 3.7818183846732024e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3754/5198 [21:21:16<9:45:55, 24.35s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3755/5198 [21:21:34<8:58:40, 22.40s/it]                                                        {'loss': 0.839, 'learning_rate': 3.776939832239125e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3755/5198 [21:21:34<8:58:40, 22.40s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3756/5198 [21:21:50<8:17:22, 20.70s/it]                                                        {'loss': 0.8514, 'learning_rate': 3.7720636958112623e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3756/5198 [21:21:50<8:17:22, 20.70s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3757/5198 [21:22:08<7:57:42, 19.89s/it]                                                        {'loss': 0.8003, 'learning_rate': 3.7671899772827113e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3757/5198 [21:22:08<7:57:42, 19.89s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3758/5198 [21:22:26<7:44:32, 19.36s/it]                                                        {'loss': 0.834, 'learning_rate': 3.7623186785456156e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3758/5198 [21:22:26<7:44:32, 19.36s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3759/5198 [21:22:45<7:37:42, 19.08s/it]                                                        {'loss': 0.7987, 'learning_rate': 3.757449801491172e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3759/5198 [21:22:45<7:37:42, 19.08s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3760/5198 [21:23:03<7:27:04, 18.65s/it]                                                        {'loss': 0.3272, 'learning_rate': 3.7525833480096575e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3760/5198 [21:23:03<7:27:04, 18.65s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3761/5198 [21:23:20<7:16:17, 18.22s/it]                                                        {'loss': 0.8022, 'learning_rate': 3.7477193199903903e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3761/5198 [21:23:20<7:16:17, 18.22s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3762/5198 [21:23:38<7:15:55, 18.21s/it]                                                        {'loss': 0.7985, 'learning_rate': 3.7428577193217563e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3762/5198 [21:23:38<7:15:55, 18.21s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3763/5198 [21:23:55<7:10:15, 17.99s/it]                                                        {'loss': 0.7937, 'learning_rate': 3.737998547891195e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3763/5198 [21:23:55<7:10:15, 17.99s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3764/5198 [21:24:13<7:03:49, 17.73s/it]                                                        {'loss': 0.8095, 'learning_rate': 3.7331418075852053e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3764/5198 [21:24:13<7:03:49, 17.73s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3765/5198 [21:24:31<7:06:34, 17.86s/it]                                                        {'loss': 0.8097, 'learning_rate': 3.728287500289339e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3765/5198 [21:24:31<7:06:34, 17.86s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3766/5198 [21:24:48<6:59:26, 17.57s/it]                                                        {'loss': 0.8126, 'learning_rate': 3.7234356278882076e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3766/5198 [21:24:48<6:59:26, 17.57s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3767/5198 [21:25:06<7:06:12, 17.87s/it]                                                        {'loss': 0.8527, 'learning_rate': 3.718586192265473e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3767/5198 [21:25:06<7:06:12, 17.87s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3768/5198 [21:25:24<7:02:29, 17.73s/it]                                                        {'loss': 0.8273, 'learning_rate': 3.7137391953038516e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3768/5198 [21:25:24<7:02:29, 17.73s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3769/5198 [21:25:41<7:02:19, 17.73s/it]                                                        {'loss': 0.328, 'learning_rate': 3.7088946388851223e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3769/5198 [21:25:41<7:02:19, 17.73s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3770/5198 [21:25:59<7:01:42, 17.72s/it]                                                        {'loss': 0.8109, 'learning_rate': 3.7040525248901003e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3770/5198 [21:25:59<7:01:42, 17.72s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3771/5198 [21:26:17<7:02:34, 17.77s/it]                                                        {'loss': 0.7846, 'learning_rate': 3.6992128551986617e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3771/5198 [21:26:17<7:02:34, 17.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3772/5198 [21:26:35<7:02:01, 17.76s/it]                                                        {'loss': 0.7712, 'learning_rate': 3.6943756316897406e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3772/5198 [21:26:35<7:02:01, 17.76s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3773/5198 [21:26:53<7:05:12, 17.90s/it]                                                        {'loss': 0.7736, 'learning_rate': 3.6895408562413027e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3773/5198 [21:26:53<7:05:12, 17.90s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3774/5198 [21:27:10<7:02:33, 17.80s/it]                                                        {'loss': 0.8061, 'learning_rate': 3.684708530730382e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3774/5198 [21:27:11<7:02:33, 17.80s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3775/5198 [21:27:29<7:05:22, 17.94s/it]                                                        {'loss': 0.7402, 'learning_rate': 3.6798786570330526e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3775/5198 [21:27:29<7:05:22, 17.94s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3776/5198 [21:28:52<14:50:55, 37.59s/it]                                                         {'loss': 0.7815, 'learning_rate': 3.6750512370244363e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3776/5198 [21:28:52<14:50:55, 37.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3777/5198 [21:29:10<12:32:38, 31.78s/it]                                                         {'loss': 0.8356, 'learning_rate': 3.670226272578704e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3777/5198 [21:29:10<12:32:38, 31.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3778/5198 [21:29:28<10:50:45, 27.50s/it]                                                         {'loss': 0.8482, 'learning_rate': 3.6654037655690732e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3778/5198 [21:29:28<10:50:45, 27.50s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3779/5198 [21:29:46<9:43:55, 24.69s/it]                                                         {'loss': 0.8033, 'learning_rate': 3.660583717867807e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3779/5198 [21:29:46<9:43:55, 24.69s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3780/5198 [21:30:04<8:58:27, 22.78s/it]                                                        {'loss': 0.825, 'learning_rate': 3.655766131346211e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3780/5198 [21:30:04<8:58:27, 22.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3781/5198 [21:30:22<8:25:12, 21.39s/it]                                                        {'loss': 0.7737, 'learning_rate': 3.650951007874648e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3781/5198 [21:30:22<8:25:12, 21.39s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3782/5198 [21:30:41<8:06:24, 20.61s/it]                                                        {'loss': 0.8439, 'learning_rate': 3.6461383493225012e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3782/5198 [21:30:41<8:06:24, 20.61s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3783/5198 [21:30:59<7:45:13, 19.73s/it]                                                        {'loss': 0.7605, 'learning_rate': 3.6413281575582194e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3783/5198 [21:30:59<7:45:13, 19.73s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3784/5198 [21:31:16<7:26:37, 18.95s/it]                                                        {'loss': 0.8192, 'learning_rate': 3.6365204344492867e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3784/5198 [21:31:16<7:26:37, 18.95s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3785/5198 [21:31:34<7:17:33, 18.58s/it]                                                        {'loss': 0.7999, 'learning_rate': 3.6317151818622154e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3785/5198 [21:31:34<7:17:33, 18.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3786/5198 [21:31:52<7:16:27, 18.55s/it]                                                        {'loss': 0.7795, 'learning_rate': 3.62691240166258e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3786/5198 [21:31:52<7:16:27, 18.55s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3787/5198 [21:32:10<7:09:06, 18.25s/it]                                                        {'loss': 0.8584, 'learning_rate': 3.6221120957149826e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3787/5198 [21:32:10<7:09:06, 18.25s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3788/5198 [21:32:27<7:04:16, 18.05s/it]                                                        {'loss': 0.7945, 'learning_rate': 3.617314265883066e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3788/5198 [21:32:27<7:04:16, 18.05s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3789/5198 [21:32:45<7:01:32, 17.95s/it]                                                        {'loss': 0.8371, 'learning_rate': 3.612518914029515e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3789/5198 [21:32:45<7:01:32, 17.95s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3790/5198 [21:33:03<6:58:36, 17.84s/it]                                                        {'loss': 0.8637, 'learning_rate': 3.6077260420160487e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3790/5198 [21:33:03<6:58:36, 17.84s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3791/5198 [21:33:20<6:56:08, 17.75s/it]                                                        {'loss': 0.7645, 'learning_rate': 3.602935651703424e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3791/5198 [21:33:20<6:56:08, 17.75s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3792/5198 [21:33:37<6:50:01, 17.50s/it]                                                        {'loss': 0.786, 'learning_rate': 3.598147744951438e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3792/5198 [21:33:37<6:50:01, 17.50s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3793/5198 [21:33:56<6:58:37, 17.88s/it]                                                        {'loss': 0.7518, 'learning_rate': 3.5933623236189198e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3793/5198 [21:33:56<6:58:37, 17.88s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3794/5198 [21:34:14<7:02:03, 18.04s/it]                                                        {'loss': 0.84, 'learning_rate': 3.58857938956373e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3794/5198 [21:34:14<7:02:03, 18.04s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3795/5198 [21:34:32<6:56:52, 17.83s/it]                                                        {'loss': 0.8302, 'learning_rate': 3.58379894464278e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3795/5198 [21:34:32<6:56:52, 17.83s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3796/5198 [21:34:49<6:54:31, 17.74s/it]                                                        {'loss': 0.786, 'learning_rate': 3.57902099071199e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3796/5198 [21:34:49<6:54:31, 17.74s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3797/5198 [21:35:07<6:54:30, 17.75s/it]                                                        {'loss': 0.7971, 'learning_rate': 3.5742455296263346e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3797/5198 [21:35:07<6:54:30, 17.75s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3798/5198 [21:35:25<6:53:47, 17.73s/it]                                                        {'loss': 0.7822, 'learning_rate': 3.569472563239814e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3798/5198 [21:35:25<6:53:47, 17.73s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3799/5198 [21:35:43<6:56:40, 17.87s/it]                                                        {'loss': 0.8084, 'learning_rate': 3.5647020934054465e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3799/5198 [21:35:43<6:56:40, 17.87s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3800/5198 [21:36:01<6:54:51, 17.81s/it]                                                        {'loss': 0.8226, 'learning_rate': 3.559934121975304e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3800/5198 [21:36:01<6:54:51, 17.81s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3801/5198 [21:37:27<14:53:43, 38.38s/it]                                                         {'loss': 0.8187, 'learning_rate': 3.5551686508004735e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3801/5198 [21:37:27<14:53:43, 38.38s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3802/5198 [21:37:44<12:27:48, 32.14s/it]                                                         {'loss': 0.8793, 'learning_rate': 3.550405681731074e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3802/5198 [21:37:45<12:27:48, 32.14s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3803/5198 [21:38:03<10:53:17, 28.10s/it]                                                         {'loss': 0.8037, 'learning_rate': 3.5456452166162547e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3803/5198 [21:38:03<10:53:17, 28.10s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3804/5198 [21:38:20<9:36:47, 24.83s/it]                                                         {'loss': 0.8293, 'learning_rate': 3.540887257304193e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3804/5198 [21:38:20<9:36:47, 24.83s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3805/5198 [21:38:38<8:42:58, 22.53s/it]                                                        {'loss': 0.7689, 'learning_rate': 3.5361318056420925e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3805/5198 [21:38:38<8:42:58, 22.53s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3806/5198 [21:38:56<8:14:27, 21.31s/it]                                                        {'loss': 0.7401, 'learning_rate': 3.531378863476178e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3806/5198 [21:38:56<8:14:27, 21.31s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3807/5198 [21:39:14<7:50:17, 20.29s/it]                                                        {'loss': 0.7782, 'learning_rate': 3.5266284326517165e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3807/5198 [21:39:14<7:50:17, 20.29s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3808/5198 [21:39:32<7:36:25, 19.70s/it]                                                        {'loss': 0.8019, 'learning_rate': 3.5218805150129755e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3808/5198 [21:39:32<7:36:25, 19.70s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3809/5198 [21:39:50<7:23:58, 19.18s/it]                                                        {'loss': 0.355, 'learning_rate': 3.5171351124032703e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3809/5198 [21:39:50<7:23:58, 19.18s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3810/5198 [21:40:08<7:17:09, 18.90s/it]                                                        {'loss': 0.7857, 'learning_rate': 3.51239222666493e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3810/5198 [21:40:08<7:17:09, 18.90s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3811/5198 [21:40:25<7:03:17, 18.31s/it]                                                        {'loss': 0.807, 'learning_rate': 3.507651859639295e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3811/5198 [21:40:25<7:03:17, 18.31s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3812/5198 [21:40:44<7:08:43, 18.56s/it]                                                        {'loss': 0.8024, 'learning_rate': 3.5029140131667493e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3812/5198 [21:40:45<7:08:43, 18.56s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3813/5198 [21:41:02<7:01:54, 18.28s/it]                                                        {'loss': 0.8081, 'learning_rate': 3.4981786890866853e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3813/5198 [21:41:02<7:01:54, 18.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3814/5198 [21:41:20<6:55:34, 18.02s/it]                                                        {'loss': 0.3221, 'learning_rate': 3.493445889237518e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3814/5198 [21:41:20<6:55:34, 18.02s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3815/5198 [21:41:37<6:50:56, 17.83s/it]                                                        {'loss': 0.742, 'learning_rate': 3.4887156154566847e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3815/5198 [21:41:37<6:50:56, 17.83s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3816/5198 [21:41:55<6:54:52, 18.01s/it]                                                        {'loss': 0.7873, 'learning_rate': 3.4839878695806385e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3816/5198 [21:41:55<6:54:52, 18.01s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3817/5198 [21:42:13<6:52:34, 17.93s/it]                                                        {'loss': 0.7762, 'learning_rate': 3.4792626534448547e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3817/5198 [21:42:13<6:52:34, 17.93s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3818/5198 [21:42:31<6:52:16, 17.93s/it]                                                        {'loss': 0.7911, 'learning_rate': 3.4745399688838243e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3818/5198 [21:42:31<6:52:16, 17.93s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3819/5198 [21:42:48<6:48:16, 17.76s/it]                                                        {'loss': 0.829, 'learning_rate': 3.469819817731056e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3819/5198 [21:42:48<6:48:16, 17.76s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3820/5198 [21:43:06<6:45:56, 17.68s/it]                                                        {'loss': 0.3519, 'learning_rate': 3.4651022018190715e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3820/5198 [21:43:06<6:45:56, 17.68s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3821/5198 [21:43:23<6:42:55, 17.56s/it]                                                        {'loss': 0.7579, 'learning_rate': 3.460387122979423e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3821/5198 [21:43:23<6:42:55, 17.56s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3822/5198 [21:43:41<6:43:20, 17.59s/it]                                                        {'loss': 0.3419, 'learning_rate': 3.455674583042652e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3822/5198 [21:43:41<6:43:20, 17.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3823/5198 [21:43:58<6:42:57, 17.58s/it]                                                        {'loss': 0.7393, 'learning_rate': 3.4509645838383386e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3823/5198 [21:43:58<6:42:57, 17.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3824/5198 [21:44:16<6:44:50, 17.68s/it]                                                        {'loss': 0.7344, 'learning_rate': 3.4462571271950674e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3824/5198 [21:44:16<6:44:50, 17.68s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3825/5198 [21:44:33<6:40:45, 17.51s/it]                                                        {'loss': 0.7463, 'learning_rate': 3.4415522149404233e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3825/5198 [21:44:33<6:40:45, 17.51s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3826/5198 [21:45:58<14:21:43, 37.68s/it]                                                         {'loss': 0.8484, 'learning_rate': 3.436849848901028e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3826/5198 [21:45:58<14:21:43, 37.68s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3827/5198 [21:46:16<12:05:54, 31.77s/it]                                                         {'loss': 0.8015, 'learning_rate': 3.432150030902497e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3827/5198 [21:46:16<12:05:54, 31.77s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3828/5198 [21:46:34<10:32:44, 27.71s/it]                                                         {'loss': 0.8406, 'learning_rate': 3.427452762769462e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3828/5198 [21:46:34<10:32:44, 27.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3829/5198 [21:46:53<9:30:19, 25.00s/it]                                                         {'loss': 0.7764, 'learning_rate': 3.4227580463255628e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3829/5198 [21:46:53<9:30:19, 25.00s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3830/5198 [21:47:11<8:41:45, 22.88s/it]                                                        {'loss': 0.7618, 'learning_rate': 3.4180658833934523e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3830/5198 [21:47:11<8:41:45, 22.88s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3831/5198 [21:47:28<8:04:10, 21.25s/it]                                                        {'loss': 0.8199, 'learning_rate': 3.4133762757947873e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3831/5198 [21:47:28<8:04:10, 21.25s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3832/5198 [21:47:47<7:42:44, 20.33s/it]                                                        {'loss': 0.8149, 'learning_rate': 3.4086892253502344e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3832/5198 [21:47:47<7:42:44, 20.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3833/5198 [21:48:05<7:27:21, 19.66s/it]                                                        {'loss': 0.8501, 'learning_rate': 3.4040047338794756e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3833/5198 [21:48:05<7:27:21, 19.66s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3834/5198 [21:48:22<7:08:03, 18.83s/it]                                                        {'loss': 0.8054, 'learning_rate': 3.3993228032011784e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3834/5198 [21:48:22<7:08:03, 18.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3835/5198 [21:48:40<7:04:29, 18.69s/it]                                                        {'loss': 0.8033, 'learning_rate': 3.3946434351330415e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3835/5198 [21:48:40<7:04:29, 18.69s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3836/5198 [21:48:57<6:54:56, 18.28s/it]                                                        {'loss': 0.8486, 'learning_rate': 3.3899666314917512e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3836/5198 [21:48:57<6:54:56, 18.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3837/5198 [21:49:15<6:47:55, 17.98s/it]                                                        {'loss': 0.8095, 'learning_rate': 3.385292394093006e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3837/5198 [21:49:15<6:47:55, 17.98s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3838/5198 [21:49:32<6:41:33, 17.72s/it]                                                        {'loss': 0.7694, 'learning_rate': 3.3806207247515068e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3838/5198 [21:49:32<6:41:33, 17.72s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3839/5198 [21:49:49<6:41:26, 17.72s/it]                                                        {'loss': 0.7946, 'learning_rate': 3.375951625280948e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3839/5198 [21:49:49<6:41:26, 17.72s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3840/5198 [21:50:07<6:40:08, 17.68s/it]                                                        {'loss': 0.753, 'learning_rate': 3.3712850974940437e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3840/5198 [21:50:07<6:40:08, 17.68s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3841/5198 [21:50:25<6:39:40, 17.67s/it]                                                        {'loss': 0.7592, 'learning_rate': 3.3666211432024974e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3841/5198 [21:50:25<6:39:40, 17.67s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3842/5198 [21:50:43<6:41:02, 17.75s/it]                                                        {'loss': 0.7781, 'learning_rate': 3.361959764217018e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3842/5198 [21:50:43<6:41:02, 17.75s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3843/5198 [21:51:00<6:40:56, 17.75s/it]                                                        {'loss': 0.7261, 'learning_rate': 3.357300962347313e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3843/5198 [21:51:00<6:40:56, 17.75s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3844/5198 [21:51:19<6:45:13, 17.96s/it]                                                        {'loss': 0.7858, 'learning_rate': 3.3526447394020887e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3844/5198 [21:51:19<6:45:13, 17.96s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3845/5198 [21:51:36<6:40:59, 17.78s/it]                                                        {'loss': 0.8663, 'learning_rate': 3.3479910971890516e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3845/5198 [21:51:36<6:40:59, 17.78s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3846/5198 [21:51:54<6:43:04, 17.89s/it]                                                        {'loss': 0.7772, 'learning_rate': 3.343340037514903e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3846/5198 [21:51:54<6:43:04, 17.89s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3847/5198 [21:52:12<6:38:45, 17.71s/it]                                                        {'loss': 0.7669, 'learning_rate': 3.3386915621853533e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3847/5198 [21:52:12<6:38:45, 17.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3848/5198 [21:52:29<6:34:31, 17.53s/it]                                                        {'loss': 0.8071, 'learning_rate': 3.3340456730050887e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3848/5198 [21:52:29<6:34:31, 17.53s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3849/5198 [21:52:46<6:31:14, 17.40s/it]                                                        {'loss': 0.8155, 'learning_rate': 3.3294023717778122e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3849/5198 [21:52:46<6:31:14, 17.40s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3850/5198 [21:53:04<6:38:27, 17.74s/it]                                                        {'loss': 0.7949, 'learning_rate': 3.324761660306215e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3850/5198 [21:53:04<6:38:27, 17.74s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3851/5198 [21:54:28<14:00:57, 37.46s/it]                                                         {'loss': 0.7611, 'learning_rate': 3.3201235403919683e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3851/5198 [21:54:28<14:00:57, 37.46s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3852/5198 [21:54:44<11:40:04, 31.21s/it]                                                         {'loss': 0.8337, 'learning_rate': 3.3154880138357626e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3852/5198 [21:54:44<11:40:04, 31.21s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3853/5198 [21:55:02<10:06:56, 27.08s/it]                                                         {'loss': 0.8224, 'learning_rate': 3.3108550824372632e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3853/5198 [21:55:02<10:06:56, 27.08s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3854/5198 [21:55:20<9:04:23, 24.30s/it]                                                         {'loss': 0.833, 'learning_rate': 3.306224747995136e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3854/5198 [21:55:20<9:04:23, 24.30s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3855/5198 [21:55:37<8:20:28, 22.36s/it]                                                        {'loss': 0.7568, 'learning_rate': 3.301597012307034e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3855/5198 [21:55:38<8:20:28, 22.36s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3856/5198 [21:55:56<7:55:00, 21.24s/it]                                                        {'loss': 0.822, 'learning_rate': 3.2969718771696047e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3856/5198 [21:55:56<7:55:00, 21.24s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3857/5198 [21:56:14<7:30:40, 20.16s/it]                                                        {'loss': 0.8223, 'learning_rate': 3.292349344378486e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3857/5198 [21:56:14<7:30:40, 20.16s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3858/5198 [21:56:32<7:19:21, 19.67s/it]                                                        {'loss': 0.7736, 'learning_rate': 3.287729415728298e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3858/5198 [21:56:32<7:19:21, 19.67s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3859/5198 [21:56:50<7:03:36, 18.98s/it]                                                        {'loss': 0.8151, 'learning_rate': 3.283112093012669e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3859/5198 [21:56:50<7:03:36, 18.98s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3860/5198 [21:57:07<6:50:32, 18.41s/it]                                                        {'loss': 0.7944, 'learning_rate': 3.278497378024187e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3860/5198 [21:57:07<6:50:32, 18.41s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3861/5198 [21:57:25<6:50:28, 18.42s/it]                                                        {'loss': 0.7982, 'learning_rate': 3.2738852725544547e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3861/5198 [21:57:25<6:50:28, 18.42s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3862/5198 [21:57:42<6:41:50, 18.05s/it]                                                        {'loss': 0.7811, 'learning_rate': 3.2692757783940467e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3862/5198 [21:57:42<6:41:50, 18.05s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3863/5198 [21:58:00<6:36:28, 17.82s/it]                                                        {'loss': 0.7622, 'learning_rate': 3.264668897332527e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3863/5198 [21:58:00<6:36:28, 17.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3864/5198 [21:58:17<6:34:36, 17.75s/it]                                                        {'loss': 0.7804, 'learning_rate': 3.2600646311584494e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3864/5198 [21:58:17<6:34:36, 17.75s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3865/5198 [21:58:35<6:32:38, 17.67s/it]                                                        {'loss': 0.797, 'learning_rate': 3.2554629816593375e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3865/5198 [21:58:35<6:32:38, 17.67s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3866/5198 [21:58:52<6:29:11, 17.53s/it]                                                        {'loss': 0.7759, 'learning_rate': 3.250863950621721e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3866/5198 [21:58:52<6:29:11, 17.53s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3867/5198 [21:59:10<6:35:04, 17.81s/it]                                                        {'loss': 0.7847, 'learning_rate': 3.2462675398310984e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3867/5198 [21:59:10<6:35:04, 17.81s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3868/5198 [21:59:28<6:35:01, 17.82s/it]                                                        {'loss': 0.7709, 'learning_rate': 3.241673751071954e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3868/5198 [21:59:28<6:35:01, 17.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3869/5198 [21:59:46<6:36:28, 17.90s/it]                                                        {'loss': 0.8406, 'learning_rate': 3.2370825861277567e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3869/5198 [21:59:46<6:36:28, 17.90s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3870/5198 [22:00:04<6:37:55, 17.98s/it]                                                        {'loss': 0.7889, 'learning_rate': 3.2324940467809527e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3870/5198 [22:00:04<6:37:55, 17.98s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3871/5198 [22:00:21<6:31:12, 17.69s/it]                                                        {'loss': 0.7312, 'learning_rate': 3.2279081348129713e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3871/5198 [22:00:21<6:31:12, 17.69s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3872/5198 [22:00:38<6:25:23, 17.44s/it]                                                        {'loss': 0.8173, 'learning_rate': 3.223324852004219e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3872/5198 [22:00:38<6:25:23, 17.44s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3873/5198 [22:00:56<6:29:12, 17.62s/it]                                                        {'loss': 0.7746, 'learning_rate': 3.2187442001340942e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3873/5198 [22:00:56<6:29:12, 17.62s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3874/5198 [22:01:14<6:26:23, 17.51s/it]                                                        {'loss': 0.7997, 'learning_rate': 3.21416618098095e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3874/5198 [22:01:14<6:26:23, 17.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3875/5198 [22:01:31<6:27:27, 17.57s/it]                                                        {'loss': 0.332, 'learning_rate': 3.2095907963221396e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3875/5198 [22:01:31<6:27:27, 17.57s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3876/5198 [22:02:56<13:52:12, 37.77s/it]                                                         {'loss': 0.7667, 'learning_rate': 3.2050180479339865e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3876/5198 [22:02:56<13:52:12, 37.77s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3877/5198 [22:03:14<11:37:50, 31.70s/it]                                                         {'loss': 0.7851, 'learning_rate': 3.2004479375917783e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3877/5198 [22:03:14<11:37:50, 31.70s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3878/5198 [22:03:31<10:03:27, 27.43s/it]                                                         {'loss': 0.3414, 'learning_rate': 3.1958804670698008e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3878/5198 [22:03:31<10:03:27, 27.43s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3879/5198 [22:03:48<8:55:08, 24.34s/it]                                                         {'loss': 0.7891, 'learning_rate': 3.191315638141297e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3879/5198 [22:03:48<8:55:08, 24.34s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3880/5198 [22:04:05<8:06:55, 22.17s/it]                                                        {'loss': 0.3349, 'learning_rate': 3.1867534525784937e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3880/5198 [22:04:05<8:06:55, 22.17s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3881/5198 [22:04:24<7:39:42, 20.94s/it]                                                        {'loss': 0.806, 'learning_rate': 3.182193912152586e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3881/5198 [22:04:24<7:39:42, 20.94s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3882/5198 [22:04:41<7:14:02, 19.79s/it]                                                        {'loss': 0.8635, 'learning_rate': 3.177637018633746e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3882/5198 [22:04:41<7:14:02, 19.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3883/5198 [22:04:58<6:55:46, 18.97s/it]                                                        {'loss': 0.8055, 'learning_rate': 3.1730827737911163e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3883/5198 [22:04:58<6:55:46, 18.97s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3884/5198 [22:05:15<6:47:22, 18.60s/it]                                                        {'loss': 0.7673, 'learning_rate': 3.1685311793928077e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3884/5198 [22:05:15<6:47:22, 18.60s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3885/5198 [22:05:33<6:37:20, 18.16s/it]                                                        {'loss': 0.8037, 'learning_rate': 3.163982237205917e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3885/5198 [22:05:33<6:37:20, 18.16s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3886/5198 [22:05:50<6:31:54, 17.92s/it]                                                        {'loss': 0.7861, 'learning_rate': 3.1594359489964853e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3886/5198 [22:05:50<6:31:54, 17.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3887/5198 [22:06:08<6:30:05, 17.85s/it]                                                        {'loss': 0.8008, 'learning_rate': 3.15489231652955e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3887/5198 [22:06:08<6:30:05, 17.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3888/5198 [22:06:25<6:27:17, 17.74s/it]                                                        {'loss': 0.3386, 'learning_rate': 3.150351341569101e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3888/5198 [22:06:25<6:27:17, 17.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3889/5198 [22:06:43<6:25:54, 17.69s/it]                                                        {'loss': 0.8345, 'learning_rate': 3.1458130258781006e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3889/5198 [22:06:43<6:25:54, 17.69s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3890/5198 [22:07:00<6:25:25, 17.68s/it]                                                        {'loss': 0.8153, 'learning_rate': 3.141277371218484e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3890/5198 [22:07:00<6:25:25, 17.68s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3891/5198 [22:07:18<6:22:52, 17.58s/it]                                                        {'loss': 0.3533, 'learning_rate': 3.136744379351139e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3891/5198 [22:07:18<6:22:52, 17.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3892/5198 [22:07:37<6:32:14, 18.02s/it]                                                        {'loss': 0.7588, 'learning_rate': 3.1322140520359366e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3892/5198 [22:07:37<6:32:14, 18.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3893/5198 [22:07:54<6:24:17, 17.67s/it]                                                        {'loss': 0.8271, 'learning_rate': 3.1276863910317057e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3893/5198 [22:07:54<6:24:17, 17.67s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3894/5198 [22:08:11<6:22:13, 17.59s/it]                                                        {'loss': 0.8143, 'learning_rate': 3.1231613980962373e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3894/5198 [22:08:11<6:22:13, 17.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3895/5198 [22:08:29<6:22:43, 17.62s/it]                                                        {'loss': 0.8169, 'learning_rate': 3.1186390749862904e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3895/5198 [22:08:29<6:22:43, 17.62s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3896/5198 [22:08:46<6:18:26, 17.44s/it]                                                        {'loss': 0.7525, 'learning_rate': 3.1141194234575878e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3896/5198 [22:08:46<6:18:26, 17.44s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3897/5198 [22:09:04<6:23:58, 17.71s/it]                                                        {'loss': 0.852, 'learning_rate': 3.1096024452648123e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3897/5198 [22:09:04<6:23:58, 17.71s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3898/5198 [22:09:21<6:19:27, 17.51s/it]                                                        {'loss': 0.7986, 'learning_rate': 3.1050881421616076e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3898/5198 [22:09:21<6:19:27, 17.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3899/5198 [22:09:39<6:23:49, 17.73s/it]                                                        {'loss': 0.8, 'learning_rate': 3.100576515900591e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3899/5198 [22:09:39<6:23:49, 17.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3900/5198 [22:09:56<6:17:38, 17.46s/it]                                                        {'loss': 0.78, 'learning_rate': 3.0960675682333186e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3900/5198 [22:09:56<6:17:38, 17.46s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3901/5198 [22:11:21<13:35:50, 37.74s/it]                                                         {'loss': 0.8059, 'learning_rate': 3.0915613009103296e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3901/5198 [22:11:21<13:35:50, 37.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3902/5198 [22:11:39<11:28:24, 31.87s/it]                                                         {'loss': 0.7671, 'learning_rate': 3.0870577156811077e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3902/5198 [22:11:39<11:28:24, 31.87s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3903/5198 [22:11:57<9:58:41, 27.74s/it]                                                         {'loss': 0.7496, 'learning_rate': 3.0825568142940998e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3903/5198 [22:11:58<9:58:41, 27.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3904/5198 [22:12:16<8:58:48, 24.98s/it]                                                        {'loss': 0.7831, 'learning_rate': 3.0780585984967113e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3904/5198 [22:12:16<8:58:48, 24.98s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3905/5198 [22:12:34<8:11:21, 22.80s/it]                                                        {'loss': 0.7627, 'learning_rate': 3.073563070035305e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3905/5198 [22:12:34<8:11:21, 22.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3906/5198 [22:12:52<7:41:22, 21.43s/it]                                                        {'loss': 0.7786, 'learning_rate': 3.069070230655198e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3906/5198 [22:12:52<7:41:22, 21.43s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3907/5198 [22:13:09<7:10:40, 20.02s/it]                                                        {'loss': 0.733, 'learning_rate': 3.0645800821006667e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3907/5198 [22:13:09<7:10:40, 20.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3908/5198 [22:13:26<6:55:19, 19.32s/it]                                                        {'loss': 0.7894, 'learning_rate': 3.060092626114941e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3908/5198 [22:13:26<6:55:19, 19.32s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3909/5198 [22:13:44<6:46:49, 18.94s/it]                                                        {'loss': 0.7808, 'learning_rate': 3.0556078644402066e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3909/5198 [22:13:44<6:46:49, 18.94s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3910/5198 [22:14:02<6:37:53, 18.54s/it]                                                        {'loss': 0.8338, 'learning_rate': 3.051125798817598e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3910/5198 [22:14:02<6:37:53, 18.54s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3911/5198 [22:14:20<6:31:55, 18.27s/it]                                                        {'loss': 0.7944, 'learning_rate': 3.0466464309872167e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3911/5198 [22:14:20<6:31:55, 18.27s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3912/5198 [22:14:37<6:28:11, 18.11s/it]                                                        {'loss': 0.7732, 'learning_rate': 3.042169762688096e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3912/5198 [22:14:37<6:28:11, 18.11s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3913/5198 [22:14:55<6:23:15, 17.90s/it]                                                        {'loss': 0.3435, 'learning_rate': 3.0376957956582452e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3913/5198 [22:14:55<6:23:15, 17.90s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3914/5198 [22:15:12<6:18:44, 17.70s/it]                                                        {'loss': 0.7492, 'learning_rate': 3.0332245316346e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3914/5198 [22:15:12<6:18:44, 17.70s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3915/5198 [22:15:30<6:19:08, 17.73s/it]                                                        {'loss': 0.8577, 'learning_rate': 3.0287559723530667e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3915/5198 [22:15:30<6:19:08, 17.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3916/5198 [22:15:48<6:20:32, 17.81s/it]                                                        {'loss': 0.767, 'learning_rate': 3.024290119548495e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3916/5198 [22:15:48<6:20:32, 17.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3917/5198 [22:16:05<6:18:41, 17.74s/it]                                                        {'loss': 0.7577, 'learning_rate': 3.019826974954674e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3917/5198 [22:16:05<6:18:41, 17.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3918/5198 [22:16:24<6:20:44, 17.85s/it]                                                        {'loss': 0.7769, 'learning_rate': 3.0153665403043586e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3918/5198 [22:16:24<6:20:44, 17.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3919/5198 [22:16:41<6:19:18, 17.79s/it]                                                        {'loss': 0.7864, 'learning_rate': 3.01090881732924e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3919/5198 [22:16:41<6:19:18, 17.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3920/5198 [22:16:58<6:14:26, 17.58s/it]                                                        {'loss': 0.8188, 'learning_rate': 3.0064538077599603e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3920/5198 [22:16:58<6:14:26, 17.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3921/5198 [22:17:17<6:18:40, 17.79s/it]                                                        {'loss': 0.8158, 'learning_rate': 3.002001513326107e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3921/5198 [22:17:17<6:18:40, 17.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3922/5198 [22:17:33<6:12:05, 17.50s/it]                                                        {'loss': 0.8534, 'learning_rate': 2.9975519357562155e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3922/5198 [22:17:33<6:12:05, 17.50s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3923/5198 [22:17:50<6:07:21, 17.29s/it]                                                        {'loss': 0.8199, 'learning_rate': 2.9931050767777626e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3923/5198 [22:17:50<6:07:21, 17.29s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3924/5198 [22:18:08<6:09:19, 17.39s/it]                                                        {'loss': 0.8018, 'learning_rate': 2.9886609381171703e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3924/5198 [22:18:08<6:09:19, 17.39s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3925/5198 [22:18:26<6:11:04, 17.49s/it]                                                        {'loss': 0.7955, 'learning_rate': 2.984219521499816e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3925/5198 [22:18:26<6:11:04, 17.49s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3926/5198 [22:19:49<13:10:12, 37.27s/it]                                                         {'loss': 0.8474, 'learning_rate': 2.9797808286499976e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3926/5198 [22:19:49<13:10:12, 37.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3927/5198 [22:20:07<11:04:40, 31.38s/it]                                                         {'loss': 0.7445, 'learning_rate': 2.9753448612909775e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3927/5198 [22:20:07<11:04:40, 31.38s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3928/5198 [22:20:23<9:30:46, 26.97s/it]                                                         {'loss': 0.7514, 'learning_rate': 2.9709116211449484e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3928/5198 [22:20:23<9:30:46, 26.97s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3929/5198 [22:20:40<8:26:12, 23.93s/it]                                                        {'loss': 0.7743, 'learning_rate': 2.966481109933047e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3929/5198 [22:20:40<8:26:12, 23.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3930/5198 [22:20:57<7:44:08, 21.96s/it]                                                        {'loss': 0.8288, 'learning_rate': 2.9620533293753495e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3930/5198 [22:20:57<7:44:08, 21.96s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3931/5198 [22:21:15<7:15:37, 20.63s/it]                                                        {'loss': 0.779, 'learning_rate': 2.957628281190873e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3931/5198 [22:21:15<7:15:37, 20.63s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3932/5198 [22:21:33<6:56:37, 19.75s/it]                                                        {'loss': 0.7997, 'learning_rate': 2.9532059670975732e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3932/5198 [22:21:33<6:56:37, 19.75s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3933/5198 [22:21:50<6:43:20, 19.13s/it]                                                        {'loss': 0.7752, 'learning_rate': 2.948786388812346e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3933/5198 [22:21:50<6:43:20, 19.13s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3934/5198 [22:22:08<6:36:12, 18.81s/it]                                                        {'loss': 0.7946, 'learning_rate': 2.9443695480510225e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3934/5198 [22:22:08<6:36:12, 18.81s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3935/5198 [22:22:27<6:32:11, 18.63s/it]                                                        {'loss': 0.7386, 'learning_rate': 2.9399554465283742e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3935/5198 [22:22:27<6:32:11, 18.63s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3936/5198 [22:22:45<6:30:38, 18.57s/it]                                                        {'loss': 0.7559, 'learning_rate': 2.935544085958102e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3936/5198 [22:22:45<6:30:38, 18.57s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3937/5198 [22:23:02<6:22:55, 18.22s/it]                                                        {'loss': 0.3578, 'learning_rate': 2.931135468052858e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3937/5198 [22:23:02<6:22:55, 18.22s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3938/5198 [22:23:21<6:22:28, 18.21s/it]                                                        {'loss': 0.801, 'learning_rate': 2.926729594524207e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3938/5198 [22:23:21<6:22:28, 18.21s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3939/5198 [22:23:39<6:20:25, 18.13s/it]                                                        {'loss': 0.8497, 'learning_rate': 2.9223264670826746e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3939/5198 [22:23:39<6:20:25, 18.13s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3940/5198 [22:23:56<6:15:59, 17.93s/it]                                                        {'loss': 0.8273, 'learning_rate': 2.9179260874376915e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3940/5198 [22:23:56<6:15:59, 17.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3941/5198 [22:24:13<6:08:21, 17.58s/it]                                                        {'loss': 0.7585, 'learning_rate': 2.9135284572976486e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3941/5198 [22:24:13<6:08:21, 17.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3942/5198 [22:24:30<6:05:13, 17.45s/it]                                                        {'loss': 0.3252, 'learning_rate': 2.9091335783698517e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3942/5198 [22:24:30<6:05:13, 17.45s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3943/5198 [22:24:47<6:00:07, 17.22s/it]                                                        {'loss': 0.8146, 'learning_rate': 2.9047414523605467e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3943/5198 [22:24:47<6:00:07, 17.22s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3944/5198 [22:25:04<6:02:20, 17.34s/it]                                                        {'loss': 0.8221, 'learning_rate': 2.9003520809749053e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3944/5198 [22:25:04<6:02:20, 17.34s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3945/5198 [22:25:22<6:05:14, 17.49s/it]                                                        {'loss': 0.8024, 'learning_rate': 2.8959654659170354e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3945/5198 [22:25:22<6:05:14, 17.49s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3946/5198 [22:25:40<6:04:45, 17.48s/it]                                                        {'loss': 0.7585, 'learning_rate': 2.8915816088899696e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3946/5198 [22:25:40<6:04:45, 17.48s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3947/5198 [22:25:57<6:03:24, 17.43s/it]                                                        {'loss': 0.7945, 'learning_rate': 2.8872005115956746e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3947/5198 [22:25:57<6:03:24, 17.43s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3948/5198 [22:26:14<6:01:31, 17.35s/it]                                                        {'loss': 0.8227, 'learning_rate': 2.8828221757350406e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3948/5198 [22:26:14<6:01:31, 17.35s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3949/5198 [22:26:32<6:07:01, 17.63s/it]                                                        {'loss': 0.7753, 'learning_rate': 2.8784466030078905e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3949/5198 [22:26:32<6:07:01, 17.63s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3950/5198 [22:26:50<6:07:52, 17.69s/it]                                                        {'loss': 0.813, 'learning_rate': 2.874073795112967e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3950/5198 [22:26:50<6:07:52, 17.69s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3951/5198 [22:28:15<13:03:27, 37.70s/it]                                                         {'loss': 0.8126, 'learning_rate': 2.8697037537479565e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3951/5198 [22:28:15<13:03:27, 37.70s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3952/5198 [22:28:32<10:54:20, 31.51s/it]                                                         {'loss': 0.3473, 'learning_rate': 2.8653364806094454e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3952/5198 [22:28:32<10:54:20, 31.51s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3953/5198 [22:28:50<9:29:45, 27.46s/it]                                                         {'loss': 0.7947, 'learning_rate': 2.86097197739297e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3953/5198 [22:28:50<9:29:45, 27.46s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3954/5198 [22:29:07<8:28:52, 24.54s/it]                                                        {'loss': 0.7458, 'learning_rate': 2.856610245792976e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3954/5198 [22:29:07<8:28:52, 24.54s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3955/5198 [22:29:25<7:45:08, 22.45s/it]                                                        {'loss': 0.8476, 'learning_rate': 2.8522512875028396e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3955/5198 [22:29:25<7:45:08, 22.45s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3956/5198 [22:29:42<7:13:17, 20.93s/it]                                                        {'loss': 0.8175, 'learning_rate': 2.847895104214856e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3956/5198 [22:29:42<7:13:17, 20.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3957/5198 [22:30:00<6:54:44, 20.05s/it]                                                        {'loss': 0.7975, 'learning_rate': 2.843541697620249e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3957/5198 [22:30:00<6:54:44, 20.05s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3958/5198 [22:30:17<6:32:17, 18.98s/it]                                                        {'loss': 0.8244, 'learning_rate': 2.8391910694091584e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3958/5198 [22:30:17<6:32:17, 18.98s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3959/5198 [22:30:35<6:25:04, 18.65s/it]                                                        {'loss': 0.7951, 'learning_rate': 2.8348432212706443e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3959/5198 [22:30:35<6:25:04, 18.65s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3960/5198 [22:30:52<6:16:26, 18.24s/it]                                                        {'loss': 0.2664, 'learning_rate': 2.8304981548927025e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3960/5198 [22:30:52<6:16:26, 18.24s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3961/5198 [22:31:10<6:12:59, 18.09s/it]                                                        {'loss': 0.8225, 'learning_rate': 2.826155871962227e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3961/5198 [22:31:10<6:12:59, 18.09s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3962/5198 [22:31:28<6:15:03, 18.21s/it]                                                        {'loss': 0.8382, 'learning_rate': 2.8218163741650415e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3962/5198 [22:31:28<6:15:03, 18.21s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3963/5198 [22:31:47<6:16:15, 18.28s/it]                                                        {'loss': 0.8121, 'learning_rate': 2.817479663185898e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3963/5198 [22:31:47<6:16:15, 18.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3964/5198 [22:32:05<6:17:59, 18.38s/it]                                                        {'loss': 0.7628, 'learning_rate': 2.813145740708445e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3964/5198 [22:32:05<6:17:59, 18.38s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3965/5198 [22:32:23<6:12:35, 18.13s/it]                                                        {'loss': 0.8333, 'learning_rate': 2.808814608415271e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3965/5198 [22:32:23<6:12:35, 18.13s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3966/5198 [22:32:41<6:09:44, 18.01s/it]                                                        {'loss': 0.7432, 'learning_rate': 2.8044862679878605e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3966/5198 [22:32:41<6:09:44, 18.01s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3967/5198 [22:32:57<6:02:23, 17.66s/it]                                                        {'loss': 0.8037, 'learning_rate': 2.800160721106633e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3967/5198 [22:32:57<6:02:23, 17.66s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3968/5198 [22:33:15<6:02:01, 17.66s/it]                                                        {'loss': 0.7789, 'learning_rate': 2.7958379694509108e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3968/5198 [22:33:15<6:02:01, 17.66s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3969/5198 [22:33:34<6:06:49, 17.91s/it]                                                        {'loss': 0.7493, 'learning_rate': 2.791518014698935e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3969/5198 [22:33:34<6:06:49, 17.91s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3970/5198 [22:33:51<6:03:08, 17.74s/it]                                                        {'loss': 0.8377, 'learning_rate': 2.787200858527862e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3970/5198 [22:33:51<6:03:08, 17.74s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3971/5198 [22:34:08<6:00:49, 17.64s/it]                                                        {'loss': 0.8131, 'learning_rate': 2.7828865026137584e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3971/5198 [22:34:08<6:00:49, 17.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3972/5198 [22:34:25<5:53:49, 17.32s/it]                                                        {'loss': 0.7901, 'learning_rate': 2.7785749486316085e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3972/5198 [22:34:25<5:53:49, 17.32s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3973/5198 [22:34:42<5:55:16, 17.40s/it]                                                        {'loss': 0.7872, 'learning_rate': 2.774266198255303e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3973/5198 [22:34:42<5:55:16, 17.40s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3974/5198 [22:35:00<5:54:19, 17.37s/it]                                                        {'loss': 0.8128, 'learning_rate': 2.7699602531576496e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3974/5198 [22:35:00<5:54:19, 17.37s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3975/5198 [22:35:17<5:50:52, 17.21s/it]                                                        {'loss': 0.8512, 'learning_rate': 2.765657115010364e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3975/5198 [22:35:17<5:50:52, 17.21s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3976/5198 [22:36:43<12:55:21, 38.07s/it]                                                         {'loss': 0.8208, 'learning_rate': 2.7613567854840685e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3976/5198 [22:36:43<12:55:21, 38.07s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3977/5198 [22:37:01<10:48:15, 31.86s/it]                                                         {'loss': 0.8008, 'learning_rate': 2.7570592662483086e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3977/5198 [22:37:01<10:48:15, 31.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3978/5198 [22:37:18<9:20:55, 27.59s/it]                                                         {'loss': 0.7633, 'learning_rate': 2.752764558971517e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3978/5198 [22:37:18<9:20:55, 27.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3979/5198 [22:37:37<8:25:07, 24.86s/it]                                                        {'loss': 0.8159, 'learning_rate': 2.748472665321056e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3979/5198 [22:37:37<8:25:07, 24.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3980/5198 [22:37:55<7:40:56, 22.71s/it]                                                        {'loss': 0.8449, 'learning_rate': 2.744183586963185e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3980/5198 [22:37:55<7:40:56, 22.71s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3981/5198 [22:38:13<7:14:15, 21.41s/it]                                                        {'loss': 0.8193, 'learning_rate': 2.739897325563069e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3981/5198 [22:38:13<7:14:15, 21.41s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3982/5198 [22:38:30<6:48:16, 20.14s/it]                                                        {'loss': 0.7404, 'learning_rate': 2.7356138827847856e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3982/5198 [22:38:30<6:48:16, 20.14s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3983/5198 [22:38:48<6:32:31, 19.38s/it]                                                        {'loss': 0.7784, 'learning_rate': 2.731333260291311e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3983/5198 [22:38:48<6:32:31, 19.38s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3984/5198 [22:39:05<6:19:41, 18.77s/it]                                                        {'loss': 0.3473, 'learning_rate': 2.7270554597445343e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3984/5198 [22:39:05<6:19:41, 18.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3985/5198 [22:39:23<6:12:25, 18.42s/it]                                                        {'loss': 0.8293, 'learning_rate': 2.7227804828052384e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3985/5198 [22:39:23<6:12:25, 18.42s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3986/5198 [22:39:40<6:06:44, 18.16s/it]                                                        {'loss': 0.797, 'learning_rate': 2.7185083311331283e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3986/5198 [22:39:40<6:06:44, 18.16s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3987/5198 [22:39:58<6:05:48, 18.12s/it]                                                        {'loss': 0.7658, 'learning_rate': 2.7142390063867896e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3987/5198 [22:39:58<6:05:48, 18.12s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3988/5198 [22:40:16<6:04:14, 18.06s/it]                                                        {'loss': 0.7413, 'learning_rate': 2.709972510223725e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3988/5198 [22:40:16<6:04:14, 18.06s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3989/5198 [22:40:35<6:06:51, 18.21s/it]                                                        {'loss': 0.7601, 'learning_rate': 2.7057088443003343e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3989/5198 [22:40:35<6:06:51, 18.21s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3990/5198 [22:40:51<5:57:45, 17.77s/it]                                                        {'loss': 0.8478, 'learning_rate': 2.7014480102719174e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3990/5198 [22:40:51<5:57:45, 17.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3991/5198 [22:41:08<5:50:08, 17.41s/it]                                                        {'loss': 0.7419, 'learning_rate': 2.697190009792685e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3991/5198 [22:41:08<5:50:08, 17.41s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3992/5198 [22:41:26<5:52:27, 17.53s/it]                                                        {'loss': 0.8371, 'learning_rate': 2.692934844515729e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3992/5198 [22:41:26<5:52:27, 17.53s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3993/5198 [22:41:44<5:54:57, 17.67s/it]                                                        {'loss': 0.7865, 'learning_rate': 2.6886825160930587e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3993/5198 [22:41:44<5:54:57, 17.67s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3994/5198 [22:42:02<5:56:01, 17.74s/it]                                                        {'loss': 0.7668, 'learning_rate': 2.6844330261755715e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3994/5198 [22:42:02<5:56:01, 17.74s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3995/5198 [22:42:18<5:48:39, 17.39s/it]                                                        {'loss': 0.768, 'learning_rate': 2.6801863764130653e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3995/5198 [22:42:18<5:48:39, 17.39s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3996/5198 [22:42:35<5:46:54, 17.32s/it]                                                        {'loss': 0.7943, 'learning_rate': 2.675942568454236e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3996/5198 [22:42:35<5:46:54, 17.32s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3997/5198 [22:42:53<5:50:18, 17.50s/it]                                                        {'loss': 0.7984, 'learning_rate': 2.671701603946678e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3997/5198 [22:42:53<5:50:18, 17.50s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3998/5198 [22:43:11<5:48:38, 17.43s/it]                                                        {'loss': 0.809, 'learning_rate': 2.667463484536876e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3998/5198 [22:43:11<5:48:38, 17.43s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3999/5198 [22:43:28<5:46:10, 17.32s/it]                                                        {'loss': 0.7329, 'learning_rate': 2.6632282118702147e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3999/5198 [22:43:28<5:46:10, 17.32s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4000/5198 [22:43:45<5:46:05, 17.33s/it]                                                        {'loss': 0.7702, 'learning_rate': 2.65899578759098e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4000/5198 [22:43:45<5:46:05, 17.33s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4001/5198 [22:45:10<12:31:27, 37.67s/it]                                                         {'loss': 0.7466, 'learning_rate': 2.654766213342335e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4001/5198 [22:45:10<12:31:27, 37.67s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4002/5198 [22:45:29<10:36:56, 31.95s/it]                                                         {'loss': 0.7517, 'learning_rate': 2.650539490766346e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4002/5198 [22:45:29<10:36:56, 31.95s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4003/5198 [22:45:48<9:17:46, 28.01s/it]                                                         {'loss': 0.7584, 'learning_rate': 2.646315621503983e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4003/5198 [22:45:48<9:17:46, 28.01s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4004/5198 [22:46:05<8:15:42, 24.91s/it]                                                        {'loss': 0.7968, 'learning_rate': 2.642094607195085e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4004/5198 [22:46:05<8:15:42, 24.91s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4005/5198 [22:46:23<7:31:25, 22.70s/it]                                                        {'loss': 0.8016, 'learning_rate': 2.6378764494784027e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4005/5198 [22:46:23<7:31:25, 22.70s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4006/5198 [22:46:41<7:05:18, 21.41s/it]                                                        {'loss': 0.8046, 'learning_rate': 2.633661149991569e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4006/5198 [22:46:41<7:05:18, 21.41s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4007/5198 [22:47:00<6:47:56, 20.55s/it]                                                        {'loss': 0.8105, 'learning_rate': 2.6294487103711064e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4007/5198 [22:47:00<6:47:56, 20.55s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4008/5198 [22:47:17<6:28:34, 19.59s/it]                                                        {'loss': 0.8278, 'learning_rate': 2.6252391322524297e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4008/5198 [22:47:17<6:28:34, 19.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4009/5198 [22:47:36<6:21:13, 19.24s/it]                                                        {'loss': 0.7759, 'learning_rate': 2.6210324172698432e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4009/5198 [22:47:36<6:21:13, 19.24s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4010/5198 [22:47:54<6:13:28, 18.86s/it]                                                        {'loss': 0.8233, 'learning_rate': 2.6168285670565374e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4010/5198 [22:47:54<6:13:28, 18.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4011/5198 [22:48:12<6:08:10, 18.61s/it]                                                        {'loss': 0.715, 'learning_rate': 2.6126275832445892e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4011/5198 [22:48:12<6:08:10, 18.61s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4012/5198 [22:48:30<6:04:55, 18.46s/it]                                                        {'loss': 0.841, 'learning_rate': 2.6084294674649734e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4012/5198 [22:48:30<6:04:55, 18.46s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4013/5198 [22:48:47<5:58:51, 18.17s/it]                                                        {'loss': 0.791, 'learning_rate': 2.6042342213475346e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4013/5198 [22:48:47<5:58:51, 18.17s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4014/5198 [22:49:04<5:53:27, 17.91s/it]                                                        {'loss': 0.808, 'learning_rate': 2.6000418465210143e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4014/5198 [22:49:04<5:53:27, 17.91s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4015/5198 [22:49:22<5:49:26, 17.72s/it]                                                        {'loss': 0.809, 'learning_rate': 2.595852344613038e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4015/5198 [22:49:22<5:49:26, 17.72s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4016/5198 [22:49:39<5:49:07, 17.72s/it]                                                        {'loss': 0.7573, 'learning_rate': 2.5916657172501103e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4016/5198 [22:49:39<5:49:07, 17.72s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4017/5198 [22:49:57<5:48:59, 17.73s/it]                                                        {'loss': 0.7529, 'learning_rate': 2.587481966057633e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4017/5198 [22:49:57<5:48:59, 17.73s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4018/5198 [22:50:15<5:51:13, 17.86s/it]                                                        {'loss': 0.7833, 'learning_rate': 2.583301092659872e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4018/5198 [22:50:15<5:51:13, 17.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4019/5198 [22:50:33<5:51:13, 17.87s/it]                                                        {'loss': 0.8237, 'learning_rate': 2.5791230986799944e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4019/5198 [22:50:33<5:51:13, 17.87s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4020/5198 [22:50:52<5:53:33, 18.01s/it]                                                        {'loss': 0.8213, 'learning_rate': 2.5749479857400383e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4020/5198 [22:50:52<5:53:33, 18.01s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4021/5198 [22:51:10<5:53:58, 18.04s/it]                                                        {'loss': 0.8385, 'learning_rate': 2.5707757554609247e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4021/5198 [22:51:10<5:53:58, 18.04s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4022/5198 [22:51:27<5:48:27, 17.78s/it]                                                        {'loss': 0.8011, 'learning_rate': 2.56660640946246e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4022/5198 [22:51:27<5:48:27, 17.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4023/5198 [22:51:44<5:44:03, 17.57s/it]                                                        {'loss': 0.8513, 'learning_rate': 2.5624399493633257e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4023/5198 [22:51:44<5:44:03, 17.57s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4024/5198 [22:52:02<5:44:58, 17.63s/it]                                                        {'loss': 0.7619, 'learning_rate': 2.558276376781086e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4024/5198 [22:52:02<5:44:58, 17.63s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4025/5198 [22:52:18<5:37:55, 17.29s/it]                                                        {'loss': 0.8023, 'learning_rate': 2.55411569333218e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4025/5198 [22:52:18<5:37:55, 17.29s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4026/5198 [22:53:43<12:13:41, 37.56s/it]                                                         {'loss': 0.8395, 'learning_rate': 2.5499579006319365e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4026/5198 [22:53:43<12:13:41, 37.56s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4027/5198 [22:54:00<10:13:03, 31.41s/it]                                                         {'loss': 0.8038, 'learning_rate': 2.5458030002945457e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4027/5198 [22:54:00<10:13:03, 31.41s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4028/5198 [22:54:18<8:50:27, 27.20s/it]                                                         {'loss': 0.3451, 'learning_rate': 2.5416509939330836e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4028/5198 [22:54:18<8:50:27, 27.20s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4029/5198 [22:54:35<7:53:24, 24.30s/it]                                                        {'loss': 0.7987, 'learning_rate': 2.537501883159509e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4029/5198 [22:54:35<7:53:24, 24.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4030/5198 [22:54:53<7:14:42, 22.33s/it]                                                        {'loss': 0.7864, 'learning_rate': 2.5333556695846384e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4030/5198 [22:54:53<7:14:42, 22.33s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4031/5198 [22:55:11<6:50:17, 21.09s/it]                                                        {'loss': 0.7681, 'learning_rate': 2.5292123548181847e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4031/5198 [22:55:11<6:50:17, 21.09s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4032/5198 [22:55:29<6:34:26, 20.30s/it]                                                        {'loss': 0.7981, 'learning_rate': 2.525071940468722e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4032/5198 [22:55:29<6:34:26, 20.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4033/5198 [22:55:47<6:20:29, 19.60s/it]                                                        {'loss': 0.3514, 'learning_rate': 2.520934428143701e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4033/5198 [22:55:47<6:20:29, 19.60s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4034/5198 [22:56:05<6:10:41, 19.11s/it]                                                        {'loss': 0.7587, 'learning_rate': 2.5167998194494468e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4034/5198 [22:56:05<6:10:41, 19.11s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4035/5198 [22:56:23<6:03:23, 18.75s/it]                                                        {'loss': 0.7869, 'learning_rate': 2.5126681159911558e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4035/5198 [22:56:23<6:03:23, 18.75s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4036/5198 [22:56:41<5:55:30, 18.36s/it]                                                        {'loss': 0.8417, 'learning_rate': 2.5085393193729e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4036/5198 [22:56:41<5:55:30, 18.36s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4037/5198 [22:56:58<5:47:24, 17.95s/it]                                                        {'loss': 0.7661, 'learning_rate': 2.5044134311976156e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4037/5198 [22:56:58<5:47:24, 17.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4038/5198 [22:57:15<5:41:35, 17.67s/it]                                                        {'loss': 0.8242, 'learning_rate': 2.5002904530671236e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4038/5198 [22:57:15<5:41:35, 17.67s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4039/5198 [22:57:33<5:46:13, 17.92s/it]                                                        {'loss': 0.8015, 'learning_rate': 2.4961703865820974e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4039/5198 [22:57:33<5:46:13, 17.92s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4040/5198 [22:57:52<5:47:47, 18.02s/it]                                                        {'loss': 0.7801, 'learning_rate': 2.492053233342091e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4040/5198 [22:57:52<5:47:47, 18.02s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4041/5198 [22:58:08<5:39:18, 17.60s/it]                                                        {'loss': 0.7662, 'learning_rate': 2.487938994945527e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4041/5198 [22:58:08<5:39:18, 17.60s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4042/5198 [22:58:25<5:37:01, 17.49s/it]                                                        {'loss': 0.3504, 'learning_rate': 2.4838276729896884e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4042/5198 [22:58:25<5:37:01, 17.49s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4043/5198 [22:58:42<5:30:32, 17.17s/it]                                                        {'loss': 0.7747, 'learning_rate': 2.479719269070743e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4043/5198 [22:58:42<5:30:32, 17.17s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4044/5198 [22:59:00<5:36:01, 17.47s/it]                                                        {'loss': 0.7718, 'learning_rate': 2.4756137847837025e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4044/5198 [22:59:00<5:36:01, 17.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4045/5198 [22:59:18<5:39:13, 17.65s/it]                                                        {'loss': 0.8495, 'learning_rate': 2.4715112217224657e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4045/5198 [22:59:18<5:39:13, 17.65s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4046/5198 [22:59:36<5:40:43, 17.75s/it]                                                        {'loss': 0.7703, 'learning_rate': 2.467411581479786e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4046/5198 [22:59:36<5:40:43, 17.75s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4047/5198 [22:59:53<5:37:54, 17.61s/it]                                                        {'loss': 0.7934, 'learning_rate': 2.463314865647286e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4047/5198 [22:59:53<5:37:54, 17.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4048/5198 [23:00:11<5:37:55, 17.63s/it]                                                        {'loss': 0.8014, 'learning_rate': 2.45922107581545e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4048/5198 [23:00:11<5:37:55, 17.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4049/5198 [23:00:29<5:39:25, 17.72s/it]                                                        {'loss': 0.7791, 'learning_rate': 2.4551302135736287e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4049/5198 [23:00:29<5:39:25, 17.72s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4050/5198 [23:00:47<5:40:36, 17.80s/it]                                                        {'loss': 0.7908, 'learning_rate': 2.4510422805100366e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4050/5198 [23:00:47<5:40:36, 17.80s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4051/5198 [23:02:12<12:06:05, 37.98s/it]                                                         {'loss': 0.7671, 'learning_rate': 2.446957278211746e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4051/5198 [23:02:12<12:06:05, 37.98s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4052/5198 [23:02:29<10:06:36, 31.76s/it]                                                         {'loss': 0.7345, 'learning_rate': 2.4428752082647044e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4052/5198 [23:02:29<10:06:36, 31.76s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4053/5198 [23:02:46<8:40:49, 27.29s/it]                                                         {'loss': 0.7669, 'learning_rate': 2.438796072253704e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4053/5198 [23:02:46<8:40:49, 27.29s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4054/5198 [23:03:03<7:42:04, 24.23s/it]                                                        {'loss': 0.8045, 'learning_rate': 2.4347198717624054e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4054/5198 [23:03:03<7:42:04, 24.23s/it]WARNING: tokenization mismatch: 1 vs. 1473. (ignored)
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4055/5198 [23:03:21<7:04:13, 22.27s/it]                                                        {'loss': 0.7603, 'learning_rate': 2.4306466083733392e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4055/5198 [23:03:21<7:04:13, 22.27s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4056/5198 [23:03:39<6:41:46, 21.11s/it]                                                        {'loss': 0.8084, 'learning_rate': 2.426576283667873e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4056/5198 [23:03:39<6:41:46, 21.11s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4057/5198 [23:03:58<6:27:00, 20.35s/it]                                                        {'loss': 0.826, 'learning_rate': 2.422508899226258e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4057/5198 [23:03:58<6:27:00, 20.35s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4058/5198 [23:04:16<6:12:46, 19.62s/it]                                                        {'loss': 0.783, 'learning_rate': 2.418444456627589e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4058/5198 [23:04:16<6:12:46, 19.62s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4059/5198 [23:04:33<6:01:34, 19.05s/it]                                                        {'loss': 0.3273, 'learning_rate': 2.4143829574498224e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4059/5198 [23:04:33<6:01:34, 19.05s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4060/5198 [23:04:52<5:58:46, 18.92s/it]                                                        {'loss': 0.7799, 'learning_rate': 2.4103244032697717e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4060/5198 [23:04:52<5:58:46, 18.92s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4061/5198 [23:05:10<5:52:08, 18.58s/it]                                                        {'loss': 0.8286, 'learning_rate': 2.406268795663108e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4061/5198 [23:05:10<5:52:08, 18.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4062/5198 [23:05:27<5:42:29, 18.09s/it]                                                        {'loss': 0.821, 'learning_rate': 2.4022161362043574e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4062/5198 [23:05:27<5:42:29, 18.09s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4063/5198 [23:05:45<5:40:16, 17.99s/it]                                                        {'loss': 0.7485, 'learning_rate': 2.3981664264669025e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4063/5198 [23:05:45<5:40:16, 17.99s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4064/5198 [23:06:03<5:40:45, 18.03s/it]                                                        {'loss': 0.7984, 'learning_rate': 2.3941196680229794e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4064/5198 [23:06:03<5:40:45, 18.03s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4065/5198 [23:06:20<5:35:33, 17.77s/it]                                                        {'loss': 0.7834, 'learning_rate': 2.3900758624436772e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4065/5198 [23:06:20<5:35:33, 17.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4066/5198 [23:06:38<5:34:55, 17.75s/it]                                                        {'loss': 0.7931, 'learning_rate': 2.3860350112989473e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4066/5198 [23:06:38<5:34:55, 17.75s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4067/5198 [23:06:56<5:37:03, 17.88s/it]                                                        {'loss': 0.8145, 'learning_rate': 2.3819971161575807e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4067/5198 [23:06:56<5:37:03, 17.88s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4068/5198 [23:07:14<5:39:47, 18.04s/it]                                                        {'loss': 0.751, 'learning_rate': 2.3779621785872252e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4068/5198 [23:07:14<5:39:47, 18.04s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4069/5198 [23:07:31<5:33:10, 17.71s/it]                                                        {'loss': 0.8732, 'learning_rate': 2.3739302001543918e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4069/5198 [23:07:31<5:33:10, 17.71s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4070/5198 [23:07:48<5:30:14, 17.57s/it]                                                        {'loss': 0.7665, 'learning_rate': 2.3699011824244234e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4070/5198 [23:07:48<5:30:14, 17.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4071/5198 [23:08:07<5:35:54, 17.88s/it]                                                        {'loss': 0.8196, 'learning_rate': 2.365875126961531e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4071/5198 [23:08:07<5:35:54, 17.88s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4072/5198 [23:08:24<5:31:36, 17.67s/it]                                                        {'loss': 0.8331, 'learning_rate': 2.3618520353287644e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4072/5198 [23:08:24<5:31:36, 17.67s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4073/5198 [23:08:42<5:33:20, 17.78s/it]                                                        {'loss': 0.8224, 'learning_rate': 2.3578319090880263e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4073/5198 [23:08:42<5:33:20, 17.78s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4074/5198 [23:08:59<5:29:06, 17.57s/it]                                                        {'loss': 0.7984, 'learning_rate': 2.3538147498000695e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4074/5198 [23:08:59<5:29:06, 17.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4075/5198 [23:09:17<5:30:37, 17.66s/it]                                                        {'loss': 0.8411, 'learning_rate': 2.349800559024492e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4075/5198 [23:09:17<5:30:37, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4076/5198 [23:10:41<11:42:31, 37.57s/it]                                                         {'loss': 0.7908, 'learning_rate': 2.3457893383197415e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4076/5198 [23:10:41<11:42:31, 37.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4077/5198 [23:10:59<9:54:13, 31.80s/it]                                                         {'loss': 0.8167, 'learning_rate': 2.3417810892431104e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4077/5198 [23:11:00<9:54:13, 31.80s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4078/5198 [23:11:17<8:35:06, 27.60s/it]                                                        {'loss': 0.8208, 'learning_rate': 2.3377758133507455e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4078/5198 [23:11:17<8:35:06, 27.60s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4079/5198 [23:11:35<7:39:08, 24.62s/it]                                                        {'loss': 0.7936, 'learning_rate': 2.3337735121976247e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4079/5198 [23:11:35<7:39:08, 24.62s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4080/5198 [23:11:52<6:57:01, 22.38s/it]                                                        {'loss': 0.794, 'learning_rate': 2.32977418733758e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4080/5198 [23:11:52<6:57:01, 22.38s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4081/5198 [23:12:11<6:35:22, 21.24s/it]                                                        {'loss': 0.785, 'learning_rate': 2.3257778403232954e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4081/5198 [23:12:11<6:35:22, 21.24s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4082/5198 [23:12:29<6:18:09, 20.33s/it]                                                        {'loss': 0.8159, 'learning_rate': 2.321784472706279e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4082/5198 [23:12:29<6:18:09, 20.33s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4083/5198 [23:12:47<6:03:13, 19.55s/it]                                                        {'loss': 0.7972, 'learning_rate': 2.317794086036901e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4083/5198 [23:12:47<6:03:13, 19.55s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4084/5198 [23:13:05<5:55:17, 19.14s/it]                                                        {'loss': 0.783, 'learning_rate': 2.3138066818643647e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4084/5198 [23:13:05<5:55:17, 19.14s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4085/5198 [23:13:21<5:40:48, 18.37s/it]                                                        {'loss': 0.8165, 'learning_rate': 2.3098222617367184e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4085/5198 [23:13:21<5:40:48, 18.37s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4086/5198 [23:13:39<5:36:38, 18.16s/it]                                                        {'loss': 0.8038, 'learning_rate': 2.30584082720085e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4086/5198 [23:13:39<5:36:38, 18.16s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4087/5198 [23:13:57<5:33:36, 18.02s/it]                                                        {'loss': 0.8168, 'learning_rate': 2.301862379802492e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4087/5198 [23:13:57<5:33:36, 18.02s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4088/5198 [23:14:14<5:31:10, 17.90s/it]                                                        {'loss': 0.7566, 'learning_rate': 2.297886921086211e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4088/5198 [23:14:14<5:31:10, 17.90s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4089/5198 [23:14:32<5:29:53, 17.85s/it]                                                        {'loss': 0.8144, 'learning_rate': 2.2939144525954194e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4089/5198 [23:14:32<5:29:53, 17.85s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4090/5198 [23:14:50<5:31:02, 17.93s/it]                                                        {'loss': 0.7102, 'learning_rate': 2.2899449758723657e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4090/5198 [23:14:50<5:31:02, 17.93s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4091/5198 [23:15:08<5:30:51, 17.93s/it]                                                        {'loss': 0.8416, 'learning_rate': 2.285978492458134e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4091/5198 [23:15:08<5:30:51, 17.93s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4092/5198 [23:15:26<5:29:14, 17.86s/it]                                                        {'loss': 0.7758, 'learning_rate': 2.282015003892659e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4092/5198 [23:15:26<5:29:14, 17.86s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4093/5198 [23:15:42<5:21:50, 17.48s/it]                                                        {'loss': 0.818, 'learning_rate': 2.2780545117146947e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4093/5198 [23:15:42<5:21:50, 17.48s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4094/5198 [23:16:00<5:20:23, 17.41s/it]                                                        {'loss': 0.7642, 'learning_rate': 2.2740970174618405e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4094/5198 [23:16:00<5:20:23, 17.41s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4095/5198 [23:16:18<5:24:50, 17.67s/it]                                                        {'loss': 0.7919, 'learning_rate': 2.270142522670541e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4095/5198 [23:16:18<5:24:50, 17.67s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4096/5198 [23:16:36<5:25:44, 17.74s/it]                                                        {'loss': 0.7947, 'learning_rate': 2.2661910288760545e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4096/5198 [23:16:36<5:25:44, 17.74s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4097/5198 [23:16:53<5:22:11, 17.56s/it]                                                        {'loss': 0.8628, 'learning_rate': 2.262242537612497e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4097/5198 [23:16:53<5:22:11, 17.56s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4098/5198 [23:17:10<5:19:12, 17.41s/it]                                                        {'loss': 0.681, 'learning_rate': 2.258297050412804e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4098/5198 [23:17:10<5:19:12, 17.41s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4099/5198 [23:17:28<5:24:39, 17.72s/it]                                                        {'loss': 0.7767, 'learning_rate': 2.254354568808752e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4099/5198 [23:17:29<5:24:39, 17.72s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4100/5198 [23:17:48<5:31:31, 18.12s/it]                                                        {'loss': 0.8143, 'learning_rate': 2.2504150943309455e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4100/5198 [23:17:48<5:31:31, 18.12s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4101/5198 [23:19:09<11:16:46, 37.02s/it]                                                         {'loss': 0.7655, 'learning_rate': 2.246478628508827e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4101/5198 [23:19:09<11:16:46, 37.02s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4102/5198 [23:19:27<9:31:32, 31.29s/it]                                                         {'loss': 0.7753, 'learning_rate': 2.242545172870665e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4102/5198 [23:19:27<9:31:32, 31.29s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4103/5198 [23:19:44<8:16:18, 27.19s/it]                                                        {'loss': 0.7293, 'learning_rate': 2.238614728943561e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4103/5198 [23:19:44<8:16:18, 27.19s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4104/5198 [23:20:03<7:27:17, 24.53s/it]                                                        {'loss': 0.7586, 'learning_rate': 2.2346872982534584e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4104/5198 [23:20:03<7:27:17, 24.53s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4105/5198 [23:20:20<6:48:52, 22.44s/it]                                                        {'loss': 0.7772, 'learning_rate': 2.2307628823251083e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4105/5198 [23:20:20<6:48:52, 22.44s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4106/5198 [23:20:37<6:19:18, 20.84s/it]                                                        {'loss': 0.839, 'learning_rate': 2.2268414826821117e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4106/5198 [23:20:37<6:19:18, 20.84s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4107/5198 [23:20:55<6:03:48, 20.01s/it]                                                        {'loss': 0.7699, 'learning_rate': 2.222923100846893e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4107/5198 [23:20:55<6:03:48, 20.01s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4108/5198 [23:21:13<5:50:55, 19.32s/it]                                                        {'loss': 0.8064, 'learning_rate': 2.2190077383406938e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4108/5198 [23:21:13<5:50:55, 19.32s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4109/5198 [23:21:30<5:38:12, 18.63s/it]                                                        {'loss': 0.7926, 'learning_rate': 2.2150953966835996e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4109/5198 [23:21:30<5:38:12, 18.63s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4110/5198 [23:21:48<5:37:03, 18.59s/it]                                                        {'loss': 0.8499, 'learning_rate': 2.211186077394516e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4110/5198 [23:21:48<5:37:03, 18.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4111/5198 [23:22:06<5:30:39, 18.25s/it]                                                        {'loss': 0.7375, 'learning_rate': 2.207279781991173e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4111/5198 [23:22:06<5:30:39, 18.25s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4112/5198 [23:22:24<5:27:18, 18.08s/it]                                                        {'loss': 0.7568, 'learning_rate': 2.2033765119901294e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4112/5198 [23:22:24<5:27:18, 18.08s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4113/5198 [23:22:41<5:25:42, 18.01s/it]                                                        {'loss': 0.8219, 'learning_rate': 2.1994762689067705e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4113/5198 [23:22:41<5:25:42, 18.01s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4114/5198 [23:22:59<5:22:40, 17.86s/it]                                                        {'loss': 0.8272, 'learning_rate': 2.1955790542553036e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4114/5198 [23:22:59<5:22:40, 17.86s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4115/5198 [23:23:17<5:21:28, 17.81s/it]                                                        {'loss': 0.7609, 'learning_rate': 2.1916848695487615e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4115/5198 [23:23:17<5:21:28, 17.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4116/5198 [23:23:34<5:17:17, 17.59s/it]                                                        {'loss': 0.8189, 'learning_rate': 2.1877937162990015e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4116/5198 [23:23:34<5:17:17, 17.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4117/5198 [23:23:51<5:16:17, 17.56s/it]                                                        {'loss': 0.8199, 'learning_rate': 2.1839055960167e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4117/5198 [23:23:51<5:16:17, 17.56s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4118/5198 [23:24:09<5:17:34, 17.64s/it]                                                        {'loss': 0.7795, 'learning_rate': 2.180020510211367e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4118/5198 [23:24:09<5:17:34, 17.64s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4119/5198 [23:24:26<5:14:38, 17.50s/it]                                                        {'loss': 0.8135, 'learning_rate': 2.1761384603913203e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4119/5198 [23:24:26<5:14:38, 17.50s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4120/5198 [23:24:44<5:16:16, 17.60s/it]                                                        {'loss': 0.7975, 'learning_rate': 2.172259448063704e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4120/5198 [23:24:44<5:16:16, 17.60s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4121/5198 [23:25:01<5:13:47, 17.48s/it]                                                        {'loss': 0.7619, 'learning_rate': 2.1683834747344913e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4121/5198 [23:25:01<5:13:47, 17.48s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4122/5198 [23:25:19<5:15:48, 17.61s/it]                                                        {'loss': 0.8267, 'learning_rate': 2.1645105419084587e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4122/5198 [23:25:19<5:15:48, 17.61s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4123/5198 [23:25:36<5:09:40, 17.28s/it]                                                        {'loss': 0.3358, 'learning_rate': 2.160640651089221e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4123/5198 [23:25:36<5:09:40, 17.28s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4124/5198 [23:25:53<5:09:58, 17.32s/it]                                                        {'loss': 0.7547, 'learning_rate': 2.1567738037791998e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4124/5198 [23:25:53<5:09:58, 17.32s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4125/5198 [23:26:11<5:15:09, 17.62s/it]                                                        {'loss': 0.7506, 'learning_rate': 2.152910001479638e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4125/5198 [23:26:11<5:15:09, 17.62s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4126/5198 [23:27:35<11:08:43, 37.43s/it]                                                         {'loss': 0.7962, 'learning_rate': 2.1490492456905964e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4126/5198 [23:27:35<11:08:43, 37.43s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4127/5198 [23:27:53<9:25:46, 31.70s/it]                                                         {'loss': 0.8188, 'learning_rate': 2.1451915379109546e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4127/5198 [23:27:53<9:25:46, 31.70s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4128/5198 [23:28:10<8:05:49, 27.24s/it]                                                        {'loss': 0.8047, 'learning_rate': 2.141336879638406e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4128/5198 [23:28:10<8:05:49, 27.24s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4129/5198 [23:28:28<7:13:59, 24.36s/it]                                                        {'loss': 0.7962, 'learning_rate': 2.1374852723694595e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4129/5198 [23:28:28<7:13:59, 24.36s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4130/5198 [23:28:46<6:39:56, 22.47s/it]                                                        {'loss': 0.7343, 'learning_rate': 2.133636717599451e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4130/5198 [23:28:46<6:39:56, 22.47s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4131/5198 [23:29:03<6:12:38, 20.95s/it]                                                        {'loss': 0.7318, 'learning_rate': 2.1297912168225086e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4131/5198 [23:29:03<6:12:38, 20.95s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4132/5198 [23:29:21<5:55:30, 20.01s/it]                                                        {'loss': 0.751, 'learning_rate': 2.1259487715316e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4132/5198 [23:29:21<5:55:30, 20.01s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4133/5198 [23:29:38<5:40:21, 19.18s/it]                                                        {'loss': 0.7787, 'learning_rate': 2.1221093832184903e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4133/5198 [23:29:38<5:40:21, 19.18s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4134/5198 [23:29:55<5:28:50, 18.54s/it]                                                        {'loss': 0.7716, 'learning_rate': 2.118273053373757e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4134/5198 [23:29:56<5:28:50, 18.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4135/5198 [23:30:13<5:21:47, 18.16s/it]                                                        {'loss': 0.782, 'learning_rate': 2.1144397834868034e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4135/5198 [23:30:13<5:21:47, 18.16s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4136/5198 [23:30:30<5:14:59, 17.80s/it]                                                        {'loss': 0.8, 'learning_rate': 2.1106095750458332e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4136/5198 [23:30:30<5:14:59, 17.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4137/5198 [23:30:48<5:16:19, 17.89s/it]                                                        {'loss': 0.8158, 'learning_rate': 2.106782429537866e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4137/5198 [23:30:48<5:16:19, 17.89s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4138/5198 [23:31:05<5:10:41, 17.59s/it]                                                        {'loss': 0.8079, 'learning_rate': 2.1029583484487315e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4138/5198 [23:31:05<5:10:41, 17.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4139/5198 [23:31:22<5:11:11, 17.63s/it]                                                        {'loss': 0.8039, 'learning_rate': 2.0991373332630683e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4139/5198 [23:31:22<5:11:11, 17.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4140/5198 [23:31:40<5:12:30, 17.72s/it]                                                        {'loss': 0.7836, 'learning_rate': 2.0953193854643274e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4140/5198 [23:31:40<5:12:30, 17.72s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4141/5198 [23:31:58<5:12:20, 17.73s/it]                                                        {'loss': 0.8201, 'learning_rate': 2.0915045065347673e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4141/5198 [23:31:58<5:12:20, 17.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4142/5198 [23:32:17<5:16:36, 17.99s/it]                                                        {'loss': 0.7794, 'learning_rate': 2.0876926979554545e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4142/5198 [23:32:17<5:16:36, 17.99s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4143/5198 [23:32:33<5:09:49, 17.62s/it]                                                        {'loss': 0.7807, 'learning_rate': 2.0838839612062633e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4143/5198 [23:32:33<5:09:49, 17.62s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4144/5198 [23:32:52<5:13:07, 17.82s/it]                                                        {'loss': 0.8246, 'learning_rate': 2.080078297765884e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4144/5198 [23:32:52<5:13:07, 17.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4145/5198 [23:33:09<5:07:27, 17.52s/it]                                                        {'loss': 0.7846, 'learning_rate': 2.0762757091117937e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4145/5198 [23:33:09<5:07:27, 17.52s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4146/5198 [23:33:27<5:13:11, 17.86s/it]                                                        {'loss': 0.7632, 'learning_rate': 2.0724761967202987e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4146/5198 [23:33:27<5:13:11, 17.86s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4147/5198 [23:33:46<5:18:09, 18.16s/it]                                                        {'loss': 0.8512, 'learning_rate': 2.0686797620664987e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4147/5198 [23:33:46<5:18:09, 18.16s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4148/5198 [23:34:04<5:16:49, 18.10s/it]                                                        {'loss': 0.336, 'learning_rate': 2.0648864066242937e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4148/5198 [23:34:04<5:16:49, 18.10s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4149/5198 [23:34:21<5:11:46, 17.83s/it]                                                        {'loss': 0.7948, 'learning_rate': 2.0610961318664013e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4149/5198 [23:34:21<5:11:46, 17.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4150/5198 [23:34:39<5:10:07, 17.76s/it]                                                        {'loss': 0.8303, 'learning_rate': 2.0573089392643362e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4150/5198 [23:34:39<5:10:07, 17.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4151/5198 [23:36:05<11:08:22, 38.30s/it]                                                         {'loss': 0.7216, 'learning_rate': 2.0535248302884147e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4151/5198 [23:36:05<11:08:22, 38.30s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4152/5198 [23:36:22<9:16:13, 31.91s/it]                                                         {'loss': 0.7903, 'learning_rate': 2.0497438064077603e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4152/5198 [23:36:22<9:16:13, 31.91s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4153/5198 [23:36:39<7:55:38, 27.31s/it]                                                        {'loss': 0.8182, 'learning_rate': 2.045965869090295e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4153/5198 [23:36:39<7:55:38, 27.31s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4154/5198 [23:36:56<7:00:50, 24.19s/it]                                                        {'loss': 0.815, 'learning_rate': 2.0421910198027452e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4154/5198 [23:36:56<7:00:50, 24.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4155/5198 [23:37:13<6:27:29, 22.29s/it]                                                        {'loss': 0.824, 'learning_rate': 2.0384192600106335e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4155/5198 [23:37:13<6:27:29, 22.29s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4156/5198 [23:37:31<6:03:32, 20.93s/it]                                                        {'loss': 0.7648, 'learning_rate': 2.0346505911782956e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4156/5198 [23:37:31<6:03:32, 20.93s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4157/5198 [23:37:48<5:42:14, 19.73s/it]                                                        {'loss': 0.7877, 'learning_rate': 2.0308850147688484e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4157/5198 [23:37:48<5:42:14, 19.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4158/5198 [23:38:06<5:34:57, 19.32s/it]                                                        {'loss': 0.8072, 'learning_rate': 2.0271225322442255e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4158/5198 [23:38:06<5:34:57, 19.32s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4159/5198 [23:38:25<5:28:18, 18.96s/it]                                                        {'loss': 0.8039, 'learning_rate': 2.0233631450651525e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4159/5198 [23:38:25<5:28:18, 18.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4160/5198 [23:38:42<5:20:36, 18.53s/it]                                                        {'loss': 0.8095, 'learning_rate': 2.019606854691145e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4160/5198 [23:38:42<5:20:36, 18.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4161/5198 [23:38:59<5:11:40, 18.03s/it]                                                        {'loss': 0.7716, 'learning_rate': 2.0158536625805325e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4161/5198 [23:38:59<5:11:40, 18.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4162/5198 [23:39:17<5:10:20, 17.97s/it]                                                        {'loss': 0.7943, 'learning_rate': 2.01210357019043e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4162/5198 [23:39:17<5:10:20, 17.97s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4163/5198 [23:39:34<5:04:09, 17.63s/it]                                                        {'loss': 0.795, 'learning_rate': 2.008356578976752e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4163/5198 [23:39:34<5:04:09, 17.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4164/5198 [23:39:51<5:04:01, 17.64s/it]                                                        {'loss': 0.8385, 'learning_rate': 2.004612690394212e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4164/5198 [23:39:51<5:04:01, 17.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4165/5198 [23:40:08<5:00:58, 17.48s/it]                                                        {'loss': 0.7973, 'learning_rate': 2.0008719058963144e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4165/5198 [23:40:08<5:00:58, 17.48s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4166/5198 [23:40:27<5:07:35, 17.88s/it]                                                        {'loss': 0.8341, 'learning_rate': 1.997134226935361e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4166/5198 [23:40:27<5:07:35, 17.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4167/5198 [23:40:45<5:06:14, 17.82s/it]                                                        {'loss': 0.8573, 'learning_rate': 1.9933996549624468e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4167/5198 [23:40:45<5:06:14, 17.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4168/5198 [23:41:03<5:05:22, 17.79s/it]                                                        {'loss': 0.8502, 'learning_rate': 1.9896681914274616e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4168/5198 [23:41:03<5:05:22, 17.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4169/5198 [23:41:20<5:05:12, 17.80s/it]                                                        {'loss': 0.8158, 'learning_rate': 1.9859398377790872e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4169/5198 [23:41:20<5:05:12, 17.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4170/5198 [23:41:39<5:06:39, 17.90s/it]                                                        {'loss': 0.8498, 'learning_rate': 1.982214595464804e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4170/5198 [23:41:39<5:06:39, 17.90s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4171/5198 [23:41:56<5:04:29, 17.79s/it]                                                        {'loss': 0.8258, 'learning_rate': 1.97849246593087e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4171/5198 [23:41:56<5:04:29, 17.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4172/5198 [23:42:14<5:02:57, 17.72s/it]                                                        {'loss': 0.8346, 'learning_rate': 1.9747734506223525e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4172/5198 [23:42:14<5:02:57, 17.72s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4173/5198 [23:42:31<5:03:02, 17.74s/it]                                                        {'loss': 0.7461, 'learning_rate': 1.9710575509831008e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4173/5198 [23:42:31<5:03:02, 17.74s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4174/5198 [23:42:50<5:04:30, 17.84s/it]                                                        {'loss': 0.7505, 'learning_rate': 1.967344768455747e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4174/5198 [23:42:50<5:04:30, 17.84s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4175/5198 [23:43:07<5:01:51, 17.70s/it]                                                        {'loss': 0.8055, 'learning_rate': 1.9636351044817292e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4175/5198 [23:43:07<5:01:51, 17.70s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4176/5198 [23:44:31<10:38:47, 37.50s/it]                                                         {'loss': 0.3344, 'learning_rate': 1.9599285605012643e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4176/5198 [23:44:31<10:38:47, 37.50s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4177/5198 [23:44:48<8:56:01, 31.50s/it]                                                         {'loss': 0.8375, 'learning_rate': 1.9562251379533593e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4177/5198 [23:44:48<8:56:01, 31.50s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4178/5198 [23:45:07<7:48:56, 27.58s/it]                                                        {'loss': 0.8404, 'learning_rate': 1.952524838275811e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4178/5198 [23:45:07<7:48:56, 27.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4179/5198 [23:45:23<6:52:59, 24.32s/it]                                                        {'loss': 0.7889, 'learning_rate': 1.9488276629052026e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4179/5198 [23:45:23<6:52:59, 24.32s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4180/5198 [23:45:40<6:14:02, 22.05s/it]                                                        {'loss': 0.8004, 'learning_rate': 1.945133613276907e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4180/5198 [23:45:40<6:14:02, 22.05s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4181/5198 [23:45:57<5:49:08, 20.60s/it]                                                        {'loss': 0.8001, 'learning_rate': 1.941442690825076e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4181/5198 [23:45:57<5:49:08, 20.60s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4182/5198 [23:46:15<5:34:25, 19.75s/it]                                                        {'loss': 0.7419, 'learning_rate': 1.937754896982663e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4182/5198 [23:46:15<5:34:25, 19.75s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4183/5198 [23:46:33<5:23:15, 19.11s/it]                                                        {'loss': 0.8564, 'learning_rate': 1.9340702331813842e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4183/5198 [23:46:33<5:23:15, 19.11s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4184/5198 [23:46:50<5:13:32, 18.55s/it]                                                        {'loss': 0.7983, 'learning_rate': 1.9303887008517618e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4184/5198 [23:46:50<5:13:32, 18.55s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4185/5198 [23:47:08<5:10:03, 18.37s/it]                                                        {'loss': 0.7171, 'learning_rate': 1.9267103014230935e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4185/5198 [23:47:08<5:10:03, 18.37s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4186/5198 [23:47:26<5:07:12, 18.21s/it]                                                        {'loss': 0.7549, 'learning_rate': 1.923035036323452e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4186/5198 [23:47:26<5:07:12, 18.21s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4187/5198 [23:47:43<5:01:00, 17.86s/it]                                                        {'loss': 0.7595, 'learning_rate': 1.91936290697971e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4187/5198 [23:47:43<5:01:00, 17.86s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4188/5198 [23:48:00<4:59:59, 17.82s/it]                                                        {'loss': 0.7789, 'learning_rate': 1.9156939148175125e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4188/5198 [23:48:00<4:59:59, 17.82s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4189/5198 [23:48:19<5:01:51, 17.95s/it]                                                        {'loss': 0.7302, 'learning_rate': 1.9120280612612873e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4189/5198 [23:48:19<5:01:51, 17.95s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4190/5198 [23:48:37<5:01:42, 17.96s/it]                                                        {'loss': 0.7422, 'learning_rate': 1.9083653477342467e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4190/5198 [23:48:37<5:01:42, 17.96s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4191/5198 [23:48:54<4:57:25, 17.72s/it]                                                        {'loss': 0.8488, 'learning_rate': 1.904705775658381e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4191/5198 [23:48:54<4:57:25, 17.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4192/5198 [23:49:12<4:58:03, 17.78s/it]                                                        {'loss': 0.7619, 'learning_rate': 1.9010493464544621e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4192/5198 [23:49:12<4:58:03, 17.78s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4193/5198 [23:49:29<4:54:45, 17.60s/it]                                                        {'loss': 0.3227, 'learning_rate': 1.8973960615420416e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4193/5198 [23:49:29<4:54:45, 17.60s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4194/5198 [23:49:47<4:58:50, 17.86s/it]                                                        {'loss': 0.7117, 'learning_rate': 1.8937459223394517e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4194/5198 [23:49:47<4:58:50, 17.86s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4195/5198 [23:50:05<4:58:35, 17.86s/it]                                                        {'loss': 0.7363, 'learning_rate': 1.8900989302637985e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4195/5198 [23:50:05<4:58:35, 17.86s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4196/5198 [23:50:23<4:58:06, 17.85s/it]                                                        {'loss': 0.7635, 'learning_rate': 1.8864550867309771e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4196/5198 [23:50:23<4:58:06, 17.85s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4197/5198 [23:50:41<4:56:30, 17.77s/it]                                                        {'loss': 0.8434, 'learning_rate': 1.8828143931556442e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4197/5198 [23:50:41<4:56:30, 17.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4198/5198 [23:50:58<4:53:49, 17.63s/it]                                                        {'loss': 0.8002, 'learning_rate': 1.8791768509512487e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4198/5198 [23:50:58<4:53:49, 17.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4199/5198 [23:51:16<4:54:48, 17.71s/it]                                                        {'loss': 0.8094, 'learning_rate': 1.875542461530011e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4199/5198 [23:51:16<4:54:48, 17.71s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4200/5198 [23:51:33<4:53:27, 17.64s/it]                                                        {'loss': 0.8197, 'learning_rate': 1.871911226302917e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4200/5198 [23:51:33<4:53:27, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4201/5198 [23:52:56<10:16:07, 37.08s/it]                                                         {'loss': 0.7702, 'learning_rate': 1.868283146679747e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4201/5198 [23:52:56<10:16:07, 37.08s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4202/5198 [23:53:14<8:42:46, 31.49s/it]                                                         {'loss': 0.8196, 'learning_rate': 1.8646582240690414e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4202/5198 [23:53:14<8:42:46, 31.49s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4203/5198 [23:53:31<7:30:28, 27.16s/it]                                                        {'loss': 0.7327, 'learning_rate': 1.8610364598781227e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4203/5198 [23:53:31<7:30:28, 27.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4204/5198 [23:53:49<6:44:40, 24.43s/it]                                                        {'loss': 0.7794, 'learning_rate': 1.8574178555130818e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4204/5198 [23:53:49<6:44:40, 24.43s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4205/5198 [23:54:07<6:11:38, 22.46s/it]                                                        {'loss': 0.7618, 'learning_rate': 1.8538024123787868e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4205/5198 [23:54:07<6:11:38, 22.46s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4206/5198 [23:54:25<5:49:25, 21.13s/it]                                                        {'loss': 0.7064, 'learning_rate': 1.8501901318788773e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4206/5198 [23:54:25<5:49:25, 21.13s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4207/5198 [23:54:42<5:29:06, 19.93s/it]                                                        {'loss': 0.7983, 'learning_rate': 1.8465810154157626e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4207/5198 [23:54:42<5:29:06, 19.93s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4208/5198 [23:55:01<5:24:11, 19.65s/it]                                                        {'loss': 0.7708, 'learning_rate': 1.8429750643906331e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4208/5198 [23:55:01<5:24:11, 19.65s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4209/5198 [23:55:19<5:13:49, 19.04s/it]                                                        {'loss': 0.774, 'learning_rate': 1.8393722802034331e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4209/5198 [23:55:19<5:13:49, 19.04s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4210/5198 [23:55:36<5:05:43, 18.57s/it]                                                        {'loss': 0.7837, 'learning_rate': 1.835772664252895e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4210/5198 [23:55:36<5:05:43, 18.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4211/5198 [23:55:53<4:57:20, 18.08s/it]                                                        {'loss': 0.7509, 'learning_rate': 1.832176217936511e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4211/5198 [23:55:53<4:57:20, 18.08s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4212/5198 [23:56:15<5:15:02, 19.17s/it]                                                        {'loss': 0.7982, 'learning_rate': 1.8285829426505453e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4212/5198 [23:56:15<5:15:02, 19.17s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4213/5198 [23:56:33<5:08:47, 18.81s/it]                                                        {'loss': 0.7715, 'learning_rate': 1.8249928397900351e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4213/5198 [23:56:33<5:08:47, 18.81s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4214/5198 [23:56:50<5:01:33, 18.39s/it]                                                        {'loss': 0.808, 'learning_rate': 1.8214059107487726e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4214/5198 [23:56:51<5:01:33, 18.39s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4215/5198 [23:57:08<4:59:02, 18.25s/it]                                                        {'loss': 0.8101, 'learning_rate': 1.8178221569193343e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4215/5198 [23:57:08<4:59:02, 18.25s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4216/5198 [23:57:27<4:58:15, 18.22s/it]                                                        {'loss': 0.8151, 'learning_rate': 1.8142415796930568e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4216/5198 [23:57:27<4:58:15, 18.22s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4217/5198 [23:57:44<4:54:04, 17.99s/it]                                                        {'loss': 0.8005, 'learning_rate': 1.8106641804600411e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4217/5198 [23:57:44<4:54:04, 17.99s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4218/5198 [23:58:01<4:50:10, 17.77s/it]                                                        {'loss': 0.7864, 'learning_rate': 1.8070899606091586e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4218/5198 [23:58:01<4:50:10, 17.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4219/5198 [23:58:19<4:50:01, 17.77s/it]                                                        {'loss': 0.7959, 'learning_rate': 1.8035189215280423e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4219/5198 [23:58:19<4:50:01, 17.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4220/5198 [23:58:38<4:54:19, 18.06s/it]                                                        {'loss': 0.7652, 'learning_rate': 1.799951064603095e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4220/5198 [23:58:38<4:54:19, 18.06s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4221/5198 [23:58:55<4:52:08, 17.94s/it]                                                        {'loss': 0.7647, 'learning_rate': 1.7963863912194768e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4221/5198 [23:58:55<4:52:08, 17.94s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4222/5198 [23:59:13<4:52:21, 17.97s/it]                                                        {'loss': 0.7273, 'learning_rate': 1.7928249027611255e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4222/5198 [23:59:13<4:52:21, 17.97s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4223/5198 [23:59:31<4:50:19, 17.87s/it]                                                        {'loss': 0.7961, 'learning_rate': 1.789266600610724e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4223/5198 [23:59:31<4:50:19, 17.87s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4224/5198 [23:59:48<4:46:10, 17.63s/it]                                                        {'loss': 0.8154, 'learning_rate': 1.7857114861497337e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4224/5198 [23:59:48<4:46:10, 17.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4225/5198 [24:00:06<4:45:31, 17.61s/it]                                                        {'loss': 0.7961, 'learning_rate': 1.782159560758373e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4225/5198 [24:00:06<4:45:31, 17.61s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4226/5198 [24:01:32<10:19:50, 38.26s/it]                                                         {'loss': 0.7752, 'learning_rate': 1.7786108258156154e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4226/5198 [24:01:32<10:19:50, 38.26s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4227/5198 [24:01:50<8:41:23, 32.22s/it]                                                         {'loss': 0.8087, 'learning_rate': 1.7750652826992077e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4227/5198 [24:01:50<8:41:23, 32.22s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4228/5198 [24:02:08<7:31:03, 27.90s/it]                                                        {'loss': 0.7442, 'learning_rate': 1.7715229327856498e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4228/5198 [24:02:08<7:31:03, 27.90s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4229/5198 [24:02:26<6:39:47, 24.75s/it]                                                        {'loss': 0.7976, 'learning_rate': 1.7679837774502052e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4229/5198 [24:02:26<6:39:47, 24.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4230/5198 [24:02:44<6:09:04, 22.88s/it]                                                        {'loss': 0.8071, 'learning_rate': 1.7644478180668945e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4230/5198 [24:02:44<6:09:04, 22.88s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4231/5198 [24:03:02<5:43:44, 21.33s/it]                                                        {'loss': 0.7666, 'learning_rate': 1.7609150560084986e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4231/5198 [24:03:02<5:43:44, 21.33s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4232/5198 [24:03:19<5:23:56, 20.12s/it]                                                        {'loss': 0.7387, 'learning_rate': 1.7573854926465582e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4232/5198 [24:03:19<5:23:56, 20.12s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4233/5198 [24:03:36<5:08:10, 19.16s/it]                                                        {'loss': 0.8207, 'learning_rate': 1.7538591293513685e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4233/5198 [24:03:36<5:08:10, 19.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4234/5198 [24:03:53<4:56:56, 18.48s/it]                                                        {'loss': 0.7874, 'learning_rate': 1.7503359674919929e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4234/5198 [24:03:53<4:56:56, 18.48s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4235/5198 [24:04:10<4:49:53, 18.06s/it]                                                        {'loss': 0.7581, 'learning_rate': 1.746816008436234e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4235/5198 [24:04:10<4:49:53, 18.06s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4236/5198 [24:04:28<4:49:39, 18.07s/it]                                                        {'loss': 0.7929, 'learning_rate': 1.7432992535506687e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4236/5198 [24:04:28<4:49:39, 18.07s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4237/5198 [24:04:45<4:44:41, 17.77s/it]                                                        {'loss': 0.8093, 'learning_rate': 1.7397857042006194e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4237/5198 [24:04:45<4:44:41, 17.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4238/5198 [24:05:03<4:44:43, 17.80s/it]                                                        {'loss': 0.842, 'learning_rate': 1.736275361750167e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4238/5198 [24:05:03<4:44:43, 17.80s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4239/5198 [24:05:20<4:42:46, 17.69s/it]                                                        {'loss': 0.8215, 'learning_rate': 1.7327682275621506e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4239/5198 [24:05:20<4:42:46, 17.69s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4240/5198 [24:05:38<4:44:22, 17.81s/it]                                                        {'loss': 0.814, 'learning_rate': 1.7292643029981525e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4240/5198 [24:05:38<4:44:22, 17.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4241/5198 [24:05:55<4:39:14, 17.51s/it]                                                        {'loss': 0.7705, 'learning_rate': 1.7257635894185232e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4241/5198 [24:05:55<4:39:14, 17.51s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4242/5198 [24:06:13<4:38:30, 17.48s/it]                                                        {'loss': 0.7891, 'learning_rate': 1.7222660881823594e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4242/5198 [24:06:13<4:38:30, 17.48s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4243/5198 [24:06:30<4:35:46, 17.33s/it]                                                        {'loss': 0.7479, 'learning_rate': 1.7187718006475117e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4243/5198 [24:06:30<4:35:46, 17.33s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4244/5198 [24:06:46<4:33:02, 17.17s/it]                                                        {'loss': 0.7904, 'learning_rate': 1.7152807281705809e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4244/5198 [24:06:46<4:33:02, 17.17s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4245/5198 [24:07:04<4:33:02, 17.19s/it]                                                        {'loss': 0.8092, 'learning_rate': 1.7117928721069233e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4245/5198 [24:07:04<4:33:02, 17.19s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4246/5198 [24:07:22<4:39:00, 17.58s/it]                                                        {'loss': 0.7432, 'learning_rate': 1.708308233810644e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4246/5198 [24:07:22<4:39:00, 17.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4247/5198 [24:07:40<4:39:08, 17.61s/it]                                                        {'loss': 0.7981, 'learning_rate': 1.704826814634597e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4247/5198 [24:07:40<4:39:08, 17.61s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4248/5198 [24:07:57<4:35:41, 17.41s/it]                                                        {'loss': 0.7777, 'learning_rate': 1.701348615930397e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4248/5198 [24:07:57<4:35:41, 17.41s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4249/5198 [24:08:15<4:38:21, 17.60s/it]                                                        {'loss': 0.6927, 'learning_rate': 1.6978736390483896e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4249/5198 [24:08:15<4:38:21, 17.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4250/5198 [24:08:32<4:34:25, 17.37s/it]                                                        {'loss': 0.7444, 'learning_rate': 1.6944018853376898e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4250/5198 [24:08:32<4:34:25, 17.37s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4251/5198 [24:09:55<9:45:40, 37.11s/it]                                                        {'loss': 0.8216, 'learning_rate': 1.6909333561461471e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4251/5198 [24:09:55<9:45:40, 37.11s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4252/5198 [24:10:13<8:13:04, 31.27s/it]                                                        {'loss': 0.781, 'learning_rate': 1.6874680528203657e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4252/5198 [24:10:13<8:13:04, 31.27s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4253/5198 [24:10:30<7:08:58, 27.24s/it]                                                        {'loss': 0.7885, 'learning_rate': 1.6840059767056949e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4253/5198 [24:10:30<7:08:58, 27.24s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4254/5198 [24:10:48<6:23:59, 24.41s/it]                                                        {'loss': 0.8255, 'learning_rate': 1.6805471291462316e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4254/5198 [24:10:48<6:23:59, 24.41s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4255/5198 [24:11:07<5:55:10, 22.60s/it]                                                        {'loss': 0.7593, 'learning_rate': 1.6770915114848197e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4255/5198 [24:11:07<5:55:10, 22.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4256/5198 [24:11:24<5:29:51, 21.01s/it]                                                        {'loss': 0.8238, 'learning_rate': 1.67363912506305e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4256/5198 [24:11:24<5:29:51, 21.01s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4257/5198 [24:11:40<5:08:37, 19.68s/it]                                                        {'loss': 0.8287, 'learning_rate': 1.6701899712212565e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4257/5198 [24:11:40<5:08:37, 19.68s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4258/5198 [24:11:58<4:59:27, 19.11s/it]                                                        {'loss': 0.7967, 'learning_rate': 1.66674405129852e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4258/5198 [24:11:58<4:59:27, 19.11s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4259/5198 [24:12:16<4:53:12, 18.74s/it]                                                        {'loss': 0.7356, 'learning_rate': 1.6633013666326636e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4259/5198 [24:12:16<4:53:12, 18.74s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4260/5198 [24:12:34<4:49:03, 18.49s/it]                                                        {'loss': 0.311, 'learning_rate': 1.6598619185602616e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4260/5198 [24:12:34<4:49:03, 18.49s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4261/5198 [24:12:52<4:44:27, 18.22s/it]                                                        {'loss': 0.7839, 'learning_rate': 1.656425708416617e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4261/5198 [24:12:52<4:44:27, 18.22s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4262/5198 [24:13:09<4:40:14, 17.96s/it]                                                        {'loss': 0.8048, 'learning_rate': 1.6529927375357957e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4262/5198 [24:13:09<4:40:14, 17.96s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4263/5198 [24:13:26<4:36:08, 17.72s/it]                                                        {'loss': 0.7718, 'learning_rate': 1.6495630072505841e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4263/5198 [24:13:26<4:36:08, 17.72s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4264/5198 [24:13:43<4:33:30, 17.57s/it]                                                        {'loss': 0.7909, 'learning_rate': 1.6461365188925304e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4264/5198 [24:13:43<4:33:30, 17.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4265/5198 [24:14:01<4:33:19, 17.58s/it]                                                        {'loss': 0.808, 'learning_rate': 1.642713273791914e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4265/5198 [24:14:01<4:33:19, 17.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4266/5198 [24:14:19<4:35:59, 17.77s/it]                                                        {'loss': 0.8515, 'learning_rate': 1.6392932732777489e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4266/5198 [24:14:19<4:35:59, 17.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4267/5198 [24:14:36<4:30:48, 17.45s/it]                                                        {'loss': 0.7726, 'learning_rate': 1.6358765186778057e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4267/5198 [24:14:36<4:30:48, 17.45s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4268/5198 [24:14:53<4:30:56, 17.48s/it]                                                        {'loss': 0.7761, 'learning_rate': 1.6324630113185835e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4268/5198 [24:14:53<4:30:56, 17.48s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4269/5198 [24:15:11<4:30:29, 17.47s/it]                                                        {'loss': 0.836, 'learning_rate': 1.629052752525323e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4269/5198 [24:15:11<4:30:29, 17.47s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4270/5198 [24:15:29<4:31:47, 17.57s/it]                                                        {'loss': 0.812, 'learning_rate': 1.625645743622003e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4270/5198 [24:15:29<4:31:47, 17.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4271/5198 [24:15:46<4:30:29, 17.51s/it]                                                        {'loss': 0.8148, 'learning_rate': 1.6222419859313443e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4271/5198 [24:15:46<4:30:29, 17.51s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4272/5198 [24:16:04<4:30:19, 17.52s/it]                                                        {'loss': 0.7863, 'learning_rate': 1.6188414807747999e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4272/5198 [24:16:04<4:30:19, 17.52s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4273/5198 [24:16:19<4:22:52, 17.05s/it]                                                        {'loss': 0.2962, 'learning_rate': 1.6154442294725636e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4273/5198 [24:16:19<4:22:52, 17.05s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4274/5198 [24:16:38<4:28:52, 17.46s/it]                                                        {'loss': 0.7735, 'learning_rate': 1.6120502333435695e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4274/5198 [24:16:38<4:28:52, 17.46s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4275/5198 [24:16:55<4:27:37, 17.40s/it]                                                        {'loss': 0.7927, 'learning_rate': 1.6086594937054767e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4275/5198 [24:16:55<4:27:37, 17.40s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4276/5198 [24:18:19<9:35:24, 37.44s/it]                                                        {'loss': 0.7884, 'learning_rate': 1.6052720118746923e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4276/5198 [24:18:19<9:35:24, 37.44s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4277/5198 [24:18:37<8:04:21, 31.55s/it]                                                        {'loss': 0.7638, 'learning_rate': 1.6018877891663521e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4277/5198 [24:18:37<8:04:21, 31.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4278/5198 [24:18:56<7:04:05, 27.66s/it]                                                        {'loss': 0.8017, 'learning_rate': 1.5985068268943283e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4278/5198 [24:18:56<7:04:05, 27.66s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4279/5198 [24:19:13<6:14:03, 24.42s/it]                                                        {'loss': 0.7997, 'learning_rate': 1.5951291263712255e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4279/5198 [24:19:13<6:14:03, 24.42s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4280/5198 [24:19:30<5:42:55, 22.41s/it]                                                        {'loss': 0.761, 'learning_rate': 1.5917546889083834e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4280/5198 [24:19:30<5:42:55, 22.41s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4281/5198 [24:19:47<5:17:55, 20.80s/it]                                                        {'loss': 0.8015, 'learning_rate': 1.5883835158158767e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4281/5198 [24:19:47<5:17:55, 20.80s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4282/5198 [24:20:05<5:02:30, 19.82s/it]                                                        {'loss': 0.8375, 'learning_rate': 1.5850156084025091e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4282/5198 [24:20:05<5:02:30, 19.82s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4283/5198 [24:20:23<4:53:03, 19.22s/it]                                                        {'loss': 0.7424, 'learning_rate': 1.5816509679758185e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4283/5198 [24:20:23<4:53:03, 19.22s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4284/5198 [24:20:41<4:48:20, 18.93s/it]                                                        {'loss': 0.8334, 'learning_rate': 1.578289595842074e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4284/5198 [24:20:41<4:48:20, 18.93s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4285/5198 [24:20:59<4:43:32, 18.63s/it]                                                        {'loss': 0.7762, 'learning_rate': 1.5749314933062754e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4285/5198 [24:20:59<4:43:32, 18.63s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4286/5198 [24:21:16<4:37:27, 18.25s/it]                                                        {'loss': 0.7895, 'learning_rate': 1.5715766616721584e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4286/5198 [24:21:16<4:37:27, 18.25s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4287/5198 [24:21:35<4:38:16, 18.33s/it]                                                        {'loss': 0.758, 'learning_rate': 1.5682251022421757e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4287/5198 [24:21:35<4:38:16, 18.33s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4288/5198 [24:21:52<4:32:12, 17.95s/it]                                                        {'loss': 0.3256, 'learning_rate': 1.5648768163175277e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4288/5198 [24:21:52<4:32:12, 17.95s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4289/5198 [24:22:10<4:31:18, 17.91s/it]                                                        {'loss': 0.7706, 'learning_rate': 1.5615318051981243e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4289/5198 [24:22:10<4:31:18, 17.91s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4290/5198 [24:22:27<4:28:35, 17.75s/it]                                                        {'loss': 0.808, 'learning_rate': 1.5581900701826226e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4290/5198 [24:22:27<4:28:35, 17.75s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4291/5198 [24:22:45<4:27:23, 17.69s/it]                                                        {'loss': 0.8173, 'learning_rate': 1.5548516125683976e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4291/5198 [24:22:45<4:27:23, 17.69s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4292/5198 [24:23:03<4:29:38, 17.86s/it]                                                        {'loss': 0.789, 'learning_rate': 1.5515164336515465e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4292/5198 [24:23:03<4:29:38, 17.86s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4293/5198 [24:23:20<4:26:43, 17.68s/it]                                                        {'loss': 0.8077, 'learning_rate': 1.5481845347269077e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4293/5198 [24:23:20<4:26:43, 17.68s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4294/5198 [24:23:39<4:30:07, 17.93s/it]                                                        {'loss': 0.8026, 'learning_rate': 1.5448559170880373e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4294/5198 [24:23:39<4:30:07, 17.93s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4295/5198 [24:23:56<4:29:31, 17.91s/it]                                                        {'loss': 0.7796, 'learning_rate': 1.5415305820272198e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4295/5198 [24:23:56<4:29:31, 17.91s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4296/5198 [24:24:14<4:26:31, 17.73s/it]                                                        {'loss': 0.8156, 'learning_rate': 1.5382085308354633e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4296/5198 [24:24:14<4:26:31, 17.73s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4297/5198 [24:24:32<4:26:54, 17.77s/it]                                                        {'loss': 0.7667, 'learning_rate': 1.534889764802503e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4297/5198 [24:24:32<4:26:54, 17.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4298/5198 [24:24:49<4:25:24, 17.69s/it]                                                        {'loss': 0.8306, 'learning_rate': 1.5315742852167992e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4298/5198 [24:24:49<4:25:24, 17.69s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4299/5198 [24:25:07<4:27:30, 17.85s/it]                                                        {'loss': 0.8397, 'learning_rate': 1.5282620933655312e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4299/5198 [24:25:07<4:27:30, 17.85s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4300/5198 [24:25:25<4:25:29, 17.74s/it]                                                        {'loss': 0.8095, 'learning_rate': 1.5249531905346138e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4300/5198 [24:25:25<4:25:29, 17.74s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4301/5198 [24:26:50<9:27:35, 37.97s/it]                                                        {'loss': 0.7343, 'learning_rate': 1.521647578008667e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4301/5198 [24:26:50<9:27:35, 37.97s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4302/5198 [24:27:08<7:57:10, 31.95s/it]                                                        {'loss': 0.3481, 'learning_rate': 1.5183452570710522e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4302/5198 [24:27:08<7:57:10, 31.95s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4303/5198 [24:27:26<6:55:24, 27.85s/it]                                                        {'loss': 0.8113, 'learning_rate': 1.5150462290038392e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4303/5198 [24:27:26<6:55:24, 27.85s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4304/5198 [24:27:43<6:07:13, 24.65s/it]                                                        {'loss': 0.8661, 'learning_rate': 1.511750495087827e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4304/5198 [24:27:43<6:07:13, 24.65s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4305/5198 [24:28:00<5:31:34, 22.28s/it]                                                        {'loss': 0.8214, 'learning_rate': 1.5084580566025309e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4305/5198 [24:28:00<5:31:34, 22.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4306/5198 [24:28:18<5:10:04, 20.86s/it]                                                        {'loss': 0.7127, 'learning_rate': 1.5051689148261895e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4306/5198 [24:28:18<5:10:04, 20.86s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4307/5198 [24:28:36<4:56:16, 19.95s/it]                                                        {'loss': 0.8062, 'learning_rate': 1.5018830710357612e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4307/5198 [24:28:36<4:56:16, 19.95s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4308/5198 [24:28:53<4:44:52, 19.20s/it]                                                        {'loss': 0.7939, 'learning_rate': 1.4986005265069204e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4308/5198 [24:28:53<4:44:52, 19.20s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4309/5198 [24:29:11<4:39:20, 18.85s/it]                                                        {'loss': 0.7359, 'learning_rate': 1.4953212825140728e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4309/5198 [24:29:11<4:39:20, 18.85s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4310/5198 [24:29:29<4:33:32, 18.48s/it]                                                        {'loss': 0.3457, 'learning_rate': 1.4920453403303249e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4310/5198 [24:29:29<4:33:32, 18.48s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4311/5198 [24:29:46<4:29:05, 18.20s/it]                                                        {'loss': 0.6631, 'learning_rate': 1.4887727012275112e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4311/5198 [24:29:46<4:29:05, 18.20s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4312/5198 [24:30:04<4:25:04, 17.95s/it]                                                        {'loss': 0.8025, 'learning_rate': 1.4855033664761898e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4312/5198 [24:30:04<4:25:04, 17.95s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4313/5198 [24:30:22<4:24:42, 17.95s/it]                                                        {'loss': 0.7769, 'learning_rate': 1.48223733734562e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4313/5198 [24:30:22<4:24:42, 17.95s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4314/5198 [24:30:39<4:22:15, 17.80s/it]                                                        {'loss': 0.3348, 'learning_rate': 1.4789746151037942e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4314/5198 [24:30:39<4:22:15, 17.80s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4315/5198 [24:30:57<4:23:30, 17.91s/it]                                                        {'loss': 0.7789, 'learning_rate': 1.475715201017407e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4315/5198 [24:30:57<4:23:30, 17.91s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4316/5198 [24:31:14<4:17:53, 17.54s/it]                                                        {'loss': 0.7588, 'learning_rate': 1.4724590963518803e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4316/5198 [24:31:14<4:17:53, 17.54s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4317/5198 [24:31:31<4:18:02, 17.57s/it]                                                        {'loss': 0.7855, 'learning_rate': 1.4692063023713444e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4317/5198 [24:31:31<4:18:02, 17.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4318/5198 [24:31:49<4:15:48, 17.44s/it]                                                        {'loss': 0.7851, 'learning_rate': 1.4659568203386464e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4318/5198 [24:31:49<4:15:48, 17.44s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4319/5198 [24:32:06<4:15:49, 17.46s/it]                                                        {'loss': 0.811, 'learning_rate': 1.4627106515153456e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4319/5198 [24:32:06<4:15:49, 17.46s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4320/5198 [24:32:25<4:19:45, 17.75s/it]                                                        {'loss': 0.7837, 'learning_rate': 1.4594677971617178e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4320/5198 [24:32:25<4:19:45, 17.75s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4321/5198 [24:32:42<4:19:52, 17.78s/it]                                                        {'loss': 0.7489, 'learning_rate': 1.4562282585367493e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4321/5198 [24:32:42<4:19:52, 17.78s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4322/5198 [24:33:00<4:17:29, 17.64s/it]                                                        {'loss': 0.7422, 'learning_rate': 1.452992036898142e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4322/5198 [24:33:00<4:17:29, 17.64s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4323/5198 [24:33:18<4:19:42, 17.81s/it]                                                        {'loss': 0.7924, 'learning_rate': 1.4497591335023087e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4323/5198 [24:33:18<4:19:42, 17.81s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4324/5198 [24:33:35<4:15:14, 17.52s/it]                                                        {'loss': 0.842, 'learning_rate': 1.446529549604373e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4324/5198 [24:33:35<4:15:14, 17.52s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4325/5198 [24:33:53<4:19:44, 17.85s/it]                                                        {'loss': 0.7377, 'learning_rate': 1.4433032864581687e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4325/5198 [24:33:53<4:19:44, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4326/5198 [24:35:19<9:14:14, 38.14s/it]                                                        {'loss': 0.8213, 'learning_rate': 1.4400803453162482e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4326/5198 [24:35:19<9:14:14, 38.14s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4327/5198 [24:35:36<7:42:34, 31.87s/it]                                                        {'loss': 0.3273, 'learning_rate': 1.4368607274298596e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4327/5198 [24:35:36<7:42:34, 31.87s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4328/5198 [24:35:54<6:39:28, 27.55s/it]                                                        {'loss': 0.7751, 'learning_rate': 1.4336444340489775e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4328/5198 [24:35:54<6:39:28, 27.55s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4329/5198 [24:36:11<5:57:16, 24.67s/it]                                                        {'loss': 0.8004, 'learning_rate': 1.430431466422273e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4329/5198 [24:36:11<5:57:16, 24.67s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4330/5198 [24:36:29<5:27:49, 22.66s/it]                                                        {'loss': 0.8092, 'learning_rate': 1.4272218257971327e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4330/5198 [24:36:29<5:27:49, 22.66s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4331/5198 [24:36:48<5:10:29, 21.49s/it]                                                        {'loss': 0.7332, 'learning_rate': 1.4240155134196499e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4331/5198 [24:36:48<5:10:29, 21.49s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4332/5198 [24:37:06<4:54:25, 20.40s/it]                                                        {'loss': 0.7742, 'learning_rate': 1.4208125305346232e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4332/5198 [24:37:06<4:54:25, 20.40s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4333/5198 [24:37:24<4:44:16, 19.72s/it]                                                        {'loss': 0.7818, 'learning_rate': 1.4176128783855636e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4333/5198 [24:37:24<4:44:16, 19.72s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4334/5198 [24:37:41<4:31:01, 18.82s/it]                                                        {'loss': 0.7611, 'learning_rate': 1.4144165582146819e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4334/5198 [24:37:41<4:31:01, 18.82s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4335/5198 [24:37:59<4:27:36, 18.61s/it]                                                        {'loss': 0.7245, 'learning_rate': 1.4112235712629063e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4335/5198 [24:37:59<4:27:36, 18.61s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4336/5198 [24:38:16<4:20:45, 18.15s/it]                                                        {'loss': 0.774, 'learning_rate': 1.40803391876986e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4336/5198 [24:38:16<4:20:45, 18.15s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4337/5198 [24:38:34<4:20:21, 18.14s/it]                                                        {'loss': 0.7631, 'learning_rate': 1.4048476019738756e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4337/5198 [24:38:34<4:20:21, 18.14s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4338/5198 [24:38:53<4:21:48, 18.27s/it]                                                        {'loss': 0.7763, 'learning_rate': 1.4016646221119912e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4338/5198 [24:38:53<4:21:48, 18.27s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4339/5198 [24:39:11<4:20:25, 18.19s/it]                                                        {'loss': 0.8351, 'learning_rate': 1.3984849804199485e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4339/5198 [24:39:11<4:20:25, 18.19s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4340/5198 [24:39:29<4:18:59, 18.11s/it]                                                        {'loss': 0.8139, 'learning_rate': 1.395308678132199e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4340/5198 [24:39:29<4:18:59, 18.11s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4341/5198 [24:39:46<4:16:53, 17.98s/it]                                                        {'loss': 0.7959, 'learning_rate': 1.392135716481885e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4341/5198 [24:39:46<4:16:53, 17.98s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4342/5198 [24:40:04<4:12:43, 17.71s/it]                                                        {'loss': 0.7916, 'learning_rate': 1.3889660967008656e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4342/5198 [24:40:04<4:12:43, 17.71s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4343/5198 [24:40:21<4:13:16, 17.77s/it]                                                        {'loss': 0.7759, 'learning_rate': 1.3857998200196943e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4343/5198 [24:40:21<4:13:16, 17.77s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4344/5198 [24:40:39<4:11:29, 17.67s/it]                                                        {'loss': 0.7882, 'learning_rate': 1.3826368876676278e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4344/5198 [24:40:39<4:11:29, 17.67s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4345/5198 [24:40:57<4:14:20, 17.89s/it]                                                        {'loss': 0.7657, 'learning_rate': 1.379477300872626e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4345/5198 [24:40:57<4:14:20, 17.89s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4346/5198 [24:41:15<4:15:03, 17.96s/it]                                                        {'loss': 0.7487, 'learning_rate': 1.3763210608613497e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4346/5198 [24:41:15<4:15:03, 17.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4347/5198 [24:41:32<4:08:37, 17.53s/it]                                                        {'loss': 0.7523, 'learning_rate': 1.3731681688591593e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4347/5198 [24:41:32<4:08:37, 17.53s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4348/5198 [24:41:50<4:10:54, 17.71s/it]                                                        {'loss': 0.7496, 'learning_rate': 1.370018626090116e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4348/5198 [24:41:50<4:10:54, 17.71s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4349/5198 [24:42:07<4:08:48, 17.58s/it]                                                        {'loss': 0.7889, 'learning_rate': 1.3668724337769823e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4349/5198 [24:42:07<4:08:48, 17.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4350/5198 [24:42:25<4:09:47, 17.67s/it]                                                        {'loss': 0.7802, 'learning_rate': 1.3637295931412153e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4350/5198 [24:42:25<4:09:47, 17.67s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4351/5198 [24:43:49<8:48:46, 37.46s/it]                                                        {'loss': 0.7671, 'learning_rate': 1.3605901054029746e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4351/5198 [24:43:49<8:48:46, 37.46s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4352/5198 [24:44:05<7:19:14, 31.15s/it]                                                        {'loss': 0.8258, 'learning_rate': 1.3574539717811231e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4352/5198 [24:44:05<7:19:14, 31.15s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4353/5198 [24:44:23<6:20:16, 27.00s/it]                                                        {'loss': 0.7738, 'learning_rate': 1.3543211934932065e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4353/5198 [24:44:23<6:20:16, 27.00s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4354/5198 [24:44:39<5:35:49, 23.87s/it]                                                        {'loss': 0.7146, 'learning_rate': 1.3511917717554846e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4354/5198 [24:44:39<5:35:49, 23.87s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4355/5198 [24:44:58<5:15:18, 22.44s/it]                                                        {'loss': 0.7415, 'learning_rate': 1.348065707782904e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4355/5198 [24:44:58<5:15:18, 22.44s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4356/5198 [24:45:16<4:54:51, 21.01s/it]                                                        {'loss': 0.7954, 'learning_rate': 1.3449430027891096e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4356/5198 [24:45:16<4:54:51, 21.01s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4357/5198 [24:45:34<4:40:36, 20.02s/it]                                                        {'loss': 0.8295, 'learning_rate': 1.3418236579864452e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4357/5198 [24:45:34<4:40:36, 20.02s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4358/5198 [24:45:51<4:27:58, 19.14s/it]                                                        {'loss': 0.8118, 'learning_rate': 1.338707674585945e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4358/5198 [24:45:51<4:27:58, 19.14s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4359/5198 [24:46:08<4:18:47, 18.51s/it]                                                        {'loss': 0.7917, 'learning_rate': 1.3355950537973438e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4359/5198 [24:46:08<4:18:47, 18.51s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4360/5198 [24:46:25<4:13:38, 18.16s/it]                                                        {'loss': 0.8263, 'learning_rate': 1.332485796829065e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4360/5198 [24:46:25<4:13:38, 18.16s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4361/5198 [24:46:42<4:09:26, 17.88s/it]                                                        {'loss': 0.7251, 'learning_rate': 1.329379904888235e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4361/5198 [24:46:42<4:09:26, 17.88s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4362/5198 [24:46:59<4:05:22, 17.61s/it]                                                        {'loss': 0.8151, 'learning_rate': 1.3262773791806617e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4362/5198 [24:46:59<4:05:22, 17.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4363/5198 [24:47:17<4:04:45, 17.59s/it]                                                        {'loss': 0.7327, 'learning_rate': 1.3231782209108546e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4363/5198 [24:47:17<4:04:45, 17.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4364/5198 [24:47:35<4:05:57, 17.69s/it]                                                        {'loss': 0.7194, 'learning_rate': 1.3200824312820137e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4364/5198 [24:47:35<4:05:57, 17.69s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4365/5198 [24:47:52<4:01:45, 17.41s/it]                                                        {'loss': 0.7986, 'learning_rate': 1.3169900114960298e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4365/5198 [24:47:52<4:01:45, 17.41s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4366/5198 [24:48:10<4:04:30, 17.63s/it]                                                        {'loss': 0.868, 'learning_rate': 1.3139009627534927e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4366/5198 [24:48:10<4:04:30, 17.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4367/5198 [24:48:27<4:04:38, 17.66s/it]                                                        {'loss': 0.7918, 'learning_rate': 1.3108152862536683e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4367/5198 [24:48:27<4:04:38, 17.66s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4368/5198 [24:48:45<4:05:03, 17.71s/it]                                                        {'loss': 0.7556, 'learning_rate': 1.3077329831945295e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4368/5198 [24:48:45<4:05:03, 17.71s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4369/5198 [24:49:03<4:03:36, 17.63s/it]                                                        {'loss': 0.7774, 'learning_rate': 1.3046540547727305e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4369/5198 [24:49:03<4:03:36, 17.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4370/5198 [24:49:20<4:02:19, 17.56s/it]                                                        {'loss': 0.7688, 'learning_rate': 1.3015785021836159e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4370/5198 [24:49:20<4:02:19, 17.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4371/5198 [24:49:38<4:02:16, 17.58s/it]                                                        {'loss': 0.7754, 'learning_rate': 1.2985063266212229e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4371/5198 [24:49:38<4:02:16, 17.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4372/5198 [24:49:55<4:00:31, 17.47s/it]                                                        {'loss': 0.8095, 'learning_rate': 1.295437529278275e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4372/5198 [24:49:55<4:00:31, 17.47s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4373/5198 [24:50:14<4:08:27, 18.07s/it]                                                        {'loss': 0.7091, 'learning_rate': 1.2923721113461852e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4373/5198 [24:50:14<4:08:27, 18.07s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4374/5198 [24:50:33<4:08:57, 18.13s/it]                                                        {'loss': 0.7697, 'learning_rate': 1.2893100740150522e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4374/5198 [24:50:33<4:08:57, 18.13s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4375/5198 [24:50:51<4:08:12, 18.09s/it]                                                        {'loss': 0.7805, 'learning_rate': 1.2862514184736695e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4375/5198 [24:50:51<4:08:12, 18.09s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4376/5198 [24:52:16<8:44:42, 38.30s/it]                                                        {'loss': 0.8093, 'learning_rate': 1.2831961459095088e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4376/5198 [24:52:16<8:44:42, 38.30s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4377/5198 [24:52:34<7:20:10, 32.17s/it]                                                        {'loss': 0.7765, 'learning_rate': 1.2801442575087296e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4377/5198 [24:52:34<7:20:10, 32.17s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4378/5198 [24:52:52<6:22:35, 27.99s/it]                                                        {'loss': 0.782, 'learning_rate': 1.2770957544561868e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4378/5198 [24:52:52<6:22:35, 27.99s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4379/5198 [24:53:09<5:37:55, 24.76s/it]                                                        {'loss': 0.7593, 'learning_rate': 1.274050637935408e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4379/5198 [24:53:09<5:37:55, 24.76s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4380/5198 [24:53:27<5:06:05, 22.45s/it]                                                        {'loss': 0.7653, 'learning_rate': 1.2710089091286148e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4380/5198 [24:53:27<5:06:05, 22.45s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4381/5198 [24:53:45<4:48:09, 21.16s/it]                                                        {'loss': 0.8379, 'learning_rate': 1.2679705692167122e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4381/5198 [24:53:45<4:48:09, 21.16s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4382/5198 [24:54:03<4:34:25, 20.18s/it]                                                        {'loss': 0.8034, 'learning_rate': 1.2649356193792873e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4382/5198 [24:54:03<4:34:25, 20.18s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4383/5198 [24:54:20<4:24:24, 19.47s/it]                                                        {'loss': 0.3478, 'learning_rate': 1.261904060794612e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4383/5198 [24:54:20<4:24:24, 19.47s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4384/5198 [24:54:38<4:15:56, 18.87s/it]                                                        {'loss': 0.7664, 'learning_rate': 1.2588758946396417e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4384/5198 [24:54:38<4:15:56, 18.87s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4385/5198 [24:54:56<4:11:33, 18.56s/it]                                                        {'loss': 0.8065, 'learning_rate': 1.2558511220900138e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4385/5198 [24:54:56<4:11:33, 18.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4386/5198 [24:55:13<4:06:38, 18.23s/it]                                                        {'loss': 0.7921, 'learning_rate': 1.2528297443200489e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4386/5198 [24:55:13<4:06:38, 18.23s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4387/5198 [24:55:31<4:04:00, 18.05s/it]                                                        {'loss': 0.7789, 'learning_rate': 1.2498117625027562e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4387/5198 [24:55:31<4:04:00, 18.05s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4388/5198 [24:55:48<4:00:13, 17.79s/it]                                                        {'loss': 0.7619, 'learning_rate': 1.246797177809812e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4388/5198 [24:55:48<4:00:13, 17.79s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4389/5198 [24:56:05<3:57:19, 17.60s/it]                                                        {'loss': 0.7546, 'learning_rate': 1.2437859914115847e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4389/5198 [24:56:05<3:57:19, 17.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4390/5198 [24:56:24<4:00:26, 17.85s/it]                                                        {'loss': 0.7536, 'learning_rate': 1.2407782044771222e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4390/5198 [24:56:24<4:00:26, 17.85s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4391/5198 [24:56:42<4:01:29, 17.95s/it]                                                        {'loss': 0.8078, 'learning_rate': 1.237773818174146e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4391/5198 [24:56:42<4:01:29, 17.95s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4392/5198 [24:57:00<4:02:45, 18.07s/it]                                                        {'loss': 0.7652, 'learning_rate': 1.23477283366907e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4392/5198 [24:57:00<4:02:45, 18.07s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4393/5198 [24:57:18<4:00:56, 17.96s/it]                                                        {'loss': 0.7394, 'learning_rate': 1.2317752521269722e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4393/5198 [24:57:18<4:00:56, 17.96s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4394/5198 [24:57:35<3:56:57, 17.68s/it]                                                        {'loss': 0.8137, 'learning_rate': 1.2287810747116224e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4394/5198 [24:57:35<3:56:57, 17.68s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4395/5198 [24:57:52<3:53:28, 17.44s/it]                                                        {'loss': 0.3485, 'learning_rate': 1.225790302585461e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4395/5198 [24:57:52<3:53:28, 17.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4396/5198 [24:58:09<3:53:18, 17.45s/it]                                                        {'loss': 0.7325, 'learning_rate': 1.2228029369096094e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4396/5198 [24:58:09<3:53:18, 17.45s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4397/5198 [24:58:26<3:50:44, 17.28s/it]                                                        {'loss': 0.7486, 'learning_rate': 1.2198189788438652e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4397/5198 [24:58:26<3:50:44, 17.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4398/5198 [24:58:45<3:55:32, 17.67s/it]                                                        {'loss': 0.7599, 'learning_rate': 1.216838429546704e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4398/5198 [24:58:45<3:55:32, 17.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4399/5198 [24:59:03<3:57:58, 17.87s/it]                                                        {'loss': 0.6973, 'learning_rate': 1.2138612901752777e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4399/5198 [24:59:03<3:57:58, 17.87s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4400/5198 [24:59:21<3:57:24, 17.85s/it]                                                        {'loss': 0.8192, 'learning_rate': 1.2108875618854122e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4400/5198 [24:59:21<3:57:24, 17.85s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4401/5198 [25:00:47<8:30:47, 38.45s/it]                                                        {'loss': 0.818, 'learning_rate': 1.2079172458316168e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4401/5198 [25:00:47<8:30:47, 38.45s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4402/5198 [25:01:05<7:07:33, 32.23s/it]                                                        {'loss': 0.3384, 'learning_rate': 1.204950343167065e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4402/5198 [25:01:05<7:07:33, 32.23s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4403/5198 [25:01:23<6:09:26, 27.88s/it]                                                        {'loss': 0.7503, 'learning_rate': 1.2019868550436099e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4403/5198 [25:01:23<6:09:26, 27.88s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4404/5198 [25:01:40<5:28:38, 24.83s/it]                                                        {'loss': 0.7715, 'learning_rate': 1.1990267826117874e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4404/5198 [25:01:40<5:28:38, 24.83s/it]WARNING: tokenization mismatch: 1 vs. 789. (ignored)
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4405/5198 [25:01:58<4:59:51, 22.69s/it]                                                        {'loss': 0.7424, 'learning_rate': 1.1960701270207885e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4405/5198 [25:01:58<4:59:51, 22.69s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4406/5198 [25:02:16<4:39:26, 21.17s/it]                                                        {'loss': 0.7713, 'learning_rate': 1.1931168894184974e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4406/5198 [25:02:16<4:39:26, 21.17s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4407/5198 [25:02:34<4:25:33, 20.14s/it]                                                        {'loss': 0.8072, 'learning_rate': 1.19016707095146e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4407/5198 [25:02:34<4:25:33, 20.14s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4408/5198 [25:02:51<4:15:44, 19.42s/it]                                                        {'loss': 0.3108, 'learning_rate': 1.187220672764897e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4408/5198 [25:02:51<4:15:44, 19.42s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4409/5198 [25:03:08<4:05:19, 18.66s/it]                                                        {'loss': 0.3403, 'learning_rate': 1.1842776960027014e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4409/5198 [25:03:08<4:05:19, 18.66s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4410/5198 [25:03:27<4:04:16, 18.60s/it]                                                        {'loss': 0.808, 'learning_rate': 1.1813381418074388e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4410/5198 [25:03:27<4:04:16, 18.60s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4411/5198 [25:03:44<4:01:02, 18.38s/it]                                                        {'loss': 0.8451, 'learning_rate': 1.1784020113203453e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4411/5198 [25:03:44<4:01:02, 18.38s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4412/5198 [25:04:03<3:59:23, 18.27s/it]                                                        {'loss': 0.8011, 'learning_rate': 1.1754693056813272e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4412/5198 [25:04:03<3:59:23, 18.27s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4413/5198 [25:04:21<3:58:22, 18.22s/it]                                                        {'loss': 0.7616, 'learning_rate': 1.172540026028962e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4413/5198 [25:04:21<3:58:22, 18.22s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4414/5198 [25:04:38<3:56:22, 18.09s/it]                                                        {'loss': 0.766, 'learning_rate': 1.169614173500494e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4414/5198 [25:04:38<3:56:22, 18.09s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4415/5198 [25:04:56<3:54:33, 17.97s/it]                                                        {'loss': 0.7297, 'learning_rate': 1.1666917492318486e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4415/5198 [25:04:56<3:54:33, 17.97s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4416/5198 [25:05:14<3:53:19, 17.90s/it]                                                        {'loss': 0.8179, 'learning_rate': 1.1637727543576027e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4416/5198 [25:05:14<3:53:19, 17.90s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4417/5198 [25:05:31<3:51:16, 17.77s/it]                                                        {'loss': 0.787, 'learning_rate': 1.1608571900110122e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4417/5198 [25:05:31<3:51:16, 17.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4418/5198 [25:05:50<3:53:49, 17.99s/it]                                                        {'loss': 0.7946, 'learning_rate': 1.1579450573240058e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4418/5198 [25:05:50<3:53:49, 17.99s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4419/5198 [25:06:08<3:52:58, 17.94s/it]                                                        {'loss': 0.7501, 'learning_rate': 1.1550363574271638e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4419/5198 [25:06:08<3:52:58, 17.94s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4420/5198 [25:06:25<3:51:39, 17.87s/it]                                                        {'loss': 0.785, 'learning_rate': 1.1521310914497518e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4420/5198 [25:06:25<3:51:39, 17.87s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4421/5198 [25:06:44<3:52:48, 17.98s/it]                                                        {'loss': 0.8402, 'learning_rate': 1.149229260519691e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4421/5198 [25:06:44<3:52:48, 17.98s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4422/5198 [25:07:02<3:53:13, 18.03s/it]                                                        {'loss': 0.7961, 'learning_rate': 1.1463308657635718e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4422/5198 [25:07:02<3:53:13, 18.03s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4423/5198 [25:07:19<3:51:29, 17.92s/it]                                                        {'loss': 0.8045, 'learning_rate': 1.1434359083066515e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4423/5198 [25:07:19<3:51:29, 17.92s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4424/5198 [25:07:36<3:46:21, 17.55s/it]                                                        {'loss': 0.768, 'learning_rate': 1.140544389272853e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4424/5198 [25:07:36<3:46:21, 17.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4425/5198 [25:07:54<3:45:50, 17.53s/it]                                                        {'loss': 0.7665, 'learning_rate': 1.1376563097847616e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4425/5198 [25:07:54<3:45:50, 17.53s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4426/5198 [25:09:19<8:09:02, 38.01s/it]                                                        {'loss': 0.7702, 'learning_rate': 1.1347716709636282e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4426/5198 [25:09:19<8:09:02, 38.01s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4427/5198 [25:09:38<6:54:04, 32.22s/it]                                                        {'loss': 0.8155, 'learning_rate': 1.1318904739293745e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4427/5198 [25:09:38<6:54:04, 32.22s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4428/5198 [25:09:56<5:59:02, 27.98s/it]                                                        {'loss': 0.7523, 'learning_rate': 1.129012719800575e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4428/5198 [25:09:56<5:59:02, 27.98s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4429/5198 [25:10:14<5:20:10, 24.98s/it]                                                        {'loss': 0.7546, 'learning_rate': 1.1261384096944728e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4429/5198 [25:10:14<5:20:10, 24.98s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4430/5198 [25:10:32<4:53:38, 22.94s/it]                                                        {'loss': 0.8313, 'learning_rate': 1.1232675447269803e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4430/5198 [25:10:32<4:53:38, 22.94s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4431/5198 [25:10:50<4:32:06, 21.29s/it]                                                        {'loss': 0.7905, 'learning_rate': 1.1204001260126574e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4431/5198 [25:10:50<4:32:06, 21.29s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4432/5198 [25:11:08<4:20:24, 20.40s/it]                                                        {'loss': 0.8107, 'learning_rate': 1.1175361546647413e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4432/5198 [25:11:08<4:20:24, 20.40s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4433/5198 [25:11:26<4:10:12, 19.62s/it]                                                        {'loss': 0.8343, 'learning_rate': 1.1146756317951224e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4433/5198 [25:11:26<4:10:12, 19.62s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4434/5198 [25:11:43<3:59:50, 18.84s/it]                                                        {'loss': 0.7672, 'learning_rate': 1.1118185585143536e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4434/5198 [25:11:43<3:59:50, 18.84s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4435/5198 [25:12:01<3:55:44, 18.54s/it]                                                        {'loss': 0.8213, 'learning_rate': 1.1089649359316501e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4435/5198 [25:12:01<3:55:44, 18.54s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4436/5198 [25:12:18<3:50:42, 18.17s/it]                                                        {'loss': 0.8269, 'learning_rate': 1.1061147651548855e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4436/5198 [25:12:18<3:50:42, 18.17s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4437/5198 [25:12:36<3:48:46, 18.04s/it]                                                        {'loss': 0.7651, 'learning_rate': 1.1032680472905932e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4437/5198 [25:12:36<3:48:46, 18.04s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4438/5198 [25:12:52<3:42:46, 17.59s/it]                                                        {'loss': 0.3043, 'learning_rate': 1.1004247834439697e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4438/5198 [25:12:52<3:42:46, 17.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4439/5198 [25:13:09<3:38:48, 17.30s/it]                                                        {'loss': 0.7594, 'learning_rate': 1.097584974718866e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4439/5198 [25:13:09<3:38:48, 17.30s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4440/5198 [25:13:26<3:39:35, 17.38s/it]                                                        {'loss': 0.3294, 'learning_rate': 1.0947486222177928e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4440/5198 [25:13:26<3:39:35, 17.38s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4441/5198 [25:13:45<3:42:55, 17.67s/it]                                                        {'loss': 0.7332, 'learning_rate': 1.0919157270419257e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4441/5198 [25:13:45<3:42:55, 17.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4442/5198 [25:14:03<3:44:50, 17.85s/it]                                                        {'loss': 0.7637, 'learning_rate': 1.0890862902910849e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4442/5198 [25:14:03<3:44:50, 17.85s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4443/5198 [25:14:21<3:43:42, 17.78s/it]                                                        {'loss': 0.8135, 'learning_rate': 1.0862603130637562e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4443/5198 [25:14:21<3:43:42, 17.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4444/5198 [25:14:39<3:45:42, 17.96s/it]                                                        {'loss': 0.7959, 'learning_rate': 1.0834377964570863e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4444/5198 [25:14:39<3:45:42, 17.96s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4445/5198 [25:14:57<3:43:32, 17.81s/it]                                                        {'loss': 0.8227, 'learning_rate': 1.0806187415668668e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4445/5198 [25:14:57<3:43:32, 17.81s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4446/5198 [25:15:14<3:40:58, 17.63s/it]                                                        {'loss': 0.7638, 'learning_rate': 1.0778031494875574e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4446/5198 [25:15:14<3:40:58, 17.63s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4447/5198 [25:15:31<3:40:57, 17.65s/it]                                                        {'loss': 0.8144, 'learning_rate': 1.0749910213122649e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4447/5198 [25:15:31<3:40:57, 17.65s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4448/5198 [25:15:48<3:36:57, 17.36s/it]                                                        {'loss': 0.7671, 'learning_rate': 1.072182358132755e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4448/5198 [25:15:48<3:36:57, 17.36s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4449/5198 [25:16:06<3:39:54, 17.62s/it]                                                        {'loss': 0.7925, 'learning_rate': 1.0693771610394477e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4449/5198 [25:16:06<3:39:54, 17.62s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4450/5198 [25:16:25<3:43:02, 17.89s/it]                                                        {'loss': 0.7543, 'learning_rate': 1.066575431121417e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4450/5198 [25:16:25<3:43:02, 17.89s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4451/5198 [25:17:51<7:55:53, 38.22s/it]                                                        {'loss': 0.7987, 'learning_rate': 1.06377716946639e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4451/5198 [25:17:51<7:55:53, 38.22s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4452/5198 [25:18:07<6:35:21, 31.80s/it]                                                        {'loss': 0.8516, 'learning_rate': 1.0609823771607487e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4452/5198 [25:18:07<6:35:21, 31.80s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4453/5198 [25:18:24<5:38:17, 27.24s/it]                                                        {'loss': 0.7499, 'learning_rate': 1.0581910552895302e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4453/5198 [25:18:24<5:38:17, 27.24s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4454/5198 [25:18:41<5:00:46, 24.26s/it]                                                        {'loss': 0.8162, 'learning_rate': 1.055403204936416e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4454/5198 [25:18:41<5:00:46, 24.26s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4455/5198 [25:18:58<4:31:22, 21.91s/it]                                                        {'loss': 0.8033, 'learning_rate': 1.0526188271837512e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4455/5198 [25:18:58<4:31:22, 21.91s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4456/5198 [25:19:16<4:15:46, 20.68s/it]                                                        {'loss': 0.7384, 'learning_rate': 1.0498379231125278e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4456/5198 [25:19:16<4:15:46, 20.68s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4457/5198 [25:19:32<4:01:15, 19.54s/it]                                                        {'loss': 0.7852, 'learning_rate': 1.047060493802381e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4457/5198 [25:19:32<4:01:15, 19.54s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4458/5198 [25:19:50<3:52:44, 18.87s/it]                                                        {'loss': 0.7559, 'learning_rate': 1.0442865403316117e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4458/5198 [25:19:50<3:52:44, 18.87s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4459/5198 [25:20:07<3:48:29, 18.55s/it]                                                        {'loss': 0.7694, 'learning_rate': 1.0415160637771604e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4459/5198 [25:20:07<3:48:29, 18.55s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4460/5198 [25:20:26<3:46:47, 18.44s/it]                                                        {'loss': 0.7919, 'learning_rate': 1.0387490652146236e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4460/5198 [25:20:26<3:46:47, 18.44s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4461/5198 [25:20:43<3:43:12, 18.17s/it]                                                        {'loss': 0.7972, 'learning_rate': 1.0359855457182455e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4461/5198 [25:20:43<3:43:12, 18.17s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4462/5198 [25:21:00<3:38:41, 17.83s/it]                                                        {'loss': 0.7965, 'learning_rate': 1.0332255063609177e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4462/5198 [25:21:00<3:38:41, 17.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4463/5198 [25:21:17<3:34:45, 17.53s/it]                                                        {'loss': 0.7682, 'learning_rate': 1.0304689482141839e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4463/5198 [25:21:17<3:34:45, 17.53s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4464/5198 [25:21:35<3:37:07, 17.75s/it]                                                        {'loss': 0.8237, 'learning_rate': 1.027715872348234e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4464/5198 [25:21:35<3:37:07, 17.75s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4465/5198 [25:21:53<3:35:57, 17.68s/it]                                                        {'loss': 0.3165, 'learning_rate': 1.0249662798319072e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4465/5198 [25:21:53<3:35:57, 17.68s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4466/5198 [25:22:11<3:38:03, 17.87s/it]                                                        {'loss': 0.7955, 'learning_rate': 1.0222201717326885e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4466/5198 [25:22:11<3:38:03, 17.87s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4467/5198 [25:22:30<3:40:39, 18.11s/it]                                                        {'loss': 0.8354, 'learning_rate': 1.0194775491167164e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4467/5198 [25:22:30<3:40:39, 18.11s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4468/5198 [25:22:47<3:36:58, 17.83s/it]                                                        {'loss': 0.883, 'learning_rate': 1.0167384130487667e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4468/5198 [25:22:47<3:36:58, 17.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4469/5198 [25:23:04<3:32:51, 17.52s/it]                                                        {'loss': 0.7872, 'learning_rate': 1.0140027645922656e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4469/5198 [25:23:04<3:32:51, 17.52s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4470/5198 [25:23:22<3:33:19, 17.58s/it]                                                        {'loss': 0.8009, 'learning_rate': 1.0112706048092924e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4470/5198 [25:23:22<3:33:19, 17.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4471/5198 [25:23:40<3:36:00, 17.83s/it]                                                        {'loss': 0.743, 'learning_rate': 1.0085419347605575e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4471/5198 [25:23:40<3:36:00, 17.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4472/5198 [25:23:58<3:36:44, 17.91s/it]                                                        {'loss': 0.7659, 'learning_rate': 1.00581675550543e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4472/5198 [25:23:58<3:36:44, 17.91s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4473/5198 [25:24:16<3:36:01, 17.88s/it]                                                        {'loss': 0.778, 'learning_rate': 1.003095068101917e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4473/5198 [25:24:16<3:36:01, 17.88s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4474/5198 [25:24:35<3:38:50, 18.14s/it]                                                        {'loss': 0.7953, 'learning_rate': 1.0003768736066722e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4474/5198 [25:24:35<3:38:50, 18.14s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4475/5198 [25:24:53<3:39:08, 18.19s/it]                                                        {'loss': 0.8047, 'learning_rate': 9.976621730749892e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4475/5198 [25:24:53<3:39:08, 18.19s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4476/5198 [25:26:20<7:47:14, 38.83s/it]                                                        {'loss': 0.7458, 'learning_rate': 9.949509675608115e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4476/5198 [25:26:20<7:47:14, 38.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4477/5198 [25:26:37<6:27:17, 32.23s/it]                                                        {'loss': 0.7574, 'learning_rate': 9.922432581167207e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4477/5198 [25:26:37<6:27:17, 32.23s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4478/5198 [25:26:54<5:31:17, 27.61s/it]                                                        {'loss': 0.8558, 'learning_rate': 9.895390457939414e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4478/5198 [25:26:54<5:31:17, 27.61s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4479/5198 [25:27:12<4:58:43, 24.93s/it]                                                        {'loss': 0.7906, 'learning_rate': 9.86838331642348e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4479/5198 [25:27:12<4:58:43, 24.93s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4480/5198 [25:27:30<4:33:15, 22.83s/it]                                                        {'loss': 0.8171, 'learning_rate': 9.84141116710442e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4480/5198 [25:27:30<4:33:15, 22.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4481/5198 [25:27:48<4:14:26, 21.29s/it]                                                        {'loss': 0.838, 'learning_rate': 9.814474020453824e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4481/5198 [25:27:48<4:14:26, 21.29s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4482/5198 [25:28:05<3:58:59, 20.03s/it]                                                        {'loss': 0.7779, 'learning_rate': 9.787571886929604e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4482/5198 [25:28:05<3:58:59, 20.03s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4483/5198 [25:28:23<3:51:11, 19.40s/it]                                                        {'loss': 0.7773, 'learning_rate': 9.76070477697605e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4483/5198 [25:28:23<3:51:11, 19.40s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4484/5198 [25:28:41<3:47:53, 19.15s/it]                                                        {'loss': 0.8289, 'learning_rate': 9.733872701023938e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4484/5198 [25:28:41<3:47:53, 19.15s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4485/5198 [25:28:59<3:41:15, 18.62s/it]                                                        {'loss': 0.8198, 'learning_rate': 9.707075669490407e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4485/5198 [25:28:59<3:41:15, 18.62s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4486/5198 [25:29:17<3:39:11, 18.47s/it]                                                        {'loss': 0.7639, 'learning_rate': 9.680313692778976e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4486/5198 [25:29:17<3:39:11, 18.47s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4487/5198 [25:29:37<3:44:24, 18.94s/it]                                                        {'loss': 0.7591, 'learning_rate': 9.653586781279567e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4487/5198 [25:29:37<3:44:24, 18.94s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4488/5198 [25:29:55<3:41:45, 18.74s/it]                                                        {'loss': 0.8281, 'learning_rate': 9.626894945368492e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4488/5198 [25:29:55<3:41:45, 18.74s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4489/5198 [25:30:13<3:36:45, 18.34s/it]                                                        {'loss': 0.7484, 'learning_rate': 9.600238195408428e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4489/5198 [25:30:13<3:36:45, 18.34s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4490/5198 [25:30:30<3:32:56, 18.05s/it]                                                        {'loss': 0.7854, 'learning_rate': 9.573616541748464e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4490/5198 [25:30:30<3:32:56, 18.05s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4491/5198 [25:30:49<3:36:15, 18.35s/it]                                                        {'loss': 0.8003, 'learning_rate': 9.547029994724023e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4491/5198 [25:30:49<3:36:15, 18.35s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4492/5198 [25:31:07<3:33:46, 18.17s/it]                                                        {'loss': 0.8215, 'learning_rate': 9.520478564656898e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4492/5198 [25:31:07<3:33:46, 18.17s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4493/5198 [25:31:24<3:31:03, 17.96s/it]                                                        {'loss': 0.3451, 'learning_rate': 9.49396226185535e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4493/5198 [25:31:24<3:31:03, 17.96s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4494/5198 [25:31:42<3:29:47, 17.88s/it]                                                        {'loss': 0.7727, 'learning_rate': 9.467481096613829e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4494/5198 [25:31:42<3:29:47, 17.88s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4495/5198 [25:32:00<3:29:47, 17.91s/it]                                                        {'loss': 0.8319, 'learning_rate': 9.441035079213267e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4495/5198 [25:32:00<3:29:47, 17.91s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4496/5198 [25:32:18<3:30:05, 17.96s/it]                                                        {'loss': 0.7951, 'learning_rate': 9.414624219920953e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4496/5198 [25:32:18<3:30:05, 17.96s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4497/5198 [25:32:36<3:28:14, 17.82s/it]                                                        {'loss': 0.7413, 'learning_rate': 9.38824852899043e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4497/5198 [25:32:36<3:28:14, 17.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4498/5198 [25:32:53<3:25:46, 17.64s/it]                                                        {'loss': 0.811, 'learning_rate': 9.361908016661703e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4498/5198 [25:32:53<3:25:46, 17.64s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4499/5198 [25:33:10<3:24:33, 17.56s/it]                                                        {'loss': 0.7673, 'learning_rate': 9.335602693161039e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4499/5198 [25:33:10<3:24:33, 17.56s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4500/5198 [25:33:28<3:24:58, 17.62s/it]                                                        {'loss': 0.7436, 'learning_rate': 9.309332568701079e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4500/5198 [25:33:28<3:24:58, 17.62s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4501/5198 [25:34:59<7:41:04, 39.69s/it]                                                        {'loss': 0.8004, 'learning_rate': 9.283097653480788e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4501/5198 [25:34:59<7:41:04, 39.69s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4502/5198 [25:35:16<6:20:10, 32.77s/it]                                                        {'loss': 0.7849, 'learning_rate': 9.256897957685463e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4502/5198 [25:35:16<6:20:10, 32.77s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4503/5198 [25:35:33<5:27:15, 28.25s/it]                                                        {'loss': 0.8008, 'learning_rate': 9.230733491486721e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4503/5198 [25:35:33<5:27:15, 28.25s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4504/5198 [25:35:51<4:48:49, 24.97s/it]                                                        {'loss': 0.7767, 'learning_rate': 9.204604265042505e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4504/5198 [25:35:51<4:48:49, 24.97s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4505/5198 [25:36:08<4:21:26, 22.64s/it]                                                        {'loss': 0.8, 'learning_rate': 9.178510288497123e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4505/5198 [25:36:08<4:21:26, 22.64s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4506/5198 [25:36:25<4:01:43, 20.96s/it]                                                        {'loss': 0.7909, 'learning_rate': 9.15245157198108e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4506/5198 [25:36:25<4:01:43, 20.96s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4507/5198 [25:36:42<3:49:25, 19.92s/it]                                                        {'loss': 0.3251, 'learning_rate': 9.126428125611342e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4507/5198 [25:36:42<3:49:25, 19.92s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4508/5198 [25:37:01<3:45:43, 19.63s/it]                                                        {'loss': 0.7917, 'learning_rate': 9.10043995949108e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4508/5198 [25:37:01<3:45:43, 19.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4509/5198 [25:37:20<3:40:18, 19.18s/it]                                                        {'loss': 0.7735, 'learning_rate': 9.074487083709759e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4509/5198 [25:37:20<3:40:18, 19.18s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4510/5198 [25:37:37<3:34:07, 18.67s/it]                                                        {'loss': 0.7552, 'learning_rate': 9.04856950834323e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4510/5198 [25:37:37<3:34:07, 18.67s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4511/5198 [25:37:54<3:28:39, 18.22s/it]                                                        {'loss': 0.7791, 'learning_rate': 9.022687243453554e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4511/5198 [25:37:54<3:28:39, 18.22s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4512/5198 [25:38:12<3:26:53, 18.10s/it]                                                        {'loss': 0.8026, 'learning_rate': 8.996840299089149e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4512/5198 [25:38:12<3:26:53, 18.10s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4513/5198 [25:38:29<3:23:43, 17.84s/it]                                                        {'loss': 0.7401, 'learning_rate': 8.971028685284655e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4513/5198 [25:38:29<3:23:43, 17.84s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4514/5198 [25:38:47<3:22:01, 17.72s/it]                                                        {'loss': 0.8236, 'learning_rate': 8.945252412061056e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4514/5198 [25:38:47<3:22:01, 17.72s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4515/5198 [25:39:05<3:23:37, 17.89s/it]                                                        {'loss': 0.7713, 'learning_rate': 8.91951148942557e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4515/5198 [25:39:05<3:23:37, 17.89s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4516/5198 [25:39:23<3:24:04, 17.95s/it]                                                        {'loss': 0.7519, 'learning_rate': 8.893805927371724e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4516/5198 [25:39:23<3:24:04, 17.95s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4517/5198 [25:39:41<3:22:34, 17.85s/it]                                                        {'loss': 0.7425, 'learning_rate': 8.868135735879291e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4517/5198 [25:39:41<3:22:34, 17.85s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4518/5198 [25:39:58<3:18:46, 17.54s/it]                                                        {'loss': 0.7764, 'learning_rate': 8.842500924914299e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4518/5198 [25:39:58<3:18:46, 17.54s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4519/5198 [25:40:16<3:22:24, 17.89s/it]                                                        {'loss': 0.7544, 'learning_rate': 8.816901504429143e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4519/5198 [25:40:16<3:22:24, 17.89s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4520/5198 [25:40:34<3:22:46, 17.95s/it]                                                        {'loss': 0.7086, 'learning_rate': 8.791337484362305e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4520/5198 [25:40:34<3:22:46, 17.95s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4521/5198 [25:40:52<3:22:34, 17.95s/it]                                                        {'loss': 0.7249, 'learning_rate': 8.765808874638682e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4521/5198 [25:40:52<3:22:34, 17.95s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4522/5198 [25:41:10<3:20:21, 17.78s/it]                                                        {'loss': 0.7609, 'learning_rate': 8.740315685169364e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4522/5198 [25:41:10<3:20:21, 17.78s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4523/5198 [25:41:28<3:20:21, 17.81s/it]                                                        {'loss': 0.7901, 'learning_rate': 8.714857925851617e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4523/5198 [25:41:28<3:20:21, 17.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4524/5198 [25:41:45<3:19:22, 17.75s/it]                                                        {'loss': 0.7886, 'learning_rate': 8.689435606569086e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4524/5198 [25:41:45<3:19:22, 17.75s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4525/5198 [25:42:03<3:19:55, 17.82s/it]                                                        {'loss': 0.7769, 'learning_rate': 8.664048737191566e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4525/5198 [25:42:03<3:19:55, 17.82s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4526/5198 [25:43:31<7:14:12, 38.77s/it]                                                        {'loss': 0.7488, 'learning_rate': 8.638697327575108e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4526/5198 [25:43:31<7:14:12, 38.77s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4527/5198 [25:43:49<6:03:00, 32.46s/it]                                                        {'loss': 0.793, 'learning_rate': 8.613381387562015e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4527/5198 [25:43:49<6:03:00, 32.46s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4528/5198 [25:44:06<5:13:03, 28.04s/it]                                                        {'loss': 0.7485, 'learning_rate': 8.588100926980802e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4528/5198 [25:44:06<5:13:03, 28.04s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4529/5198 [25:44:25<4:40:45, 25.18s/it]                                                        {'loss': 0.7734, 'learning_rate': 8.56285595564621e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4529/5198 [25:44:25<4:40:45, 25.18s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4530/5198 [25:44:42<4:15:20, 22.93s/it]                                                        {'loss': 0.7937, 'learning_rate': 8.537646483359185e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4530/5198 [25:44:42<4:15:20, 22.93s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4531/5198 [25:45:00<3:56:17, 21.26s/it]                                                        {'loss': 0.7605, 'learning_rate': 8.512472519906978e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4531/5198 [25:45:00<3:56:17, 21.26s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4532/5198 [25:45:17<3:42:43, 20.07s/it]                                                        {'loss': 0.8227, 'learning_rate': 8.487334075062914e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4532/5198 [25:45:17<3:42:43, 20.07s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4533/5198 [25:45:36<3:39:06, 19.77s/it]                                                        {'loss': 0.7557, 'learning_rate': 8.462231158586654e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4533/5198 [25:45:36<3:39:06, 19.77s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4534/5198 [25:45:53<3:30:02, 18.98s/it]                                                        {'loss': 0.8566, 'learning_rate': 8.437163780224011e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4534/5198 [25:45:53<3:30:02, 18.98s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4535/5198 [25:46:11<3:24:56, 18.55s/it]                                                        {'loss': 0.7825, 'learning_rate': 8.412131949706958e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4535/5198 [25:46:11<3:24:56, 18.55s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4536/5198 [25:46:29<3:23:21, 18.43s/it]                                                        {'loss': 0.7363, 'learning_rate': 8.387135676753755e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4536/5198 [25:46:29<3:23:21, 18.43s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4537/5198 [25:46:48<3:25:15, 18.63s/it]                                                        {'loss': 0.8014, 'learning_rate': 8.362174971068804e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4537/5198 [25:46:48<3:25:15, 18.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4538/5198 [25:47:06<3:22:11, 18.38s/it]                                                        {'loss': 0.8071, 'learning_rate': 8.337249842342721e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4538/5198 [25:47:06<3:22:11, 18.38s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4539/5198 [25:47:24<3:20:23, 18.24s/it]                                                        {'loss': 0.7914, 'learning_rate': 8.312360300252287e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4539/5198 [25:47:24<3:20:23, 18.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4540/5198 [25:47:42<3:20:41, 18.30s/it]                                                        {'loss': 0.7885, 'learning_rate': 8.287506354460484e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4540/5198 [25:47:42<3:20:41, 18.30s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4541/5198 [25:48:00<3:17:04, 18.00s/it]                                                        {'loss': 0.801, 'learning_rate': 8.26268801461646e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4541/5198 [25:48:00<3:17:04, 18.00s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4542/5198 [25:48:18<3:16:50, 18.00s/it]                                                        {'loss': 0.7915, 'learning_rate': 8.237905290355563e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4542/5198 [25:48:18<3:16:50, 18.00s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4543/5198 [25:48:35<3:13:43, 17.75s/it]                                                        {'loss': 0.8071, 'learning_rate': 8.213158191299297e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4543/5198 [25:48:35<3:13:43, 17.75s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4544/5198 [25:48:53<3:14:38, 17.86s/it]                                                        {'loss': 0.7346, 'learning_rate': 8.188446727055311e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4544/5198 [25:48:53<3:14:38, 17.86s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4545/5198 [25:49:10<3:11:49, 17.63s/it]                                                        {'loss': 0.7612, 'learning_rate': 8.163770907217506e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4545/5198 [25:49:10<3:11:49, 17.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4546/5198 [25:49:28<3:13:21, 17.79s/it]                                                        {'loss': 0.7182, 'learning_rate': 8.139130741365819e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4546/5198 [25:49:28<3:13:21, 17.79s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4547/5198 [25:49:45<3:11:17, 17.63s/it]                                                        {'loss': 0.7991, 'learning_rate': 8.114526239066456e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4547/5198 [25:49:45<3:11:17, 17.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4548/5198 [25:50:03<3:10:57, 17.63s/it]                                                        {'loss': 0.7774, 'learning_rate': 8.08995740987173e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4548/5198 [25:50:03<3:10:57, 17.63s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4549/5198 [25:50:20<3:07:44, 17.36s/it]                                                        {'loss': 0.8047, 'learning_rate': 8.065424263320054e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4549/5198 [25:50:20<3:07:44, 17.36s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4550/5198 [25:50:39<3:12:19, 17.81s/it]                                                        {'loss': 0.8044, 'learning_rate': 8.040926808936112e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4550/5198 [25:50:39<3:12:19, 17.81s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4551/5198 [25:52:04<6:51:03, 38.12s/it]                                                        {'loss': 0.833, 'learning_rate': 8.016465056230616e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4551/5198 [25:52:04<6:51:03, 38.12s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4552/5198 [25:52:21<5:42:55, 31.85s/it]                                                        {'loss': 0.7479, 'learning_rate': 7.99203901470047e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4552/5198 [25:52:21<5:42:55, 31.85s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4553/5198 [25:52:40<5:00:42, 27.97s/it]                                                        {'loss': 0.7344, 'learning_rate': 7.967648693828712e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4553/5198 [25:52:40<5:00:42, 27.97s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4554/5198 [25:52:58<4:29:06, 25.07s/it]                                                        {'loss': 0.8063, 'learning_rate': 7.943294103084487e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4554/5198 [25:52:58<4:29:06, 25.07s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4555/5198 [25:53:16<4:03:44, 22.74s/it]                                                        {'loss': 0.7695, 'learning_rate': 7.9189752519231e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4555/5198 [25:53:16<4:03:44, 22.74s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4556/5198 [25:53:33<3:44:42, 21.00s/it]                                                        {'loss': 0.7804, 'learning_rate': 7.894692149785954e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4556/5198 [25:53:33<3:44:42, 21.00s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4557/5198 [25:53:50<3:33:55, 20.02s/it]                                                        {'loss': 0.8232, 'learning_rate': 7.870444806100619e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4557/5198 [25:53:50<3:33:55, 20.02s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4558/5198 [25:54:08<3:25:18, 19.25s/it]                                                        {'loss': 0.7739, 'learning_rate': 7.846233230280698e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4558/5198 [25:54:08<3:25:18, 19.25s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4559/5198 [25:54:27<3:23:42, 19.13s/it]                                                        {'loss': 0.773, 'learning_rate': 7.822057431725994e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4559/5198 [25:54:27<3:23:42, 19.13s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4560/5198 [25:54:44<3:16:24, 18.47s/it]                                                        {'loss': 0.783, 'learning_rate': 7.797917419822377e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4560/5198 [25:54:44<3:16:24, 18.47s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4561/5198 [25:55:02<3:14:45, 18.34s/it]                                                        {'loss': 0.7999, 'learning_rate': 7.773813203941827e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4561/5198 [25:55:02<3:14:45, 18.34s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4562/5198 [25:55:20<3:13:26, 18.25s/it]                                                        {'loss': 0.7179, 'learning_rate': 7.749744793442448e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4562/5198 [25:55:20<3:13:26, 18.25s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4563/5198 [25:55:38<3:13:23, 18.27s/it]                                                        {'loss': 0.7407, 'learning_rate': 7.725712197668378e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4563/5198 [25:55:38<3:13:23, 18.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4564/5198 [25:55:56<3:10:44, 18.05s/it]                                                        {'loss': 0.8157, 'learning_rate': 7.701715425949952e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4564/5198 [25:55:56<3:10:44, 18.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4565/5198 [25:56:13<3:06:41, 17.70s/it]                                                        {'loss': 0.8177, 'learning_rate': 7.677754487603517e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4565/5198 [25:56:13<3:06:41, 17.70s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4566/5198 [25:56:30<3:07:14, 17.78s/it]                                                        {'loss': 0.8041, 'learning_rate': 7.653829391931533e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4566/5198 [25:56:30<3:07:14, 17.78s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4567/5198 [25:56:49<3:08:44, 17.95s/it]                                                        {'loss': 0.7767, 'learning_rate': 7.629940148222559e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4567/5198 [25:56:49<3:08:44, 17.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4568/5198 [25:57:06<3:06:23, 17.75s/it]                                                        {'loss': 0.3384, 'learning_rate': 7.606086765751209e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4568/5198 [25:57:06<3:06:23, 17.75s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4569/5198 [25:57:24<3:05:25, 17.69s/it]                                                        {'loss': 0.7881, 'learning_rate': 7.582269253778185e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4569/5198 [25:57:24<3:05:25, 17.69s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4570/5198 [25:57:41<3:03:25, 17.52s/it]                                                        {'loss': 0.7868, 'learning_rate': 7.55848762155027e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4570/5198 [25:57:41<3:03:25, 17.52s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4571/5198 [25:57:59<3:05:15, 17.73s/it]                                                        {'loss': 0.8118, 'learning_rate': 7.534741878300333e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4571/5198 [25:57:59<3:05:15, 17.73s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4572/5198 [25:58:16<3:03:02, 17.54s/it]                                                        {'loss': 0.8362, 'learning_rate': 7.511032033247256e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4572/5198 [25:58:16<3:03:02, 17.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4573/5198 [25:58:35<3:06:47, 17.93s/it]                                                        {'loss': 0.7874, 'learning_rate': 7.487358095596031e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4573/5198 [25:58:35<3:06:47, 17.93s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4574/5198 [25:58:53<3:05:47, 17.86s/it]                                                        {'loss': 0.772, 'learning_rate': 7.463720074537728e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4574/5198 [25:58:53<3:05:47, 17.86s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4575/5198 [25:59:10<3:04:03, 17.73s/it]                                                        {'loss': 0.7489, 'learning_rate': 7.440117979249362e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4575/5198 [25:59:10<3:04:03, 17.73s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4576/5198 [26:00:38<6:40:58, 38.68s/it]                                                        {'loss': 0.7877, 'learning_rate': 7.416551818894158e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4576/5198 [26:00:38<6:40:58, 38.68s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4577/5198 [26:00:56<5:36:54, 32.55s/it]                                                        {'loss': 0.8064, 'learning_rate': 7.393021602621264e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4577/5198 [26:00:56<5:36:54, 32.55s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4578/5198 [26:01:13<4:48:23, 27.91s/it]                                                        {'loss': 0.7987, 'learning_rate': 7.369527339565951e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4578/5198 [26:01:13<4:48:23, 27.91s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4579/5198 [26:01:32<4:19:15, 25.13s/it]                                                        {'loss': 0.8066, 'learning_rate': 7.346069038849469e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4579/5198 [26:01:32<4:19:15, 25.13s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4580/5198 [26:01:49<3:56:19, 22.94s/it]                                                        {'loss': 0.7407, 'learning_rate': 7.322646709579173e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4580/5198 [26:01:49<3:56:19, 22.94s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4581/5198 [26:02:08<3:42:43, 21.66s/it]                                                        {'loss': 0.7518, 'learning_rate': 7.299260360848382e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4581/5198 [26:02:08<3:42:43, 21.66s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4582/5198 [26:02:25<3:27:32, 20.21s/it]                                                        {'loss': 0.8278, 'learning_rate': 7.275910001736497e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4582/5198 [26:02:25<3:27:32, 20.21s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4583/5198 [26:02:43<3:21:32, 19.66s/it]                                                        {'loss': 0.7898, 'learning_rate': 7.252595641308957e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4583/5198 [26:02:43<3:21:32, 19.66s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4584/5198 [26:03:00<3:12:27, 18.81s/it]                                                        {'loss': 0.7254, 'learning_rate': 7.229317288617144e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4584/5198 [26:03:00<3:12:27, 18.81s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4585/5198 [26:03:18<3:09:50, 18.58s/it]                                                        {'loss': 0.7876, 'learning_rate': 7.20607495269856e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4585/5198 [26:03:18<3:09:50, 18.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4586/5198 [26:03:35<3:05:24, 18.18s/it]                                                        {'loss': 0.815, 'learning_rate': 7.182868642576679e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4586/5198 [26:03:35<3:05:24, 18.18s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4587/5198 [26:03:53<3:03:10, 17.99s/it]                                                        {'loss': 0.8501, 'learning_rate': 7.15969836726097e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4587/5198 [26:03:53<3:03:10, 17.99s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4588/5198 [26:04:10<3:01:29, 17.85s/it]                                                        {'loss': 0.8241, 'learning_rate': 7.13656413574696e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4588/5198 [26:04:11<3:01:29, 17.85s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4589/5198 [26:04:28<3:00:18, 17.76s/it]                                                        {'loss': 0.8035, 'learning_rate': 7.113465957016097e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4589/5198 [26:04:28<3:00:18, 17.76s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4590/5198 [26:04:47<3:03:12, 18.08s/it]                                                        {'loss': 0.811, 'learning_rate': 7.090403840035942e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4590/5198 [26:04:47<3:03:12, 18.08s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4591/5198 [26:05:05<3:03:42, 18.16s/it]                                                        {'loss': 0.7833, 'learning_rate': 7.067377793759999e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4591/5198 [26:05:05<3:03:42, 18.16s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4592/5198 [26:05:23<3:03:34, 18.18s/it]                                                        {'loss': 0.7491, 'learning_rate': 7.044387827127752e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4592/5198 [26:05:23<3:03:34, 18.18s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4593/5198 [26:05:40<2:59:02, 17.76s/it]                                                        {'loss': 0.7413, 'learning_rate': 7.021433949064704e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4593/5198 [26:05:40<2:59:02, 17.76s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4594/5198 [26:05:58<2:59:00, 17.78s/it]                                                        {'loss': 0.7825, 'learning_rate': 6.99851616848235e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4594/5198 [26:05:58<2:59:00, 17.78s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4595/5198 [26:06:16<2:57:53, 17.70s/it]                                                        {'loss': 0.794, 'learning_rate': 6.975634494278149e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4595/5198 [26:06:16<2:57:53, 17.70s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4596/5198 [26:06:33<2:55:53, 17.53s/it]                                                        {'loss': 0.826, 'learning_rate': 6.952788935335541e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4596/5198 [26:06:33<2:55:53, 17.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4597/5198 [26:06:50<2:54:17, 17.40s/it]                                                        {'loss': 0.7702, 'learning_rate': 6.92997950052402e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4597/5198 [26:06:50<2:54:17, 17.40s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4598/5198 [26:07:08<2:55:20, 17.53s/it]                                                        {'loss': 0.7269, 'learning_rate': 6.907206198698912e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4598/5198 [26:07:08<2:55:20, 17.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4599/5198 [26:07:25<2:54:07, 17.44s/it]                                                        {'loss': 0.7815, 'learning_rate': 6.884469038701646e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4599/5198 [26:07:25<2:54:07, 17.44s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4600/5198 [26:07:43<2:55:16, 17.59s/it]                                                        {'loss': 0.7544, 'learning_rate': 6.861768029359595e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4600/5198 [26:07:43<2:55:16, 17.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4601/5198 [26:09:11<6:27:02, 38.90s/it]                                                        {'loss': 0.7923, 'learning_rate': 6.839103179485995e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4601/5198 [26:09:11<6:27:02, 38.90s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4602/5198 [26:09:29<5:23:47, 32.60s/it]                                                        {'loss': 0.3686, 'learning_rate': 6.816474497880177e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4602/5198 [26:09:29<5:23:47, 32.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4603/5198 [26:09:47<4:39:29, 28.18s/it]                                                        {'loss': 0.7719, 'learning_rate': 6.793881993327366e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4603/5198 [26:09:47<4:39:29, 28.18s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4604/5198 [26:10:05<4:07:50, 25.03s/it]                                                        {'loss': 0.7789, 'learning_rate': 6.77132567459875e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4604/5198 [26:10:05<4:07:50, 25.03s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4605/5198 [26:10:23<3:46:39, 22.93s/it]                                                        {'loss': 0.797, 'learning_rate': 6.748805550451453e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4605/5198 [26:10:23<3:46:39, 22.93s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4606/5198 [26:10:41<3:31:27, 21.43s/it]                                                        {'loss': 0.7848, 'learning_rate': 6.726321629628585e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4606/5198 [26:10:41<3:31:27, 21.43s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4607/5198 [26:10:57<3:16:27, 19.94s/it]                                                        {'loss': 0.7803, 'learning_rate': 6.703873920859161e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4607/5198 [26:10:57<3:16:27, 19.94s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4608/5198 [26:11:15<3:08:43, 19.19s/it]                                                        {'loss': 0.7816, 'learning_rate': 6.681462432858154e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4608/5198 [26:11:15<3:08:43, 19.19s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4609/5198 [26:11:33<3:05:03, 18.85s/it]                                                        {'loss': 0.7034, 'learning_rate': 6.659087174326506e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4609/5198 [26:11:33<3:05:03, 18.85s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4610/5198 [26:11:51<3:03:42, 18.75s/it]                                                        {'loss': 0.7904, 'learning_rate': 6.636748153951e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4610/5198 [26:11:51<3:03:42, 18.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4611/5198 [26:12:09<3:01:03, 18.51s/it]                                                        {'loss': 0.7807, 'learning_rate': 6.614445380404478e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4611/5198 [26:12:09<3:01:03, 18.51s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4612/5198 [26:12:28<3:01:36, 18.60s/it]                                                        {'loss': 0.8091, 'learning_rate': 6.592178862345622e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4612/5198 [26:12:28<3:01:36, 18.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4613/5198 [26:12:45<2:57:45, 18.23s/it]                                                        {'loss': 0.6975, 'learning_rate': 6.569948608419041e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4613/5198 [26:12:45<2:57:45, 18.23s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4614/5198 [26:13:02<2:53:25, 17.82s/it]                                                        {'loss': 0.7243, 'learning_rate': 6.547754627255332e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4614/5198 [26:13:02<2:53:25, 17.82s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4615/5198 [26:13:20<2:51:47, 17.68s/it]                                                        {'loss': 0.7921, 'learning_rate': 6.52559692747089e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4615/5198 [26:13:20<2:51:47, 17.68s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4616/5198 [26:13:37<2:50:25, 17.57s/it]                                                        {'loss': 0.785, 'learning_rate': 6.503475517668168e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4616/5198 [26:13:37<2:50:25, 17.57s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4617/5198 [26:13:54<2:49:50, 17.54s/it]                                                        {'loss': 0.7359, 'learning_rate': 6.481390406435417e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4617/5198 [26:13:54<2:49:50, 17.54s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4618/5198 [26:14:12<2:50:52, 17.68s/it]                                                        {'loss': 0.7706, 'learning_rate': 6.459341602346858e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4618/5198 [26:14:12<2:50:52, 17.68s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4619/5198 [26:14:30<2:50:46, 17.70s/it]                                                        {'loss': 0.7496, 'learning_rate': 6.437329113962576e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4619/5198 [26:14:30<2:50:46, 17.70s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4620/5198 [26:14:48<2:50:18, 17.68s/it]                                                        {'loss': 0.8058, 'learning_rate': 6.415352949828601e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4620/5198 [26:14:48<2:50:18, 17.68s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4621/5198 [26:15:06<2:51:02, 17.79s/it]                                                        {'loss': 0.7486, 'learning_rate': 6.393413118476821e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4621/5198 [26:15:06<2:51:02, 17.79s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4622/5198 [26:15:24<2:50:35, 17.77s/it]                                                        {'loss': 0.8199, 'learning_rate': 6.371509628425021e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4622/5198 [26:15:24<2:50:35, 17.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4623/5198 [26:15:41<2:49:00, 17.64s/it]                                                        {'loss': 0.749, 'learning_rate': 6.349642488176943e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4623/5198 [26:15:41<2:49:00, 17.64s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4624/5198 [26:15:58<2:47:39, 17.53s/it]                                                        {'loss': 0.7833, 'learning_rate': 6.327811706222097e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4624/5198 [26:15:58<2:47:39, 17.53s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4625/5198 [26:16:16<2:48:40, 17.66s/it]                                                        {'loss': 0.7763, 'learning_rate': 6.306017291035981e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4625/5198 [26:16:16<2:48:40, 17.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4626/5198 [26:17:43<6:05:41, 38.36s/it]                                                        {'loss': 0.8014, 'learning_rate': 6.284259251079939e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4626/5198 [26:17:43<6:05:41, 38.36s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4627/5198 [26:18:01<5:07:02, 32.26s/it]                                                        {'loss': 0.8173, 'learning_rate': 6.262537594801177e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4627/5198 [26:18:01<5:07:02, 32.26s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4628/5198 [26:18:18<4:22:43, 27.66s/it]                                                        {'loss': 0.7709, 'learning_rate': 6.240852330632796e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4628/5198 [26:18:18<4:22:43, 27.66s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4629/5198 [26:18:36<3:56:31, 24.94s/it]                                                        {'loss': 0.7622, 'learning_rate': 6.219203466993762e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4629/5198 [26:18:36<3:56:31, 24.94s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4630/5198 [26:18:54<3:34:58, 22.71s/it]                                                        {'loss': 0.3282, 'learning_rate': 6.197591012288918e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4630/5198 [26:18:54<3:34:58, 22.71s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4631/5198 [26:19:11<3:17:33, 20.90s/it]                                                        {'loss': 0.8112, 'learning_rate': 6.17601497490895e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4631/5198 [26:19:11<3:17:33, 20.90s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4632/5198 [26:19:29<3:09:52, 20.13s/it]                                                        {'loss': 0.7658, 'learning_rate': 6.154475363230417e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4632/5198 [26:19:29<3:09:52, 20.13s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4633/5198 [26:19:46<3:01:45, 19.30s/it]                                                        {'loss': 0.786, 'learning_rate': 6.132972185615749e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4633/5198 [26:19:46<3:01:45, 19.30s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4634/5198 [26:20:03<2:54:57, 18.61s/it]                                                        {'loss': 0.8359, 'learning_rate': 6.111505450413202e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4634/5198 [26:20:03<2:54:57, 18.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4635/5198 [26:20:21<2:52:08, 18.35s/it]                                                        {'loss': 0.7527, 'learning_rate': 6.090075165956943e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4635/5198 [26:20:21<2:52:08, 18.35s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4636/5198 [26:20:39<2:50:17, 18.18s/it]                                                        {'loss': 0.3607, 'learning_rate': 6.068681340566896e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4636/5198 [26:20:39<2:50:17, 18.18s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4637/5198 [26:20:57<2:49:56, 18.18s/it]                                                        {'loss': 0.7741, 'learning_rate': 6.047323982548924e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4637/5198 [26:20:57<2:49:56, 18.18s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4638/5198 [26:21:15<2:49:46, 18.19s/it]                                                        {'loss': 0.7782, 'learning_rate': 6.026003100194633e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4638/5198 [26:21:15<2:49:46, 18.19s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4639/5198 [26:21:33<2:47:50, 18.01s/it]                                                        {'loss': 0.7802, 'learning_rate': 6.004718701781575e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4639/5198 [26:21:33<2:47:50, 18.01s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4640/5198 [26:21:51<2:48:41, 18.14s/it]                                                        {'loss': 0.7961, 'learning_rate': 5.983470795573088e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4640/5198 [26:21:51<2:48:41, 18.14s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4641/5198 [26:22:09<2:46:48, 17.97s/it]                                                        {'loss': 0.7616, 'learning_rate': 5.962259389818292e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4641/5198 [26:22:09<2:46:48, 17.97s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4642/5198 [26:22:27<2:46:56, 18.02s/it]                                                        {'loss': 0.778, 'learning_rate': 5.941084492752236e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4642/5198 [26:22:27<2:46:56, 18.02s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4643/5198 [26:22:44<2:44:48, 17.82s/it]                                                        {'loss': 0.7321, 'learning_rate': 5.91994611259572e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4643/5198 [26:22:44<2:44:48, 17.82s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4644/5198 [26:23:02<2:45:22, 17.91s/it]                                                        {'loss': 0.7422, 'learning_rate': 5.898844257555392e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4644/5198 [26:23:02<2:45:22, 17.91s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4645/5198 [26:23:21<2:46:04, 18.02s/it]                                                        {'loss': 0.7422, 'learning_rate': 5.87777893582372e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4645/5198 [26:23:21<2:46:04, 18.02s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4646/5198 [26:23:38<2:45:00, 17.93s/it]                                                        {'loss': 0.7514, 'learning_rate': 5.856750155578983e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4646/5198 [26:23:38<2:45:00, 17.93s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4647/5198 [26:23:56<2:43:10, 17.77s/it]                                                        {'loss': 0.7736, 'learning_rate': 5.835757924985286e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4647/5198 [26:23:56<2:43:10, 17.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4648/5198 [26:24:14<2:43:04, 17.79s/it]                                                        {'loss': 0.7835, 'learning_rate': 5.81480225219252e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4648/5198 [26:24:14<2:43:04, 17.79s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4649/5198 [26:24:32<2:45:05, 18.04s/it]                                                        {'loss': 0.7305, 'learning_rate': 5.793883145336443e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4649/5198 [26:24:32<2:45:05, 18.04s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4650/5198 [26:24:50<2:44:16, 17.99s/it]                                                        {'loss': 0.747, 'learning_rate': 5.773000612538505e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4650/5198 [26:24:50<2:44:16, 17.99s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4651/5198 [26:26:17<5:51:18, 38.53s/it]                                                        {'loss': 0.8129, 'learning_rate': 5.752154661906085e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4651/5198 [26:26:17<5:51:18, 38.53s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4652/5198 [26:26:34<4:53:18, 32.23s/it]                                                        {'loss': 0.7996, 'learning_rate': 5.731345301532265e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4652/5198 [26:26:34<4:53:18, 32.23s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4653/5198 [26:26:52<4:13:30, 27.91s/it]                                                        {'loss': 0.8466, 'learning_rate': 5.710572539495962e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4653/5198 [26:26:52<4:13:30, 27.91s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4654/5198 [26:27:10<3:45:43, 24.90s/it]                                                        {'loss': 0.8124, 'learning_rate': 5.68983638386188e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4654/5198 [26:27:10<3:45:43, 24.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4655/5198 [26:27:27<3:24:09, 22.56s/it]                                                        {'loss': 0.8239, 'learning_rate': 5.669136842680512e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4655/5198 [26:27:27<3:24:09, 22.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4656/5198 [26:27:45<3:11:19, 21.18s/it]                                                        {'loss': 0.8073, 'learning_rate': 5.648473923988129e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4656/5198 [26:27:45<3:11:19, 21.18s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4657/5198 [26:28:02<3:00:35, 20.03s/it]                                                        {'loss': 0.7665, 'learning_rate': 5.627847635806771e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4657/5198 [26:28:02<3:00:35, 20.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4658/5198 [26:28:20<2:54:21, 19.37s/it]                                                        {'loss': 0.8026, 'learning_rate': 5.607257986144321e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4658/5198 [26:28:20<2:54:21, 19.37s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4659/5198 [26:28:38<2:49:46, 18.90s/it]                                                        {'loss': 0.7729, 'learning_rate': 5.58670498299434e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4659/5198 [26:28:38<2:49:46, 18.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4660/5198 [26:28:56<2:47:50, 18.72s/it]                                                        {'loss': 0.7599, 'learning_rate': 5.566188634336212e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4660/5198 [26:28:56<2:47:50, 18.72s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4661/5198 [26:29:13<2:42:17, 18.13s/it]                                                        {'loss': 0.3284, 'learning_rate': 5.545708948135142e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4661/5198 [26:29:13<2:42:17, 18.13s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4662/5198 [26:29:30<2:38:39, 17.76s/it]                                                        {'loss': 0.7923, 'learning_rate': 5.525265932341984e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4662/5198 [26:29:30<2:38:39, 17.76s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4663/5198 [26:29:47<2:37:18, 17.64s/it]                                                        {'loss': 0.7934, 'learning_rate': 5.504859594893475e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4663/5198 [26:29:47<2:37:18, 17.64s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4664/5198 [26:30:05<2:37:22, 17.68s/it]                                                        {'loss': 0.8175, 'learning_rate': 5.484489943712013e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4664/5198 [26:30:05<2:37:22, 17.68s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4665/5198 [26:30:23<2:36:55, 17.67s/it]                                                        {'loss': 0.794, 'learning_rate': 5.464156986705826e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4665/5198 [26:30:23<2:36:55, 17.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4666/5198 [26:30:39<2:34:07, 17.38s/it]                                                        {'loss': 0.7673, 'learning_rate': 5.443860731768869e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4666/5198 [26:30:39<2:34:07, 17.38s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4667/5198 [26:30:56<2:33:15, 17.32s/it]                                                        {'loss': 0.803, 'learning_rate': 5.423601186780836e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4667/5198 [26:30:56<2:33:15, 17.32s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4668/5198 [26:31:14<2:32:38, 17.28s/it]                                                        {'loss': 0.8161, 'learning_rate': 5.403378359607181e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4668/5198 [26:31:14<2:32:38, 17.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4669/5198 [26:31:32<2:34:57, 17.58s/it]                                                        {'loss': 0.7184, 'learning_rate': 5.383192258099113e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4669/5198 [26:31:32<2:34:57, 17.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4670/5198 [26:31:50<2:37:07, 17.86s/it]                                                        {'loss': 0.7638, 'learning_rate': 5.36304289009355e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4670/5198 [26:31:50<2:37:07, 17.86s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4671/5198 [26:32:08<2:36:46, 17.85s/it]                                                        {'loss': 0.7466, 'learning_rate': 5.342930263413193e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4671/5198 [26:32:08<2:36:46, 17.85s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4672/5198 [26:32:25<2:32:23, 17.38s/it]                                                        {'loss': 0.837, 'learning_rate': 5.322854385866439e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4672/5198 [26:32:25<2:32:23, 17.38s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4673/5198 [26:32:43<2:34:00, 17.60s/it]                                                        {'loss': 0.6993, 'learning_rate': 5.302815265247452e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4673/5198 [26:32:43<2:34:00, 17.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4674/5198 [26:33:01<2:36:37, 17.93s/it]                                                        {'loss': 0.756, 'learning_rate': 5.282812909336077e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4674/5198 [26:33:01<2:36:37, 17.93s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4675/5198 [26:33:20<2:38:29, 18.18s/it]                                                        {'loss': 0.7806, 'learning_rate': 5.262847325897968e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4675/5198 [26:33:20<2:38:29, 18.18s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4676/5198 [26:34:48<5:40:36, 39.15s/it]                                                        {'loss': 0.7941, 'learning_rate': 5.242918522684392e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4676/5198 [26:34:48<5:40:36, 39.15s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4677/5198 [26:35:05<4:40:57, 32.36s/it]                                                        {'loss': 0.7292, 'learning_rate': 5.22302650743245e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4677/5198 [26:35:05<4:40:57, 32.36s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4678/5198 [26:35:23<4:03:52, 28.14s/it]                                                        {'loss': 0.7419, 'learning_rate': 5.203171287864872e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4678/5198 [26:35:23<4:03:52, 28.14s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4679/5198 [26:35:41<3:38:04, 25.21s/it]                                                        {'loss': 0.7369, 'learning_rate': 5.183352871690162e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4679/5198 [26:35:41<3:38:04, 25.21s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4680/5198 [26:36:00<3:21:15, 23.31s/it]                                                        {'loss': 0.8503, 'learning_rate': 5.163571266602485e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4680/5198 [26:36:00<3:21:15, 23.31s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4681/5198 [26:36:17<3:04:33, 21.42s/it]                                                        {'loss': 0.7924, 'learning_rate': 5.143826480281778e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4681/5198 [26:36:17<3:04:33, 21.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4682/5198 [26:36:35<2:54:54, 20.34s/it]                                                        {'loss': 0.7412, 'learning_rate': 5.124118520393606e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4682/5198 [26:36:35<2:54:54, 20.34s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4683/5198 [26:36:53<2:49:09, 19.71s/it]                                                        {'loss': 0.7177, 'learning_rate': 5.104447394589295e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4683/5198 [26:36:53<2:49:09, 19.71s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4684/5198 [26:37:11<2:42:29, 18.97s/it]                                                        {'loss': 0.3478, 'learning_rate': 5.084813110505871e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4684/5198 [26:37:11<2:42:29, 18.97s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4685/5198 [26:37:28<2:37:58, 18.48s/it]                                                        {'loss': 0.8004, 'learning_rate': 5.065215675766023e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4685/5198 [26:37:28<2:37:58, 18.48s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4686/5198 [26:37:46<2:35:45, 18.25s/it]                                                        {'loss': 0.3211, 'learning_rate': 5.045655097978131e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4686/5198 [26:37:46<2:35:45, 18.25s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4687/5198 [26:38:03<2:33:57, 18.08s/it]                                                        {'loss': 0.7477, 'learning_rate': 5.026131384736321e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4687/5198 [26:38:03<2:33:57, 18.08s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4688/5198 [26:38:20<2:31:19, 17.80s/it]                                                        {'loss': 0.8074, 'learning_rate': 5.006644543620342e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4688/5198 [26:38:20<2:31:19, 17.80s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4689/5198 [26:38:39<2:32:18, 17.95s/it]                                                        {'loss': 0.7547, 'learning_rate': 4.987194582195687e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4689/5198 [26:38:39<2:32:18, 17.95s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4690/5198 [26:38:56<2:30:02, 17.72s/it]                                                        {'loss': 0.3066, 'learning_rate': 4.967781508013459e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4690/5198 [26:38:56<2:30:02, 17.72s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4691/5198 [26:39:13<2:28:57, 17.63s/it]                                                        {'loss': 0.7843, 'learning_rate': 4.948405328610506e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4691/5198 [26:39:13<2:28:57, 17.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4692/5198 [26:39:31<2:29:18, 17.70s/it]                                                        {'loss': 0.728, 'learning_rate': 4.929066051509346e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4692/5198 [26:39:31<2:29:18, 17.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4693/5198 [26:39:49<2:28:31, 17.65s/it]                                                        {'loss': 0.7831, 'learning_rate': 4.909763684218116e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4693/5198 [26:39:49<2:28:31, 17.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4694/5198 [26:40:07<2:29:53, 17.84s/it]                                                        {'loss': 0.7933, 'learning_rate': 4.890498234230689e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4694/5198 [26:40:07<2:29:53, 17.84s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4695/5198 [26:40:24<2:28:36, 17.73s/it]                                                        {'loss': 0.7697, 'learning_rate': 4.871269709026561e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4695/5198 [26:40:24<2:28:36, 17.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4696/5198 [26:40:42<2:28:21, 17.73s/it]                                                        {'loss': 0.7593, 'learning_rate': 4.852078116070902e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4696/5198 [26:40:42<2:28:21, 17.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4697/5198 [26:41:00<2:28:45, 17.82s/it]                                                        {'loss': 0.7295, 'learning_rate': 4.832923462814565e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4697/5198 [26:41:00<2:28:45, 17.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4698/5198 [26:41:19<2:31:16, 18.15s/it]                                                        {'loss': 0.7729, 'learning_rate': 4.813805756694035e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4698/5198 [26:41:19<2:31:16, 18.15s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4699/5198 [26:41:37<2:30:43, 18.12s/it]                                                        {'loss': 0.794, 'learning_rate': 4.794725005131462e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4699/5198 [26:41:37<2:30:43, 18.12s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4700/5198 [26:41:55<2:29:55, 18.06s/it]                                                        {'loss': 0.7655, 'learning_rate': 4.775681215534656e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4700/5198 [26:41:55<2:29:55, 18.06s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4701/5198 [26:43:24<5:26:20, 39.40s/it]                                                        {'loss': 0.7983, 'learning_rate': 4.7566743952970894e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4701/5198 [26:43:24<5:26:20, 39.40s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4702/5198 [26:43:42<4:31:24, 32.83s/it]                                                        {'loss': 0.3362, 'learning_rate': 4.7377045517978173e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4702/5198 [26:43:42<4:31:24, 32.83s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4703/5198 [26:44:00<3:55:40, 28.57s/it]                                                        {'loss': 0.7999, 'learning_rate': 4.7187716924016355e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4703/5198 [26:44:00<3:55:40, 28.57s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4704/5198 [26:44:18<3:27:54, 25.25s/it]                                                        {'loss': 0.8316, 'learning_rate': 4.6998758244588995e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4704/5198 [26:44:18<3:27:54, 25.25s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4705/5198 [26:44:36<3:09:05, 23.01s/it]                                                        {'loss': 0.7406, 'learning_rate': 4.6810169553056616e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4705/5198 [26:44:36<3:09:05, 23.01s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4706/5198 [26:44:54<2:55:48, 21.44s/it]                                                        {'loss': 0.3195, 'learning_rate': 4.662195092263566e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4706/5198 [26:44:54<2:55:48, 21.44s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4707/5198 [26:45:11<2:45:55, 20.28s/it]                                                        {'loss': 0.7538, 'learning_rate': 4.643410242639912e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4707/5198 [26:45:11<2:45:55, 20.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4708/5198 [26:45:29<2:40:41, 19.68s/it]                                                        {'loss': 0.7665, 'learning_rate': 4.6246624137276206e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4708/5198 [26:45:29<2:40:41, 19.68s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4709/5198 [26:45:46<2:33:50, 18.88s/it]                                                        {'loss': 0.7529, 'learning_rate': 4.605951612805237e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4709/5198 [26:45:46<2:33:50, 18.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4710/5198 [26:46:04<2:30:45, 18.54s/it]                                                        {'loss': 0.3157, 'learning_rate': 4.587277847136984e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4710/5198 [26:46:04<2:30:45, 18.54s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4711/5198 [26:46:22<2:28:23, 18.28s/it]                                                        {'loss': 0.8187, 'learning_rate': 4.568641123972606e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4711/5198 [26:46:22<2:28:23, 18.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4712/5198 [26:46:39<2:26:03, 18.03s/it]                                                        {'loss': 0.8058, 'learning_rate': 4.550041450547549e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4712/5198 [26:46:39<2:26:03, 18.03s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4713/5198 [26:46:57<2:24:33, 17.88s/it]                                                        {'loss': 0.8548, 'learning_rate': 4.5314788340828365e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4713/5198 [26:46:57<2:24:33, 17.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4714/5198 [26:47:14<2:23:03, 17.73s/it]                                                        {'loss': 0.7511, 'learning_rate': 4.512953281785104e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4714/5198 [26:47:14<2:23:03, 17.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4715/5198 [26:47:32<2:23:49, 17.87s/it]                                                        {'loss': 0.8005, 'learning_rate': 4.494464800846654e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4715/5198 [26:47:32<2:23:49, 17.87s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4716/5198 [26:47:50<2:23:03, 17.81s/it]                                                        {'loss': 0.7829, 'learning_rate': 4.476013398445289e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4716/5198 [26:47:50<2:23:03, 17.81s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4717/5198 [26:48:07<2:21:19, 17.63s/it]                                                        {'loss': 0.3006, 'learning_rate': 4.4575990817445234e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4717/5198 [26:48:07<2:21:19, 17.63s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4718/5198 [26:48:26<2:22:50, 17.86s/it]                                                        {'loss': 0.8202, 'learning_rate': 4.4392218578934164e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4718/5198 [26:48:26<2:22:50, 17.86s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4719/5198 [26:48:43<2:21:55, 17.78s/it]                                                        {'loss': 0.7462, 'learning_rate': 4.4208817340266385e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4719/5198 [26:48:43<2:21:55, 17.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4720/5198 [26:49:01<2:22:25, 17.88s/it]                                                        {'loss': 0.8421, 'learning_rate': 4.4025787172644495e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4720/5198 [26:49:01<2:22:25, 17.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4721/5198 [26:49:20<2:24:13, 18.14s/it]                                                        {'loss': 0.7114, 'learning_rate': 4.384312814712721e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4721/5198 [26:49:20<2:24:13, 18.14s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4722/5198 [26:49:38<2:22:44, 17.99s/it]                                                        {'loss': 0.7001, 'learning_rate': 4.366084033462914e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4722/5198 [26:49:38<2:22:44, 17.99s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4723/5198 [26:49:55<2:21:14, 17.84s/it]                                                        {'loss': 0.8653, 'learning_rate': 4.3478923805920335e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4723/5198 [26:49:55<2:21:14, 17.84s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4724/5198 [26:50:13<2:21:16, 17.88s/it]                                                        {'loss': 0.7744, 'learning_rate': 4.329737863162753e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4724/5198 [26:50:13<2:21:16, 17.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4725/5198 [26:50:30<2:17:55, 17.50s/it]                                                        {'loss': 0.7589, 'learning_rate': 4.311620488223256e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4725/5198 [26:50:30<2:17:55, 17.50s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4726/5198 [26:51:57<5:02:26, 38.45s/it]                                                        {'loss': 0.7551, 'learning_rate': 4.2935402628073166e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4726/5198 [26:51:57<5:02:26, 38.45s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4727/5198 [26:52:14<4:12:06, 32.12s/it]                                                        {'loss': 0.7698, 'learning_rate': 4.27549719393433e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4727/5198 [26:52:14<4:12:06, 32.12s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4728/5198 [26:52:32<3:37:12, 27.73s/it]                                                        {'loss': 0.801, 'learning_rate': 4.2574912886092166e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4728/5198 [26:52:32<3:37:12, 27.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4729/5198 [26:52:50<3:13:44, 24.79s/it]                                                        {'loss': 0.7647, 'learning_rate': 4.239522553822495e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4729/5198 [26:52:50<3:13:44, 24.79s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4730/5198 [26:53:09<2:59:19, 22.99s/it]                                                        {'loss': 0.7223, 'learning_rate': 4.221590996550251e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4730/5198 [26:53:09<2:59:19, 22.99s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4731/5198 [26:53:26<2:46:39, 21.41s/it]                                                        {'loss': 0.8309, 'learning_rate': 4.203696623754139e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4731/5198 [26:53:26<2:46:39, 21.41s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4732/5198 [26:53:44<2:37:23, 20.27s/it]                                                        {'loss': 0.797, 'learning_rate': 4.1858394423813563e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4732/5198 [26:53:44<2:37:23, 20.27s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4733/5198 [26:54:02<2:31:16, 19.52s/it]                                                        {'loss': 0.7197, 'learning_rate': 4.1680194593646696e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4733/5198 [26:54:02<2:31:16, 19.52s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4734/5198 [26:54:19<2:26:09, 18.90s/it]                                                        {'loss': 0.7588, 'learning_rate': 4.1502366816224327e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4734/5198 [26:54:19<2:26:09, 18.90s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4735/5198 [26:54:36<2:21:28, 18.33s/it]                                                        {'loss': 0.8054, 'learning_rate': 4.1324911160585014e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4735/5198 [26:54:36<2:21:28, 18.33s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4736/5198 [26:54:54<2:20:27, 18.24s/it]                                                        {'loss': 0.7919, 'learning_rate': 4.1147827695623643e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4736/5198 [26:54:54<2:20:27, 18.24s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4737/5198 [26:55:12<2:18:43, 18.06s/it]                                                        {'loss': 0.8373, 'learning_rate': 4.097111649008967e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4737/5198 [26:55:12<2:18:43, 18.06s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4738/5198 [26:55:29<2:17:04, 17.88s/it]                                                        {'loss': 0.3164, 'learning_rate': 4.0794777612588543e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4738/5198 [26:55:29<2:17:04, 17.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4739/5198 [26:55:48<2:17:52, 18.02s/it]                                                        {'loss': 0.7572, 'learning_rate': 4.061881113158117e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4739/5198 [26:55:48<2:17:52, 18.02s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4740/5198 [26:56:06<2:17:14, 17.98s/it]                                                        {'loss': 0.8167, 'learning_rate': 4.044321711538368e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4740/5198 [26:56:06<2:17:14, 17.98s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4741/5198 [26:56:23<2:15:43, 17.82s/it]                                                        {'loss': 0.7591, 'learning_rate': 4.02679956321681e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4741/5198 [26:56:23<2:15:43, 17.82s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4742/5198 [26:56:41<2:16:43, 17.99s/it]                                                        {'loss': 0.7893, 'learning_rate': 4.00931467499609e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4742/5198 [26:56:41<2:16:43, 17.99s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4743/5198 [26:56:59<2:15:42, 17.90s/it]                                                        {'loss': 0.7221, 'learning_rate': 3.9918670536644776e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4743/5198 [26:56:59<2:15:42, 17.90s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4744/5198 [26:57:17<2:16:07, 17.99s/it]                                                        {'loss': 0.7746, 'learning_rate': 3.974456705995733e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4744/5198 [26:57:17<2:16:07, 17.99s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4745/5198 [26:57:35<2:15:42, 17.97s/it]                                                        {'loss': 0.7516, 'learning_rate': 3.9570836387491487e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4745/5198 [26:57:35<2:15:42, 17.97s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4746/5198 [26:57:53<2:14:09, 17.81s/it]                                                        {'loss': 0.7961, 'learning_rate': 3.9397478586695513e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4746/5198 [26:57:53<2:14:09, 17.81s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4747/5198 [26:58:10<2:12:53, 17.68s/it]                                                        {'loss': 0.7925, 'learning_rate': 3.9224493724872915e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4747/5198 [26:58:10<2:12:53, 17.68s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4748/5198 [26:58:28<2:12:40, 17.69s/it]                                                        {'loss': 0.7148, 'learning_rate': 3.90518818691823e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4748/5198 [26:58:28<2:12:40, 17.69s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4749/5198 [26:58:46<2:13:40, 17.86s/it]                                                        {'loss': 0.8155, 'learning_rate': 3.8879643086637384e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4749/5198 [26:58:46<2:13:40, 17.86s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4750/5198 [26:59:04<2:13:14, 17.84s/it]                                                        {'loss': 0.8104, 'learning_rate': 3.8707777444107697e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4750/5198 [26:59:04<2:13:14, 17.84s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4751/5198 [27:00:30<4:45:21, 38.30s/it]                                                        {'loss': 0.8105, 'learning_rate': 3.8536285008316854e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4751/5198 [27:00:30<4:45:21, 38.30s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4752/5198 [27:00:48<4:00:08, 32.31s/it]                                                        {'loss': 0.7755, 'learning_rate': 3.8365165845844266e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4752/5198 [27:00:48<4:00:08, 32.31s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4753/5198 [27:01:06<3:27:22, 27.96s/it]                                                        {'loss': 0.3535, 'learning_rate': 3.819442002312457e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4753/5198 [27:01:06<3:27:22, 27.96s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4754/5198 [27:01:24<3:05:13, 25.03s/it]                                                        {'loss': 0.7603, 'learning_rate': 3.8024047606446736e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4754/5198 [27:01:24<3:05:13, 25.03s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4755/5198 [27:01:42<2:48:38, 22.84s/it]                                                        {'loss': 0.7526, 'learning_rate': 3.785404866195552e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4755/5198 [27:01:42<2:48:38, 22.84s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4756/5198 [27:01:59<2:36:08, 21.20s/it]                                                        {'loss': 0.7991, 'learning_rate': 3.768442325565036e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4756/5198 [27:01:59<2:36:08, 21.20s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4757/5198 [27:02:17<2:27:12, 20.03s/it]                                                        {'loss': 0.8352, 'learning_rate': 3.751517145338546e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4757/5198 [27:02:17<2:27:12, 20.03s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4758/5198 [27:02:33<2:19:43, 19.05s/it]                                                        {'loss': 0.7443, 'learning_rate': 3.7346293320870363e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4758/5198 [27:02:33<2:19:43, 19.05s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4759/5198 [27:02:51<2:17:09, 18.75s/it]                                                        {'loss': 0.7585, 'learning_rate': 3.717778892366941e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4759/5198 [27:02:51<2:17:09, 18.75s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4760/5198 [27:03:08<2:12:55, 18.21s/it]                                                        {'loss': 0.8019, 'learning_rate': 3.700965832720171e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4760/5198 [27:03:08<2:12:55, 18.21s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4761/5198 [27:03:25<2:10:13, 17.88s/it]                                                        {'loss': 0.765, 'learning_rate': 3.684190159674117e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4761/5198 [27:03:25<2:10:13, 17.88s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4762/5198 [27:03:43<2:08:50, 17.73s/it]                                                        {'loss': 0.83, 'learning_rate': 3.6674518797417236e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4762/5198 [27:03:43<2:08:50, 17.73s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4763/5198 [27:04:01<2:09:54, 17.92s/it]                                                        {'loss': 0.74, 'learning_rate': 3.6507509994213155e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4763/5198 [27:04:01<2:09:54, 17.92s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4764/5198 [27:04:18<2:07:43, 17.66s/it]                                                        {'loss': 0.7479, 'learning_rate': 3.6340875251967946e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4764/5198 [27:04:18<2:07:43, 17.66s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4765/5198 [27:04:37<2:08:47, 17.85s/it]                                                        {'loss': 0.7586, 'learning_rate': 3.617461463537464e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4765/5198 [27:04:37<2:08:47, 17.85s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4766/5198 [27:04:55<2:09:27, 17.98s/it]                                                        {'loss': 0.7839, 'learning_rate': 3.6008728208981157e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4766/5198 [27:04:55<2:09:27, 17.98s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4767/5198 [27:05:12<2:06:35, 17.62s/it]                                                        {'loss': 0.8244, 'learning_rate': 3.5843216037190873e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4767/5198 [27:05:12<2:06:35, 17.62s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4768/5198 [27:05:29<2:05:15, 17.48s/it]                                                        {'loss': 0.7796, 'learning_rate': 3.5678078184260834e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4768/5198 [27:05:29<2:05:15, 17.48s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4769/5198 [27:05:47<2:06:56, 17.76s/it]                                                        {'loss': 0.7309, 'learning_rate': 3.5513314714303524e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4769/5198 [27:05:47<2:06:56, 17.76s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4770/5198 [27:06:05<2:06:00, 17.67s/it]                                                        {'loss': 0.727, 'learning_rate': 3.5348925691285675e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4770/5198 [27:06:05<2:06:00, 17.67s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4771/5198 [27:06:22<2:05:42, 17.66s/it]                                                        {'loss': 0.8191, 'learning_rate': 3.518491117902878e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4771/5198 [27:06:22<2:05:42, 17.66s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4772/5198 [27:06:39<2:04:14, 17.50s/it]                                                        {'loss': 0.8083, 'learning_rate': 3.502127124120891e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4772/5198 [27:06:39<2:04:14, 17.50s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4773/5198 [27:06:57<2:04:46, 17.62s/it]                                                        {'loss': 0.7277, 'learning_rate': 3.48580059413568e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4773/5198 [27:06:57<2:04:46, 17.62s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4774/5198 [27:07:15<2:04:36, 17.63s/it]                                                        {'loss': 0.8062, 'learning_rate': 3.4695115342857524e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4774/5198 [27:07:15<2:04:36, 17.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4775/5198 [27:07:32<2:03:23, 17.50s/it]                                                        {'loss': 0.766, 'learning_rate': 3.4532599508950826e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4775/5198 [27:07:32<2:03:23, 17.50s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4776/5198 [27:08:56<4:24:00, 37.54s/it]                                                        {'loss': 0.7786, 'learning_rate': 3.437045850273113e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4776/5198 [27:08:56<4:24:00, 37.54s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4777/5198 [27:09:14<3:40:55, 31.49s/it]                                                        {'loss': 0.7743, 'learning_rate': 3.420869238714708e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4777/5198 [27:09:14<3:40:55, 31.49s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4778/5198 [27:09:32<3:13:08, 27.59s/it]                                                        {'loss': 0.7349, 'learning_rate': 3.404730122500155e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4778/5198 [27:09:32<3:13:08, 27.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4779/5198 [27:09:50<2:52:05, 24.64s/it]                                                        {'loss': 0.7779, 'learning_rate': 3.3886285078952753e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4779/5198 [27:09:50<2:52:05, 24.64s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4780/5198 [27:10:08<2:38:05, 22.69s/it]                                                        {'loss': 0.7239, 'learning_rate': 3.3725644011512125e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4780/5198 [27:10:08<2:38:05, 22.69s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4781/5198 [27:10:26<2:28:03, 21.30s/it]                                                        {'loss': 0.8124, 'learning_rate': 3.356537808504634e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4781/5198 [27:10:26<2:28:03, 21.30s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4782/5198 [27:10:43<2:18:53, 20.03s/it]                                                        {'loss': 0.7609, 'learning_rate': 3.3405487361776177e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4782/5198 [27:10:43<2:18:53, 20.03s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4783/5198 [27:11:00<2:12:22, 19.14s/it]                                                        {'loss': 0.7686, 'learning_rate': 3.3245971903776654e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4783/5198 [27:11:00<2:12:22, 19.14s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4784/5198 [27:11:18<2:09:14, 18.73s/it]                                                        {'loss': 0.7188, 'learning_rate': 3.308683177297711e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4784/5198 [27:11:18<2:09:14, 18.73s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4785/5198 [27:11:35<2:05:29, 18.23s/it]                                                        {'loss': 0.7089, 'learning_rate': 3.292806703116125e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4785/5198 [27:11:35<2:05:29, 18.23s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4786/5198 [27:11:53<2:05:03, 18.21s/it]                                                        {'loss': 0.7745, 'learning_rate': 3.2769677739966975e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4786/5198 [27:11:53<2:05:03, 18.21s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4787/5198 [27:12:11<2:02:48, 17.93s/it]                                                        {'loss': 0.767, 'learning_rate': 3.2611663960886665e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4787/5198 [27:12:11<2:02:48, 17.93s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4788/5198 [27:12:28<2:01:56, 17.84s/it]                                                        {'loss': 0.7704, 'learning_rate': 3.245402575526646e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4788/5198 [27:12:28<2:01:56, 17.84s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4789/5198 [27:12:46<2:01:38, 17.84s/it]                                                        {'loss': 0.7429, 'learning_rate': 3.2296763184306965e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4789/5198 [27:12:46<2:01:38, 17.84s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4790/5198 [27:13:05<2:02:40, 18.04s/it]                                                        {'loss': 0.7126, 'learning_rate': 3.2139876309063233e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4790/5198 [27:13:05<2:02:40, 18.04s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4791/5198 [27:13:22<2:00:09, 17.71s/it]                                                        {'loss': 0.8102, 'learning_rate': 3.198336519044376e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4791/5198 [27:13:22<2:00:09, 17.71s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4792/5198 [27:13:40<2:01:00, 17.88s/it]                                                        {'loss': 0.7609, 'learning_rate': 3.182722988921161e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4792/5198 [27:13:40<2:01:00, 17.88s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4793/5198 [27:13:57<1:58:47, 17.60s/it]                                                        {'loss': 0.7605, 'learning_rate': 3.167147046598418e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4793/5198 [27:13:57<1:58:47, 17.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4794/5198 [27:14:15<1:58:42, 17.63s/it]                                                        {'loss': 0.7762, 'learning_rate': 3.151608698123232e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4794/5198 [27:14:15<1:58:42, 17.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4795/5198 [27:14:32<1:57:33, 17.50s/it]                                                        {'loss': 0.7934, 'learning_rate': 3.1361079495281443e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4795/5198 [27:14:32<1:57:33, 17.50s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4796/5198 [27:14:50<1:57:49, 17.59s/it]                                                        {'loss': 0.7847, 'learning_rate': 3.1206448068310635e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4796/5198 [27:14:50<1:57:49, 17.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4797/5198 [27:15:08<1:58:29, 17.73s/it]                                                        {'loss': 0.7598, 'learning_rate': 3.1052192760353316e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4797/5198 [27:15:08<1:58:29, 17.73s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4798/5198 [27:15:25<1:56:58, 17.55s/it]                                                        {'loss': 0.7905, 'learning_rate': 3.0898313631296586e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4798/5198 [27:15:25<1:56:58, 17.55s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4799/5198 [27:15:42<1:56:44, 17.56s/it]                                                        {'loss': 0.8204, 'learning_rate': 3.0744810740881646e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4799/5198 [27:15:42<1:56:44, 17.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4800/5198 [27:16:00<1:57:18, 17.68s/it]                                                        {'loss': 0.7998, 'learning_rate': 3.0591684148703617e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4800/5198 [27:16:00<1:57:18, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4801/5198 [27:17:29<4:18:32, 39.08s/it]                                                        {'loss': 0.8212, 'learning_rate': 3.043893391421149e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4801/5198 [27:17:29<4:18:32, 39.08s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4802/5198 [27:17:47<3:35:49, 32.70s/it]                                                        {'loss': 0.7042, 'learning_rate': 3.0286560096708275e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4802/5198 [27:17:47<3:35:49, 32.70s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4803/5198 [27:18:05<3:05:50, 28.23s/it]                                                        {'loss': 0.7987, 'learning_rate': 3.013456275535054e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4803/5198 [27:18:05<3:05:50, 28.23s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4804/5198 [27:18:23<2:44:27, 25.04s/it]                                                        {'loss': 0.7885, 'learning_rate': 2.998294194914897e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4804/5198 [27:18:23<2:44:27, 25.04s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4805/5198 [27:18:41<2:30:18, 22.95s/it]                                                        {'loss': 0.7523, 'learning_rate': 2.983169773696815e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4805/5198 [27:18:41<2:30:18, 22.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4806/5198 [27:18:58<2:18:21, 21.18s/it]                                                        {'loss': 0.803, 'learning_rate': 2.968083017752599e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4806/5198 [27:18:58<2:18:21, 21.18s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4807/5198 [27:19:15<2:09:38, 19.89s/it]                                                        {'loss': 0.8074, 'learning_rate': 2.953033932939464e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4807/5198 [27:19:15<2:09:38, 19.89s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4808/5198 [27:19:32<2:04:42, 19.19s/it]                                                        {'loss': 0.3172, 'learning_rate': 2.938022525099982e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4808/5198 [27:19:32<2:04:42, 19.19s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4809/5198 [27:19:49<1:59:41, 18.46s/it]                                                        {'loss': 0.8318, 'learning_rate': 2.9230488000621003e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4809/5198 [27:19:49<1:59:41, 18.46s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4810/5198 [27:20:07<1:58:51, 18.38s/it]                                                        {'loss': 0.7737, 'learning_rate': 2.908112763639137e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4810/5198 [27:20:07<1:58:51, 18.38s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4811/5198 [27:20:24<1:55:17, 17.87s/it]                                                        {'loss': 0.7393, 'learning_rate': 2.8932144216297643e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4811/5198 [27:20:24<1:55:17, 17.87s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4812/5198 [27:20:41<1:54:47, 17.84s/it]                                                        {'loss': 0.7541, 'learning_rate': 2.878353779818044e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4812/5198 [27:20:41<1:54:47, 17.84s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4813/5198 [27:21:00<1:55:37, 18.02s/it]                                                        {'loss': 0.7529, 'learning_rate': 2.863530843973372e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4813/5198 [27:21:00<1:55:37, 18.02s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4814/5198 [27:21:19<1:56:44, 18.24s/it]                                                        {'loss': 0.7986, 'learning_rate': 2.848745619850546e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4814/5198 [27:21:19<1:56:44, 18.24s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4815/5198 [27:21:37<1:55:54, 18.16s/it]                                                        {'loss': 0.7819, 'learning_rate': 2.833998113189662e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4815/5198 [27:21:37<1:55:54, 18.16s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4816/5198 [27:21:55<1:55:11, 18.09s/it]                                                        {'loss': 0.7721, 'learning_rate': 2.8192883297162634e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4816/5198 [27:21:55<1:55:11, 18.09s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4817/5198 [27:22:13<1:55:24, 18.17s/it]                                                        {'loss': 0.7579, 'learning_rate': 2.804616275141148e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4817/5198 [27:22:13<1:55:24, 18.17s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4818/5198 [27:22:30<1:52:47, 17.81s/it]                                                        {'loss': 0.7898, 'learning_rate': 2.7899819551605256e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4818/5198 [27:22:30<1:52:47, 17.81s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4819/5198 [27:22:47<1:50:20, 17.47s/it]                                                        {'loss': 0.7829, 'learning_rate': 2.7753853754559634e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4819/5198 [27:22:47<1:50:20, 17.47s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4820/5198 [27:23:03<1:48:59, 17.30s/it]                                                        {'loss': 0.7992, 'learning_rate': 2.760826541694328e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4820/5198 [27:23:03<1:48:59, 17.30s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4821/5198 [27:23:20<1:48:04, 17.20s/it]                                                        {'loss': 0.771, 'learning_rate': 2.746305459527876e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4821/5198 [27:23:20<1:48:04, 17.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4822/5198 [27:23:37<1:47:32, 17.16s/it]                                                        {'loss': 0.8229, 'learning_rate': 2.7318221345941865e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4822/5198 [27:23:38<1:47:32, 17.16s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4823/5198 [27:23:55<1:48:09, 17.31s/it]                                                        {'loss': 0.3281, 'learning_rate': 2.717376572516184e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4823/5198 [27:23:55<1:48:09, 17.31s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4824/5198 [27:24:13<1:48:52, 17.47s/it]                                                        {'loss': 0.7797, 'learning_rate': 2.7029687789021377e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4824/5198 [27:24:13<1:48:52, 17.47s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4825/5198 [27:24:30<1:48:21, 17.43s/it]                                                        {'loss': 0.7678, 'learning_rate': 2.688598759345651e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4825/5198 [27:24:30<1:48:21, 17.43s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4826/5198 [27:25:58<3:59:29, 38.63s/it]                                                        {'loss': 0.786, 'learning_rate': 2.67426651942565e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4826/5198 [27:25:58<3:59:29, 38.63s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4827/5198 [27:26:16<3:19:20, 32.24s/it]                                                        {'loss': 0.785, 'learning_rate': 2.659972064706406e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4827/5198 [27:26:16<3:19:20, 32.24s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4828/5198 [27:26:33<2:51:20, 27.78s/it]                                                        {'loss': 0.8085, 'learning_rate': 2.645715400737536e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4828/5198 [27:26:33<2:51:20, 27.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4829/5198 [27:26:51<2:32:09, 24.74s/it]                                                        {'loss': 0.7404, 'learning_rate': 2.631496533053934e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4829/5198 [27:26:51<2:32:09, 24.74s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4830/5198 [27:27:10<2:20:54, 22.97s/it]                                                        {'loss': 0.7667, 'learning_rate': 2.6173154671758847e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4830/5198 [27:27:10<2:20:54, 22.97s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4831/5198 [27:27:27<2:10:25, 21.32s/it]                                                        {'loss': 0.7723, 'learning_rate': 2.603172208608962e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4831/5198 [27:27:27<2:10:25, 21.32s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4832/5198 [27:27:46<2:05:06, 20.51s/it]                                                        {'loss': 0.8125, 'learning_rate': 2.589066762844039e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4832/5198 [27:27:46<2:05:06, 20.51s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4833/5198 [27:28:04<1:59:55, 19.71s/it]                                                        {'loss': 0.7126, 'learning_rate': 2.57499913535737e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4833/5198 [27:28:04<1:59:55, 19.71s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4834/5198 [27:28:21<1:55:12, 18.99s/it]                                                        {'loss': 0.7858, 'learning_rate': 2.5609693316104745e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4834/5198 [27:28:21<1:55:12, 18.99s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4835/5198 [27:28:39<1:53:44, 18.80s/it]                                                        {'loss': 0.7579, 'learning_rate': 2.5469773570502063e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4835/5198 [27:28:39<1:53:44, 18.80s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4836/5198 [27:28:57<1:50:57, 18.39s/it]                                                        {'loss': 0.7815, 'learning_rate': 2.5330232171087433e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4836/5198 [27:28:57<1:50:57, 18.39s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4837/5198 [27:29:14<1:49:18, 18.17s/it]                                                        {'loss': 0.799, 'learning_rate': 2.51910691720354e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4837/5198 [27:29:14<1:49:18, 18.17s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4838/5198 [27:29:32<1:48:34, 18.10s/it]                                                        {'loss': 0.7699, 'learning_rate': 2.5052284627374077e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4838/5198 [27:29:32<1:48:34, 18.10s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4839/5198 [27:29:50<1:48:19, 18.10s/it]                                                        {'loss': 0.7796, 'learning_rate': 2.491387859098426e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4839/5198 [27:29:50<1:48:19, 18.10s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4840/5198 [27:30:08<1:47:03, 17.94s/it]                                                        {'loss': 0.7685, 'learning_rate': 2.477585111659997e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4840/5198 [27:30:08<1:47:03, 17.94s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4841/5198 [27:30:26<1:47:49, 18.12s/it]                                                        {'loss': 0.8065, 'learning_rate': 2.463820225780811e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4841/5198 [27:30:26<1:47:49, 18.12s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4842/5198 [27:30:44<1:47:01, 18.04s/it]                                                        {'loss': 0.3294, 'learning_rate': 2.4500932068049046e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4842/5198 [27:30:44<1:47:01, 18.04s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4843/5198 [27:31:03<1:48:27, 18.33s/it]                                                        {'loss': 0.7429, 'learning_rate': 2.4364040600615477e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4843/5198 [27:31:03<1:48:27, 18.33s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4844/5198 [27:31:20<1:46:04, 17.98s/it]                                                        {'loss': 0.8092, 'learning_rate': 2.422752790865346e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4844/5198 [27:31:20<1:46:04, 17.98s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4845/5198 [27:31:38<1:44:16, 17.72s/it]                                                        {'loss': 0.8117, 'learning_rate': 2.409139404516203e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4845/5198 [27:31:38<1:44:16, 17.72s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4846/5198 [27:31:55<1:43:43, 17.68s/it]                                                        {'loss': 0.7329, 'learning_rate': 2.3955639062992696e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4846/5198 [27:31:55<1:43:43, 17.68s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4847/5198 [27:32:13<1:42:47, 17.57s/it]                                                        {'loss': 0.7796, 'learning_rate': 2.3820263014850741e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4847/5198 [27:32:13<1:42:47, 17.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4848/5198 [27:32:30<1:42:53, 17.64s/it]                                                        {'loss': 0.8069, 'learning_rate': 2.3685265953293345e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4848/5198 [27:32:30<1:42:53, 17.64s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4849/5198 [27:32:48<1:42:38, 17.65s/it]                                                        {'loss': 0.7611, 'learning_rate': 2.3550647930731362e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4849/5198 [27:32:48<1:42:38, 17.65s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4850/5198 [27:33:07<1:43:53, 17.91s/it]                                                        {'loss': 0.7748, 'learning_rate': 2.3416408999427876e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4850/5198 [27:33:07<1:43:53, 17.91s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4851/5198 [27:34:32<3:40:36, 38.15s/it]                                                        {'loss': 0.7955, 'learning_rate': 2.3282549211499307e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4851/5198 [27:34:32<3:40:36, 38.15s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4852/5198 [27:34:49<3:03:30, 31.82s/it]                                                        {'loss': 0.3554, 'learning_rate': 2.3149068618914417e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4852/5198 [27:34:49<3:03:30, 31.82s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4853/5198 [27:35:06<2:38:09, 27.51s/it]                                                        {'loss': 0.854, 'learning_rate': 2.3015967273494867e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4853/5198 [27:35:06<2:38:09, 27.51s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4854/5198 [27:35:25<2:22:38, 24.88s/it]                                                        {'loss': 0.701, 'learning_rate': 2.2883245226915652e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4854/5198 [27:35:25<2:22:38, 24.88s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4855/5198 [27:35:42<2:09:07, 22.59s/it]                                                        {'loss': 0.7861, 'learning_rate': 2.2750902530703667e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4855/5198 [27:35:42<2:09:07, 22.59s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4856/5198 [27:36:00<2:00:11, 21.08s/it]                                                        {'loss': 0.811, 'learning_rate': 2.2618939236238924e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4856/5198 [27:36:00<2:00:11, 21.08s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4857/5198 [27:36:16<1:51:49, 19.68s/it]                                                        {'loss': 0.7938, 'learning_rate': 2.2487355394754328e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4857/5198 [27:36:16<1:51:49, 19.68s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4858/5198 [27:36:34<1:47:38, 19.00s/it]                                                        {'loss': 0.805, 'learning_rate': 2.2356151057334908e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4858/5198 [27:36:34<1:47:38, 19.00s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4859/5198 [27:36:51<1:44:47, 18.55s/it]                                                        {'loss': 0.8034, 'learning_rate': 2.2225326274919135e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4859/5198 [27:36:51<1:44:47, 18.55s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4860/5198 [27:37:09<1:43:05, 18.30s/it]                                                        {'loss': 0.7416, 'learning_rate': 2.209488109829727e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4860/5198 [27:37:09<1:43:05, 18.30s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4861/5198 [27:37:26<1:40:51, 17.96s/it]                                                        {'loss': 0.7477, 'learning_rate': 2.196481557811303e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4861/5198 [27:37:26<1:40:51, 17.96s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4862/5198 [27:37:44<1:40:59, 18.04s/it]                                                        {'loss': 0.7969, 'learning_rate': 2.1835129764861907e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4862/5198 [27:37:44<1:40:59, 18.04s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4863/5198 [27:38:01<1:37:40, 17.49s/it]                                                        {'loss': 0.777, 'learning_rate': 2.1705823708892737e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4863/5198 [27:38:01<1:37:40, 17.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4864/5198 [27:38:18<1:37:40, 17.55s/it]                                                        {'loss': 0.7634, 'learning_rate': 2.1576897460406477e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4864/5198 [27:38:18<1:37:40, 17.55s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4865/5198 [27:38:36<1:38:16, 17.71s/it]                                                        {'loss': 0.7411, 'learning_rate': 2.144835106945664e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4865/5198 [27:38:36<1:38:16, 17.71s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4866/5198 [27:38:53<1:36:35, 17.46s/it]                                                        {'loss': 0.8068, 'learning_rate': 2.1320184585949532e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4866/5198 [27:38:53<1:36:35, 17.46s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4867/5198 [27:39:11<1:36:05, 17.42s/it]                                                        {'loss': 0.7532, 'learning_rate': 2.119239805964357e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4867/5198 [27:39:11<1:36:05, 17.42s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4868/5198 [27:39:28<1:35:50, 17.42s/it]                                                        {'loss': 0.8249, 'learning_rate': 2.106499154015018e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4868/5198 [27:39:28<1:35:50, 17.42s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4869/5198 [27:39:46<1:36:40, 17.63s/it]                                                        {'loss': 0.8255, 'learning_rate': 2.0937965076932576e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4869/5198 [27:39:46<1:36:40, 17.63s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4870/5198 [27:40:04<1:37:20, 17.80s/it]                                                        {'loss': 0.8005, 'learning_rate': 2.0811318719307194e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4870/5198 [27:40:04<1:37:20, 17.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4871/5198 [27:40:22<1:37:30, 17.89s/it]                                                        {'loss': 0.7758, 'learning_rate': 2.0685052516442373e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4871/5198 [27:40:22<1:37:30, 17.89s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4872/5198 [27:40:40<1:36:44, 17.81s/it]                                                        {'loss': 0.7596, 'learning_rate': 2.0559166517358787e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4872/5198 [27:40:40<1:36:44, 17.81s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4873/5198 [27:40:59<1:37:48, 18.06s/it]                                                        {'loss': 0.7856, 'learning_rate': 2.0433660770930009e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4873/5198 [27:40:59<1:37:48, 18.06s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4874/5198 [27:41:16<1:36:52, 17.94s/it]                                                        {'loss': 0.7686, 'learning_rate': 2.0308535325881616e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4874/5198 [27:41:16<1:36:52, 17.94s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4875/5198 [27:41:35<1:37:16, 18.07s/it]                                                        {'loss': 0.8466, 'learning_rate': 2.0183790230791532e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4875/5198 [27:41:35<1:37:16, 18.07s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4876/5198 [27:43:01<3:27:08, 38.60s/it]                                                        {'loss': 0.8233, 'learning_rate': 2.0059425534090128e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4876/5198 [27:43:01<3:27:08, 38.60s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4877/5198 [27:43:19<2:53:24, 32.41s/it]                                                        {'loss': 0.8349, 'learning_rate': 1.9935441284059998e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4877/5198 [27:43:19<2:53:24, 32.41s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4878/5198 [27:43:36<2:28:40, 27.88s/it]                                                        {'loss': 0.7879, 'learning_rate': 1.981183752883631e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4878/5198 [27:43:36<2:28:40, 27.88s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4879/5198 [27:43:54<2:12:28, 24.92s/it]                                                        {'loss': 0.7172, 'learning_rate': 1.9688614316406006e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4879/5198 [27:43:54<2:12:28, 24.92s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4880/5198 [27:44:12<2:00:10, 22.67s/it]                                                        {'loss': 0.366, 'learning_rate': 1.9565771694608937e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4880/5198 [27:44:12<2:00:10, 22.67s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4881/5198 [27:44:29<1:50:42, 20.95s/it]                                                        {'loss': 0.8124, 'learning_rate': 1.9443309711136393e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4881/5198 [27:44:29<1:50:42, 20.95s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4882/5198 [27:44:47<1:45:36, 20.05s/it]                                                        {'loss': 0.7497, 'learning_rate': 1.9321228413532788e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4882/5198 [27:44:47<1:45:36, 20.05s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4883/5198 [27:45:05<1:41:56, 19.42s/it]                                                        {'loss': 0.7998, 'learning_rate': 1.9199527849194098e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4883/5198 [27:45:05<1:41:56, 19.42s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4884/5198 [27:45:22<1:38:58, 18.91s/it]                                                        {'loss': 0.8089, 'learning_rate': 1.907820806536842e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4884/5198 [27:45:22<1:38:58, 18.91s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4885/5198 [27:45:41<1:37:54, 18.77s/it]                                                        {'loss': 0.7357, 'learning_rate': 1.895726910915663e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4885/5198 [27:45:41<1:37:54, 18.77s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4886/5198 [27:45:58<1:35:04, 18.28s/it]                                                        {'loss': 0.3135, 'learning_rate': 1.883671102751128e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4886/5198 [27:45:58<1:35:04, 18.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4887/5198 [27:46:17<1:35:16, 18.38s/it]                                                        {'loss': 0.7519, 'learning_rate': 1.8716533867237153e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4887/5198 [27:46:17<1:35:16, 18.38s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4888/5198 [27:46:35<1:34:47, 18.35s/it]                                                        {'loss': 0.7058, 'learning_rate': 1.859673767499115e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4888/5198 [27:46:35<1:34:47, 18.35s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4889/5198 [27:46:53<1:34:44, 18.40s/it]                                                        {'loss': 0.8279, 'learning_rate': 1.847732249728218e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4889/5198 [27:46:53<1:34:44, 18.40s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4890/5198 [27:47:11<1:33:43, 18.26s/it]                                                        {'loss': 0.3533, 'learning_rate': 1.83582883804716e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4890/5198 [27:47:11<1:33:43, 18.26s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4891/5198 [27:47:29<1:32:39, 18.11s/it]                                                        {'loss': 0.3499, 'learning_rate': 1.8239635370772223e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4891/5198 [27:47:29<1:32:39, 18.11s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4892/5198 [27:47:46<1:31:06, 17.86s/it]                                                        {'loss': 0.7795, 'learning_rate': 1.8121363514249534e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4892/5198 [27:47:46<1:31:06, 17.86s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4893/5198 [27:48:03<1:29:38, 17.63s/it]                                                        {'loss': 0.7805, 'learning_rate': 1.8003472856820469e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4893/5198 [27:48:03<1:29:38, 17.63s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4894/5198 [27:48:21<1:28:40, 17.50s/it]                                                        {'loss': 0.7711, 'learning_rate': 1.7885963444254528e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4894/5198 [27:48:21<1:28:40, 17.50s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4895/5198 [27:48:38<1:28:38, 17.55s/it]                                                        {'loss': 0.8138, 'learning_rate': 1.7768835322172552e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4895/5198 [27:48:38<1:28:38, 17.55s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4896/5198 [27:48:56<1:28:37, 17.61s/it]                                                        {'loss': 0.3021, 'learning_rate': 1.7652088536048052e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4896/5198 [27:48:56<1:28:37, 17.61s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4897/5198 [27:49:13<1:26:53, 17.32s/it]                                                        {'loss': 0.7425, 'learning_rate': 1.7535723131206106e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4897/5198 [27:49:13<1:26:53, 17.32s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4898/5198 [27:49:30<1:26:34, 17.32s/it]                                                        {'loss': 0.7524, 'learning_rate': 1.7419739152823468e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4898/5198 [27:49:30<1:26:34, 17.32s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4899/5198 [27:49:49<1:28:13, 17.70s/it]                                                        {'loss': 0.794, 'learning_rate': 1.7304136645929448e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4899/5198 [27:49:49<1:28:13, 17.70s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4900/5198 [27:50:06<1:27:37, 17.64s/it]                                                        {'loss': 0.8119, 'learning_rate': 1.7188915655404814e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4900/5198 [27:50:06<1:27:37, 17.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4901/5198 [27:51:36<3:14:33, 39.30s/it]                                                        {'loss': 0.7459, 'learning_rate': 1.707407622598223e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4901/5198 [27:51:36<3:14:33, 39.30s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4902/5198 [27:51:53<2:40:25, 32.52s/it]                                                        {'loss': 0.8138, 'learning_rate': 1.695961840224636e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4902/5198 [27:51:53<2:40:25, 32.52s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4903/5198 [27:52:10<2:18:02, 28.07s/it]                                                        {'loss': 0.7739, 'learning_rate': 1.6845542228633772e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4903/5198 [27:52:10<2:18:02, 28.07s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4904/5198 [27:52:27<2:01:07, 24.72s/it]                                                        {'loss': 0.7876, 'learning_rate': 1.6731847749432705e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4904/5198 [27:52:27<2:01:07, 24.72s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4905/5198 [27:52:45<1:50:52, 22.71s/it]                                                        {'loss': 0.7856, 'learning_rate': 1.6618535008783075e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4905/5198 [27:52:45<1:50:52, 22.71s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4906/5198 [27:53:02<1:42:11, 21.00s/it]                                                        {'loss': 0.8508, 'learning_rate': 1.6505604050677249e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4906/5198 [27:53:02<1:42:11, 21.00s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4907/5198 [27:53:19<1:35:50, 19.76s/it]                                                        {'loss': 0.758, 'learning_rate': 1.6393054918958373e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4907/5198 [27:53:19<1:35:50, 19.76s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4908/5198 [27:53:38<1:33:36, 19.37s/it]                                                        {'loss': 0.7949, 'learning_rate': 1.6280887657322276e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4908/5198 [27:53:38<1:33:36, 19.37s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4909/5198 [27:53:55<1:30:35, 18.81s/it]                                                        {'loss': 0.7896, 'learning_rate': 1.616910230931612e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4909/5198 [27:53:55<1:30:35, 18.81s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4910/5198 [27:54:13<1:29:32, 18.65s/it]                                                        {'loss': 0.8156, 'learning_rate': 1.6057698918338526e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4910/5198 [27:54:13<1:29:32, 18.65s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4911/5198 [27:54:31<1:27:27, 18.28s/it]                                                        {'loss': 0.7922, 'learning_rate': 1.5946677527640563e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4911/5198 [27:54:31<1:27:27, 18.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4912/5198 [27:54:48<1:25:25, 17.92s/it]                                                        {'loss': 0.7543, 'learning_rate': 1.5836038180324198e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4912/5198 [27:54:48<1:25:25, 17.92s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4913/5198 [27:55:06<1:24:52, 17.87s/it]                                                        {'loss': 0.7849, 'learning_rate': 1.5725780919343624e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4913/5198 [27:55:06<1:24:52, 17.87s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4914/5198 [27:55:23<1:23:34, 17.66s/it]                                                        {'loss': 0.7361, 'learning_rate': 1.561590578750438e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4914/5198 [27:55:23<1:23:34, 17.66s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4915/5198 [27:55:41<1:23:36, 17.73s/it]                                                        {'loss': 0.8483, 'learning_rate': 1.55064128274639e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4915/5198 [27:55:41<1:23:36, 17.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4916/5198 [27:55:59<1:23:50, 17.84s/it]                                                        {'loss': 0.704, 'learning_rate': 1.5397302081731069e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4916/5198 [27:55:59<1:23:50, 17.84s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4917/5198 [27:56:18<1:24:54, 18.13s/it]                                                        {'loss': 0.7584, 'learning_rate': 1.5288573592666445e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4917/5198 [27:56:18<1:24:54, 18.13s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4918/5198 [27:56:35<1:23:05, 17.80s/it]                                                        {'loss': 0.7635, 'learning_rate': 1.518022740248215e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4918/5198 [27:56:35<1:23:05, 17.80s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4919/5198 [27:56:54<1:24:43, 18.22s/it]                                                        {'loss': 0.7979, 'learning_rate': 1.5072263553241872e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4919/5198 [27:56:54<1:24:43, 18.22s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4920/5198 [27:57:12<1:23:46, 18.08s/it]                                                        {'loss': 0.7645, 'learning_rate': 1.4964682086861082e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4920/5198 [27:57:12<1:23:46, 18.08s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4921/5198 [27:57:30<1:23:17, 18.04s/it]                                                        {'loss': 0.7881, 'learning_rate': 1.4857483045106258e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4921/5198 [27:57:30<1:23:17, 18.04s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4922/5198 [27:57:46<1:21:02, 17.62s/it]                                                        {'loss': 0.834, 'learning_rate': 1.475066646959611e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4922/5198 [27:57:46<1:21:02, 17.62s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4923/5198 [27:58:04<1:21:38, 17.81s/it]                                                        {'loss': 0.7741, 'learning_rate': 1.4644232401800352e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4923/5198 [27:58:04<1:21:38, 17.81s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4924/5198 [27:58:22<1:21:30, 17.85s/it]                                                        {'loss': 0.7989, 'learning_rate': 1.4538180883040264e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4924/5198 [27:58:22<1:21:30, 17.85s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4925/5198 [27:58:40<1:20:27, 17.68s/it]                                                        {'loss': 0.7704, 'learning_rate': 1.4432511954488915e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4925/5198 [27:58:40<1:20:27, 17.68s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4926/5198 [28:00:05<2:52:27, 38.04s/it]                                                        {'loss': 0.7931, 'learning_rate': 1.4327225657170485e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4926/5198 [28:00:05<2:52:27, 38.04s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4927/5198 [28:00:23<2:24:35, 32.01s/it]                                                        {'loss': 0.8, 'learning_rate': 1.4222322031960723e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4927/5198 [28:00:23<2:24:35, 32.01s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4928/5198 [28:00:40<2:04:09, 27.59s/it]                                                        {'loss': 0.3145, 'learning_rate': 1.411780111958694e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4928/5198 [28:00:40<2:04:09, 27.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4929/5198 [28:00:59<1:51:39, 24.90s/it]                                                        {'loss': 0.8102, 'learning_rate': 1.4013662960627562e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4929/5198 [28:00:59<1:51:39, 24.90s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4930/5198 [28:01:16<1:40:17, 22.45s/it]                                                        {'loss': 0.7862, 'learning_rate': 1.3909907595512806e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4930/5198 [28:01:16<1:40:17, 22.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4931/5198 [28:01:33<1:33:15, 20.96s/it]                                                        {'loss': 0.7905, 'learning_rate': 1.3806535064524006e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4931/5198 [28:01:33<1:33:15, 20.96s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4932/5198 [28:01:51<1:28:32, 19.97s/it]                                                        {'loss': 0.7805, 'learning_rate': 1.3703545407793951e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4932/5198 [28:01:51<1:28:32, 19.97s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4933/5198 [28:02:09<1:24:59, 19.24s/it]                                                        {'loss': 0.7914, 'learning_rate': 1.360093866530665e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4933/5198 [28:02:09<1:24:59, 19.24s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4934/5198 [28:02:27<1:23:50, 19.05s/it]                                                        {'loss': 0.7874, 'learning_rate': 1.34987148768978e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4934/5198 [28:02:27<1:23:50, 19.05s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4935/5198 [28:02:45<1:21:46, 18.66s/it]                                                        {'loss': 0.7904, 'learning_rate': 1.3396874082253986e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4935/5198 [28:02:45<1:21:46, 18.66s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4936/5198 [28:03:02<1:19:19, 18.17s/it]                                                        {'loss': 0.7738, 'learning_rate': 1.3295416320913357e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4936/5198 [28:03:02<1:19:19, 18.17s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4937/5198 [28:03:18<1:16:44, 17.64s/it]                                                        {'loss': 0.7559, 'learning_rate': 1.3194341632265518e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4937/5198 [28:03:18<1:16:44, 17.64s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4938/5198 [28:03:35<1:15:20, 17.39s/it]                                                        {'loss': 0.7394, 'learning_rate': 1.3093650055550855e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4938/5198 [28:03:35<1:15:20, 17.39s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4939/5198 [28:03:52<1:14:42, 17.31s/it]                                                        {'loss': 0.8084, 'learning_rate': 1.2993341629861432e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4939/5198 [28:03:52<1:14:42, 17.31s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4940/5198 [28:04:09<1:14:13, 17.26s/it]                                                        {'loss': 0.7927, 'learning_rate': 1.2893416394140323e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4940/5198 [28:04:09<1:14:13, 17.26s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4941/5198 [28:04:26<1:13:47, 17.23s/it]                                                        {'loss': 0.8018, 'learning_rate': 1.279387438718216e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4941/5198 [28:04:27<1:13:47, 17.23s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4942/5198 [28:04:45<1:14:50, 17.54s/it]                                                        {'loss': 0.7561, 'learning_rate': 1.269471564763247e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4942/5198 [28:04:45<1:14:50, 17.54s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4943/5198 [28:05:03<1:15:14, 17.70s/it]                                                        {'loss': 0.7799, 'learning_rate': 1.2595940213988024e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4943/5198 [28:05:03<1:15:14, 17.70s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4944/5198 [28:05:20<1:14:14, 17.54s/it]                                                        {'loss': 0.8501, 'learning_rate': 1.2497548124597026e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4944/5198 [28:05:20<1:14:14, 17.54s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4945/5198 [28:05:38<1:14:54, 17.76s/it]                                                        {'loss': 0.7629, 'learning_rate': 1.2399539417658368e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4945/5198 [28:05:38<1:14:54, 17.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4946/5198 [28:05:56<1:13:57, 17.61s/it]                                                        {'loss': 0.7829, 'learning_rate': 1.2301914131222726e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4946/5198 [28:05:56<1:13:57, 17.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4947/5198 [28:06:14<1:14:31, 17.81s/it]                                                        {'loss': 0.7982, 'learning_rate': 1.2204672303191335e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4947/5198 [28:06:14<1:14:31, 17.81s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4948/5198 [28:06:32<1:14:33, 17.90s/it]                                                        {'loss': 0.8026, 'learning_rate': 1.2107813971317106e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4948/5198 [28:06:32<1:14:33, 17.90s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4949/5198 [28:06:50<1:14:40, 18.00s/it]                                                        {'loss': 0.7847, 'learning_rate': 1.201133917320363e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4949/5198 [28:06:50<1:14:40, 18.00s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4950/5198 [28:07:09<1:14:54, 18.12s/it]                                                        {'loss': 0.7365, 'learning_rate': 1.1915247946305498e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4950/5198 [28:07:09<1:14:54, 18.12s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4951/5198 [28:08:34<2:37:08, 38.17s/it]                                                        {'loss': 0.8019, 'learning_rate': 1.1819540327929092e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4951/5198 [28:08:34<2:37:08, 38.17s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4952/5198 [28:08:52<2:12:49, 32.40s/it]                                                        {'loss': 0.7524, 'learning_rate': 1.1724216355231022e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4952/5198 [28:08:52<2:12:49, 32.40s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4953/5198 [28:09:10<1:53:41, 27.84s/it]                                                        {'loss': 0.7881, 'learning_rate': 1.1629276065219575e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4953/5198 [28:09:10<1:53:41, 27.84s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4954/5198 [28:09:27<1:40:04, 24.61s/it]                                                        {'loss': 0.7857, 'learning_rate': 1.1534719494753821e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4954/5198 [28:09:27<1:40:04, 24.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4955/5198 [28:09:45<1:31:47, 22.67s/it]                                                        {'loss': 0.8365, 'learning_rate': 1.144054668054373e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4955/5198 [28:09:45<1:31:47, 22.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4956/5198 [28:10:03<1:25:53, 21.29s/it]                                                        {'loss': 0.769, 'learning_rate': 1.1346757659150498e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4956/5198 [28:10:03<1:25:53, 21.29s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4957/5198 [28:10:20<1:20:08, 19.95s/it]                                                        {'loss': 0.8055, 'learning_rate': 1.1253352466986334e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4957/5198 [28:10:20<1:20:08, 19.95s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4958/5198 [28:10:37<1:16:41, 19.17s/it]                                                        {'loss': 0.7482, 'learning_rate': 1.116033114031434e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4958/5198 [28:10:37<1:16:41, 19.17s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4959/5198 [28:10:55<1:14:14, 18.64s/it]                                                        {'loss': 0.8036, 'learning_rate': 1.1067693715248406e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4959/5198 [28:10:55<1:14:14, 18.64s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4960/5198 [28:11:12<1:12:40, 18.32s/it]                                                        {'loss': 0.7873, 'learning_rate': 1.0975440227753764e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4960/5198 [28:11:12<1:12:40, 18.32s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4961/5198 [28:11:29<1:10:53, 17.95s/it]                                                        {'loss': 0.7048, 'learning_rate': 1.0883570713646318e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4961/5198 [28:11:29<1:10:53, 17.95s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4962/5198 [28:11:48<1:11:13, 18.11s/it]                                                        {'loss': 0.7804, 'learning_rate': 1.0792085208593095e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4962/5198 [28:11:48<1:11:13, 18.11s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4963/5198 [28:12:06<1:10:51, 18.09s/it]                                                        {'loss': 0.7555, 'learning_rate': 1.0700983748111792e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4963/5198 [28:12:06<1:10:51, 18.09s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4964/5198 [28:12:23<1:09:09, 17.73s/it]                                                        {'loss': 0.7582, 'learning_rate': 1.061026636757101e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4964/5198 [28:12:23<1:09:09, 17.73s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4965/5198 [28:12:40<1:08:52, 17.73s/it]                                                        {'loss': 0.7168, 'learning_rate': 1.0519933102190682e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4965/5198 [28:12:40<1:08:52, 17.73s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4966/5198 [28:12:57<1:07:33, 17.47s/it]                                                        {'loss': 0.7776, 'learning_rate': 1.0429983987041092e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4966/5198 [28:12:57<1:07:33, 17.47s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4967/5198 [28:13:14<1:06:43, 17.33s/it]                                                        {'loss': 0.7722, 'learning_rate': 1.0340419057043527e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4967/5198 [28:13:14<1:06:43, 17.33s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4968/5198 [28:13:33<1:07:51, 17.70s/it]                                                        {'loss': 0.7831, 'learning_rate': 1.0251238346970393e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4968/5198 [28:13:33<1:07:51, 17.70s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4969/5198 [28:13:50<1:06:32, 17.43s/it]                                                        {'loss': 0.7544, 'learning_rate': 1.0162441891444441e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4969/5198 [28:13:50<1:06:32, 17.43s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4970/5198 [28:14:07<1:06:35, 17.53s/it]                                                        {'loss': 0.8148, 'learning_rate': 1.007402972493976e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4970/5198 [28:14:07<1:06:35, 17.53s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4971/5198 [28:14:25<1:06:33, 17.59s/it]                                                        {'loss': 0.7547, 'learning_rate': 9.986001881780783e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4971/5198 [28:14:25<1:06:33, 17.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4972/5198 [28:14:43<1:07:00, 17.79s/it]                                                        {'loss': 0.7748, 'learning_rate': 9.898358396143171e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4972/5198 [28:14:43<1:07:00, 17.79s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4973/5198 [28:15:02<1:07:18, 17.95s/it]                                                        {'loss': 0.7982, 'learning_rate': 9.811099302052928e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4973/5198 [28:15:02<1:07:18, 17.95s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4974/5198 [28:15:20<1:06:54, 17.92s/it]                                                        {'loss': 0.3293, 'learning_rate': 9.72422463338718e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4974/5198 [28:15:20<1:06:54, 17.92s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4975/5198 [28:15:38<1:06:59, 18.02s/it]                                                        {'loss': 0.7935, 'learning_rate': 9.637734423873612e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4975/5198 [28:15:38<1:06:59, 18.02s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4976/5198 [28:17:03<2:21:04, 38.13s/it]                                                        {'loss': 0.8212, 'learning_rate': 9.55162870709081e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4976/5198 [28:17:03<2:21:04, 38.13s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4977/5198 [28:17:21<1:58:01, 32.04s/it]                                                        {'loss': 0.7703, 'learning_rate': 9.465907516467698e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4977/5198 [28:17:21<1:58:01, 32.04s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4978/5198 [28:17:37<1:40:45, 27.48s/it]                                                        {'loss': 0.783, 'learning_rate': 9.380570885284546e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4978/5198 [28:17:37<1:40:45, 27.48s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4979/5198 [28:17:55<1:29:35, 24.55s/it]                                                        {'loss': 0.7994, 'learning_rate': 9.295618846671739e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4979/5198 [28:17:55<1:29:35, 24.55s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4980/5198 [28:18:13<1:22:15, 22.64s/it]                                                        {'loss': 0.8277, 'learning_rate': 9.211051433610674e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4980/5198 [28:18:13<1:22:15, 22.64s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4981/5198 [28:18:30<1:15:39, 20.92s/it]                                                        {'loss': 0.8132, 'learning_rate': 9.126868678933198e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4981/5198 [28:18:30<1:15:39, 20.92s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4982/5198 [28:18:48<1:12:09, 20.05s/it]                                                        {'loss': 0.7533, 'learning_rate': 9.04307061532217e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4982/5198 [28:18:48<1:12:09, 20.05s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4983/5198 [28:19:06<1:09:07, 19.29s/it]                                                        {'loss': 0.7755, 'learning_rate': 8.959657275310674e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4983/5198 [28:19:06<1:09:07, 19.29s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4984/5198 [28:19:23<1:06:55, 18.77s/it]                                                        {'loss': 0.7757, 'learning_rate': 8.876628691282918e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4984/5198 [28:19:23<1:06:55, 18.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4985/5198 [28:19:42<1:06:06, 18.62s/it]                                                        {'loss': 0.8329, 'learning_rate': 8.793984895473117e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4985/5198 [28:19:42<1:06:06, 18.62s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4986/5198 [28:19:58<1:03:52, 18.08s/it]                                                        {'loss': 0.3015, 'learning_rate': 8.711725919966718e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4986/5198 [28:19:58<1:03:52, 18.08s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4987/5198 [28:20:16<1:03:25, 18.03s/it]                                                        {'loss': 0.8271, 'learning_rate': 8.629851796699284e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4987/5198 [28:20:16<1:03:25, 18.03s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4988/5198 [28:20:34<1:03:11, 18.05s/it]                                                        {'loss': 0.7085, 'learning_rate': 8.54836255745728e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4988/5198 [28:20:34<1:03:11, 18.05s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4989/5198 [28:20:53<1:03:02, 18.10s/it]                                                        {'loss': 0.7733, 'learning_rate': 8.467258233877728e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4989/5198 [28:20:53<1:03:02, 18.10s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4990/5198 [28:21:11<1:03:22, 18.28s/it]                                                        {'loss': 0.8025, 'learning_rate': 8.386538857447779e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4990/5198 [28:21:11<1:03:22, 18.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4991/5198 [28:21:29<1:02:01, 17.98s/it]                                                        {'loss': 0.772, 'learning_rate': 8.306204459505807e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4991/5198 [28:21:29<1:02:01, 17.98s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4992/5198 [28:21:45<1:00:10, 17.53s/it]                                                        {'loss': 0.7836, 'learning_rate': 8.226255071240308e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4992/5198 [28:21:45<1:00:10, 17.53s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4993/5198 [28:22:03<1:00:00, 17.57s/it]                                                        {'loss': 0.8007, 'learning_rate': 8.146690723690342e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4993/5198 [28:22:03<1:00:00, 17.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4994/5198 [28:22:20<59:38, 17.54s/it]                                                        {'loss': 0.7462, 'learning_rate': 8.067511447745535e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4994/5198 [28:22:20<59:38, 17.54s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4995/5198 [28:22:38<59:07, 17.48s/it]                                                      {'loss': 0.352, 'learning_rate': 7.988717274146074e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4995/5198 [28:22:38<59:07, 17.48s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4996/5198 [28:22:56<59:30, 17.67s/it]                                                      {'loss': 0.7837, 'learning_rate': 7.910308233482488e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4996/5198 [28:22:56<59:30, 17.67s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4997/5198 [28:23:13<58:40, 17.52s/it]                                                      {'loss': 0.7748, 'learning_rate': 7.832284356195764e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4997/5198 [28:23:13<58:40, 17.52s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4998/5198 [28:23:30<58:15, 17.48s/it]                                                      {'loss': 0.8032, 'learning_rate': 7.754645672577776e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4998/5198 [28:23:30<58:15, 17.48s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4999/5198 [28:23:48<58:14, 17.56s/it]                                                      {'loss': 0.7835, 'learning_rate': 7.677392212770196e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4999/5198 [28:23:48<58:14, 17.56s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5000/5198 [28:24:06<58:48, 17.82s/it]                                                      {'loss': 0.7571, 'learning_rate': 7.600524006765808e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5000/5198 [28:24:06<58:48, 17.82s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5001/5198 [28:25:34<2:07:18, 38.78s/it]                                                        {'loss': 0.8112, 'learning_rate': 7.524041084407185e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5001/5198 [28:25:34<2:07:18, 38.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5002/5198 [28:25:52<1:46:18, 32.54s/it]                                                        {'loss': 0.7474, 'learning_rate': 7.447943475387797e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5002/5198 [28:25:52<1:46:18, 32.54s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5003/5198 [28:26:10<1:31:54, 28.28s/it]                                                        {'loss': 0.7347, 'learning_rate': 7.372231209251346e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5003/5198 [28:26:10<1:31:54, 28.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5004/5198 [28:26:29<1:21:54, 25.33s/it]                                                        {'loss': 0.7913, 'learning_rate': 7.296904315391873e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5004/5198 [28:26:29<1:21:54, 25.33s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5005/5198 [28:26:46<1:13:58, 23.00s/it]                                                        {'loss': 0.7843, 'learning_rate': 7.221962823053874e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5005/5198 [28:26:46<1:13:58, 23.00s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5006/5198 [28:27:05<1:09:12, 21.63s/it]                                                        {'loss': 0.7852, 'learning_rate': 7.147406761332298e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5006/5198 [28:27:05<1:09:12, 21.63s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5007/5198 [28:27:22<1:04:56, 20.40s/it]                                                        {'loss': 0.764, 'learning_rate': 7.073236159172325e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5007/5198 [28:27:22<1:04:56, 20.40s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5008/5198 [28:27:40<1:02:06, 19.61s/it]                                                        {'loss': 0.8203, 'learning_rate': 6.999451045369587e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5008/5198 [28:27:40<1:02:06, 19.61s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5009/5198 [28:27:58<1:00:05, 19.07s/it]                                                        {'loss': 0.744, 'learning_rate': 6.926051448569948e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5009/5198 [28:27:58<1:00:05, 19.07s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5010/5198 [28:28:15<57:56, 18.49s/it]                                                        {'loss': 0.8957, 'learning_rate': 6.853037397269724e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5010/5198 [28:28:15<57:56, 18.49s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5011/5198 [28:28:33<56:49, 18.23s/it]                                                      {'loss': 0.7949, 'learning_rate': 6.78040891981524e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5011/5198 [28:28:33<56:49, 18.23s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5012/5198 [28:28:50<55:48, 18.00s/it]                                                      {'loss': 0.7786, 'learning_rate': 6.70816604440383e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5012/5198 [28:28:50<55:48, 18.00s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5013/5198 [28:29:07<54:46, 17.77s/it]                                                      {'loss': 0.7937, 'learning_rate': 6.63630879908217e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5013/5198 [28:29:07<54:46, 17.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5014/5198 [28:29:25<54:14, 17.69s/it]                                                      {'loss': 0.3198, 'learning_rate': 6.564837211748054e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5014/5198 [28:29:25<54:14, 17.69s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5015/5198 [28:29:43<53:54, 17.68s/it]                                                      {'loss': 0.7923, 'learning_rate': 6.493751310149177e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5015/5198 [28:29:43<53:54, 17.68s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5016/5198 [28:30:00<53:00, 17.48s/it]                                                      {'loss': 0.7655, 'learning_rate': 6.42305112188335e-08, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5016/5198 [28:30:00<53:00, 17.48s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5017/5198 [28:30:17<52:46, 17.49s/it]                                                      {'loss': 0.8579, 'learning_rate': 6.352736674398951e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5017/5198 [28:30:17<52:46, 17.49s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5018/5198 [28:30:36<53:37, 17.88s/it]                                                      {'loss': 0.7643, 'learning_rate': 6.282807994994477e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5018/5198 [28:30:36<53:37, 17.88s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5019/5198 [28:30:53<52:35, 17.63s/it]                                                      {'loss': 0.762, 'learning_rate': 6.213265110818656e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5019/5198 [28:30:53<52:35, 17.63s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5020/5198 [28:31:11<52:57, 17.85s/it]                                                      {'loss': 0.7543, 'learning_rate': 6.144108048870335e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5020/5198 [28:31:11<52:57, 17.85s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5021/5198 [28:31:30<53:18, 18.07s/it]                                                      {'loss': 0.7925, 'learning_rate': 6.075336835998813e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5021/5198 [28:31:30<53:18, 18.07s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5022/5198 [28:31:48<52:35, 17.93s/it]                                                      {'loss': 0.7783, 'learning_rate': 6.00695149890329e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5022/5198 [28:31:48<52:35, 17.93s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5023/5198 [28:32:06<52:33, 18.02s/it]                                                      {'loss': 0.7559, 'learning_rate': 5.938952064133419e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5023/5198 [28:32:06<52:33, 18.02s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5024/5198 [28:32:24<52:21, 18.06s/it]                                                      {'loss': 0.766, 'learning_rate': 5.871338558088857e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5024/5198 [28:32:24<52:21, 18.06s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5025/5198 [28:32:41<51:37, 17.90s/it]                                                      {'loss': 0.8254, 'learning_rate': 5.8041110070194976e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5025/5198 [28:32:41<51:37, 17.90s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5026/5198 [28:34:10<1:52:19, 39.18s/it]                                                        {'loss': 0.8033, 'learning_rate': 5.7372694370254614e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5026/5198 [28:34:10<1:52:19, 39.18s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5027/5198 [28:34:28<1:33:15, 32.72s/it]                                                        {'loss': 0.7939, 'learning_rate': 5.67081387405688e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5027/5198 [28:34:28<1:33:15, 32.72s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5028/5198 [28:34:46<1:20:16, 28.33s/it]                                                        {'loss': 0.747, 'learning_rate': 5.6047443439141146e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5028/5198 [28:34:46<1:20:16, 28.33s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5029/5198 [28:35:04<1:10:50, 25.15s/it]                                                        {'loss': 0.7964, 'learning_rate': 5.539060872247537e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5029/5198 [28:35:04<1:10:50, 25.15s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5030/5198 [28:35:22<1:04:47, 23.14s/it]                                                        {'loss': 0.7393, 'learning_rate': 5.47376348455797e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5030/5198 [28:35:22<1:04:47, 23.14s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5031/5198 [28:35:39<59:24, 21.35s/it]                                                        {'loss': 0.7801, 'learning_rate': 5.408852206195914e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5031/5198 [28:35:39<59:24, 21.35s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5032/5198 [28:35:56<55:00, 19.88s/it]                                                      {'loss': 0.2965, 'learning_rate': 5.344327062362098e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5032/5198 [28:35:56<55:00, 19.88s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5033/5198 [28:36:13<52:49, 19.21s/it]                                                      {'loss': 0.7852, 'learning_rate': 5.2801880781075954e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5033/5198 [28:36:13<52:49, 19.21s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5034/5198 [28:36:32<51:40, 18.91s/it]                                                      {'loss': 0.7541, 'learning_rate': 5.216435278333376e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5034/5198 [28:36:32<51:40, 18.91s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5035/5198 [28:36:50<51:08, 18.83s/it]                                                      {'loss': 0.7803, 'learning_rate': 5.153068687790197e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5035/5198 [28:36:50<51:08, 18.83s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5036/5198 [28:37:08<50:14, 18.61s/it]                                                      {'loss': 0.7801, 'learning_rate': 5.0900883310794903e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5036/5198 [28:37:08<50:14, 18.61s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5037/5198 [28:37:27<50:10, 18.70s/it]                                                      {'loss': 0.7875, 'learning_rate': 5.0274942326521414e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5037/5198 [28:37:27<50:10, 18.70s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5038/5198 [28:37:45<49:11, 18.44s/it]                                                      {'loss': 0.7457, 'learning_rate': 4.9652864168096e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5038/5198 [28:37:45<49:11, 18.44s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5039/5198 [28:38:03<48:28, 18.29s/it]                                                      {'loss': 0.7546, 'learning_rate': 4.9034649077027706e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5039/5198 [28:38:03<48:28, 18.29s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5040/5198 [28:38:21<48:06, 18.27s/it]                                                      {'loss': 0.7848, 'learning_rate': 4.84202972933312e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5040/5198 [28:38:21<48:06, 18.27s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5041/5198 [28:38:39<47:35, 18.19s/it]                                                      {'loss': 0.7835, 'learning_rate': 4.7809809055517906e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5041/5198 [28:38:39<47:35, 18.19s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5042/5198 [28:38:57<47:01, 18.08s/it]                                                      {'loss': 0.7827, 'learning_rate': 4.720318460060047e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5042/5198 [28:38:57<47:01, 18.08s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5043/5198 [28:39:15<46:21, 17.94s/it]                                                      {'loss': 0.8038, 'learning_rate': 4.6600424164091606e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5043/5198 [28:39:15<46:21, 17.94s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5044/5198 [28:39:32<45:24, 17.69s/it]                                                      {'loss': 0.7825, 'learning_rate': 4.6001527980004125e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5044/5198 [28:39:32<45:24, 17.69s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5045/5198 [28:39:50<45:08, 17.70s/it]                                                      {'loss': 0.8284, 'learning_rate': 4.54064962808487e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5045/5198 [28:39:50<45:08, 17.70s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5046/5198 [28:40:09<45:52, 18.11s/it]                                                      {'loss': 0.8064, 'learning_rate': 4.4815329297639434e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5046/5198 [28:40:09<45:52, 18.11s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5047/5198 [28:40:26<45:15, 17.98s/it]                                                      {'loss': 0.8052, 'learning_rate': 4.422802725988606e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5047/5198 [28:40:26<45:15, 17.98s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5048/5198 [28:40:45<45:11, 18.07s/it]                                                      {'loss': 0.7678, 'learning_rate': 4.364459039559843e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5048/5198 [28:40:45<45:11, 18.07s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5049/5198 [28:41:02<44:35, 17.96s/it]                                                      {'loss': 0.7608, 'learning_rate': 4.3065018931289784e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5049/5198 [28:41:02<44:35, 17.96s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5050/5198 [28:41:21<44:45, 18.15s/it]                                                      {'loss': 0.8054, 'learning_rate': 4.248931309196791e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5050/5198 [28:41:21<44:45, 18.15s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5051/5198 [28:42:46<1:33:42, 38.25s/it]                                                        {'loss': 0.7569, 'learning_rate': 4.1917473101140696e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5051/5198 [28:42:46<1:33:42, 38.25s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5052/5198 [28:43:04<1:18:21, 32.20s/it]                                                        {'loss': 0.825, 'learning_rate': 4.134949918081832e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5052/5198 [28:43:04<1:18:21, 32.20s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5053/5198 [28:43:21<1:06:34, 27.55s/it]                                                        {'loss': 0.7855, 'learning_rate': 4.0785391551506626e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5053/5198 [28:43:21<1:06:34, 27.55s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5054/5198 [28:43:38<58:56, 24.56s/it]                                                        {'loss': 0.8014, 'learning_rate': 4.022515043221154e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5054/5198 [28:43:38<58:56, 24.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5055/5198 [28:43:56<53:44, 22.55s/it]                                                      {'loss': 0.7522, 'learning_rate': 3.966877604043795e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5055/5198 [28:43:56<53:44, 22.55s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5056/5198 [28:44:13<49:20, 20.85s/it]                                                      {'loss': 0.7822, 'learning_rate': 3.9116268592189755e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5056/5198 [28:44:13<49:20, 20.85s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5057/5198 [28:44:31<46:54, 19.96s/it]                                                      {'loss': 0.8081, 'learning_rate': 3.8567628301969806e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5057/5198 [28:44:31<46:54, 19.96s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5058/5198 [28:44:49<44:59, 19.28s/it]                                                      {'loss': 0.7891, 'learning_rate': 3.802285538277772e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5058/5198 [28:44:49<44:59, 19.28s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5059/5198 [28:45:07<43:53, 18.94s/it]                                                      {'loss': 0.7574, 'learning_rate': 3.748195004611543e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5059/5198 [28:45:07<43:53, 18.94s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5060/5198 [28:45:26<43:41, 18.99s/it]                                                      {'loss': 0.7236, 'learning_rate': 3.69449125019794e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5060/5198 [28:45:26<43:41, 18.99s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5061/5198 [28:45:45<43:14, 18.94s/it]                                                      {'loss': 0.7775, 'learning_rate': 3.6411742958866184e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5061/5198 [28:45:45<43:14, 18.94s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5062/5198 [28:46:02<41:31, 18.32s/it]                                                      {'loss': 0.843, 'learning_rate': 3.588244162377019e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5062/5198 [28:46:02<41:31, 18.32s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5063/5198 [28:46:19<40:34, 18.03s/it]                                                      {'loss': 0.8084, 'learning_rate': 3.5357008702185945e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5063/5198 [28:46:19<40:34, 18.03s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5064/5198 [28:46:37<40:00, 17.92s/it]                                                      {'loss': 0.792, 'learning_rate': 3.483544439810249e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5064/5198 [28:46:37<40:00, 17.92s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5065/5198 [28:46:54<39:22, 17.77s/it]                                                      {'loss': 0.7588, 'learning_rate': 3.4317748914011187e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5065/5198 [28:46:54<39:22, 17.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5066/5198 [28:47:11<38:19, 17.42s/it]                                                      {'loss': 0.8222, 'learning_rate': 3.3803922450897917e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5066/5198 [28:47:11<38:19, 17.42s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5067/5198 [28:47:29<38:21, 17.57s/it]                                                      {'loss': 0.7494, 'learning_rate': 3.329396520824757e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5067/5198 [28:47:29<38:21, 17.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5068/5198 [28:47:45<37:29, 17.30s/it]                                                      {'loss': 0.7973, 'learning_rate': 3.2787877384045095e-08, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5068/5198 [28:47:45<37:29, 17.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5069/5198 [28:48:03<37:40, 17.52s/it]                                                      {'loss': 0.8313, 'learning_rate': 3.228565917476889e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5069/5198 [28:48:03<37:40, 17.52s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5070/5198 [28:48:21<37:17, 17.48s/it]                                                      {'loss': 0.8081, 'learning_rate': 3.178731077539743e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5070/5198 [28:48:21<37:17, 17.48s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5071/5198 [28:48:39<37:37, 17.78s/it]                                                      {'loss': 0.774, 'learning_rate': 3.129283237940928e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5071/5198 [28:48:39<37:37, 17.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5072/5198 [28:48:56<36:55, 17.58s/it]                                                      {'loss': 0.815, 'learning_rate': 3.080222417877421e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5072/5198 [28:48:56<36:55, 17.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5073/5198 [28:49:15<37:05, 17.80s/it]                                                      {'loss': 0.7994, 'learning_rate': 3.031548636396764e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5073/5198 [28:49:15<37:05, 17.80s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5074/5198 [28:49:32<36:25, 17.63s/it]                                                      {'loss': 0.766, 'learning_rate': 2.983261912395397e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5074/5198 [28:49:32<36:25, 17.63s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5075/5198 [28:49:50<36:28, 17.80s/it]                                                      {'loss': 0.7973, 'learning_rate': 2.9353622646199898e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5075/5198 [28:49:50<36:28, 17.80s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5076/5198 [28:51:16<1:17:43, 38.23s/it]                                                        {'loss': 0.7763, 'learning_rate': 2.8878497116671124e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5076/5198 [28:51:16<1:17:43, 38.23s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5077/5198 [28:51:34<1:04:43, 32.09s/it]                                                        {'loss': 0.3479, 'learning_rate': 2.8407242719823424e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5077/5198 [28:51:34<1:04:43, 32.09s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5078/5198 [28:51:51<55:33, 27.78s/it]                                                        {'loss': 0.8026, 'learning_rate': 2.7939859638617118e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5078/5198 [28:51:51<55:33, 27.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5079/5198 [28:52:08<48:32, 24.47s/it]                                                      {'loss': 0.7932, 'learning_rate': 2.7476348054504832e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5079/5198 [28:52:08<48:32, 24.47s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5080/5198 [28:52:27<44:33, 22.66s/it]                                                      {'loss': 0.762, 'learning_rate': 2.7016708147439285e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5080/5198 [28:52:27<44:33, 22.66s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5081/5198 [28:52:45<41:33, 21.31s/it]                                                      {'loss': 0.7939, 'learning_rate': 2.6560940095866626e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5081/5198 [28:52:45<41:33, 21.31s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5082/5198 [28:53:03<39:08, 20.24s/it]                                                      {'loss': 0.7695, 'learning_rate': 2.6109044076733092e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5082/5198 [28:53:03<39:08, 20.24s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5083/5198 [28:53:20<37:17, 19.46s/it]                                                      {'loss': 0.8231, 'learning_rate': 2.5661020265479452e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5083/5198 [28:53:20<37:17, 19.46s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5084/5198 [28:53:37<35:27, 18.66s/it]                                                      {'loss': 0.8295, 'learning_rate': 2.5216868836043242e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5084/5198 [28:53:37<35:27, 18.66s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5085/5198 [28:53:54<34:10, 18.14s/it]                                                      {'loss': 0.8357, 'learning_rate': 2.4776589960862074e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5085/5198 [28:53:54<34:10, 18.14s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5086/5198 [28:54:12<33:34, 17.99s/it]                                                      {'loss': 0.7393, 'learning_rate': 2.434018381086589e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5086/5198 [28:54:12<33:34, 17.99s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5087/5198 [28:54:30<33:32, 18.13s/it]                                                      {'loss': 0.7483, 'learning_rate': 2.3907650555481387e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5087/5198 [28:54:30<33:32, 18.13s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5088/5198 [28:54:48<33:11, 18.10s/it]                                                      {'loss': 0.73, 'learning_rate': 2.3478990362634235e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5088/5198 [28:54:48<33:11, 18.10s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5089/5198 [28:55:06<32:57, 18.14s/it]                                                      {'loss': 0.7998, 'learning_rate': 2.3054203398743537e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5089/5198 [28:55:06<32:57, 18.14s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5090/5198 [28:55:23<31:56, 17.75s/it]                                                      {'loss': 0.7761, 'learning_rate': 2.263328982872959e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5090/5198 [28:55:23<31:56, 17.75s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5091/5198 [28:55:42<32:03, 17.97s/it]                                                      {'loss': 0.7681, 'learning_rate': 2.221624981600168e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5091/5198 [28:55:42<32:03, 17.97s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5092/5198 [28:56:00<31:56, 18.08s/it]                                                      {'loss': 0.737, 'learning_rate': 2.1803083522471402e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5092/5198 [28:56:00<31:56, 18.08s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5093/5198 [28:56:18<31:46, 18.16s/it]                                                      {'loss': 0.7595, 'learning_rate': 2.1393791108542672e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5093/5198 [28:56:18<31:46, 18.16s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5094/5198 [28:56:36<30:58, 17.87s/it]                                                      {'loss': 0.7989, 'learning_rate': 2.098837273311838e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5094/5198 [28:56:36<30:58, 17.87s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5095/5198 [28:56:53<30:35, 17.82s/it]                                                      {'loss': 0.7703, 'learning_rate': 2.058682855359595e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5095/5198 [28:56:53<30:35, 17.82s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5096/5198 [28:57:11<30:10, 17.75s/it]                                                      {'loss': 0.7471, 'learning_rate': 2.0189158725867353e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5096/5198 [28:57:11<30:10, 17.75s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5097/5198 [28:57:27<29:03, 17.26s/it]                                                      {'loss': 0.844, 'learning_rate': 1.979536340432131e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5097/5198 [28:57:27<29:03, 17.26s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5098/5198 [28:57:45<29:03, 17.43s/it]                                                      {'loss': 0.819, 'learning_rate': 1.9405442741844415e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5098/5198 [28:57:45<29:03, 17.43s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5099/5198 [28:58:03<29:05, 17.63s/it]                                                      {'loss': 0.7832, 'learning_rate': 1.9019396889816688e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5099/5198 [28:58:03<29:05, 17.63s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5100/5198 [28:58:20<28:47, 17.63s/it]                                                      {'loss': 0.8034, 'learning_rate': 1.8637225998114904e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5100/5198 [28:58:20<28:47, 17.63s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5101/5198 [28:59:49<1:02:58, 38.95s/it]                                                        {'loss': 0.7868, 'learning_rate': 1.825893021510927e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5101/5198 [28:59:49<1:02:58, 38.95s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5102/5198 [29:00:07<52:21, 32.72s/it]                                                        {'loss': 0.81, 'learning_rate': 1.7884509687668972e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5102/5198 [29:00:07<52:21, 32.72s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5103/5198 [29:00:25<44:39, 28.20s/it]                                                      {'loss': 0.8196, 'learning_rate': 1.7513964561156617e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5103/5198 [29:00:25<44:39, 28.20s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5104/5198 [29:00:41<38:33, 24.61s/it]                                                      {'loss': 0.8728, 'learning_rate': 1.714729497942935e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5104/5198 [29:00:41<38:33, 24.61s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5105/5198 [29:01:00<35:16, 22.76s/it]                                                      {'loss': 0.7741, 'learning_rate': 1.6784501084843307e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5105/5198 [29:01:00<35:16, 22.76s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5106/5198 [29:01:18<32:42, 21.33s/it]                                                      {'loss': 0.7574, 'learning_rate': 1.6425583018244706e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5106/5198 [29:01:18<32:42, 21.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5107/5198 [29:01:36<30:47, 20.30s/it]                                                      {'loss': 0.8281, 'learning_rate': 1.607054091897986e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5107/5198 [29:01:36<30:47, 20.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5108/5198 [29:01:53<29:06, 19.41s/it]                                                      {'loss': 0.7313, 'learning_rate': 1.57193749248874e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5108/5198 [29:01:53<29:06, 19.41s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5109/5198 [29:02:10<27:44, 18.71s/it]                                                      {'loss': 0.799, 'learning_rate': 1.537208517230271e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5109/5198 [29:02:10<27:44, 18.71s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5110/5198 [29:02:27<26:52, 18.32s/it]                                                      {'loss': 0.7398, 'learning_rate': 1.5028671796055715e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5110/5198 [29:02:27<26:52, 18.32s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5111/5198 [29:02:45<26:13, 18.09s/it]                                                      {'loss': 0.8626, 'learning_rate': 1.4689134929470884e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5111/5198 [29:02:45<26:13, 18.09s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5112/5198 [29:03:02<25:36, 17.87s/it]                                                      {'loss': 0.7928, 'learning_rate': 1.435347470436832e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5112/5198 [29:03:02<25:36, 17.87s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5113/5198 [29:03:20<25:04, 17.69s/it]                                                      {'loss': 0.7299, 'learning_rate': 1.4021691251062675e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5113/5198 [29:03:20<25:04, 17.69s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5114/5198 [29:03:37<24:28, 17.49s/it]                                                      {'loss': 0.7794, 'learning_rate': 1.3693784698363133e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5114/5198 [29:03:37<24:28, 17.49s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5115/5198 [29:03:54<24:07, 17.43s/it]                                                      {'loss': 0.3596, 'learning_rate': 1.3369755173575639e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5115/5198 [29:03:54<24:07, 17.43s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5116/5198 [29:04:12<23:54, 17.49s/it]                                                      {'loss': 0.7908, 'learning_rate': 1.3049602802498451e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5116/5198 [29:04:12<23:54, 17.49s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5117/5198 [29:04:29<23:28, 17.39s/it]                                                      {'loss': 0.7651, 'learning_rate': 1.273332770942659e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5117/5198 [29:04:29<23:28, 17.39s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5118/5198 [29:04:46<23:06, 17.33s/it]                                                      {'loss': 0.7552, 'learning_rate': 1.2420930017148503e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5118/5198 [29:04:46<23:06, 17.33s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5119/5198 [29:05:04<23:12, 17.63s/it]                                                      {'loss': 0.7757, 'learning_rate': 1.2112409846947171e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5119/5198 [29:05:04<23:12, 17.63s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5120/5198 [29:05:23<23:11, 17.84s/it]                                                      {'loss': 0.8347, 'learning_rate': 1.1807767318602337e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5120/5198 [29:05:23<23:11, 17.84s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5121/5198 [29:05:40<22:42, 17.70s/it]                                                      {'loss': 0.7393, 'learning_rate': 1.150700255038606e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5121/5198 [29:05:40<22:42, 17.70s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5122/5198 [29:05:57<22:16, 17.58s/it]                                                      {'loss': 0.7602, 'learning_rate': 1.1210115659063825e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5122/5198 [29:05:57<22:16, 17.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5123/5198 [29:06:15<22:09, 17.73s/it]                                                      {'loss': 0.7501, 'learning_rate': 1.0917106759900097e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5123/5198 [29:06:15<22:09, 17.73s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5124/5198 [29:06:34<22:04, 17.89s/it]                                                      {'loss': 0.7219, 'learning_rate': 1.0627975966649439e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5124/5198 [29:06:34<22:04, 17.89s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5125/5198 [29:06:52<21:47, 17.91s/it]                                                      {'loss': 0.7962, 'learning_rate': 1.034272339156206e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5125/5198 [29:06:52<21:47, 17.91s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5126/5198 [29:08:24<48:25, 40.35s/it]                                                      {'loss': 0.824, 'learning_rate': 1.0061349145383814e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5126/5198 [29:08:24<48:25, 40.35s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5127/5198 [29:08:42<39:49, 33.66s/it]                                                      {'loss': 0.7976, 'learning_rate': 9.783853337353987e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5127/5198 [29:08:42<39:49, 33.66s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5128/5198 [29:08:59<33:24, 28.63s/it]                                                      {'loss': 0.8207, 'learning_rate': 9.510236075205292e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5128/5198 [29:08:59<33:24, 28.63s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5129/5198 [29:09:17<29:12, 25.40s/it]                                                      {'loss': 0.7875, 'learning_rate': 9.240497465164978e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5129/5198 [29:09:17<29:12, 25.40s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5130/5198 [29:09:35<26:06, 23.03s/it]                                                      {'loss': 0.7491, 'learning_rate': 8.974637611955939e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5130/5198 [29:09:35<26:06, 23.03s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5131/5198 [29:09:52<23:55, 21.42s/it]                                                      {'loss': 0.7962, 'learning_rate': 8.712656618793391e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5131/5198 [29:09:52<23:55, 21.42s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5132/5198 [29:10:10<22:22, 20.33s/it]                                                      {'loss': 0.7857, 'learning_rate': 8.454554587388198e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5132/5198 [29:10:10<22:22, 20.33s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5133/5198 [29:10:29<21:35, 19.94s/it]                                                      {'loss': 0.8099, 'learning_rate': 8.200331617943535e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5133/5198 [29:10:29<21:35, 19.94s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5134/5198 [29:10:47<20:31, 19.24s/it]                                                      {'loss': 0.7722, 'learning_rate': 7.949987809158232e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5134/5198 [29:10:47<20:31, 19.24s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5135/5198 [29:11:04<19:43, 18.78s/it]                                                      {'loss': 0.8374, 'learning_rate': 7.703523258223433e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5135/5198 [29:11:04<19:43, 18.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5136/5198 [29:11:22<18:54, 18.29s/it]                                                      {'loss': 0.8419, 'learning_rate': 7.460938060825929e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5136/5198 [29:11:22<18:54, 18.29s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5137/5198 [29:11:39<18:16, 17.98s/it]                                                      {'loss': 0.7615, 'learning_rate': 7.222232311145938e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5137/5198 [29:11:39<18:16, 17.98s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5138/5198 [29:11:56<17:49, 17.83s/it]                                                      {'loss': 0.7794, 'learning_rate': 6.987406101855998e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5138/5198 [29:11:56<17:49, 17.83s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5139/5198 [29:12:13<17:07, 17.42s/it]                                                      {'loss': 0.8798, 'learning_rate': 6.756459524125403e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5139/5198 [29:12:13<17:07, 17.42s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5140/5198 [29:12:30<16:56, 17.52s/it]                                                      {'loss': 0.7521, 'learning_rate': 6.5293926676135434e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5140/5198 [29:12:30<16:56, 17.52s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5141/5198 [29:12:49<16:54, 17.79s/it]                                                      {'loss': 0.7695, 'learning_rate': 6.306205620477679e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5141/5198 [29:12:49<16:54, 17.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5142/5198 [29:13:08<16:52, 18.08s/it]                                                      {'loss': 0.8131, 'learning_rate': 6.086898469365166e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5142/5198 [29:13:08<16:52, 18.08s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5143/5198 [29:13:26<16:38, 18.15s/it]                                                      {'loss': 0.8027, 'learning_rate': 5.871471299419007e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5143/5198 [29:13:26<16:38, 18.15s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5144/5198 [29:13:43<16:04, 17.87s/it]                                                      {'loss': 0.7972, 'learning_rate': 5.6599241942767445e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5144/5198 [29:13:43<16:04, 17.87s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5145/5198 [29:14:00<15:38, 17.70s/it]                                                      {'loss': 0.8062, 'learning_rate': 5.452257236066017e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5145/5198 [29:14:00<15:38, 17.70s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5146/5198 [29:14:18<15:13, 17.58s/it]                                                      {'loss': 0.7327, 'learning_rate': 5.248470505412328e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5146/5198 [29:14:18<15:13, 17.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5147/5198 [29:14:35<14:57, 17.60s/it]                                                      {'loss': 0.7934, 'learning_rate': 5.0485640814312844e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5147/5198 [29:14:35<14:57, 17.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5148/5198 [29:14:52<14:27, 17.34s/it]                                                      {'loss': 0.7581, 'learning_rate': 4.8525380417330234e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5148/5198 [29:14:52<14:27, 17.34s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5149/5198 [29:15:10<14:16, 17.48s/it]                                                      {'loss': 0.7857, 'learning_rate': 4.660392462424446e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5149/5198 [29:15:10<14:16, 17.48s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5150/5198 [29:15:28<14:08, 17.67s/it]                                                      {'loss': 0.8001, 'learning_rate': 4.472127418099215e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5150/5198 [29:15:28<14:08, 17.67s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5151/5198 [29:16:54<29:48, 38.04s/it]                                                      {'loss': 0.7067, 'learning_rate': 4.287742981851084e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5151/5198 [29:16:54<29:48, 38.04s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5152/5198 [29:17:12<24:38, 32.14s/it]                                                      {'loss': 0.7147, 'learning_rate': 4.1072392252639034e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5152/5198 [29:17:12<24:38, 32.14s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5153/5198 [29:17:30<20:56, 27.93s/it]                                                      {'loss': 0.8041, 'learning_rate': 3.930616218414951e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5153/5198 [29:17:30<20:56, 27.93s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5154/5198 [29:17:49<18:23, 25.08s/it]                                                      {'loss': 0.7868, 'learning_rate': 3.757874029874931e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5154/5198 [29:17:49<18:23, 25.08s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5155/5198 [29:18:06<16:24, 22.90s/it]                                                      {'loss': 0.7847, 'learning_rate': 3.5890127267090844e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5155/5198 [29:18:06<16:24, 22.90s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5156/5198 [29:18:24<14:49, 21.19s/it]                                                      {'loss': 0.8097, 'learning_rate': 3.424032374476083e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5156/5198 [29:18:24<14:49, 21.19s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5157/5198 [29:18:41<13:46, 20.15s/it]                                                      {'loss': 0.7248, 'learning_rate': 3.2629330372246915e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5157/5198 [29:18:41<13:46, 20.15s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5158/5198 [29:18:59<12:57, 19.43s/it]                                                      {'loss': 0.7418, 'learning_rate': 3.105714777501545e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5158/5198 [29:18:59<12:57, 19.43s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5159/5198 [29:19:17<12:16, 18.89s/it]                                                      {'loss': 0.7897, 'learning_rate': 2.9523776563422644e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5159/5198 [29:19:17<12:16, 18.89s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5160/5198 [29:19:34<11:41, 18.45s/it]                                                      {'loss': 0.7716, 'learning_rate': 2.802921733278119e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5160/5198 [29:19:34<11:41, 18.45s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5161/5198 [29:19:51<11:06, 18.01s/it]                                                      {'loss': 0.6868, 'learning_rate': 2.657347066333804e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5161/5198 [29:19:51<11:06, 18.01s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5162/5198 [29:20:09<10:48, 18.02s/it]                                                      {'loss': 0.7928, 'learning_rate': 2.5156537120263335e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5162/5198 [29:20:09<10:48, 18.02s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5163/5198 [29:20:27<10:29, 17.99s/it]                                                      {'loss': 0.8139, 'learning_rate': 2.3778417253650376e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5163/5198 [29:20:27<10:29, 17.99s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5164/5198 [29:20:44<09:58, 17.61s/it]                                                      {'loss': 0.3294, 'learning_rate': 2.2439111598537844e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5164/5198 [29:20:44<09:58, 17.61s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5165/5198 [29:21:01<09:36, 17.47s/it]                                                      {'loss': 0.7663, 'learning_rate': 2.113862067488759e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5165/5198 [29:21:01<09:36, 17.47s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5166/5198 [29:21:17<09:08, 17.13s/it]                                                      {'loss': 0.8368, 'learning_rate': 1.987694498760684e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5166/5198 [29:21:17<09:08, 17.13s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5167/5198 [29:21:35<08:55, 17.27s/it]                                                      {'loss': 0.8162, 'learning_rate': 1.865408502650379e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5167/5198 [29:21:35<08:55, 17.27s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5168/5198 [29:21:53<08:43, 17.44s/it]                                                      {'loss': 0.7433, 'learning_rate': 1.747004126635421e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5168/5198 [29:21:53<08:43, 17.44s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5169/5198 [29:22:10<08:25, 17.44s/it]                                                      {'loss': 0.7503, 'learning_rate': 1.6324814166823744e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5169/5198 [29:22:10<08:25, 17.44s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5170/5198 [29:22:28<08:12, 17.58s/it]                                                      {'loss': 0.759, 'learning_rate': 1.5218404172545609e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5170/5198 [29:22:28<08:12, 17.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5171/5198 [29:22:46<07:55, 17.63s/it]                                                      {'loss': 0.7519, 'learning_rate': 1.415081171305399e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5171/5198 [29:22:46<07:55, 17.63s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5172/5198 [29:23:04<07:44, 17.87s/it]                                                      {'loss': 0.7638, 'learning_rate': 1.3122037202828452e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5172/5198 [29:23:04<07:44, 17.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5173/5198 [29:23:21<07:21, 17.66s/it]                                                      {'loss': 0.8046, 'learning_rate': 1.2132081041282829e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5173/5198 [29:23:21<07:21, 17.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5174/5198 [29:23:40<07:09, 17.91s/it]                                                      {'loss': 0.7588, 'learning_rate': 1.1180943612754124e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5174/5198 [29:23:40<07:09, 17.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5175/5198 [29:23:59<06:59, 18.26s/it]                                                      {'loss': 0.7653, 'learning_rate': 1.026862528649142e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5175/5198 [29:23:59<06:59, 18.26s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5176/5198 [29:25:27<14:24, 39.28s/it]                                                      {'loss': 0.7454, 'learning_rate': 9.39512641668916e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5176/5198 [29:25:27<14:24, 39.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5177/5198 [29:25:45<11:30, 32.86s/it]                                                      {'loss': 0.7766, 'learning_rate': 8.560447342487177e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5177/5198 [29:25:45<11:30, 32.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5178/5198 [29:26:03<09:26, 28.32s/it]                                                      {'loss': 0.7881, 'learning_rate': 7.764588387915161e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5178/5198 [29:26:03<09:26, 28.32s/it]WARNING: tokenization mismatch: 1 vs. 64. (ignored)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5179/5198 [29:26:21<07:58, 25.16s/it]                                                      {'loss': 0.7996, 'learning_rate': 7.007549861970387e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5179/5198 [29:26:21<07:58, 25.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5180/5198 [29:26:39<06:55, 23.09s/it]                                                      {'loss': 0.7411, 'learning_rate': 6.289332058551089e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5180/5198 [29:26:39<06:55, 23.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5181/5198 [29:26:56<06:03, 21.41s/it]                                                      {'loss': 0.8004, 'learning_rate': 5.609935256500887e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5181/5198 [29:26:56<06:03, 21.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5182/5198 [29:27:14<05:25, 20.37s/it]                                                      {'loss': 0.8172, 'learning_rate': 4.969359719586563e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5182/5198 [29:27:14<05:25, 20.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5183/5198 [29:27:32<04:53, 19.58s/it]                                                      {'loss': 0.8397, 'learning_rate': 4.3676056964869764e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5183/5198 [29:27:32<04:53, 19.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5184/5198 [29:27:49<04:23, 18.85s/it]                                                      {'loss': 0.7876, 'learning_rate': 3.804673420837457e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5184/5198 [29:27:49<04:23, 18.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5185/5198 [29:28:08<04:03, 18.74s/it]                                                      {'loss': 0.7696, 'learning_rate': 3.2805631111743064e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5185/5198 [29:28:08<04:03, 18.74s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5186/5198 [29:28:25<03:41, 18.44s/it]                                                      {'loss': 0.8417, 'learning_rate': 2.795274971001405e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5186/5198 [29:28:25<03:41, 18.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5187/5198 [29:28:43<03:21, 18.28s/it]                                                      {'loss': 0.7799, 'learning_rate': 2.3488091886902933e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5187/5198 [29:28:43<03:21, 18.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5188/5198 [29:29:00<02:58, 17.88s/it]                                                      {'loss': 0.7756, 'learning_rate': 1.941165937602296e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5188/5198 [29:29:00<02:58, 17.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5189/5198 [29:29:18<02:41, 17.94s/it]                                                      {'loss': 0.7609, 'learning_rate': 1.5723453759886042e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5189/5198 [29:29:18<02:41, 17.94s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5190/5198 [29:29:36<02:22, 17.78s/it]                                                      {'loss': 0.8254, 'learning_rate': 1.2423476470346808e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5190/5198 [29:29:36<02:22, 17.78s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5191/5198 [29:29:53<02:03, 17.62s/it]                                                      {'loss': 0.2894, 'learning_rate': 9.511728788602625e-11, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5191/5198 [29:29:53<02:03, 17.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5192/5198 [29:30:11<01:46, 17.78s/it]                                                      {'loss': 0.8058, 'learning_rate': 6.988211845082582e-11, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5192/5198 [29:30:11<01:46, 17.78s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5193/5198 [29:30:29<01:28, 17.69s/it]                                                      {'loss': 0.868, 'learning_rate': 4.852926619447473e-11, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5193/5198 [29:30:29<01:28, 17.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5194/5198 [29:30:47<01:11, 17.89s/it]                                                      {'loss': 0.8037, 'learning_rate': 3.105873940811854e-11, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5194/5198 [29:30:47<01:11, 17.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5195/5198 [29:31:04<00:53, 17.73s/it]                                                      {'loss': 0.7603, 'learning_rate': 1.7470544874109706e-11, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5195/5198 [29:31:04<00:53, 17.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5196/5198 [29:31:22<00:35, 17.67s/it]                                                      {'loss': 0.8111, 'learning_rate': 7.764687866007592e-12, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5196/5198 [29:31:22<00:35, 17.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5197/5198 [29:31:40<00:17, 17.89s/it]                                                      {'loss': 0.5565, 'learning_rate': 1.9411721552398123e-12, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5197/5198 [29:31:40<00:17, 17.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5198/5198 [29:31:59<00:00, 18.07s/it]                                                      {'loss': 0.4438, 'learning_rate': 0.0, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5198/5198 [29:31:59<00:00, 18.07s/it]                                                      {'train_runtime': 106322.8534, 'train_samples_per_second': 6.257, 'train_steps_per_second': 0.049, 'train_loss': 0.8086810814916927, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5198/5198 [29:31:59<00:00, 18.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5198/5198 [29:31:59<00:00, 20.45s/it]
[2024-02-26 18:01:13,119] [INFO] [launch.py:347:main] Process 1971582 exits successfully.
[2024-02-26 18:01:13,119] [INFO] [launch.py:347:main] Process 1971583 exits successfully.
[2024-02-26 18:01:16,122] [INFO] [launch.py:347:main] Process 1971581 exits successfully.
wandb: - 14.879 MB of 14.879 MB uploadedwandb: \ 14.879 MB of 14.879 MB uploadedwandb: | 14.879 MB of 14.879 MB uploadedwandb: / 14.879 MB of 14.879 MB uploadedwandb: - 14.879 MB of 14.879 MB uploadedwandb: \ 14.914 MB of 19.196 MB uploaded (0.002 MB deduped)wandb: | 19.196 MB of 19.196 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                     train/loss â–ˆâ–‡â–‡â–‡â–â–‡â–†â–‡â–‡â–â–‡â–†â–‡â–‡â–†â–†â–‡â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 5198
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.4438
wandb:               train/total_flos 1.4816945029382144e+16
wandb:               train/train_loss 0.80868
wandb:            train/train_runtime 106322.8534
wandb: train/train_samples_per_second 6.257
wandb:   train/train_steps_per_second 0.049
wandb: 
wandb: ðŸš€ View run dupl-glbltok-pretrained-grllava-7b-it at: https://wandb.ai/compyle/LLaVA-llava_train/runs/764y9rsm
wandb: ï¸âš¡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v17
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240225_122857-764y9rsm/logs
[2024-02-26 18:01:49,157] [INFO] [launch.py:347:main] Process 1971580 exits successfully.
