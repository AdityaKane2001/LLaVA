[2024-02-19 13:52:07,204] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:09,354] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 13:52:09,355] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 13:52:12,016] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:14,035] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 13:52:14,036] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 13:52:14,036] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 13:52:14,036] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 13:52:14,036] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 13:52:17,982] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:19,098] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:19,281] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:19,536] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 13:52:19,595] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 13:52:20,597] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 13:52:20,835] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 13:52:21,530] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 13:52:21,530] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 13:52:37,895] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.84s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.91s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.23s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.76s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.45s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.71s/it]
[2024-02-19 13:52:57,836] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_135345-4k7ln09t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/4k7ln09t

  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 17%|█▋        | 1/6 [00:42<03:31, 42.29s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

 17%|█▋        | 1/6 [00:42<03:31, 42.29s/it]
 33%|███▎      | 2/6 [01:04<02:00, 30.21s/it]
                                             
{'loss': 2.5048, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.0}

 33%|███▎      | 2/6 [01:04<02:00, 30.21s/it]
 50%|█████     | 3/6 [01:25<01:18, 26.22s/it]
                                             
{'loss': 1.5471, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.0}

 50%|█████     | 3/6 [01:25<01:18, 26.22s/it]
 67%|██████▋   | 4/6 [01:47<00:48, 24.43s/it]
                                             
{'loss': 1.354, 'learning_rate': 6.909830056250527e-06, 'epoch': 0.0}

 67%|██████▋   | 4/6 [01:47<00:48, 24.43s/it]
 83%|████████▎ | 5/6 [02:08<00:23, 23.23s/it]
                                             
{'loss': 1.2683, 'learning_rate': 1.9098300562505266e-06, 'epoch': 0.0}

 83%|████████▎ | 5/6 [02:08<00:23, 23.23s/it]
100%|██████████| 6/6 [02:29<00:00, 22.51s/it]
                                             
{'loss': 1.2272, 'learning_rate': 0.0, 'epoch': 0.0}

100%|██████████| 6/6 [02:29<00:00, 22.51s/it]
                                             
{'train_runtime': 154.8526, 'train_samples_per_second': 4.96, 'train_steps_per_second': 0.039, 'train_loss': 1.7307867606480916, 'epoch': 0.0}

100%|██████████| 6/6 [02:29<00:00, 22.51s/it]
100%|██████████| 6/6 [02:29<00:00, 24.98s/it]
[2024-02-19 13:56:35,364] [INFO] [launch.py:347:main] Process 3279906 exits successfully.
[2024-02-19 13:56:36,366] [INFO] [launch.py:347:main] Process 3279905 exits successfully.
[2024-02-19 13:56:36,366] [INFO] [launch.py:347:main] Process 3279904 exits successfully.
wandb: - 0.667 MB of 0.667 MB uploaded
wandb: \ 0.667 MB of 0.667 MB uploaded
wandb: | 0.667 MB of 0.667 MB uploaded
wandb: / 0.667 MB of 0.667 MB uploaded
wandb: - 1.330 MB of 1.344 MB uploaded (0.002 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▁▁▁
wandb:              train/global_step ▁▂▄▅▇██
wandb:            train/learning_rate █▇▆▃▂▁
wandb:                     train/loss ██▃▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 6
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.2272
wandb:               train/total_flos 15965849649152.0
wandb:               train/train_loss 1.73079
wandb:            train/train_runtime 154.8526
wandb: train/train_samples_per_second 4.96
wandb:   train/train_steps_per_second 0.039
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/4k7ln09t
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v3
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_135345-4k7ln09t/logs
[2024-02-19 13:57:12,407] [INFO] [launch.py:347:main] Process 3279903 exits successfully.
[2024-02-19 14:06:12,759] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:14,843] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 14:06:14,844] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 14:06:18,714] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:21,306] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 14:06:21,307] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 14:06:21,307] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 14:06:21,307] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 14:06:21,307] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 14:06:25,986] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:26,047] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:26,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:26,680] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:06:27,504] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:06:27,641] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:06:27,641] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 14:06:28,408] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:06:28,514] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 14:06:42,790] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.71s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.72s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.75s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.47s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.96s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(

Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.61s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.07s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(

Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  8.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.06s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-02-19 14:07:03,408] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_140750-dnw1l9p2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/dnw1l9p2

  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 17%|█▋        | 1/6 [00:41<03:27, 41.43s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

 17%|█▋        | 1/6 [00:41<03:27, 41.43s/it]
 33%|███▎      | 2/6 [01:03<01:59, 29.87s/it]
                                             
{'loss': 2.5048, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.0}

 33%|███▎      | 2/6 [01:03<01:59, 29.87s/it]
 50%|█████     | 3/6 [01:24<01:18, 26.10s/it]
                                             
{'loss': 1.5474, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.0}

 50%|█████     | 3/6 [01:24<01:18, 26.10s/it]
 67%|██████▋   | 4/6 [01:46<00:48, 24.28s/it]
                                             
{'loss': 1.3542, 'learning_rate': 6.909830056250527e-06, 'epoch': 0.0}

 67%|██████▋   | 4/6 [01:46<00:48, 24.28s/it]
 83%|████████▎ | 5/6 [02:07<00:23, 23.12s/it]
                                             
{'loss': 1.2676, 'learning_rate': 1.9098300562505266e-06, 'epoch': 0.0}

 83%|████████▎ | 5/6 [02:07<00:23, 23.12s/it]
100%|██████████| 6/6 [02:29<00:00, 22.62s/it]
                                             
{'loss': 1.2273, 'learning_rate': 0.0, 'epoch': 0.0}

100%|██████████| 6/6 [02:29<00:00, 22.62s/it]
                                             
{'train_runtime': 155.1117, 'train_samples_per_second': 4.951, 'train_steps_per_second': 0.039, 'train_loss': 1.730780343214671, 'epoch': 0.0}

100%|██████████| 6/6 [02:29<00:00, 22.62s/it]
100%|██████████| 6/6 [02:29<00:00, 24.92s/it]
 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 559, in save_pretrained
    raise ValueError(str([w.message for w in caught_warnings]))
ValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2887, in save_model
    self._save(output_dir, state_dict=state_dict)
  File "/home/akane38/LLaVA/llava/train/llava_trainer.py", line 255, in _save
    super(LLaVATrainer, self)._save(output_dir, state_dict)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2972, in _save
    self.model.save_pretrained(
  File "/home/akane38/LLaVA/transformers/src/transformers/modeling_utils.py", line 2369, in save_pretrained
    model_to_save.generation_config.save_pretrained(save_directory)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 561, in save_pretrained
    raise ValueError(
ValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.

Thrown during validation:
[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 559, in save_pretrained
    raise ValueError(str([w.message for w in caught_warnings]))
ValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 996, in train
    safe_save_model_for_hf_trainer(trainer=trainer,
  File "/home/akane38/LLaVA/llava/train/train.py", line 215, in safe_save_model_for_hf_trainer
    trainer.save_model(output_dir)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2894, in save_model
    self._save(output_dir, state_dict={})
  File "/home/akane38/LLaVA/llava/train/llava_trainer.py", line 255, in _save
    super(LLaVATrainer, self)._save(output_dir, state_dict)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2972, in _save
    self.model.save_pretrained(
  File "/home/akane38/LLaVA/transformers/src/transformers/modeling_utils.py", line 2369, in save_pretrained
    model_to_save.generation_config.save_pretrained(save_directory)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 561, in save_pretrained
    raise ValueError(
ValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.

Thrown during validation:
[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]
wandb: - 0.682 MB of 0.682 MB uploaded
wandb: \ 0.682 MB of 0.682 MB uploaded
wandb: | 0.682 MB of 0.682 MB uploaded
[2024-02-19 14:10:40,588] [INFO] [launch.py:347:main] Process 3840286 exits successfully.
wandb: / 0.682 MB of 0.682 MB uploaded
[2024-02-19 14:10:41,590] [INFO] [launch.py:347:main] Process 3840288 exits successfully.
[2024-02-19 14:10:41,590] [INFO] [launch.py:347:main] Process 3840287 exits successfully.
wandb: - 0.682 MB of 0.682 MB uploaded
wandb: \ 1.361 MB of 1.384 MB uploaded (0.027 MB deduped)
wandb: | 1.384 MB of 1.384 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▁▁▁
wandb:              train/global_step ▁▂▄▅▇██
wandb:            train/learning_rate █▇▆▃▂▁
wandb:                     train/loss ██▃▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 6
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.2273
wandb:               train/total_flos 15965849649152.0
wandb:               train/train_loss 1.73078
wandb:            train/train_runtime 155.1117
wandb: train/train_samples_per_second 4.951
wandb:   train/train_steps_per_second 0.039
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/dnw1l9p2
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v4
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_140750-dnw1l9p2/logs
[2024-02-19 14:10:54,606] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3840285
[2024-02-19 14:10:54,607] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3840286
[2024-02-19 14:10:54,607] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3840287
[2024-02-19 14:10:54,607] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3840288
[2024-02-19 14:10:54,608] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--mm_vision_use_granular_tokens', 'True', '--mm_vision_granular_select_layers', '6 12 18', '--mm_vision_granular_tokens_strategy', 'pool', '--mm_vision_granular_tokens_per_layer', '192', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data1/akane/grllava-v1.5-7b/checkpoints', '--max_steps', '6', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'grllava-7b-it-dryrun'] exits with return code = 1


[2024-02-19 14:17:49,550] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:17:51,738] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 14:17:51,738] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 1 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 14:17:55,016] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:17:57,325] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 14:17:57,325] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 14:17:57,326] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 14:17:57,326] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 14:17:57,326] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 14:18:02,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:18:02,352] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:18:02,354] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:18:02,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:18:03,845] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:18:03,847] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:18:04,140] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:18:04,338] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:18:04,338] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-02-19 14:18:21,819] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.91s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.99s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.55s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Merged granular config!

Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.42s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Merged granular config!

Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.67s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Merged granular config!

Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 10.56s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.09s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Merged granular config!
[2024-02-19 14:18:46,596] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_141943-de0p17in
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/de0p17in

  0%|          | 0/1 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 1/1 [00:43<00:00, 43.25s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

100%|██████████| 1/1 [00:43<00:00, 43.25s/it]
                                             
{'train_runtime': 48.5387, 'train_samples_per_second': 2.637, 'train_steps_per_second': 0.021, 'train_loss': 2.4834094047546387, 'epoch': 0.0}

100%|██████████| 1/1 [00:43<00:00, 43.25s/it]
100%|██████████| 1/1 [00:43<00:00, 43.77s/it]
 stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights
Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 559, in save_pretrained
    raise ValueError(str([w.message for w in caught_warnings]))
ValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2887, in save_model
    self._save(output_dir, state_dict=state_dict)
  File "/home/akane38/LLaVA/llava/train/llava_trainer.py", line 255, in _save
    super(LLaVATrainer, self)._save(output_dir, state_dict)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2972, in _save
    self.model.save_pretrained(
  File "/home/akane38/LLaVA/transformers/src/transformers/modeling_utils.py", line 2369, in save_pretrained
    model_to_save.generation_config.save_pretrained(save_directory)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 561, in save_pretrained
    raise ValueError(
ValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.

Thrown during validation:
[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 559, in save_pretrained
    raise ValueError(str([w.message for w in caught_warnings]))
ValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 996, in train
    safe_save_model_for_hf_trainer(trainer=trainer,
  File "/home/akane38/LLaVA/llava/train/train.py", line 215, in safe_save_model_for_hf_trainer
    trainer.save_model(output_dir)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2894, in save_model
    self._save(output_dir, state_dict={})
  File "/home/akane38/LLaVA/llava/train/llava_trainer.py", line 255, in _save
    super(LLaVATrainer, self)._save(output_dir, state_dict)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2972, in _save
    self.model.save_pretrained(
  File "/home/akane38/LLaVA/transformers/src/transformers/modeling_utils.py", line 2369, in save_pretrained
    model_to_save.generation_config.save_pretrained(save_directory)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py", line 561, in save_pretrained
    raise ValueError(
ValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.

Thrown during validation:
[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]
wandb: - 0.707 MB of 0.707 MB uploaded
wandb: \ 0.707 MB of 0.707 MB uploaded
wandb: | 0.707 MB of 0.707 MB uploaded
wandb: / 0.707 MB of 0.707 MB uploaded
[2024-02-19 14:20:48,548] [INFO] [launch.py:347:main] Process 126359 exits successfully.
[2024-02-19 14:20:48,549] [INFO] [launch.py:347:main] Process 126358 exits successfully.
wandb: - 0.707 MB of 0.707 MB uploaded
wandb: \ 1.410 MB of 1.410 MB uploaded (0.002 MB deduped)
[2024-02-19 14:20:49,550] [INFO] [launch.py:347:main] Process 126357 exits successfully.
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:            train/learning_rate ▁
wandb:                     train/loss ▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 1
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 2.4834
wandb:               train/total_flos 2707941228544.0
wandb:               train/train_loss 2.48341
wandb:            train/train_runtime 48.5387
wandb: train/train_samples_per_second 2.637
wandb:   train/train_steps_per_second 0.021
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/de0p17in
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v5
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_141943-de0p17in/logs
[2024-02-19 14:21:01,563] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 126356
[2024-02-19 14:21:01,563] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 126357
[2024-02-19 14:21:01,563] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 126358
[2024-02-19 14:21:01,563] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 126359
[2024-02-19 14:21:01,563] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--mm_vision_use_granular_tokens', 'True', '--mm_vision_granular_select_layers', '6 12 18', '--mm_vision_granular_tokens_strategy', 'pool', '--mm_vision_granular_tokens_per_layer', '192', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data1/akane/grllava-v1.5-7b/checkpoints', '--max_steps', '1', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'grllava-7b-it-dryrun'] exits with return code = 1
[2024-02-19 14:45:09,589] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:12,034] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 14:45:12,035] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens False --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/llava-v1.5-7b/checkpoints --max_steps 1 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 14:45:15,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:17,685] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 14:45:17,685] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 14:45:17,685] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 14:45:17,685] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 14:45:17,685] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 14:45:22,980] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:23,073] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:23,373] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:23,434] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 14:45:24,682] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:45:24,821] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:45:24,821] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 14:45:25,261] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 14:45:25,329] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 14:45:39,877] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.29s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.28s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.66s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.10s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.65s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.55s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 14:46:01,548] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_144650-nsprnzvt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/nsprnzvt

  0%|          | 0/1 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 1/1 [00:36<00:00, 36.59s/it]
                                             
{'loss': 1.3154, 'learning_rate': 2e-05, 'epoch': 0.0}

100%|██████████| 1/1 [00:36<00:00, 36.59s/it]
                                             
{'train_runtime': 42.346, 'train_samples_per_second': 3.023, 'train_steps_per_second': 0.024, 'train_loss': 1.3153908252716064, 'epoch': 0.0}

100%|██████████| 1/1 [00:37<00:00, 36.59s/it]
100%|██████████| 1/1 [00:37<00:00, 37.05s/it]
[2024-02-19 14:47:49,895] [INFO] [launch.py:347:main] Process 946182 exits successfully.
[2024-02-19 14:47:49,895] [INFO] [launch.py:347:main] Process 946181 exits successfully.
[2024-02-19 14:47:49,896] [INFO] [launch.py:347:main] Process 946183 exits successfully.
wandb: - 0.731 MB of 0.731 MB uploaded
wandb: \ 0.731 MB of 0.731 MB uploaded
wandb: | 0.731 MB of 0.731 MB uploaded
wandb: / 0.731 MB of 0.731 MB uploaded
wandb: - 0.731 MB of 0.731 MB uploaded
wandb: \ 1.458 MB of 1.461 MB uploaded (0.027 MB deduped)
wandb: | 1.476 MB of 1.476 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:            train/learning_rate ▁
wandb:                     train/loss ▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 1
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 1.3154
wandb:               train/total_flos 1421417119744.0
wandb:               train/train_loss 1.31539
wandb:            train/train_runtime 42.346
wandb: train/train_samples_per_second 3.023
wandb:   train/train_steps_per_second 0.024
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/nsprnzvt
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v6
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_144650-nsprnzvt/logs
[2024-02-19 14:48:29,945] [INFO] [launch.py:347:main] Process 946179 exits successfully.
[2024-02-19 16:03:01,691] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:03,981] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 16:03:03,982] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/llava-v1.5-7b/checkpoints --max_steps 1 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 16:03:06,489] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:08,871] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 16:03:08,871] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 16:03:08,872] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 16:03:08,872] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 16:03:08,872] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 16:03:14,597] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:14,694] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:14,898] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:14,991] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:03:16,494] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:03:16,672] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:03:16,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:03:16,701] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 16:03:16,833] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 16:03:32,670] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.26s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.33s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.37s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.65s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.71s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.97s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None

Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.66s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 16:03:52,785] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_160444-onbqn3tz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/onbqn3tz

  0%|          | 0/1 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 1/1 [00:41<00:00, 41.93s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

100%|██████████| 1/1 [00:41<00:00, 41.93s/it]
                                             
{'train_runtime': 46.7642, 'train_samples_per_second': 2.737, 'train_steps_per_second': 0.021, 'train_loss': 2.4834094047546387, 'epoch': 0.0}

100%|██████████| 1/1 [00:42<00:00, 41.93s/it]
100%|██████████| 1/1 [00:42<00:00, 42.47s/it]
[2024-02-19 16:05:47,044] [INFO] [launch.py:347:main] Process 1219891 exits successfully.
[2024-02-19 16:05:48,050] [INFO] [launch.py:347:main] Process 1219890 exits successfully.
[2024-02-19 16:05:49,051] [INFO] [launch.py:347:main] Process 1219892 exits successfully.
wandb: - 0.749 MB of 0.749 MB uploaded
wandb: \ 0.749 MB of 0.749 MB uploaded
wandb: | 0.749 MB of 0.749 MB uploaded
wandb: / 0.749 MB of 0.749 MB uploaded
wandb: - 1.494 MB of 1.496 MB uploaded (0.027 MB deduped)
wandb: \ 1.512 MB of 1.512 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:            train/learning_rate ▁
wandb:                     train/loss ▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 1
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 2.4834
wandb:               train/total_flos 2707941228544.0
wandb:               train/train_loss 2.48341
wandb:            train/train_runtime 46.7642
wandb: train/train_samples_per_second 2.737
wandb:   train/train_steps_per_second 0.021
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/onbqn3tz
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v7
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_160444-onbqn3tz/logs
[2024-02-19 16:06:34,102] [INFO] [launch.py:347:main] Process 1219889 exits successfully.
[2024-02-19 16:11:01,031] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:03,416] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 16:11:03,417] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/llava-v1.5-7b/checkpoints --max_steps 1 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 16:11:06,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:08,700] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 16:11:08,701] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 16:11:08,701] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 16:11:08,701] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 16:11:08,701] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 16:11:13,991] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:14,189] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:14,239] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:14,633] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:11:16,136] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:11:16,137] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 16:11:16,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:11:16,244] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:11:16,416] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 16:11:31,691] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.58s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.66s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.90s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.73s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None

Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  8.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.09s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 16:11:52,836] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_161244-t3my96q6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/t3my96q6

  0%|          | 0/1 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 1/1 [00:41<00:00, 41.52s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

100%|██████████| 1/1 [00:41<00:00, 41.52s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(

                                             
{'train_runtime': 119.5547, 'train_samples_per_second': 1.071, 'train_steps_per_second': 0.008, 'train_loss': 2.4834094047546387, 'epoch': 0.0}

100%|██████████| 1/1 [01:55<00:00, 41.52s/it]
100%|██████████| 1/1 [01:55<00:00, 115.61s/it]
[2024-02-19 16:14:58,024] [INFO] [launch.py:347:main] Process 1253436 exits successfully.
[2024-02-19 16:14:59,026] [INFO] [launch.py:347:main] Process 1253437 exits successfully.
[2024-02-19 16:14:59,026] [INFO] [launch.py:347:main] Process 1253438 exits successfully.
wandb: - 0.763 MB of 0.763 MB uploaded
wandb: \ 0.763 MB of 0.763 MB uploaded
wandb: | 0.763 MB of 0.763 MB uploaded
wandb: / 0.763 MB of 0.763 MB uploaded
wandb: - 0.763 MB of 0.763 MB uploaded
wandb: \ 1.523 MB of 1.523 MB uploaded (0.027 MB deduped)
wandb: | 1.542 MB of 1.542 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:            train/learning_rate ▁
wandb:                     train/loss ▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 1
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 2.4834
wandb:               train/total_flos 2707941228544.0
wandb:               train/train_loss 2.48341
wandb:            train/train_runtime 119.5547
wandb: train/train_samples_per_second 1.071
wandb:   train/train_steps_per_second 0.008
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/t3my96q6
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v8
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_161244-t3my96q6/logs
[2024-02-19 16:15:41,075] [INFO] [launch.py:347:main] Process 1253435 exits successfully.
[2024-02-19 16:21:24,304] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:26,615] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 16:21:26,615] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens False --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/llava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 16:21:29,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:31,324] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 16:21:31,325] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 16:21:31,325] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 16:21:31,325] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 16:21:31,326] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 16:21:36,191] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:36,204] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:36,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:36,498] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 16:21:37,782] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:21:37,785] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:21:37,786] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 16:21:37,967] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 16:21:38,045] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 16:21:52,057] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.13s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.29s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.39s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.38s/it]
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.16s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None

Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.96s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 16:22:10,561] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_162251-af0favbx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/af0favbx

  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 17%|█▋        | 1/6 [00:34<02:52, 34.55s/it]
                                             
{'loss': 1.3154, 'learning_rate': 2e-05, 'epoch': 0.0}

 17%|█▋        | 1/6 [00:34<02:52, 34.55s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 33%|███▎      | 2/6 [02:04<04:27, 66.89s/it]
                                             
{'loss': 1.3334, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.0}

 33%|███▎      | 2/6 [02:04<04:27, 66.89s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 50%|█████     | 3/6 [03:34<03:52, 77.59s/it]
                                             
{'loss': 1.1116, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.0}

 50%|█████     | 3/6 [03:34<03:52, 77.59s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 67%|██████▋   | 4/6 [05:03<02:43, 81.95s/it]
                                             
{'loss': 1.1231, 'learning_rate': 6.909830056250527e-06, 'epoch': 0.0}

 67%|██████▋   | 4/6 [05:03<02:43, 81.95s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 83%|████████▎ | 5/6 [06:38<01:26, 86.76s/it]
                                             
{'loss': 1.1473, 'learning_rate': 1.9098300562505266e-06, 'epoch': 0.0}

 83%|████████▎ | 5/6 [06:38<01:26, 86.76s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 6/6 [08:15<00:00, 90.42s/it]
                                             
{'loss': 1.0773, 'learning_rate': 0.0, 'epoch': 0.0}

100%|██████████| 6/6 [08:15<00:00, 90.42s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(

                                             
{'train_runtime': 580.5701, 'train_samples_per_second': 1.323, 'train_steps_per_second': 0.01, 'train_loss': 1.1846861044565837, 'epoch': 0.0}

100%|██████████| 6/6 [09:36<00:00, 90.42s/it]
100%|██████████| 6/6 [09:36<00:00, 96.07s/it]
[2024-02-19 16:32:47,113] [INFO] [launch.py:347:main] Process 1296212 exits successfully.
[2024-02-19 16:32:47,114] [INFO] [launch.py:347:main] Process 1296209 exits successfully.
[2024-02-19 16:32:48,115] [INFO] [launch.py:347:main] Process 1296214 exits successfully.
wandb: - 0.780 MB of 0.780 MB uploaded
wandb: \ 0.780 MB of 0.780 MB uploaded
wandb: | 0.780 MB of 0.780 MB uploaded
wandb: / 0.780 MB of 0.780 MB uploaded
wandb: - 1.556 MB of 1.580 MB uploaded (0.027 MB deduped)
wandb: \ 1.580 MB of 1.580 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▁▁▁
wandb:              train/global_step ▁▂▄▅▇██
wandb:            train/learning_rate █▇▆▃▂▁
wandb:                     train/loss ██▂▂▃▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 6
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.0773
wandb:               train/total_flos 8817902485504.0
wandb:               train/train_loss 1.18469
wandb:            train/train_runtime 580.5701
wandb: train/train_samples_per_second 1.323
wandb:   train/train_steps_per_second 0.01
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/af0favbx
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v9
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_162251-af0favbx/logs
[2024-02-19 16:33:21,148] [INFO] [launch.py:347:main] Process 1296207 exits successfully.
[2024-02-19 17:16:55,131] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:16:57,151] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 17:16:57,151] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 17:16:59,653] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:17:02,007] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 17:17:02,007] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 17:17:02,008] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 17:17:02,008] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 17:17:02,008] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 17:17:06,660] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:17:06,737] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:17:06,757] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:17:07,022] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 17:17:08,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 17:17:08,148] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 17:17:08,219] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 17:17:08,422] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 17:17:08,507] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
model class instantiated
[2024-02-19 17:17:21,683] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]checking if state dict is not none
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]checking if state dict is not none
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.14s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.22s/it]
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.50s/it]
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.50s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method

Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.92s/it]
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
Merged granular config!
Granular tokens config loaded!
[2024-02-19 17:17:38,078] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_171818-b0d9jisz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/b0d9jisz

  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 17%|█▋        | 1/6 [00:40<03:21, 40.30s/it]
                                             
{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}

 17%|█▋        | 1/6 [00:40<03:21, 40.30s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 33%|███▎      | 2/6 [02:23<05:09, 77.41s/it]
                                             
{'loss': 2.5048, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.0}

 33%|███▎      | 2/6 [02:23<05:09, 77.41s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 50%|█████     | 3/6 [04:06<04:27, 89.07s/it]
                                             
{'loss': 1.5467, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.0}

 50%|█████     | 3/6 [04:06<04:27, 89.07s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 67%|██████▋   | 4/6 [05:42<03:03, 91.64s/it]
                                             
{'loss': 1.3539, 'learning_rate': 6.909830056250527e-06, 'epoch': 0.0}

 67%|██████▋   | 4/6 [05:42<03:03, 91.64s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

 83%|████████▎ | 5/6 [07:36<01:39, 99.66s/it]
                                             
{'loss': 1.2676, 'learning_rate': 1.9098300562505266e-06, 'epoch': 0.0}

 83%|████████▎ | 5/6 [07:36<01:39, 99.66s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

100%|██████████| 6/6 [09:33<00:00, 105.55s/it]
                                              
{'loss': 1.2268, 'learning_rate': 0.0, 'epoch': 0.0}

100%|██████████| 6/6 [09:33<00:00, 105.55s/it]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(

                                              
{'train_runtime': 670.5494, 'train_samples_per_second': 1.145, 'train_steps_per_second': 0.009, 'train_loss': 1.7305291295051575, 'epoch': 0.0}

100%|██████████| 6/6 [11:06<00:00, 105.55s/it]
100%|██████████| 6/6 [11:06<00:00, 111.08s/it]
[2024-02-19 17:29:42,894] [INFO] [launch.py:347:main] Process 1515130 exits successfully.
[2024-02-19 17:29:43,897] [INFO] [launch.py:347:main] Process 1515131 exits successfully.
[2024-02-19 17:29:45,900] [INFO] [launch.py:347:main] Process 1515132 exits successfully.
wandb: - 0.803 MB of 0.803 MB uploaded
wandb: \ 0.803 MB of 0.803 MB uploaded
wandb: | 0.803 MB of 0.803 MB uploaded
wandb: / 0.803 MB of 0.803 MB uploaded
wandb: - 1.602 MB of 1.626 MB uploaded (0.027 MB deduped)
wandb: \ 1.626 MB of 1.626 MB uploaded (0.027 MB deduped)
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▁▁▁
wandb:              train/global_step ▁▂▄▅▇██
wandb:            train/learning_rate █▇▆▃▂▁
wandb:                     train/loss ██▃▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 6
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.2268
wandb:               train/total_flos 15965849649152.0
wandb:               train/train_loss 1.73053
wandb:            train/train_runtime 670.5494
wandb: train/train_samples_per_second 1.145
wandb:   train/train_steps_per_second 0.009
wandb: 
wandb: 🚀 View run grllava-7b-it-dryrun at: https://wandb.ai/compyle/LLaVA-llava_train/runs/b0d9jisz
wandb: ️⚡ View job at https://wandb.ai/compyle/LLaVA-llava_train/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MDgyNTM0Mg==/version_details/v10
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240219_171818-b0d9jisz/logs
[2024-02-19 17:30:21,940] [INFO] [launch.py:347:main] Process 1515129 exits successfully.
[2024-02-19 18:28:48,368] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:28:50,590] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 18:28:50,591] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 18:28:53,295] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:28:55,747] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 18:28:55,747] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 18:28:55,747] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 18:28:55,747] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 18:28:55,747] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 18:29:00,225] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:29:00,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:29:00,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:29:00,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:29:01,628] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:29:01,884] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:29:01,884] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 18:29:02,055] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:29:02,232] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
[2024-02-19 18:29:15,916] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.13s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.28s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.32s/it]
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.51s/it]model class instantiated
trying to load pretrained model using class specific method
model class instantiated
in base class load pretrained method
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None

Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.54s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 18:29:33,586] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 921, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 921, in train
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin'
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 921, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 921, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin'
[2024-02-19 18:29:39,851] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1788407
[2024-02-19 18:29:39,851] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1788408
[2024-02-19 18:29:39,944] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1788411
[2024-02-19 18:29:40,006] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1788412
[2024-02-19 18:29:40,069] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/grllava-v1.5-7b/pretrained/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--mm_vision_use_granular_tokens', 'True', '--mm_vision_granular_select_layers', '6 12 18', '--mm_vision_granular_tokens_strategy', 'pool', '--mm_vision_granular_tokens_per_layer', '192', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data1/akane/grllava-v1.5-7b/checkpoints', '--max_steps', '6', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '1', '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'grllava-7b-it-dryrun'] exits with return code = 1

[2024-02-19 18:31:31,530] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:33,690] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 18:31:33,691] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 18:31:36,257] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:38,298] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 18:31:38,298] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 18:31:38,298] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 18:31:38,298] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 18:31:38,298] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 18:31:43,150] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:43,247] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:43,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:43,531] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:31:44,586] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:31:44,775] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:31:44,924] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:31:45,220] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:31:45,221] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
[2024-02-19 18:32:00,096] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]checking if state dict is not none
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.51s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.55s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.54s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.57s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.63s/it]
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!

Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.08s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None

Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.75s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 18:32:18,151] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_183258-s0e7a96c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/s0e7a96c
  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 17%|█▋        | 1/6 [00:40<03:23, 40.63s/it]                                             {'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.0}
 17%|█▋        | 1/6 [00:40<03:23, 40.63s/it][2024-02-19 18:34:06,903] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1798008
[2024-02-19 18:34:08,895] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1798009
[2024-02-19 18:34:10,046] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1798010
[2024-02-19 18:34:11,213] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1798011
[2024-02-19 18:34:12,714] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2024-02-19 18:34:28,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:30,885] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-02-19 18:34:30,885] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --mm_vision_use_granular_tokens True --mm_vision_granular_select_layers 6 12 18 --mm_vision_granular_tokens_strategy pool --mm_vision_granular_tokens_per_layer 192 --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data1/akane/grllava-v1.5-7b/checkpoints --max_steps 6 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name grllava-7b-it-dryrun
[2024-02-19 18:34:33,299] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:35,478] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-19 18:34:35,479] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-19 18:34:35,479] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-19 18:34:35,479] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-19 18:34:35,479] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-19 18:34:40,012] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:40,471] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:40,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:40,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 18:34:41,408] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:34:41,913] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:34:42,018] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 18:34:42,019] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 18:34:42,050] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-19 18:34:56,218] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
model class instantiated
in base class load pretrained method
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]checking if state dict is not none
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
checking if state dict is not none
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.18s/it]
Merged granular config!
Granular tokens config loaded!
Merged granular config!
Granular tokens config loaded!
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]
Merged granular config!
Granular tokens config loaded!
Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.66s/it]model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
checking if state dict is not none
in state_dict not None
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.56s/it]
Merged granular config!
Granular tokens config loaded!
[2024-02-19 18:35:11,894] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
model class instantiated
trying to load pretrained model using class specific method
in base class load pretrained method
checking if state dict is not none
in state_dict not None
model.embed_tokens.weight: True
model.layers.0.self_attn.q_proj.weight: True
model.layers.0.self_attn.k_proj.weight: True
model.layers.0.self_attn.v_proj.weight: True
model.layers.0.self_attn.o_proj.weight: True
model.layers.0.mlp.gate_proj.weight: True
model.layers.0.mlp.up_proj.weight: True
model.layers.0.mlp.down_proj.weight: True
model.layers.0.input_layernorm.weight: True
model.layers.0.post_attention_layernorm.weight: True
model.layers.1.self_attn.q_proj.weight: True
model.layers.1.self_attn.k_proj.weight: True
model.layers.1.self_attn.v_proj.weight: True
model.layers.1.self_attn.o_proj.weight: True
model.layers.1.mlp.gate_proj.weight: True
model.layers.1.mlp.up_proj.weight: True
model.layers.1.mlp.down_proj.weight: True
model.layers.1.input_layernorm.weight: True
model.layers.1.post_attention_layernorm.weight: True
model.layers.2.self_attn.q_proj.weight: True
model.layers.2.self_attn.k_proj.weight: True
model.layers.2.self_attn.v_proj.weight: True
model.layers.2.self_attn.o_proj.weight: True
model.layers.2.mlp.gate_proj.weight: True
model.layers.2.mlp.up_proj.weight: True
model.layers.2.mlp.down_proj.weight: True
model.layers.2.input_layernorm.weight: True
model.layers.2.post_attention_layernorm.weight: True
model.layers.3.self_attn.q_proj.weight: True
model.layers.3.self_attn.k_proj.weight: True
model.layers.3.self_attn.v_proj.weight: True
model.layers.3.self_attn.o_proj.weight: True
model.layers.3.mlp.gate_proj.weight: True
model.layers.3.mlp.up_proj.weight: True
model.layers.3.mlp.down_proj.weight: True
model.layers.3.input_layernorm.weight: True
model.layers.3.post_attention_layernorm.weight: True
model.layers.4.self_attn.q_proj.weight: True
model.layers.4.self_attn.k_proj.weight: True
model.layers.4.self_attn.v_proj.weight: True
model.layers.4.self_attn.o_proj.weight: True
model.layers.4.mlp.gate_proj.weight: True
model.layers.4.mlp.up_proj.weight: True
model.layers.4.mlp.down_proj.weight: True
model.layers.4.input_layernorm.weight: True
model.layers.4.post_attention_layernorm.weight: True
model.layers.5.self_attn.q_proj.weight: True
model.layers.5.self_attn.k_proj.weight: True
model.layers.5.self_attn.v_proj.weight: True
model.layers.5.self_attn.o_proj.weight: True
model.layers.5.mlp.gate_proj.weight: True
model.layers.5.mlp.up_proj.weight: True
model.layers.5.mlp.down_proj.weight: True
model.layers.5.input_layernorm.weight: True
model.layers.5.post_attention_layernorm.weight: True
model.layers.6.self_attn.q_proj.weight: True
model.layers.6.self_attn.k_proj.weight: True
model.layers.6.self_attn.v_proj.weight: True
model.layers.6.self_attn.o_proj.weight: True
model.layers.6.mlp.gate_proj.weight: True
model.layers.6.mlp.up_proj.weight: True
model.layers.6.mlp.down_proj.weight: True
model.layers.6.input_layernorm.weight: True
model.layers.6.post_attention_layernorm.weight: True
model.layers.7.self_attn.q_proj.weight: True
model.layers.7.self_attn.k_proj.weight: True
model.layers.7.self_attn.v_proj.weight: True
model.layers.7.self_attn.o_proj.weight: True
model.layers.7.mlp.gate_proj.weight: True
model.layers.7.mlp.up_proj.weight: True
model.layers.7.mlp.down_proj.weight: True
model.layers.7.input_layernorm.weight: True
model.layers.7.post_attention_layernorm.weight: True
model.layers.8.self_attn.q_proj.weight: True
model.layers.8.self_attn.k_proj.weight: True
model.layers.8.self_attn.v_proj.weight: True
model.layers.8.self_attn.o_proj.weight: True
model.layers.8.mlp.gate_proj.weight: True
model.layers.8.mlp.up_proj.weight: True
model.layers.8.mlp.down_proj.weight: True
model.layers.8.input_layernorm.weight: True
model.layers.8.post_attention_layernorm.weight: True
model.layers.9.self_attn.q_proj.weight: True
model.layers.9.self_attn.k_proj.weight: True
model.layers.9.self_attn.v_proj.weight: True
model.layers.9.self_attn.o_proj.weight: True
model.layers.9.mlp.gate_proj.weight: True
model.layers.9.mlp.up_proj.weight: True
model.layers.9.mlp.down_proj.weight: True
model.layers.9.input_layernorm.weight: True
model.layers.9.post_attention_layernorm.weight: True
model.layers.10.self_attn.q_proj.weight: True
model.layers.10.self_attn.k_proj.weight: True
model.layers.10.self_attn.v_proj.weight: True
model.layers.10.self_attn.o_proj.weight: True
model.layers.10.mlp.gate_proj.weight: True
model.layers.10.mlp.up_proj.weight: True
model.layers.10.mlp.down_proj.weight: True
model.layers.10.input_layernorm.weight: True
model.layers.10.post_attention_layernorm.weight: True
model.layers.11.self_attn.q_proj.weight: True
model.layers.11.self_attn.k_proj.weight: True
model.layers.11.self_attn.v_proj.weight: True
model.layers.11.self_attn.o_proj.weight: True
model.layers.11.mlp.gate_proj.weight: True
model.layers.11.mlp.up_proj.weight: True
model.layers.11.mlp.down_proj.weight: True
model.layers.11.input_layernorm.weight: True
model.layers.11.post_attention_layernorm.weight: True
model.layers.12.self_attn.q_proj.weight: True
model.layers.12.self_attn.k_proj.weight: True
model.layers.12.self_attn.v_proj.weight: True
model.layers.12.self_attn.o_proj.weight: True
model.layers.12.mlp.gate_proj.weight: True
model.layers.12.mlp.up_proj.weight: True
model.layers.12.mlp.down_proj.weight: True
model.layers.12.input_layernorm.weight: True
model.layers.12.post_attention_layernorm.weight: True
model.layers.13.self_attn.q_proj.weight: True
model.layers.13.self_attn.k_proj.weight: True
model.layers.13.self_attn.v_proj.weight: True
model.layers.13.self_attn.o_proj.weight: True
model.layers.13.mlp.gate_proj.weight: True
model.layers.13.mlp.up_proj.weight: True
model.layers.13.mlp.down_proj.weight: True
model.layers.13.input_layernorm.weight: True
model.layers.13.post_attention_layernorm.weight: True
model.layers.14.self_attn.q_proj.weight: True
model.layers.14.self_attn.k_proj.weight: True
model.layers.14.self_attn.v_proj.weight: True
model.layers.14.self_attn.o_proj.weight: True
model.layers.14.mlp.gate_proj.weight: True
model.layers.14.mlp.up_proj.weight: True
model.layers.14.mlp.down_proj.weight: True
model.layers.14.input_layernorm.weight: True
model.layers.14.post_attention_layernorm.weight: True
model.layers.15.self_attn.q_proj.weight: True
model.layers.15.self_attn.k_proj.weight: True
model.layers.15.self_attn.v_proj.weight: True
model.layers.15.self_attn.o_proj.weight: True
model.layers.15.mlp.gate_proj.weight: True
model.layers.15.mlp.up_proj.weight: True
model.layers.15.mlp.down_proj.weight: True
model.layers.15.input_layernorm.weight: True
model.layers.15.post_attention_layernorm.weight: True
model.layers.16.self_attn.q_proj.weight: True
model.layers.16.self_attn.k_proj.weight: True
model.layers.16.self_attn.v_proj.weight: True
model.layers.16.self_attn.o_proj.weight: True
model.layers.16.mlp.gate_proj.weight: True
model.layers.16.mlp.up_proj.weight: True
model.layers.16.mlp.down_proj.weight: True
model.layers.16.input_layernorm.weight: True
model.layers.16.post_attention_layernorm.weight: True
model.layers.17.self_attn.q_proj.weight: True
model.layers.17.self_attn.k_proj.weight: True
model.layers.17.self_attn.v_proj.weight: True
model.layers.17.self_attn.o_proj.weight: True
model.layers.17.mlp.gate_proj.weight: True
model.layers.17.mlp.up_proj.weight: True
model.layers.17.mlp.down_proj.weight: True
model.layers.17.input_layernorm.weight: True
model.layers.17.post_attention_layernorm.weight: True
model.layers.18.self_attn.q_proj.weight: True
model.layers.18.self_attn.k_proj.weight: True
model.layers.18.self_attn.v_proj.weight: True
model.layers.18.self_attn.o_proj.weight: True
model.layers.18.mlp.gate_proj.weight: True
model.layers.18.mlp.up_proj.weight: True
model.layers.18.mlp.down_proj.weight: True
model.layers.18.input_layernorm.weight: True
model.layers.18.post_attention_layernorm.weight: True
model.layers.19.self_attn.q_proj.weight: True
model.layers.19.self_attn.k_proj.weight: True
model.layers.19.self_attn.v_proj.weight: True
model.layers.19.self_attn.o_proj.weight: True
model.layers.19.mlp.gate_proj.weight: True
model.layers.19.mlp.up_proj.weight: True
model.layers.19.mlp.down_proj.weight: True
model.layers.19.input_layernorm.weight: True
model.layers.19.post_attention_layernorm.weight: True
model.layers.20.self_attn.q_proj.weight: True
model.layers.20.self_attn.k_proj.weight: True
model.layers.20.self_attn.v_proj.weight: True
model.layers.20.self_attn.o_proj.weight: True
model.layers.20.mlp.gate_proj.weight: True
model.layers.20.mlp.up_proj.weight: True
model.layers.20.mlp.down_proj.weight: True
model.layers.20.input_layernorm.weight: True
model.layers.20.post_attention_layernorm.weight: True
model.layers.21.self_attn.q_proj.weight: True
model.layers.21.self_attn.k_proj.weight: True
model.layers.21.self_attn.v_proj.weight: True
model.layers.21.self_attn.o_proj.weight: True
model.layers.21.mlp.gate_proj.weight: True
model.layers.21.mlp.up_proj.weight: True
model.layers.21.mlp.down_proj.weight: True
model.layers.21.input_layernorm.weight: True
model.layers.21.post_attention_layernorm.weight: True
model.layers.22.self_attn.q_proj.weight: True
model.layers.22.self_attn.k_proj.weight: True
model.layers.22.self_attn.v_proj.weight: True
model.layers.22.self_attn.o_proj.weight: True
model.layers.22.mlp.gate_proj.weight: True
model.layers.22.mlp.up_proj.weight: True
model.layers.22.mlp.down_proj.weight: True
model.layers.22.input_layernorm.weight: True
model.layers.22.post_attention_layernorm.weight: True
model.layers.23.self_attn.q_proj.weight: True
model.layers.23.self_attn.k_proj.weight: True
model.layers.23.self_attn.v_proj.weight: True
model.layers.23.self_attn.o_proj.weight: True
model.layers.23.mlp.gate_proj.weight: True
model.layers.23.mlp.up_proj.weight: True
model.layers.23.mlp.down_proj.weight: True
model.layers.23.input_layernorm.weight: True
model.layers.23.post_attention_layernorm.weight: True
model.layers.24.self_attn.q_proj.weight: True
model.layers.24.self_attn.k_proj.weight: True
model.layers.24.self_attn.v_proj.weight: True
model.layers.24.self_attn.o_proj.weight: True
model.layers.24.mlp.gate_proj.weight: True
model.layers.24.mlp.up_proj.weight: True
model.layers.24.mlp.down_proj.weight: True
model.layers.24.input_layernorm.weight: True
model.layers.24.post_attention_layernorm.weight: True
model.layers.25.self_attn.q_proj.weight: True
model.layers.25.self_attn.k_proj.weight: True
model.layers.25.self_attn.v_proj.weight: True
model.layers.25.self_attn.o_proj.weight: True
model.layers.25.mlp.gate_proj.weight: True
model.layers.25.mlp.up_proj.weight: True
model.layers.25.mlp.down_proj.weight: True
model.layers.25.input_layernorm.weight: True
model.layers.25.post_attention_layernorm.weight: True
model.layers.26.self_attn.q_proj.weight: True
model.layers.26.self_attn.k_proj.weight: True
model.layers.26.self_attn.v_proj.weight: True
model.layers.26.self_attn.o_proj.weight: True
model.layers.26.mlp.gate_proj.weight: True
model.layers.26.mlp.up_proj.weight: True
model.layers.26.mlp.down_proj.weight: True
model.layers.26.input_layernorm.weight: True
model.layers.26.post_attention_layernorm.weight: True
model.layers.27.self_attn.q_proj.weight: True
model.layers.27.self_attn.k_proj.weight: True
model.layers.27.self_attn.v_proj.weight: True
model.layers.27.self_attn.o_proj.weight: True
model.layers.27.mlp.gate_proj.weight: True
model.layers.27.mlp.up_proj.weight: True
model.layers.27.mlp.down_proj.weight: True
model.layers.27.input_layernorm.weight: True
model.layers.27.post_attention_layernorm.weight: True
model.layers.28.self_attn.q_proj.weight: True
model.layers.28.self_attn.k_proj.weight: True
model.layers.28.self_attn.v_proj.weight: True
model.layers.28.self_attn.o_proj.weight: True
model.layers.28.mlp.gate_proj.weight: True
model.layers.28.mlp.up_proj.weight: True
model.layers.28.mlp.down_proj.weight: True
model.layers.28.input_layernorm.weight: True
model.layers.28.post_attention_layernorm.weight: True
model.layers.29.self_attn.q_proj.weight: True
model.layers.29.self_attn.k_proj.weight: True
model.layers.29.self_attn.v_proj.weight: True
model.layers.29.self_attn.o_proj.weight: True
model.layers.29.mlp.gate_proj.weight: True
model.layers.29.mlp.up_proj.weight: True
model.layers.29.mlp.down_proj.weight: True
model.layers.29.input_layernorm.weight: True
model.layers.29.post_attention_layernorm.weight: True
model.layers.30.self_attn.q_proj.weight: True
model.layers.30.self_attn.k_proj.weight: True
model.layers.30.self_attn.v_proj.weight: True
model.layers.30.self_attn.o_proj.weight: True
model.layers.30.mlp.gate_proj.weight: True
model.layers.30.mlp.up_proj.weight: True
model.layers.30.mlp.down_proj.weight: True
model.layers.30.input_layernorm.weight: True
model.layers.30.post_attention_layernorm.weight: True
model.layers.31.self_attn.q_proj.weight: True
model.layers.31.self_attn.k_proj.weight: True
model.layers.31.self_attn.v_proj.weight: True
model.layers.31.self_attn.o_proj.weight: True
model.layers.31.mlp.gate_proj.weight: True
model.layers.31.mlp.up_proj.weight: True
model.layers.31.mlp.down_proj.weight: True
model.layers.31.input_layernorm.weight: True
model.layers.31.post_attention_layernorm.weight: True
model.norm.weight: True
model.vision_tower.vision_tower.vision_model.embeddings.class_embedding: False
model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight: False
model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.post_layernorm.weight: False
model.vision_tower.vision_tower.vision_model.post_layernorm.bias: False
model.mm_projector.0.weight: True
model.mm_projector.0.bias: True
model.mm_projector.2.weight: True
model.mm_projector.2.bias: True
model.granular_mm_projector.0.weight: True
model.granular_mm_projector.0.bias: True
model.granular_mm_projector.2.weight: True
model.granular_mm_projector.2.bias: True
lm_head.weight: True
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
model.embed_tokens.weight: True
model.layers.0.self_attn.q_proj.weight: True
model.layers.0.self_attn.k_proj.weight: True
model.layers.0.self_attn.v_proj.weight: True
model.layers.0.self_attn.o_proj.weight: True
model.layers.0.mlp.gate_proj.weight: True
model.layers.0.mlp.up_proj.weight: True
model.layers.0.mlp.down_proj.weight: True
model.layers.0.input_layernorm.weight: True
model.layers.0.post_attention_layernorm.weight: True
model.layers.1.self_attn.q_proj.weight: True
model.layers.1.self_attn.k_proj.weight: True
model.layers.1.self_attn.v_proj.weight: True
model.layers.1.self_attn.o_proj.weight: True
model.layers.1.mlp.gate_proj.weight: True
model.layers.1.mlp.up_proj.weight: True
model.layers.1.mlp.down_proj.weight: True
model.layers.1.input_layernorm.weight: True
model.layers.1.post_attention_layernorm.weight: True
model.layers.2.self_attn.q_proj.weight: True
model.layers.2.self_attn.k_proj.weight: True
model.layers.2.self_attn.v_proj.weight: True
model.layers.2.self_attn.o_proj.weight: True
model.layers.2.mlp.gate_proj.weight: True
model.layers.2.mlp.up_proj.weight: True
model.layers.2.mlp.down_proj.weight: True
model.layers.2.input_layernorm.weight: True
model.layers.2.post_attention_layernorm.weight: True
model.layers.3.self_attn.q_proj.weight: True
model.layers.3.self_attn.k_proj.weight: True
model.layers.3.self_attn.v_proj.weight: True
model.layers.3.self_attn.o_proj.weight: True
model.layers.3.mlp.gate_proj.weight: True
model.layers.3.mlp.up_proj.weight: True
model.layers.3.mlp.down_proj.weight: True
model.layers.3.input_layernorm.weight: True
model.layers.3.post_attention_layernorm.weight: True
model.layers.4.self_attn.q_proj.weight: True
model.layers.4.self_attn.k_proj.weight: True
model.layers.4.self_attn.v_proj.weight: True
model.layers.4.self_attn.o_proj.weight: True
model.layers.4.mlp.gate_proj.weight: True
model.layers.4.mlp.up_proj.weight: True
model.layers.4.mlp.down_proj.weight: True
model.layers.4.input_layernorm.weight: True
model.layers.4.post_attention_layernorm.weight: True
model.layers.5.self_attn.q_proj.weight: True
model.layers.5.self_attn.k_proj.weight: True
model.layers.5.self_attn.v_proj.weight: True
model.layers.5.self_attn.o_proj.weight: True
model.layers.5.mlp.gate_proj.weight: True
model.layers.5.mlp.up_proj.weight: True
model.layers.5.mlp.down_proj.weight: True
model.layers.5.input_layernorm.weight: True
model.layers.5.post_attention_layernorm.weight: True
model.layers.6.self_attn.q_proj.weight: True
model.layers.6.self_attn.k_proj.weight: True
model.layers.6.self_attn.v_proj.weight: True
model.layers.6.self_attn.o_proj.weight: True
model.layers.6.mlp.gate_proj.weight: True
model.layers.6.mlp.up_proj.weight: True
model.layers.6.mlp.down_proj.weight: True
model.layers.6.input_layernorm.weight: True
model.layers.6.post_attention_layernorm.weight: True
model.layers.7.self_attn.q_proj.weight: True
model.layers.7.self_attn.k_proj.weight: True
model.layers.7.self_attn.v_proj.weight: True
model.layers.7.self_attn.o_proj.weight: True
model.layers.7.mlp.gate_proj.weight: True
model.layers.7.mlp.up_proj.weight: True
model.layers.7.mlp.down_proj.weight: True
model.layers.7.input_layernorm.weight: True
model.layers.7.post_attention_layernorm.weight: True
model.layers.8.self_attn.q_proj.weight: True
model.layers.8.self_attn.k_proj.weight: True
model.layers.8.self_attn.v_proj.weight: True
model.layers.8.self_attn.o_proj.weight: True
model.layers.8.mlp.gate_proj.weight: True
model.layers.8.mlp.up_proj.weight: True
model.layers.8.mlp.down_proj.weight: True
model.layers.8.input_layernorm.weight: True
model.layers.8.post_attention_layernorm.weight: True
model.layers.9.self_attn.q_proj.weight: True
model.layers.9.self_attn.k_proj.weight: True
model.layers.9.self_attn.v_proj.weight: True
model.layers.9.self_attn.o_proj.weight: True
model.layers.9.mlp.gate_proj.weight: True
model.layers.9.mlp.up_proj.weight: True
model.layers.9.mlp.down_proj.weight: True
model.layers.9.input_layernorm.weight: True
model.layers.9.post_attention_layernorm.weight: True
model.layers.10.self_attn.q_proj.weight: True
model.layers.10.self_attn.k_proj.weight: True
model.layers.10.self_attn.v_proj.weight: True
model.layers.10.self_attn.o_proj.weight: True
model.layers.10.mlp.gate_proj.weight: True
model.layers.10.mlp.up_proj.weight: True
model.layers.10.mlp.down_proj.weight: True
model.layers.10.input_layernorm.weight: True
model.layers.10.post_attention_layernorm.weight: True
model.layers.11.self_attn.q_proj.weight: True
model.layers.11.self_attn.k_proj.weight: True
model.layers.11.self_attn.v_proj.weight: True
model.layers.11.self_attn.o_proj.weight: True
model.layers.11.mlp.gate_proj.weight: True
model.layers.11.mlp.up_proj.weight: True
model.layers.11.mlp.down_proj.weight: True
model.layers.11.input_layernorm.weight: True
model.layers.11.post_attention_layernorm.weight: True
model.layers.12.self_attn.q_proj.weight: True
model.layers.12.self_attn.k_proj.weight: True
model.layers.12.self_attn.v_proj.weight: True
model.layers.12.self_attn.o_proj.weight: True
model.layers.12.mlp.gate_proj.weight: True
model.layers.12.mlp.up_proj.weight: True
model.layers.12.mlp.down_proj.weight: True
model.layers.12.input_layernorm.weight: True
model.layers.12.post_attention_layernorm.weight: True
model.layers.13.self_attn.q_proj.weight: True
model.layers.13.self_attn.k_proj.weight: True
model.layers.13.self_attn.v_proj.weight: True
model.layers.13.self_attn.o_proj.weight: True
model.layers.13.mlp.gate_proj.weight: True
model.layers.13.mlp.up_proj.weight: True
model.layers.13.mlp.down_proj.weight: True
model.layers.13.input_layernorm.weight: True
model.layers.13.post_attention_layernorm.weight: True
model.layers.14.self_attn.q_proj.weight: True
model.layers.14.self_attn.k_proj.weight: True
model.layers.14.self_attn.v_proj.weight: True
model.layers.14.self_attn.o_proj.weight: True
model.layers.14.mlp.gate_proj.weight: True
model.layers.14.mlp.up_proj.weight: True
model.layers.14.mlp.down_proj.weight: True
model.layers.14.input_layernorm.weight: True
model.layers.14.post_attention_layernorm.weight: True
model.layers.15.self_attn.q_proj.weight: True
model.layers.15.self_attn.k_proj.weight: True
model.layers.15.self_attn.v_proj.weight: True
model.layers.15.self_attn.o_proj.weight: True
model.layers.15.mlp.gate_proj.weight: True
model.layers.15.mlp.up_proj.weight: True
model.layers.15.mlp.down_proj.weight: True
model.layers.15.input_layernorm.weight: True
model.layers.15.post_attention_layernorm.weight: True
model.layers.16.self_attn.q_proj.weight: True
model.layers.16.self_attn.k_proj.weight: True
model.layers.16.self_attn.v_proj.weight: True
model.layers.16.self_attn.o_proj.weight: True
model.layers.16.mlp.gate_proj.weight: True
model.layers.16.mlp.up_proj.weight: True
model.layers.16.mlp.down_proj.weight: True
model.layers.16.input_layernorm.weight: True
model.layers.16.post_attention_layernorm.weight: True
model.layers.17.self_attn.q_proj.weight: True
model.layers.17.self_attn.k_proj.weight: True
model.layers.17.self_attn.v_proj.weight: True
model.layers.17.self_attn.o_proj.weight: True
model.layers.17.mlp.gate_proj.weight: True
model.layers.17.mlp.up_proj.weight: True
model.layers.17.mlp.down_proj.weight: True
model.layers.17.input_layernorm.weight: True
model.layers.17.post_attention_layernorm.weight: True
model.layers.18.self_attn.q_proj.weight: True
model.layers.18.self_attn.k_proj.weight: True
model.layers.18.self_attn.v_proj.weight: True
model.layers.18.self_attn.o_proj.weight: True
model.layers.18.mlp.gate_proj.weight: True
model.layers.18.mlp.up_proj.weight: True
model.layers.18.mlp.down_proj.weight: True
model.layers.18.input_layernorm.weight: True
model.layers.18.post_attention_layernorm.weight: True
model.layers.19.self_attn.q_proj.weight: True
model.layers.19.self_attn.k_proj.weight: True
model.layers.19.self_attn.v_proj.weight: True
model.layers.19.self_attn.o_proj.weight: True
model.layers.19.mlp.gate_proj.weight: True
model.layers.19.mlp.up_proj.weight: True
model.layers.19.mlp.down_proj.weight: True
model.layers.19.input_layernorm.weight: True
model.layers.19.post_attention_layernorm.weight: True
model.layers.20.self_attn.q_proj.weight: True
model.layers.20.self_attn.k_proj.weight: True
model.layers.20.self_attn.v_proj.weight: True
model.layers.20.self_attn.o_proj.weight: True
model.layers.20.mlp.gate_proj.weight: True
model.layers.20.mlp.up_proj.weight: True
model.layers.20.mlp.down_proj.weight: True
model.layers.20.input_layernorm.weight: True
model.layers.20.post_attention_layernorm.weight: True
model.layers.21.self_attn.q_proj.weight: True
model.layers.21.self_attn.k_proj.weight: True
model.layers.21.self_attn.v_proj.weight: True
model.layers.21.self_attn.o_proj.weight: True
model.layers.21.mlp.gate_proj.weight: True
model.layers.21.mlp.up_proj.weight: True
model.layers.21.mlp.down_proj.weight: True
model.layers.21.input_layernorm.weight: True
model.layers.21.post_attention_layernorm.weight: True
model.layers.22.self_attn.q_proj.weight: True
model.layers.22.self_attn.k_proj.weight: True
model.layers.22.self_attn.v_proj.weight: True
model.layers.22.self_attn.o_proj.weight: True
model.layers.22.mlp.gate_proj.weight: True
model.layers.22.mlp.up_proj.weight: True
model.layers.22.mlp.down_proj.weight: True
model.layers.22.input_layernorm.weight: True
model.layers.22.post_attention_layernorm.weight: True
model.layers.23.self_attn.q_proj.weight: True
model.layers.23.self_attn.k_proj.weight: True
model.layers.23.self_attn.v_proj.weight: True
model.layers.23.self_attn.o_proj.weight: True
model.layers.23.mlp.gate_proj.weight: True
model.layers.23.mlp.up_proj.weight: True
model.layers.23.mlp.down_proj.weight: True
model.layers.23.input_layernorm.weight: True
model.layers.23.post_attention_layernorm.weight: True
model.layers.24.self_attn.q_proj.weight: True
model.layers.24.self_attn.k_proj.weight: True
model.layers.24.self_attn.v_proj.weight: True
model.layers.24.self_attn.o_proj.weight: True
model.layers.24.mlp.gate_proj.weight: True
model.layers.24.mlp.up_proj.weight: True
model.layers.24.mlp.down_proj.weight: True
model.layers.24.input_layernorm.weight: True
model.layers.24.post_attention_layernorm.weight: True
model.layers.25.self_attn.q_proj.weight: True
model.layers.25.self_attn.k_proj.weight: True
model.layers.25.self_attn.v_proj.weight: True
model.layers.25.self_attn.o_proj.weight: True
model.layers.25.mlp.gate_proj.weight: True
model.layers.25.mlp.up_proj.weight: True
model.layers.25.mlp.down_proj.weight: True
model.layers.25.input_layernorm.weight: True
model.layers.25.post_attention_layernorm.weight: True
model.layers.26.self_attn.q_proj.weight: True
model.layers.26.self_attn.k_proj.weight: True
model.layers.26.self_attn.v_proj.weight: True
model.layers.26.self_attn.o_proj.weight: True
model.layers.26.mlp.gate_proj.weight: True
model.layers.26.mlp.up_proj.weight: True
model.layers.26.mlp.down_proj.weight: True
model.layers.26.input_layernorm.weight: True
model.layers.26.post_attention_layernorm.weight: True
model.layers.27.self_attn.q_proj.weight: True
model.layers.27.self_attn.k_proj.weight: True
model.layers.27.self_attn.v_proj.weight: True
model.layers.27.self_attn.o_proj.weight: True
model.layers.27.mlp.gate_proj.weight: True
model.layers.27.mlp.up_proj.weight: True
model.layers.27.mlp.down_proj.weight: True
model.layers.27.input_layernorm.weight: True
model.layers.27.post_attention_layernorm.weight: True
model.layers.28.self_attn.q_proj.weight: True
model.layers.28.self_attn.k_proj.weight: True
model.layers.28.self_attn.v_proj.weight: True
model.layers.28.self_attn.o_proj.weight: True
model.layers.28.mlp.gate_proj.weight: True
model.layers.28.mlp.up_proj.weight: True
model.layers.28.mlp.down_proj.weight: True
model.layers.28.input_layernorm.weight: True
model.layers.28.post_attention_layernorm.weight: True
model.layers.29.self_attn.q_proj.weight: True
model.layers.29.self_attn.k_proj.weight: True
model.layers.29.self_attn.v_proj.weight: True
model.layers.29.self_attn.o_proj.weight: True
model.layers.29.mlp.gate_proj.weight: True
model.layers.29.mlp.up_proj.weight: True
model.layers.29.mlp.down_proj.weight: True
model.layers.29.input_layernorm.weight: True
model.layers.29.post_attention_layernorm.weight: True
model.layers.30.self_attn.q_proj.weight: True
model.layers.30.self_attn.k_proj.weight: True
model.layers.30.self_attn.v_proj.weight: True
model.layers.30.self_attn.o_proj.weight: True
model.layers.30.mlp.gate_proj.weight: True
model.layers.30.mlp.up_proj.weight: True
model.layers.30.mlp.down_proj.weight: True
model.layers.30.input_layernorm.weight: True
model.layers.30.post_attention_layernorm.weight: True
model.layers.31.self_attn.q_proj.weight: True
model.layers.31.self_attn.k_proj.weight: True
model.layers.31.self_attn.v_proj.weight: True
model.layers.31.self_attn.o_proj.weight: True
model.layers.31.mlp.gate_proj.weight: True
model.layers.31.mlp.up_proj.weight: True
model.layers.31.mlp.down_proj.weight: True
model.layers.31.input_layernorm.weight: True
model.layers.31.post_attention_layernorm.weight: True
model.norm.weight: True
model.vision_tower.vision_tower.vision_model.embeddings.class_embedding: False
model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight: False
model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.post_layernorm.weight: False
model.vision_tower.vision_tower.vision_model.post_layernorm.bias: False
model.mm_projector.0.weight: True
model.mm_projector.0.bias: True
model.mm_projector.2.weight: True
model.mm_projector.2.bias: True
model.granular_mm_projector.0.weight: True
model.granular_mm_projector.0.bias: True
model.granular_mm_projector.2.weight: True
model.granular_mm_projector.2.bias: True
lm_head.weight: True
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
model.embed_tokens.weight: True
model.layers.0.self_attn.q_proj.weight: True
model.layers.0.self_attn.k_proj.weight: True
model.layers.0.self_attn.v_proj.weight: True
model.layers.0.self_attn.o_proj.weight: True
model.layers.0.mlp.gate_proj.weight: True
model.layers.0.mlp.up_proj.weight: True
model.layers.0.mlp.down_proj.weight: True
model.layers.0.input_layernorm.weight: True
model.layers.0.post_attention_layernorm.weight: True
model.layers.1.self_attn.q_proj.weight: True
model.layers.1.self_attn.k_proj.weight: True
model.layers.1.self_attn.v_proj.weight: True
model.layers.1.self_attn.o_proj.weight: True
model.layers.1.mlp.gate_proj.weight: True
model.layers.1.mlp.up_proj.weight: True
model.layers.1.mlp.down_proj.weight: True
model.layers.1.input_layernorm.weight: True
model.layers.1.post_attention_layernorm.weight: True
model.layers.2.self_attn.q_proj.weight: True
model.layers.2.self_attn.k_proj.weight: True
model.layers.2.self_attn.v_proj.weight: True
model.layers.2.self_attn.o_proj.weight: True
model.layers.2.mlp.gate_proj.weight: True
model.layers.2.mlp.up_proj.weight: True
model.layers.2.mlp.down_proj.weight: True
model.layers.2.input_layernorm.weight: True
model.layers.2.post_attention_layernorm.weight: True
model.layers.3.self_attn.q_proj.weight: True
model.layers.3.self_attn.k_proj.weight: True
model.layers.3.self_attn.v_proj.weight: True
model.layers.3.self_attn.o_proj.weight: True
model.layers.3.mlp.gate_proj.weight: True
model.layers.3.mlp.up_proj.weight: True
model.layers.3.mlp.down_proj.weight: True
model.layers.3.input_layernorm.weight: True
model.layers.3.post_attention_layernorm.weight: True
model.layers.4.self_attn.q_proj.weight: True
model.layers.4.self_attn.k_proj.weight: True
model.layers.4.self_attn.v_proj.weight: True
model.layers.4.self_attn.o_proj.weight: True
model.layers.4.mlp.gate_proj.weight: True
model.layers.4.mlp.up_proj.weight: True
model.layers.4.mlp.down_proj.weight: True
model.layers.4.input_layernorm.weight: True
model.layers.4.post_attention_layernorm.weight: True
model.layers.5.self_attn.q_proj.weight: True
model.layers.5.self_attn.k_proj.weight: True
model.layers.5.self_attn.v_proj.weight: True
model.layers.5.self_attn.o_proj.weight: True
model.layers.5.mlp.gate_proj.weight: True
model.layers.5.mlp.up_proj.weight: True
model.layers.5.mlp.down_proj.weight: True
model.layers.5.input_layernorm.weight: True
model.layers.5.post_attention_layernorm.weight: True
model.layers.6.self_attn.q_proj.weight: True
model.layers.6.self_attn.k_proj.weight: True
model.layers.6.self_attn.v_proj.weight: True
model.layers.6.self_attn.o_proj.weight: True
model.layers.6.mlp.gate_proj.weight: True
model.layers.6.mlp.up_proj.weight: True
model.layers.6.mlp.down_proj.weight: True
model.layers.6.input_layernorm.weight: True
model.layers.6.post_attention_layernorm.weight: True
model.layers.7.self_attn.q_proj.weight: True
model.layers.7.self_attn.k_proj.weight: True
model.layers.7.self_attn.v_proj.weight: True
model.layers.7.self_attn.o_proj.weight: True
model.layers.7.mlp.gate_proj.weight: True
model.layers.7.mlp.up_proj.weight: True
model.layers.7.mlp.down_proj.weight: True
model.layers.7.input_layernorm.weight: True
model.layers.7.post_attention_layernorm.weight: True
model.layers.8.self_attn.q_proj.weight: True
model.layers.8.self_attn.k_proj.weight: True
model.layers.8.self_attn.v_proj.weight: True
model.layers.8.self_attn.o_proj.weight: True
model.layers.8.mlp.gate_proj.weight: True
model.layers.8.mlp.up_proj.weight: True
model.layers.8.mlp.down_proj.weight: True
model.layers.8.input_layernorm.weight: True
model.layers.8.post_attention_layernorm.weight: True
model.layers.9.self_attn.q_proj.weight: True
model.layers.9.self_attn.k_proj.weight: True
model.layers.9.self_attn.v_proj.weight: True
model.layers.9.self_attn.o_proj.weight: True
model.layers.9.mlp.gate_proj.weight: True
model.layers.9.mlp.up_proj.weight: True
model.layers.9.mlp.down_proj.weight: True
model.layers.9.input_layernorm.weight: True
model.layers.9.post_attention_layernorm.weight: True
model.layers.10.self_attn.q_proj.weight: True
model.layers.10.self_attn.k_proj.weight: True
model.layers.10.self_attn.v_proj.weight: True
model.layers.10.self_attn.o_proj.weight: True
model.layers.10.mlp.gate_proj.weight: True
model.layers.10.mlp.up_proj.weight: True
model.layers.10.mlp.down_proj.weight: True
model.layers.10.input_layernorm.weight: True
model.layers.10.post_attention_layernorm.weight: True
model.layers.11.self_attn.q_proj.weight: True
model.layers.11.self_attn.k_proj.weight: True
model.layers.11.self_attn.v_proj.weight: True
model.layers.11.self_attn.o_proj.weight: True
model.layers.11.mlp.gate_proj.weight: True
model.layers.11.mlp.up_proj.weight: True
model.layers.11.mlp.down_proj.weight: True
model.layers.11.input_layernorm.weight: True
model.layers.11.post_attention_layernorm.weight: True
model.layers.12.self_attn.q_proj.weight: True
model.layers.12.self_attn.k_proj.weight: True
model.layers.12.self_attn.v_proj.weight: True
model.layers.12.self_attn.o_proj.weight: True
model.layers.12.mlp.gate_proj.weight: True
model.layers.12.mlp.up_proj.weight: True
model.layers.12.mlp.down_proj.weight: True
model.layers.12.input_layernorm.weight: True
model.layers.12.post_attention_layernorm.weight: True
model.layers.13.self_attn.q_proj.weight: True
model.layers.13.self_attn.k_proj.weight: True
model.layers.13.self_attn.v_proj.weight: True
model.layers.13.self_attn.o_proj.weight: True
model.layers.13.mlp.gate_proj.weight: True
model.layers.13.mlp.up_proj.weight: True
model.layers.13.mlp.down_proj.weight: True
model.layers.13.input_layernorm.weight: True
model.layers.13.post_attention_layernorm.weight: True
model.layers.14.self_attn.q_proj.weight: True
model.layers.14.self_attn.k_proj.weight: True
model.layers.14.self_attn.v_proj.weight: True
model.layers.14.self_attn.o_proj.weight: True
model.layers.14.mlp.gate_proj.weight: True
model.layers.14.mlp.up_proj.weight: True
model.layers.14.mlp.down_proj.weight: True
model.layers.14.input_layernorm.weight: True
model.layers.14.post_attention_layernorm.weight: True
model.layers.15.self_attn.q_proj.weight: True
model.layers.15.self_attn.k_proj.weight: True
model.layers.15.self_attn.v_proj.weight: True
model.layers.15.self_attn.o_proj.weight: True
model.layers.15.mlp.gate_proj.weight: True
model.layers.15.mlp.up_proj.weight: True
model.layers.15.mlp.down_proj.weight: True
model.layers.15.input_layernorm.weight: True
model.layers.15.post_attention_layernorm.weight: True
model.layers.16.self_attn.q_proj.weight: True
model.layers.16.self_attn.k_proj.weight: True
model.layers.16.self_attn.v_proj.weight: True
model.layers.16.self_attn.o_proj.weight: True
model.layers.16.mlp.gate_proj.weight: True
model.layers.16.mlp.up_proj.weight: True
model.layers.16.mlp.down_proj.weight: True
model.layers.16.input_layernorm.weight: True
model.layers.16.post_attention_layernorm.weight: True
model.layers.17.self_attn.q_proj.weight: True
model.layers.17.self_attn.k_proj.weight: True
model.layers.17.self_attn.v_proj.weight: True
model.layers.17.self_attn.o_proj.weight: True
model.layers.17.mlp.gate_proj.weight: True
model.layers.17.mlp.up_proj.weight: True
model.layers.17.mlp.down_proj.weight: True
model.layers.17.input_layernorm.weight: True
model.layers.17.post_attention_layernorm.weight: True
model.layers.18.self_attn.q_proj.weight: True
model.layers.18.self_attn.k_proj.weight: True
model.layers.18.self_attn.v_proj.weight: True
model.layers.18.self_attn.o_proj.weight: True
model.layers.18.mlp.gate_proj.weight: True
model.layers.18.mlp.up_proj.weight: True
model.layers.18.mlp.down_proj.weight: True
model.layers.18.input_layernorm.weight: True
model.layers.18.post_attention_layernorm.weight: True
model.layers.19.self_attn.q_proj.weight: True
model.layers.19.self_attn.k_proj.weight: True
model.layers.19.self_attn.v_proj.weight: True
model.layers.19.self_attn.o_proj.weight: True
model.layers.19.mlp.gate_proj.weight: True
model.layers.19.mlp.up_proj.weight: True
model.layers.19.mlp.down_proj.weight: True
model.layers.19.input_layernorm.weight: True
model.layers.19.post_attention_layernorm.weight: True
model.layers.20.self_attn.q_proj.weight: True
model.layers.20.self_attn.k_proj.weight: True
model.layers.20.self_attn.v_proj.weight: True
model.layers.20.self_attn.o_proj.weight: True
model.layers.20.mlp.gate_proj.weight: True
model.layers.20.mlp.up_proj.weight: True
model.layers.20.mlp.down_proj.weight: True
model.layers.20.input_layernorm.weight: True
model.layers.20.post_attention_layernorm.weight: True
model.layers.21.self_attn.q_proj.weight: True
model.layers.21.self_attn.k_proj.weight: True
model.layers.21.self_attn.v_proj.weight: True
model.layers.21.self_attn.o_proj.weight: True
model.layers.21.mlp.gate_proj.weight: True
model.layers.21.mlp.up_proj.weight: True
model.layers.21.mlp.down_proj.weight: True
model.layers.21.input_layernorm.weight: True
model.layers.21.post_attention_layernorm.weight: True
model.layers.22.self_attn.q_proj.weight: True
model.layers.22.self_attn.k_proj.weight: True
model.layers.22.self_attn.v_proj.weight: True
model.layers.22.self_attn.o_proj.weight: True
model.layers.22.mlp.gate_proj.weight: True
model.layers.22.mlp.up_proj.weight: True
model.layers.22.mlp.down_proj.weight: True
model.layers.22.input_layernorm.weight: True
model.layers.22.post_attention_layernorm.weight: True
model.layers.23.self_attn.q_proj.weight: True
model.layers.23.self_attn.k_proj.weight: True
model.layers.23.self_attn.v_proj.weight: True
model.layers.23.self_attn.o_proj.weight: True
model.layers.23.mlp.gate_proj.weight: True
model.layers.23.mlp.up_proj.weight: True
model.layers.23.mlp.down_proj.weight: True
model.layers.23.input_layernorm.weight: True
model.layers.23.post_attention_layernorm.weight: True
model.layers.24.self_attn.q_proj.weight: True
model.layers.24.self_attn.k_proj.weight: True
model.layers.24.self_attn.v_proj.weight: True
model.layers.24.self_attn.o_proj.weight: True
model.layers.24.mlp.gate_proj.weight: True
model.layers.24.mlp.up_proj.weight: True
model.layers.24.mlp.down_proj.weight: True
model.layers.24.input_layernorm.weight: True
model.layers.24.post_attention_layernorm.weight: True
model.layers.25.self_attn.q_proj.weight: True
model.layers.25.self_attn.k_proj.weight: True
model.layers.25.self_attn.v_proj.weight: True
model.layers.25.self_attn.o_proj.weight: True
model.layers.25.mlp.gate_proj.weight: True
model.layers.25.mlp.up_proj.weight: True
model.layers.25.mlp.down_proj.weight: True
model.layers.25.input_layernorm.weight: True
model.layers.25.post_attention_layernorm.weight: True
model.layers.26.self_attn.q_proj.weight: True
model.layers.26.self_attn.k_proj.weight: True
model.layers.26.self_attn.v_proj.weight: True
model.layers.26.self_attn.o_proj.weight: True
model.layers.26.mlp.gate_proj.weight: True
model.layers.26.mlp.up_proj.weight: True
model.layers.26.mlp.down_proj.weight: True
model.layers.26.input_layernorm.weight: True
model.layers.26.post_attention_layernorm.weight: True
model.layers.27.self_attn.q_proj.weight: True
model.layers.27.self_attn.k_proj.weight: True
model.layers.27.self_attn.v_proj.weight: True
model.layers.27.self_attn.o_proj.weight: True
model.layers.27.mlp.gate_proj.weight: True
model.layers.27.mlp.up_proj.weight: True
model.layers.27.mlp.down_proj.weight: True
model.layers.27.input_layernorm.weight: True
model.layers.27.post_attention_layernorm.weight: True
model.layers.28.self_attn.q_proj.weight: True
model.layers.28.self_attn.k_proj.weight: True
model.layers.28.self_attn.v_proj.weight: True
model.layers.28.self_attn.o_proj.weight: True
model.layers.28.mlp.gate_proj.weight: True
model.layers.28.mlp.up_proj.weight: True
model.layers.28.mlp.down_proj.weight: True
model.layers.28.input_layernorm.weight: True
model.layers.28.post_attention_layernorm.weight: True
model.layers.29.self_attn.q_proj.weight: True
model.layers.29.self_attn.k_proj.weight: True
model.layers.29.self_attn.v_proj.weight: True
model.layers.29.self_attn.o_proj.weight: True
model.layers.29.mlp.gate_proj.weight: True
model.layers.29.mlp.up_proj.weight: True
model.layers.29.mlp.down_proj.weight: True
model.layers.29.input_layernorm.weight: True
model.layers.29.post_attention_layernorm.weight: True
model.layers.30.self_attn.q_proj.weight: True
model.layers.30.self_attn.k_proj.weight: True
model.layers.30.self_attn.v_proj.weight: True
model.layers.30.self_attn.o_proj.weight: True
model.layers.30.mlp.gate_proj.weight: True
model.layers.30.mlp.up_proj.weight: True
model.layers.30.mlp.down_proj.weight: True
model.layers.30.input_layernorm.weight: True
model.layers.30.post_attention_layernorm.weight: True
model.layers.31.self_attn.q_proj.weight: True
model.layers.31.self_attn.k_proj.weight: True
model.layers.31.self_attn.v_proj.weight: True
model.layers.31.self_attn.o_proj.weight: True
model.layers.31.mlp.gate_proj.weight: True
model.layers.31.mlp.up_proj.weight: True
model.layers.31.mlp.down_proj.weight: True
model.layers.31.input_layernorm.weight: True
model.layers.31.post_attention_layernorm.weight: True
model.norm.weight: True
model.vision_tower.vision_tower.vision_model.embeddings.class_embedding: False
model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight: False
model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.post_layernorm.weight: False
model.vision_tower.vision_tower.vision_model.post_layernorm.bias: False
model.mm_projector.0.weight: True
model.mm_projector.0.bias: True
model.mm_projector.2.weight: True
model.mm_projector.2.bias: True
model.granular_mm_projector.0.weight: True
model.granular_mm_projector.0.bias: True
model.granular_mm_projector.2.weight: True
model.granular_mm_projector.2.bias: True
lm_head.weight: True
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
model.embed_tokens.weight: True
model.layers.0.self_attn.q_proj.weight: True
model.layers.0.self_attn.k_proj.weight: True
model.layers.0.self_attn.v_proj.weight: True
model.layers.0.self_attn.o_proj.weight: True
model.layers.0.mlp.gate_proj.weight: True
model.layers.0.mlp.up_proj.weight: True
model.layers.0.mlp.down_proj.weight: True
model.layers.0.input_layernorm.weight: True
model.layers.0.post_attention_layernorm.weight: True
model.layers.1.self_attn.q_proj.weight: True
model.layers.1.self_attn.k_proj.weight: True
model.layers.1.self_attn.v_proj.weight: True
model.layers.1.self_attn.o_proj.weight: True
model.layers.1.mlp.gate_proj.weight: True
model.layers.1.mlp.up_proj.weight: True
model.layers.1.mlp.down_proj.weight: True
model.layers.1.input_layernorm.weight: True
model.layers.1.post_attention_layernorm.weight: True
model.layers.2.self_attn.q_proj.weight: True
model.layers.2.self_attn.k_proj.weight: True
model.layers.2.self_attn.v_proj.weight: True
model.layers.2.self_attn.o_proj.weight: True
model.layers.2.mlp.gate_proj.weight: True
model.layers.2.mlp.up_proj.weight: True
model.layers.2.mlp.down_proj.weight: True
model.layers.2.input_layernorm.weight: True
model.layers.2.post_attention_layernorm.weight: True
model.layers.3.self_attn.q_proj.weight: True
model.layers.3.self_attn.k_proj.weight: True
model.layers.3.self_attn.v_proj.weight: True
model.layers.3.self_attn.o_proj.weight: True
model.layers.3.mlp.gate_proj.weight: True
model.layers.3.mlp.up_proj.weight: True
model.layers.3.mlp.down_proj.weight: True
model.layers.3.input_layernorm.weight: True
model.layers.3.post_attention_layernorm.weight: True
model.layers.4.self_attn.q_proj.weight: True
model.layers.4.self_attn.k_proj.weight: True
model.layers.4.self_attn.v_proj.weight: True
model.layers.4.self_attn.o_proj.weight: True
model.layers.4.mlp.gate_proj.weight: True
model.layers.4.mlp.up_proj.weight: True
model.layers.4.mlp.down_proj.weight: True
model.layers.4.input_layernorm.weight: True
model.layers.4.post_attention_layernorm.weight: True
model.layers.5.self_attn.q_proj.weight: True
model.layers.5.self_attn.k_proj.weight: True
model.layers.5.self_attn.v_proj.weight: True
model.layers.5.self_attn.o_proj.weight: True
model.layers.5.mlp.gate_proj.weight: True
model.layers.5.mlp.up_proj.weight: True
model.layers.5.mlp.down_proj.weight: True
model.layers.5.input_layernorm.weight: True
model.layers.5.post_attention_layernorm.weight: True
model.layers.6.self_attn.q_proj.weight: True
model.layers.6.self_attn.k_proj.weight: True
model.layers.6.self_attn.v_proj.weight: True
model.layers.6.self_attn.o_proj.weight: True
model.layers.6.mlp.gate_proj.weight: True
model.layers.6.mlp.up_proj.weight: True
model.layers.6.mlp.down_proj.weight: True
model.layers.6.input_layernorm.weight: True
model.layers.6.post_attention_layernorm.weight: True
model.layers.7.self_attn.q_proj.weight: True
model.layers.7.self_attn.k_proj.weight: True
model.layers.7.self_attn.v_proj.weight: True
model.layers.7.self_attn.o_proj.weight: True
model.layers.7.mlp.gate_proj.weight: True
model.layers.7.mlp.up_proj.weight: True
model.layers.7.mlp.down_proj.weight: True
model.layers.7.input_layernorm.weight: True
model.layers.7.post_attention_layernorm.weight: True
model.layers.8.self_attn.q_proj.weight: True
model.layers.8.self_attn.k_proj.weight: True
model.layers.8.self_attn.v_proj.weight: True
model.layers.8.self_attn.o_proj.weight: True
model.layers.8.mlp.gate_proj.weight: True
model.layers.8.mlp.up_proj.weight: True
model.layers.8.mlp.down_proj.weight: True
model.layers.8.input_layernorm.weight: True
model.layers.8.post_attention_layernorm.weight: True
model.layers.9.self_attn.q_proj.weight: True
model.layers.9.self_attn.k_proj.weight: True
model.layers.9.self_attn.v_proj.weight: True
model.layers.9.self_attn.o_proj.weight: True
model.layers.9.mlp.gate_proj.weight: True
model.layers.9.mlp.up_proj.weight: True
model.layers.9.mlp.down_proj.weight: True
model.layers.9.input_layernorm.weight: True
model.layers.9.post_attention_layernorm.weight: True
model.layers.10.self_attn.q_proj.weight: True
model.layers.10.self_attn.k_proj.weight: True
model.layers.10.self_attn.v_proj.weight: True
model.layers.10.self_attn.o_proj.weight: True
model.layers.10.mlp.gate_proj.weight: True
model.layers.10.mlp.up_proj.weight: True
model.layers.10.mlp.down_proj.weight: True
model.layers.10.input_layernorm.weight: True
model.layers.10.post_attention_layernorm.weight: True
model.layers.11.self_attn.q_proj.weight: True
model.layers.11.self_attn.k_proj.weight: True
model.layers.11.self_attn.v_proj.weight: True
model.layers.11.self_attn.o_proj.weight: True
model.layers.11.mlp.gate_proj.weight: True
model.layers.11.mlp.up_proj.weight: True
model.layers.11.mlp.down_proj.weight: True
model.layers.11.input_layernorm.weight: True
model.layers.11.post_attention_layernorm.weight: True
model.layers.12.self_attn.q_proj.weight: True
model.layers.12.self_attn.k_proj.weight: True
model.layers.12.self_attn.v_proj.weight: True
model.layers.12.self_attn.o_proj.weight: True
model.layers.12.mlp.gate_proj.weight: True
model.layers.12.mlp.up_proj.weight: True
model.layers.12.mlp.down_proj.weight: True
model.layers.12.input_layernorm.weight: True
model.layers.12.post_attention_layernorm.weight: True
model.layers.13.self_attn.q_proj.weight: True
model.layers.13.self_attn.k_proj.weight: True
model.layers.13.self_attn.v_proj.weight: True
model.layers.13.self_attn.o_proj.weight: True
model.layers.13.mlp.gate_proj.weight: True
model.layers.13.mlp.up_proj.weight: True
model.layers.13.mlp.down_proj.weight: True
model.layers.13.input_layernorm.weight: True
model.layers.13.post_attention_layernorm.weight: True
model.layers.14.self_attn.q_proj.weight: True
model.layers.14.self_attn.k_proj.weight: True
model.layers.14.self_attn.v_proj.weight: True
model.layers.14.self_attn.o_proj.weight: True
model.layers.14.mlp.gate_proj.weight: True
model.layers.14.mlp.up_proj.weight: True
model.layers.14.mlp.down_proj.weight: True
model.layers.14.input_layernorm.weight: True
model.layers.14.post_attention_layernorm.weight: True
model.layers.15.self_attn.q_proj.weight: True
model.layers.15.self_attn.k_proj.weight: True
model.layers.15.self_attn.v_proj.weight: True
model.layers.15.self_attn.o_proj.weight: True
model.layers.15.mlp.gate_proj.weight: True
model.layers.15.mlp.up_proj.weight: True
model.layers.15.mlp.down_proj.weight: True
model.layers.15.input_layernorm.weight: True
model.layers.15.post_attention_layernorm.weight: True
model.layers.16.self_attn.q_proj.weight: True
model.layers.16.self_attn.k_proj.weight: True
model.layers.16.self_attn.v_proj.weight: True
model.layers.16.self_attn.o_proj.weight: True
model.layers.16.mlp.gate_proj.weight: True
model.layers.16.mlp.up_proj.weight: True
model.layers.16.mlp.down_proj.weight: True
model.layers.16.input_layernorm.weight: True
model.layers.16.post_attention_layernorm.weight: True
model.layers.17.self_attn.q_proj.weight: True
model.layers.17.self_attn.k_proj.weight: True
model.layers.17.self_attn.v_proj.weight: True
model.layers.17.self_attn.o_proj.weight: True
model.layers.17.mlp.gate_proj.weight: True
model.layers.17.mlp.up_proj.weight: True
model.layers.17.mlp.down_proj.weight: True
model.layers.17.input_layernorm.weight: True
model.layers.17.post_attention_layernorm.weight: True
model.layers.18.self_attn.q_proj.weight: True
model.layers.18.self_attn.k_proj.weight: True
model.layers.18.self_attn.v_proj.weight: True
model.layers.18.self_attn.o_proj.weight: True
model.layers.18.mlp.gate_proj.weight: True
model.layers.18.mlp.up_proj.weight: True
model.layers.18.mlp.down_proj.weight: True
model.layers.18.input_layernorm.weight: True
model.layers.18.post_attention_layernorm.weight: True
model.layers.19.self_attn.q_proj.weight: True
model.layers.19.self_attn.k_proj.weight: True
model.layers.19.self_attn.v_proj.weight: True
model.layers.19.self_attn.o_proj.weight: True
model.layers.19.mlp.gate_proj.weight: True
model.layers.19.mlp.up_proj.weight: True
model.layers.19.mlp.down_proj.weight: True
model.layers.19.input_layernorm.weight: True
model.layers.19.post_attention_layernorm.weight: True
model.layers.20.self_attn.q_proj.weight: True
model.layers.20.self_attn.k_proj.weight: True
model.layers.20.self_attn.v_proj.weight: True
model.layers.20.self_attn.o_proj.weight: True
model.layers.20.mlp.gate_proj.weight: True
model.layers.20.mlp.up_proj.weight: True
model.layers.20.mlp.down_proj.weight: True
model.layers.20.input_layernorm.weight: True
model.layers.20.post_attention_layernorm.weight: True
model.layers.21.self_attn.q_proj.weight: True
model.layers.21.self_attn.k_proj.weight: True
model.layers.21.self_attn.v_proj.weight: True
model.layers.21.self_attn.o_proj.weight: True
model.layers.21.mlp.gate_proj.weight: True
model.layers.21.mlp.up_proj.weight: True
model.layers.21.mlp.down_proj.weight: True
model.layers.21.input_layernorm.weight: True
model.layers.21.post_attention_layernorm.weight: True
model.layers.22.self_attn.q_proj.weight: True
model.layers.22.self_attn.k_proj.weight: True
model.layers.22.self_attn.v_proj.weight: True
model.layers.22.self_attn.o_proj.weight: True
model.layers.22.mlp.gate_proj.weight: True
model.layers.22.mlp.up_proj.weight: True
model.layers.22.mlp.down_proj.weight: True
model.layers.22.input_layernorm.weight: True
model.layers.22.post_attention_layernorm.weight: True
model.layers.23.self_attn.q_proj.weight: True
model.layers.23.self_attn.k_proj.weight: True
model.layers.23.self_attn.v_proj.weight: True
model.layers.23.self_attn.o_proj.weight: True
model.layers.23.mlp.gate_proj.weight: True
model.layers.23.mlp.up_proj.weight: True
model.layers.23.mlp.down_proj.weight: True
model.layers.23.input_layernorm.weight: True
model.layers.23.post_attention_layernorm.weight: True
model.layers.24.self_attn.q_proj.weight: True
model.layers.24.self_attn.k_proj.weight: True
model.layers.24.self_attn.v_proj.weight: True
model.layers.24.self_attn.o_proj.weight: True
model.layers.24.mlp.gate_proj.weight: True
model.layers.24.mlp.up_proj.weight: True
model.layers.24.mlp.down_proj.weight: True
model.layers.24.input_layernorm.weight: True
model.layers.24.post_attention_layernorm.weight: True
model.layers.25.self_attn.q_proj.weight: True
model.layers.25.self_attn.k_proj.weight: True
model.layers.25.self_attn.v_proj.weight: True
model.layers.25.self_attn.o_proj.weight: True
model.layers.25.mlp.gate_proj.weight: True
model.layers.25.mlp.up_proj.weight: True
model.layers.25.mlp.down_proj.weight: True
model.layers.25.input_layernorm.weight: True
model.layers.25.post_attention_layernorm.weight: True
model.layers.26.self_attn.q_proj.weight: True
model.layers.26.self_attn.k_proj.weight: True
model.layers.26.self_attn.v_proj.weight: True
model.layers.26.self_attn.o_proj.weight: True
model.layers.26.mlp.gate_proj.weight: True
model.layers.26.mlp.up_proj.weight: True
model.layers.26.mlp.down_proj.weight: True
model.layers.26.input_layernorm.weight: True
model.layers.26.post_attention_layernorm.weight: True
model.layers.27.self_attn.q_proj.weight: True
model.layers.27.self_attn.k_proj.weight: True
model.layers.27.self_attn.v_proj.weight: True
model.layers.27.self_attn.o_proj.weight: True
model.layers.27.mlp.gate_proj.weight: True
model.layers.27.mlp.up_proj.weight: True
model.layers.27.mlp.down_proj.weight: True
model.layers.27.input_layernorm.weight: True
model.layers.27.post_attention_layernorm.weight: True
model.layers.28.self_attn.q_proj.weight: True
model.layers.28.self_attn.k_proj.weight: True
model.layers.28.self_attn.v_proj.weight: True
model.layers.28.self_attn.o_proj.weight: True
model.layers.28.mlp.gate_proj.weight: True
model.layers.28.mlp.up_proj.weight: True
model.layers.28.mlp.down_proj.weight: True
model.layers.28.input_layernorm.weight: True
model.layers.28.post_attention_layernorm.weight: True
model.layers.29.self_attn.q_proj.weight: True
model.layers.29.self_attn.k_proj.weight: True
model.layers.29.self_attn.v_proj.weight: True
model.layers.29.self_attn.o_proj.weight: True
model.layers.29.mlp.gate_proj.weight: True
model.layers.29.mlp.up_proj.weight: True
model.layers.29.mlp.down_proj.weight: True
model.layers.29.input_layernorm.weight: True
model.layers.29.post_attention_layernorm.weight: True
model.layers.30.self_attn.q_proj.weight: True
model.layers.30.self_attn.k_proj.weight: True
model.layers.30.self_attn.v_proj.weight: True
model.layers.30.self_attn.o_proj.weight: True
model.layers.30.mlp.gate_proj.weight: True
model.layers.30.mlp.up_proj.weight: True
model.layers.30.mlp.down_proj.weight: True
model.layers.30.input_layernorm.weight: True
model.layers.30.post_attention_layernorm.weight: True
model.layers.31.self_attn.q_proj.weight: True
model.layers.31.self_attn.k_proj.weight: True
model.layers.31.self_attn.v_proj.weight: True
model.layers.31.self_attn.o_proj.weight: True
model.layers.31.mlp.gate_proj.weight: True
model.layers.31.mlp.up_proj.weight: True
model.layers.31.mlp.down_proj.weight: True
model.layers.31.input_layernorm.weight: True
model.layers.31.post_attention_layernorm.weight: True
model.norm.weight: True
model.vision_tower.vision_tower.vision_model.embeddings.class_embedding: False
model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight: False
model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight: False
model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight: False
model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias: False
model.vision_tower.vision_tower.vision_model.post_layernorm.weight: False
model.vision_tower.vision_tower.vision_model.post_layernorm.bias: False
model.mm_projector.0.weight: True
model.mm_projector.0.bias: True
model.mm_projector.2.weight: True
model.mm_projector.2.bias: True
model.granular_mm_projector.0.weight: True
model.granular_mm_projector.0.bias: True
model.granular_mm_projector.2.weight: True
model.granular_mm_projector.2.bias: True
lm_head.weight: True
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 607232 in 314 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240219_183553-jl7z662b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grllava-7b-it-dryrun
wandb: ⭐️ View project at https://wandb.ai/compyle/LLaVA-llava_train
wandb: 🚀 View run at https://wandb.ai/compyle/LLaVA-llava_train/runs/jl7z662b
  0%|          | 0/6 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2024-02-19 18:36:23,934] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1811261
[2024-02-19 18:36:24,764] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1811262
[2024-02-19 18:36:25,801] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1811263
[2024-02-19 18:36:26,746] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1811264
[2024-02-19 18:36:27,747] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
