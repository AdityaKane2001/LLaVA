
================================================= Thu Apr 11 06:53:33 AM UTC 2024 =========================================================

[2024-04-11 02:53:35,560] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:38,135] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 02:53:38,136] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 02:53:40,093] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:41,876] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 02:53:41,876] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 02:53:41,876] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 02:53:41,876] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 02:53:41,876] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 02:53:45,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:45,617] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:45,623] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:45,691] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:53:46,747] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:53:46,747] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 02:53:46,771] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:53:46,809] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:53:46,999] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.05s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.96s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.63s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.29s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.40s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Formatting inputs...Skip in lazy mode
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240411_025506-7y7bgv1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mve-clip-dino-pretrain-7b
wandb: â­ï¸ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: ðŸš€ View run at https://wandb.ai/compyle/multi-ve-llava/runs/7y7bgv1j
  0%|          | 0/2180 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2180 [00:21<13:05:44, 21.64s/it]                                                   {'loss': 8.1981, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:21<13:05:44, 21.64s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 2/2180 [00:31<8:44:14, 14.44s/it]                                                   {'loss': 8.1258, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:31<8:44:14, 14.44s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 3/2180 [00:40<7:22:36, 12.20s/it]                                                  {'loss': 7.3409, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [00:40<7:22:36, 12.20s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 4/2180 [00:50<6:44:12, 11.15s/it]                                                  {'loss': 6.6819, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [00:50<6:44:12, 11.15s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 5/2180 [00:59<6:22:15, 10.54s/it]                                                  {'loss': 5.8024, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [00:59<6:22:15, 10.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 6/2180 [01:09<6:08:49, 10.18s/it]                                                  {'loss': 5.5418, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [01:09<6:08:49, 10.18s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 7/2180 [01:18<6:00:33,  9.96s/it]                                                  {'loss': 5.3796, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2180 [01:18<6:00:33,  9.96s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 8/2180 [01:28<5:55:10,  9.81s/it]                                                  {'loss': 5.2359, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2180 [01:28<5:55:10,  9.81s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 9/2180 [01:37<5:51:14,  9.71s/it]                                                  {'loss': 5.1084, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2180 [01:37<5:51:14,  9.71s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 10/2180 [01:47<5:48:30,  9.64s/it]                                                   {'loss': 4.8765, 'learning_rate': 0.00015151515151515152, 'epoch': 0.0}
  0%|          | 10/2180 [01:47<5:48:30,  9.64s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 11/2180 [01:56<5:46:58,  9.60s/it]                                                   {'loss': 4.7344, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 11/2180 [01:56<5:46:58,  9.60s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 12/2180 [02:05<5:45:27,  9.56s/it]                                                   {'loss': 4.5862, 'learning_rate': 0.00018181818181818183, 'epoch': 0.01}
  1%|          | 12/2180 [02:05<5:45:27,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 13/2180 [02:15<5:44:19,  9.53s/it]                                                   {'loss': 4.5761, 'learning_rate': 0.00019696969696969695, 'epoch': 0.01}
  1%|          | 13/2180 [02:15<5:44:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

  1%|          | 14/2180 [02:24<5:43:41,  9.52s/it]                                                   {'loss': 4.4759, 'learning_rate': 0.00021212121212121213, 'epoch': 0.01}
  1%|          | 14/2180 [02:24<5:43:41,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 15/2180 [02:34<5:43:03,  9.51s/it]                                                   {'loss': 4.413, 'learning_rate': 0.00022727272727272727, 'epoch': 0.01}
  1%|          | 15/2180 [02:34<5:43:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 16/2180 [02:43<5:43:13,  9.52s/it]                                                   {'loss': 4.2649, 'learning_rate': 0.00024242424242424245, 'epoch': 0.01}
  1%|          | 16/2180 [02:43<5:43:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 17/2180 [02:53<5:42:37,  9.50s/it]                                                   {'loss': 4.3349, 'learning_rate': 0.00025757575757575756, 'epoch': 0.01}
  1%|          | 17/2180 [02:53<5:42:37,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 18/2180 [03:02<5:42:17,  9.50s/it]                                                   {'loss': 4.2208, 'learning_rate': 0.00027272727272727274, 'epoch': 0.01}
  1%|          | 18/2180 [03:02<5:42:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 19/2180 [03:12<5:41:48,  9.49s/it]                                                   {'loss': 4.1715, 'learning_rate': 0.0002878787878787879, 'epoch': 0.01}
  1%|          | 19/2180 [03:12<5:41:48,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 20/2180 [03:22<5:47:05,  9.64s/it]                                                   {'loss': 3.9521, 'learning_rate': 0.00030303030303030303, 'epoch': 0.01}
  1%|          | 20/2180 [03:22<5:47:05,  9.64s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 21/2180 [03:31<5:45:20,  9.60s/it]                                                   {'loss': 4.0505, 'learning_rate': 0.0003181818181818182, 'epoch': 0.01}
  1%|          | 21/2180 [03:31<5:45:20,  9.60s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 22/2180 [03:41<5:45:12,  9.60s/it]                                                   {'loss': 3.8612, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 22/2180 [03:41<5:45:12,  9.60s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 23/2180 [03:51<5:44:14,  9.58s/it]                                                   {'loss': 3.7673, 'learning_rate': 0.0003484848484848485, 'epoch': 0.01}
  1%|          | 23/2180 [03:51<5:44:14,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 24/2180 [04:00<5:43:04,  9.55s/it]                                                   {'loss': 3.8977, 'learning_rate': 0.00036363636363636367, 'epoch': 0.01}
  1%|          | 24/2180 [04:00<5:43:04,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 25/2180 [04:09<5:42:03,  9.52s/it]                                                   {'loss': 3.7882, 'learning_rate': 0.0003787878787878788, 'epoch': 0.01}
  1%|          | 25/2180 [04:09<5:42:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 26/2180 [04:19<5:41:20,  9.51s/it]                                                   {'loss': 3.6956, 'learning_rate': 0.0003939393939393939, 'epoch': 0.01}
  1%|          | 26/2180 [04:19<5:41:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|          | 27/2180 [04:28<5:41:27,  9.52s/it]                                                   {'loss': 3.7652, 'learning_rate': 0.00040909090909090913, 'epoch': 0.01}
  1%|          | 27/2180 [04:28<5:41:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  1%|â–         | 28/2180 [04:38<5:41:10,  9.51s/it]                                                   {'loss': 3.7218, 'learning_rate': 0.00042424242424242425, 'epoch': 0.01}
  1%|â–         | 28/2180 [04:38<5:41:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|â–         | 29/2180 [04:48<5:42:01,  9.54s/it]                                                   {'loss': 3.595, 'learning_rate': 0.0004393939393939394, 'epoch': 0.01}
  1%|â–         | 29/2180 [04:48<5:42:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|â–         | 30/2180 [04:57<5:40:59,  9.52s/it]                                                   {'loss': 3.6498, 'learning_rate': 0.00045454545454545455, 'epoch': 0.01}
  1%|â–         | 30/2180 [04:57<5:40:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|â–         | 31/2180 [05:07<5:40:55,  9.52s/it]                                                   {'loss': 3.5716, 'learning_rate': 0.0004696969696969697, 'epoch': 0.01}
  1%|â–         | 31/2180 [05:07<5:40:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  1%|â–         | 32/2180 [05:16<5:40:20,  9.51s/it]                                                   {'loss': 3.5009, 'learning_rate': 0.0004848484848484849, 'epoch': 0.01}
  1%|â–         | 32/2180 [05:16<5:40:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 33/2180 [05:26<5:39:58,  9.50s/it]                                                   {'loss': 3.4531, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|â–         | 33/2180 [05:26<5:39:58,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 34/2180 [05:35<5:39:28,  9.49s/it]                                                   {'loss': 3.4881, 'learning_rate': 0.0005151515151515151, 'epoch': 0.02}
  2%|â–         | 34/2180 [05:35<5:39:28,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 35/2180 [05:44<5:38:58,  9.48s/it]                                                   {'loss': 3.4082, 'learning_rate': 0.0005303030303030302, 'epoch': 0.02}
  2%|â–         | 35/2180 [05:44<5:38:58,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 36/2180 [05:54<5:38:41,  9.48s/it]                                                   {'loss': 3.3697, 'learning_rate': 0.0005454545454545455, 'epoch': 0.02}
  2%|â–         | 36/2180 [05:54<5:38:41,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 37/2180 [06:03<5:38:37,  9.48s/it]                                                   {'loss': 3.4225, 'learning_rate': 0.0005606060606060606, 'epoch': 0.02}
  2%|â–         | 37/2180 [06:03<5:38:37,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 38/2180 [06:13<5:38:25,  9.48s/it]                                                   {'loss': 3.4674, 'learning_rate': 0.0005757575757575758, 'epoch': 0.02}
  2%|â–         | 38/2180 [06:13<5:38:25,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 39/2180 [06:22<5:38:10,  9.48s/it]                                                   {'loss': 3.5024, 'learning_rate': 0.0005909090909090909, 'epoch': 0.02}
  2%|â–         | 39/2180 [06:22<5:38:10,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 40/2180 [06:32<5:38:27,  9.49s/it]                                                   {'loss': 3.4306, 'learning_rate': 0.0006060606060606061, 'epoch': 0.02}
  2%|â–         | 40/2180 [06:32<5:38:27,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 41/2180 [06:41<5:38:31,  9.50s/it]                                                   {'loss': 3.2912, 'learning_rate': 0.0006212121212121212, 'epoch': 0.02}
  2%|â–         | 41/2180 [06:41<5:38:31,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 42/2180 [06:51<5:38:15,  9.49s/it]                                                   {'loss': 3.3984, 'learning_rate': 0.0006363636363636364, 'epoch': 0.02}
  2%|â–         | 42/2180 [06:51<5:38:15,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 43/2180 [07:00<5:37:42,  9.48s/it]                                                   {'loss': 3.3452, 'learning_rate': 0.0006515151515151515, 'epoch': 0.02}
  2%|â–         | 43/2180 [07:00<5:37:42,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 44/2180 [07:10<5:37:46,  9.49s/it]                                                   {'loss': 3.2628, 'learning_rate': 0.0006666666666666666, 'epoch': 0.02}
  2%|â–         | 44/2180 [07:10<5:37:46,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 45/2180 [07:19<5:37:31,  9.49s/it]                                                   {'loss': 3.184, 'learning_rate': 0.0006818181818181818, 'epoch': 0.02}
  2%|â–         | 45/2180 [07:19<5:37:31,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 46/2180 [07:29<5:37:12,  9.48s/it]                                                   {'loss': 3.3002, 'learning_rate': 0.000696969696969697, 'epoch': 0.02}
  2%|â–         | 46/2180 [07:29<5:37:12,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])



  2%|â–         | 47/2180 [07:38<5:37:55,  9.51s/it]                                                   {'loss': 3.3113, 'learning_rate': 0.0007121212121212122, 'epoch': 0.02}
  2%|â–         | 47/2180 [07:38<5:37:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 48/2180 [07:48<5:37:50,  9.51s/it]                                                   {'loss': 3.2937, 'learning_rate': 0.0007272727272727273, 'epoch': 0.02}
  2%|â–         | 48/2180 [07:48<5:37:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 49/2180 [07:57<5:38:23,  9.53s/it]                                                   {'loss': 3.2942, 'learning_rate': 0.0007424242424242425, 'epoch': 0.02}
  2%|â–         | 49/2180 [07:57<5:38:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 50/2180 [08:07<5:37:26,  9.51s/it]                                                   {'loss': 3.3498, 'learning_rate': 0.0007575757575757576, 'epoch': 0.02}
  2%|â–         | 50/2180 [08:07<5:37:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 51/2180 [08:16<5:37:03,  9.50s/it]                                                   {'loss': 3.2594, 'learning_rate': 0.0007727272727272727, 'epoch': 0.02}
  2%|â–         | 51/2180 [08:16<5:37:03,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 52/2180 [08:26<5:36:30,  9.49s/it]                                                   {'loss': 3.293, 'learning_rate': 0.0007878787878787878, 'epoch': 0.02}
  2%|â–         | 52/2180 [08:26<5:36:30,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 53/2180 [08:35<5:36:20,  9.49s/it]                                                   {'loss': 3.2175, 'learning_rate': 0.000803030303030303, 'epoch': 0.02}
  2%|â–         | 53/2180 [08:35<5:36:20,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  2%|â–         | 54/2180 [08:45<5:35:53,  9.48s/it]                                                   {'loss': 3.2643, 'learning_rate': 0.0008181818181818183, 'epoch': 0.02}
  2%|â–         | 54/2180 [08:45<5:35:53,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 55/2180 [08:54<5:35:45,  9.48s/it]                                                   {'loss': 3.2951, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}
  3%|â–Ž         | 55/2180 [08:54<5:35:45,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 56/2180 [09:04<5:36:17,  9.50s/it]                                                   {'loss': 3.3392, 'learning_rate': 0.0008484848484848485, 'epoch': 0.03}
  3%|â–Ž         | 56/2180 [09:04<5:36:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 57/2180 [09:13<5:36:22,  9.51s/it]                                                   {'loss': 3.2229, 'learning_rate': 0.0008636363636363636, 'epoch': 0.03}Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

  3%|â–Ž         | 57/2180 [09:13<5:36:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 58/2180 [09:23<5:36:36,  9.52s/it]                                                   {'loss': 3.2625, 'learning_rate': 0.0008787878787878789, 'epoch': 0.03}
  3%|â–Ž         | 58/2180 [09:23<5:36:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 59/2180 [09:32<5:36:32,  9.52s/it]                                                   {'loss': 3.1849, 'learning_rate': 0.000893939393939394, 'epoch': 0.03}
  3%|â–Ž         | 59/2180 [09:32<5:36:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 60/2180 [09:42<5:36:22,  9.52s/it]                                                   {'loss': 3.2367, 'learning_rate': 0.0009090909090909091, 'epoch': 0.03}
  3%|â–Ž         | 60/2180 [09:42<5:36:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 61/2180 [09:51<5:36:08,  9.52s/it]                                                   {'loss': 3.1867, 'learning_rate': 0.0009242424242424242, 'epoch': 0.03}
  3%|â–Ž         | 61/2180 [09:51<5:36:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 62/2180 [10:01<5:36:00,  9.52s/it]                                                   {'loss': 3.166, 'learning_rate': 0.0009393939393939394, 'epoch': 0.03}
  3%|â–Ž         | 62/2180 [10:01<5:36:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  3%|â–Ž         | 63/2180 [10:11<5:36:20,  9.53s/it]                                                   {'loss': 3.2221, 'learning_rate': 0.0009545454545454546, 'epoch': 0.03}
  3%|â–Ž         | 63/2180 [10:11<5:36:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 64/2180 [10:20<5:34:58,  9.50s/it]                                                   {'loss': 3.1117, 'learning_rate': 0.0009696969696969698, 'epoch': 0.03}
  3%|â–Ž         | 64/2180 [10:20<5:34:58,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 65/2180 [10:29<5:35:14,  9.51s/it]                                                   {'loss': 3.1623, 'learning_rate': 0.000984848484848485, 'epoch': 0.03}
  3%|â–Ž         | 65/2180 [10:29<5:35:14,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 66/2180 [10:39<5:34:57,  9.51s/it]                                                   {'loss': 3.076, 'learning_rate': 0.001, 'epoch': 0.03}
  3%|â–Ž         | 66/2180 [10:39<5:34:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 67/2180 [10:48<5:34:45,  9.51s/it]                                                   {'loss': 3.0857, 'learning_rate': 0.0009999994478847943, 'epoch': 0.03}
  3%|â–Ž         | 67/2180 [10:48<5:34:45,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 68/2180 [10:58<5:34:23,  9.50s/it]                                                   {'loss': 3.0976, 'learning_rate': 0.0009999977915403962, 'epoch': 0.03}
  3%|â–Ž         | 68/2180 [10:58<5:34:23,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 69/2180 [11:08<5:34:42,  9.51s/it]                                                   {'loss': 3.039, 'learning_rate': 0.0009999950309704639, 'epoch': 0.03}
  3%|â–Ž         | 69/2180 [11:08<5:34:42,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 70/2180 [11:17<5:34:20,  9.51s/it]                                                   {'loss': 3.1175, 'learning_rate': 0.000999991166181094, 'epoch': 0.03}
  3%|â–Ž         | 70/2180 [11:17<5:34:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 71/2180 [11:26<5:33:59,  9.50s/it]                                                   {'loss': 3.0294, 'learning_rate': 0.0009999861971808216, 'epoch': 0.03}
  3%|â–Ž         | 71/2180 [11:26<5:33:59,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 72/2180 [11:36<5:34:06,  9.51s/it]                                                   {'loss': 3.0724, 'learning_rate': 0.0009999801239806208, 'epoch': 0.03}
  3%|â–Ž         | 72/2180 [11:36<5:34:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 73/2180 [11:46<5:33:59,  9.51s/it]                                                   {'loss': 2.9484, 'learning_rate': 0.0009999729465939035, 'epoch': 0.03}
  3%|â–Ž         | 73/2180 [11:46<5:33:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 74/2180 [11:55<5:34:00,  9.52s/it]                                                   {'loss': 3.0917, 'learning_rate': 0.0009999646650365212, 'epoch': 0.03}
  3%|â–Ž         | 74/2180 [11:55<5:34:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 75/2180 [12:05<5:33:56,  9.52s/it]                                                   {'loss': 2.9773, 'learning_rate': 0.0009999552793267634, 'epoch': 0.03}
  3%|â–Ž         | 75/2180 [12:05<5:33:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  3%|â–Ž         | 76/2180 [12:14<5:34:00,  9.52s/it]                                                   {'loss': 3.0241, 'learning_rate': 0.0009999447894853577, 'epoch': 0.03}
  3%|â–Ž         | 76/2180 [12:14<5:34:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–Ž         | 77/2180 [12:24<5:33:33,  9.52s/it]                                                   {'loss': 3.1054, 'learning_rate': 0.0009999331955354708, 'epoch': 0.04}
  4%|â–Ž         | 77/2180 [12:24<5:33:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–Ž         | 78/2180 [12:33<5:33:01,  9.51s/it]                                                   {'loss': 2.8901, 'learning_rate': 0.0009999204975027073, 'epoch': 0.04}
  4%|â–Ž         | 78/2180 [12:33<5:33:01,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–Ž         | 79/2180 [12:43<5:33:53,  9.54s/it]                                                   {'loss': 2.8962, 'learning_rate': 0.0009999066954151103, 'epoch': 0.04}
  4%|â–Ž         | 79/2180 [12:43<5:33:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
wandb: Network error (ReadTimeout), entering retry loop.
  4%|â–Ž         | 80/2180 [12:52<5:33:13,  9.52s/it]                                                   {'loss': 3.0432, 'learning_rate': 0.0009998917893031614, 'epoch': 0.04}
  4%|â–Ž         | 80/2180 [12:52<5:33:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–Ž         | 81/2180 [13:02<5:32:52,  9.52s/it]                                                   {'loss': 3.0612, 'learning_rate': 0.0009998757791997801, 'epoch': 0.04}
  4%|â–Ž         | 81/2180 [13:02<5:32:52,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 82/2180 [13:11<5:33:03,  9.52s/it]                                                   {'loss': 3.0192, 'learning_rate': 0.0009998586651403238, 'epoch': 0.04}
  4%|â–         | 82/2180 [13:11<5:33:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 83/2180 [13:21<5:32:29,  9.51s/it]                                                   {'loss': 2.9305, 'learning_rate': 0.0009998404471625885, 'epoch': 0.04}
  4%|â–         | 83/2180 [13:21<5:32:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])

  4%|â–         | 84/2180 [13:30<5:32:24,  9.52s/it]                                                   {'loss': 2.8443, 'learning_rate': 0.0009998211253068078, 'epoch': 0.04}
  4%|â–         | 84/2180 [13:30<5:32:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 85/2180 [13:40<5:32:21,  9.52s/it]                                                   {'loss': 2.9782, 'learning_rate': 0.0009998006996156535, 'epoch': 0.04}
  4%|â–         | 85/2180 [13:40<5:32:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 86/2180 [13:49<5:32:20,  9.52s/it]                                                   {'loss': 2.8326, 'learning_rate': 0.0009997791701342347, 'epoch': 0.04}
  4%|â–         | 86/2180 [13:49<5:32:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 87/2180 [13:59<5:31:36,  9.51s/it]                                                   {'loss': 2.9335, 'learning_rate': 0.0009997565369100983, 'epoch': 0.04}
  4%|â–         | 87/2180 [13:59<5:31:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  4%|â–         | 88/2180 [14:08<5:32:00,  9.52s/it]                                                   {'loss': 2.8462, 'learning_rate': 0.0009997327999932291, 'epoch': 0.04}
  4%|â–         | 88/2180 [14:08<5:32:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 89/2180 [14:18<5:31:42,  9.52s/it]                                                   {'loss': 2.8833, 'learning_rate': 0.000999707959436049, 'epoch': 0.04}
  4%|â–         | 89/2180 [14:18<5:31:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 90/2180 [14:27<5:31:26,  9.52s/it]                                                   {'loss': 2.8966, 'learning_rate': 0.0009996820152934176, 'epoch': 0.04}
  4%|â–         | 90/2180 [14:27<5:31:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 91/2180 [14:37<5:31:19,  9.52s/it]                                                   {'loss': 2.8288, 'learning_rate': 0.000999654967622631, 'epoch': 0.04}
  4%|â–         | 91/2180 [14:37<5:31:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 92/2180 [14:46<5:31:52,  9.54s/it]                                                   {'loss': 2.8282, 'learning_rate': 0.0009996268164834238, 'epoch': 0.04}
  4%|â–         | 92/2180 [14:46<5:31:52,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 93/2180 [14:56<5:31:02,  9.52s/it]                                                   {'loss': 2.8208, 'learning_rate': 0.000999597561937966, 'epoch': 0.04}
  4%|â–         | 93/2180 [14:56<5:31:02,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 94/2180 [15:05<5:31:09,  9.52s/it]                                                   {'loss': 2.7805, 'learning_rate': 0.0009995672040508656, 'epoch': 0.04}
  4%|â–         | 94/2180 [15:05<5:31:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 95/2180 [15:15<5:30:37,  9.51s/it]                                                   {'loss': 2.8162, 'learning_rate': 0.0009995357428891662, 'epoch': 0.04}
  4%|â–         | 95/2180 [15:15<5:30:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 96/2180 [15:24<5:30:21,  9.51s/it]                                                   {'loss': 2.7862, 'learning_rate': 0.0009995031785223491, 'epoch': 0.04}
  4%|â–         | 96/2180 [15:24<5:30:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 97/2180 [15:34<5:30:07,  9.51s/it]                                                   {'loss': 2.9306, 'learning_rate': 0.000999469511022331, 'epoch': 0.04}
  4%|â–         | 97/2180 [15:34<5:30:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  4%|â–         | 98/2180 [15:43<5:29:44,  9.50s/it]                                                   {'loss': 2.7548, 'learning_rate': 0.0009994347404634657, 'epoch': 0.04}
  4%|â–         | 98/2180 [15:43<5:29:44,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 99/2180 [15:53<5:29:21,  9.50s/it]                                                   {'loss': 2.8509, 'learning_rate': 0.0009993988669225423, 'epoch': 0.05}
  5%|â–         | 99/2180 [15:53<5:29:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 100/2180 [16:02<5:29:07,  9.49s/it]                                                    {'loss': 2.7871, 'learning_rate': 0.000999361890478786, 'epoch': 0.05}
  5%|â–         | 100/2180 [16:02<5:29:07,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 101/2180 [16:12<5:29:00,  9.50s/it]                                                    {'loss': 2.8444, 'learning_rate': 0.0009993238112138583, 'epoch': 0.05}
  5%|â–         | 101/2180 [16:12<5:29:00,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 102/2180 [16:21<5:28:51,  9.50s/it]                                                    {'loss': 2.7477, 'learning_rate': 0.0009992846292118554, 'epoch': 0.05}
  5%|â–         | 102/2180 [16:21<5:28:51,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 103/2180 [16:31<5:28:45,  9.50s/it]                                                    {'loss': 2.7605, 'learning_rate': 0.000999244344559309, 'epoch': 0.05}
  5%|â–         | 103/2180 [16:31<5:28:45,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 104/2180 [16:41<5:29:48,  9.53s/it]                                                    {'loss': 2.6791, 'learning_rate': 0.0009992029573451869, 'epoch': 0.05}
  5%|â–         | 104/2180 [16:41<5:29:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 105/2180 [16:50<5:29:14,  9.52s/it]                                                    {'loss': 2.7206, 'learning_rate': 0.0009991604676608905, 'epoch': 0.05}
  5%|â–         | 105/2180 [16:50<5:29:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  5%|â–         | 106/2180 [17:00<5:28:48,  9.51s/it]                                                    {'loss': 2.7149, 'learning_rate': 0.0009991168756002568, 'epoch': 0.05}
  5%|â–         | 106/2180 [17:00<5:28:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 107/2180 [17:09<5:28:07,  9.50s/it]                                                    {'loss': 2.6879, 'learning_rate': 0.0009990721812595574, 'epoch': 0.05}
  5%|â–         | 107/2180 [17:09<5:28:07,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–         | 108/2180 [17:19<5:28:21,  9.51s/it]                                                    {'loss': 2.6814, 'learning_rate': 0.0009990263847374976, 'epoch': 0.05}
  5%|â–         | 108/2180 [17:19<5:28:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 109/2180 [17:28<5:28:05,  9.51s/it]                                                    {'loss': 2.6867, 'learning_rate': 0.0009989794861352173, 'epoch': 0.05}
  5%|â–Œ         | 109/2180 [17:28<5:28:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 110/2180 [17:37<5:27:34,  9.50s/it]                                                    {'loss': 2.6845, 'learning_rate': 0.0009989314855562905, 'epoch': 0.05}
  5%|â–Œ         | 110/2180 [17:37<5:27:34,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 111/2180 [17:47<5:27:16,  9.49s/it]                                                    {'loss': 2.6215, 'learning_rate': 0.0009988823831067245, 'epoch': 0.05}
  5%|â–Œ         | 111/2180 [17:47<5:27:16,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 112/2180 [17:56<5:26:50,  9.48s/it]                                                    {'loss': 2.6964, 'learning_rate': 0.0009988321788949597, 'epoch': 0.05}
  5%|â–Œ         | 112/2180 [17:56<5:26:50,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 113/2180 [18:06<5:28:30,  9.54s/it]                                                    {'loss': 2.6485, 'learning_rate': 0.0009987808730318709, 'epoch': 0.05}
  5%|â–Œ         | 113/2180 [18:06<5:28:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 114/2180 [18:16<5:28:26,  9.54s/it]                                                    {'loss': 2.7843, 'learning_rate': 0.0009987284656307644, 'epoch': 0.05}
  5%|â–Œ         | 114/2180 [18:16<5:28:26,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 115/2180 [18:25<5:28:39,  9.55s/it]                                                    {'loss': 2.6202, 'learning_rate': 0.0009986749568073802, 'epoch': 0.05}
  5%|â–Œ         | 115/2180 [18:25<5:28:39,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 116/2180 [18:35<5:28:18,  9.54s/it]                                                    {'loss': 2.583, 'learning_rate': 0.0009986203466798905, 'epoch': 0.05}
  5%|â–Œ         | 116/2180 [18:35<5:28:18,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 117/2180 [18:44<5:27:34,  9.53s/it]                                                    {'loss': 2.6356, 'learning_rate': 0.0009985646353688996, 'epoch': 0.05}
  5%|â–Œ         | 117/2180 [18:44<5:27:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 118/2180 [18:54<5:27:11,  9.52s/it]                                                    {'loss': 2.6923, 'learning_rate': 0.0009985078229974437, 'epoch': 0.05}
  5%|â–Œ         | 118/2180 [18:54<5:27:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  5%|â–Œ         | 119/2180 [19:03<5:26:41,  9.51s/it]                                                    {'loss': 2.6533, 'learning_rate': 0.0009984499096909905, 'epoch': 0.05}
  5%|â–Œ         | 119/2180 [19:03<5:26:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 120/2180 [19:13<5:26:54,  9.52s/it]                                                    {'loss': 2.6616, 'learning_rate': 0.0009983908955774397, 'epoch': 0.06}
  6%|â–Œ         | 120/2180 [19:13<5:26:54,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  6%|â–Œ         | 121/2180 [19:22<5:26:18,  9.51s/it]                                                    {'loss': 2.6336, 'learning_rate': 0.0009983307807871211, 'epoch': 0.06}
  6%|â–Œ         | 121/2180 [19:22<5:26:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 122/2180 [19:32<5:25:59,  9.50s/it]                                                    {'loss': 2.6878, 'learning_rate': 0.0009982695654527965, 'epoch': 0.06}
  6%|â–Œ         | 122/2180 [19:32<5:25:59,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 123/2180 [19:41<5:26:08,  9.51s/it]                                                    {'loss': 2.6914, 'learning_rate': 0.0009982072497096571, 'epoch': 0.06}
  6%|â–Œ         | 123/2180 [19:41<5:26:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 124/2180 [19:51<5:26:20,  9.52s/it]                                                    {'loss': 2.6198, 'learning_rate': 0.000998143833695325, 'epoch': 0.06}
  6%|â–Œ         | 124/2180 [19:51<5:26:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 125/2180 [20:00<5:25:59,  9.52s/it]                                                    {'loss': 2.5851, 'learning_rate': 0.0009980793175498517, 'epoch': 0.06}
  6%|â–Œ         | 125/2180 [20:00<5:25:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 126/2180 [20:10<5:25:54,  9.52s/it]                                                    {'loss': 2.5959, 'learning_rate': 0.000998013701415719, 'epoch': 0.06}
  6%|â–Œ         | 126/2180 [20:10<5:25:54,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  6%|â–Œ         | 127/2180 [20:19<5:26:10,  9.53s/it]                                                    {'loss': 2.5955, 'learning_rate': 0.0009979469854378372, 'epoch': 0.06}
  6%|â–Œ         | 127/2180 [20:19<5:26:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 128/2180 [20:29<5:25:52,  9.53s/it]                                                    {'loss': 2.6124, 'learning_rate': 0.000997879169763546, 'epoch': 0.06}
  6%|â–Œ         | 128/2180 [20:29<5:25:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  6%|â–Œ         | 129/2180 [20:38<5:25:29,  9.52s/it]                                                    {'loss': 2.5833, 'learning_rate': 0.000997810254542614, 'epoch': 0.06}
  6%|â–Œ         | 129/2180 [20:38<5:25:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  6%|â–Œ         | 130/2180 [20:48<5:25:17,  9.52s/it]                                                    {'loss': 2.5665, 'learning_rate': 0.0009977402399272374, 'epoch': 0.06}
  6%|â–Œ         | 130/2180 [20:48<5:25:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 131/2180 [20:57<5:24:35,  9.50s/it]                                                    {'loss': 2.5884, 'learning_rate': 0.0009976691260720407, 'epoch': 0.06}
  6%|â–Œ         | 131/2180 [20:57<5:24:35,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 132/2180 [21:07<5:24:08,  9.50s/it]                                                    {'loss': 2.5546, 'learning_rate': 0.0009975969131340763, 'epoch': 0.06}
  6%|â–Œ         | 132/2180 [21:07<5:24:08,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 133/2180 [21:16<5:24:47,  9.52s/it]                                                    {'loss': 2.5646, 'learning_rate': 0.0009975236012728236, 'epoch': 0.06}
  6%|â–Œ         | 133/2180 [21:16<5:24:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 134/2180 [21:26<5:25:02,  9.53s/it]                                                    {'loss': 2.5545, 'learning_rate': 0.0009974491906501886, 'epoch': 0.06}
  6%|â–Œ         | 134/2180 [21:26<5:25:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 135/2180 [21:36<5:24:58,  9.53s/it]                                                    {'loss': 2.5593, 'learning_rate': 0.0009973736814305049, 'epoch': 0.06}
  6%|â–Œ         | 135/2180 [21:36<5:24:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–Œ         | 136/2180 [21:45<5:24:44,  9.53s/it]                                                    {'loss': 2.5857, 'learning_rate': 0.0009972970737805312, 'epoch': 0.06}
  6%|â–Œ         | 136/2180 [21:45<5:24:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–‹         | 137/2180 [21:55<5:24:16,  9.52s/it]                                                    {'loss': 2.545, 'learning_rate': 0.0009972193678694525, 'epoch': 0.06}
  6%|â–‹         | 137/2180 [21:55<5:24:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–‹         | 138/2180 [22:04<5:23:55,  9.52s/it]                                                    {'loss': 2.4984, 'learning_rate': 0.0009971405638688794, 'epoch': 0.06}
  6%|â–‹         | 138/2180 [22:04<5:23:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–‹         | 139/2180 [22:14<5:24:31,  9.54s/it]                                                    {'loss': 2.5502, 'learning_rate': 0.0009970606619528475, 'epoch': 0.06}
  6%|â–‹         | 139/2180 [22:14<5:24:31,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  6%|â–‹         | 140/2180 [22:23<5:23:45,  9.52s/it]                                                    {'loss': 2.5263, 'learning_rate': 0.000996979662297817, 'epoch': 0.06}
  6%|â–‹         | 140/2180 [22:23<5:23:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  6%|â–‹         | 141/2180 [22:33<5:23:58,  9.53s/it]                                                    {'loss': 2.5689, 'learning_rate': 0.0009968975650826721, 'epoch': 0.06}
  6%|â–‹         | 141/2180 [22:33<5:23:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 142/2180 [22:42<5:23:18,  9.52s/it]                                                    {'loss': 2.5843, 'learning_rate': 0.000996814370488722, 'epoch': 0.07}
  7%|â–‹         | 142/2180 [22:42<5:23:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 143/2180 [22:52<5:22:51,  9.51s/it]                                                    {'loss': 2.5243, 'learning_rate': 0.000996730078699698, 'epoch': 0.07}
  7%|â–‹         | 143/2180 [22:52<5:22:51,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 144/2180 [23:01<5:22:25,  9.50s/it]                                                    {'loss': 2.6063, 'learning_rate': 0.0009966446899017558, 'epoch': 0.07}
  7%|â–‹         | 144/2180 [23:01<5:22:25,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 145/2180 [23:11<5:22:20,  9.50s/it]                                                    {'loss': 2.5235, 'learning_rate': 0.0009965582042834728, 'epoch': 0.07}
  7%|â–‹         | 145/2180 [23:11<5:22:20,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 146/2180 [23:20<5:21:59,  9.50s/it]                                                    {'loss': 2.5191, 'learning_rate': 0.0009964706220358492, 'epoch': 0.07}
  7%|â–‹         | 146/2180 [23:20<5:21:59,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 147/2180 [23:30<5:23:41,  9.55s/it]                                                    {'loss': 2.4571, 'learning_rate': 0.000996381943352307, 'epoch': 0.07}
  7%|â–‹         | 147/2180 [23:30<5:23:41,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 148/2180 [23:39<5:23:02,  9.54s/it]                                                    {'loss': 2.5942, 'learning_rate': 0.0009962921684286896, 'epoch': 0.07}
  7%|â–‹         | 148/2180 [23:39<5:23:02,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 149/2180 [23:49<5:22:45,  9.53s/it]                                                    {'loss': 2.5198, 'learning_rate': 0.0009962012974632614, 'epoch': 0.07}
  7%|â–‹         | 149/2180 [23:49<5:22:45,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 150/2180 [23:58<5:22:13,  9.52s/it]                                                    {'loss': 2.5271, 'learning_rate': 0.0009961093306567075, 'epoch': 0.07}
  7%|â–‹         | 150/2180 [23:58<5:22:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 151/2180 [24:08<5:23:14,  9.56s/it]                                                    {'loss': 2.429, 'learning_rate': 0.0009960162682121328, 'epoch': 0.07}
  7%|â–‹         | 151/2180 [24:08<5:23:14,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 152/2180 [24:18<5:22:30,  9.54s/it]                                                    {'loss': 2.4858, 'learning_rate': 0.0009959221103350623, 'epoch': 0.07}
  7%|â–‹         | 152/2180 [24:18<5:22:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 153/2180 [24:27<5:21:37,  9.52s/it]                                                    {'loss': 2.5555, 'learning_rate': 0.0009958268572334394, 'epoch': 0.07}
  7%|â–‹         | 153/2180 [24:27<5:21:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 154/2180 [24:37<5:21:09,  9.51s/it]                                                    {'loss': 2.4893, 'learning_rate': 0.0009957305091176274, 'epoch': 0.07}
  7%|â–‹         | 154/2180 [24:37<5:21:09,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 155/2180 [24:46<5:21:10,  9.52s/it]                                                    {'loss': 2.4904, 'learning_rate': 0.0009956330662004075, 'epoch': 0.07}
  7%|â–‹         | 155/2180 [24:46<5:21:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 156/2180 [24:56<5:21:00,  9.52s/it]                                                    {'loss': 2.5294, 'learning_rate': 0.0009955345286969779, 'epoch': 0.07}
  7%|â–‹         | 156/2180 [24:56<5:21:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 157/2180 [25:05<5:21:14,  9.53s/it]                                                    {'loss': 2.4945, 'learning_rate': 0.0009954348968249551, 'epoch': 0.07}
  7%|â–‹         | 157/2180 [25:05<5:21:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 158/2180 [25:15<5:20:46,  9.52s/it]                                                    {'loss': 2.438, 'learning_rate': 0.0009953341708043724, 'epoch': 0.07}
  7%|â–‹         | 158/2180 [25:15<5:20:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 159/2180 [25:24<5:20:30,  9.52s/it]                                                    {'loss': 2.42, 'learning_rate': 0.0009952323508576793, 'epoch': 0.07}
  7%|â–‹         | 159/2180 [25:24<5:20:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 160/2180 [25:34<5:21:49,  9.56s/it]                                                    {'loss': 2.4479, 'learning_rate': 0.0009951294372097406, 'epoch': 0.07}
  7%|â–‹         | 160/2180 [25:34<5:21:49,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 161/2180 [25:43<5:20:39,  9.53s/it]                                                    {'loss': 2.3745, 'learning_rate': 0.0009950254300878378, 'epoch': 0.07}
  7%|â–‹         | 161/2180 [25:43<5:20:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 162/2180 [25:53<5:20:33,  9.53s/it]                                                    {'loss': 2.4051, 'learning_rate': 0.000994920329721666, 'epoch': 0.07}
  7%|â–‹         | 162/2180 [25:53<5:20:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  7%|â–‹         | 163/2180 [26:02<5:19:54,  9.52s/it]                                                    {'loss': 2.3347, 'learning_rate': 0.0009948141363433356, 'epoch': 0.07}
  7%|â–‹         | 163/2180 [26:02<5:19:54,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 164/2180 [26:12<5:20:30,  9.54s/it]                                                    {'loss': 2.3651, 'learning_rate': 0.00099470685018737, 'epoch': 0.08}
  8%|â–Š         | 164/2180 [26:12<5:20:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 165/2180 [26:21<5:20:16,  9.54s/it]                                                    {'loss': 2.3802, 'learning_rate': 0.0009945984714907073, 'epoch': 0.08}
  8%|â–Š         | 165/2180 [26:21<5:20:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  8%|â–Š         | 166/2180 [26:31<5:19:47,  9.53s/it]                                                    {'loss': 2.4385, 'learning_rate': 0.000994489000492697, 'epoch': 0.08}
  8%|â–Š         | 166/2180 [26:31<5:19:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 167/2180 [26:40<5:19:21,  9.52s/it]                                                    {'loss': 2.4652, 'learning_rate': 0.0009943784374351016, 'epoch': 0.08}
  8%|â–Š         | 167/2180 [26:40<5:19:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 168/2180 [26:50<5:18:37,  9.50s/it]                                                    {'loss': 2.4367, 'learning_rate': 0.0009942667825620951, 'epoch': 0.08}
  8%|â–Š         | 168/2180 [26:50<5:18:37,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 169/2180 [26:59<5:18:29,  9.50s/it]                                                    {'loss': 2.4592, 'learning_rate': 0.0009941540361202634, 'epoch': 0.08}
  8%|â–Š         | 169/2180 [26:59<5:18:29,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 170/2180 [27:09<5:19:11,  9.53s/it]                                                    {'loss': 2.3821, 'learning_rate': 0.0009940401983586022, 'epoch': 0.08}
  8%|â–Š         | 170/2180 [27:09<5:19:11,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 171/2180 [27:18<5:18:23,  9.51s/it]                                                    {'loss': 2.3991, 'learning_rate': 0.000993925269528518, 'epoch': 0.08}
  8%|â–Š         | 171/2180 [27:18<5:18:23,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 172/2180 [27:28<5:17:55,  9.50s/it]                                                    {'loss': 2.3924, 'learning_rate': 0.0009938092498838265, 'epoch': 0.08}
  8%|â–Š         | 172/2180 [27:28<5:17:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 173/2180 [27:37<5:17:48,  9.50s/it]                                                    {'loss': 2.4024, 'learning_rate': 0.0009936921396807524, 'epoch': 0.08}
  8%|â–Š         | 173/2180 [27:37<5:17:48,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 174/2180 [27:47<5:17:31,  9.50s/it]                                                    {'loss': 2.3405, 'learning_rate': 0.0009935739391779292, 'epoch': 0.08}
  8%|â–Š         | 174/2180 [27:47<5:17:31,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 175/2180 [27:56<5:17:00,  9.49s/it]                                                    {'loss': 2.468, 'learning_rate': 0.000993454648636398, 'epoch': 0.08}
  8%|â–Š         | 175/2180 [27:56<5:17:00,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 176/2180 [28:06<5:16:56,  9.49s/it]                                                    {'loss': 2.4554, 'learning_rate': 0.0009933342683196074, 'epoch': 0.08}
  8%|â–Š         | 176/2180 [28:06<5:16:56,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 177/2180 [28:15<5:17:01,  9.50s/it]                                                    {'loss': 2.3209, 'learning_rate': 0.0009932127984934125, 'epoch': 0.08}
  8%|â–Š         | 177/2180 [28:15<5:17:01,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 178/2180 [28:25<5:16:51,  9.50s/it]                                                    {'loss': 2.4242, 'learning_rate': 0.0009930902394260745, 'epoch': 0.08}
  8%|â–Š         | 178/2180 [28:25<5:16:51,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 179/2180 [28:34<5:16:53,  9.50s/it]                                                    {'loss': 2.3856, 'learning_rate': 0.0009929665913882607, 'epoch': 0.08}
  8%|â–Š         | 179/2180 [28:34<5:16:53,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 180/2180 [28:44<5:16:38,  9.50s/it]                                                    {'loss': 2.3874, 'learning_rate': 0.0009928418546530425, 'epoch': 0.08}
  8%|â–Š         | 180/2180 [28:44<5:16:38,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 181/2180 [28:53<5:16:16,  9.49s/it]                                                    {'loss': 2.3399, 'learning_rate': 0.0009927160294958964, 'epoch': 0.08}
  8%|â–Š         | 181/2180 [28:53<5:16:16,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 182/2180 [29:03<5:16:49,  9.51s/it]                                                    {'loss': 2.4196, 'learning_rate': 0.000992589116194702, 'epoch': 0.08}
  8%|â–Š         | 182/2180 [29:03<5:16:49,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 183/2180 [29:12<5:16:26,  9.51s/it]                                                    {'loss': 2.428, 'learning_rate': 0.000992461115029743, 'epoch': 0.08}
  8%|â–Š         | 183/2180 [29:12<5:16:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 184/2180 [29:22<5:16:09,  9.50s/it]                                                    {'loss': 2.3393, 'learning_rate': 0.000992332026283704, 'epoch': 0.08}
  8%|â–Š         | 184/2180 [29:22<5:16:09,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  8%|â–Š         | 185/2180 [29:31<5:15:53,  9.50s/it]                                                    {'loss': 2.3451, 'learning_rate': 0.0009922018502416736, 'epoch': 0.08}
  8%|â–Š         | 185/2180 [29:31<5:15:53,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–Š         | 186/2180 [29:41<5:15:30,  9.49s/it]                                                    {'loss': 2.3073, 'learning_rate': 0.0009920705871911395, 'epoch': 0.09}
  9%|â–Š         | 186/2180 [29:41<5:15:30,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–Š         | 187/2180 [29:50<5:15:22,  9.49s/it]                                                    {'loss': 2.3809, 'learning_rate': 0.0009919382374219915, 'epoch': 0.09}
  9%|â–Š         | 187/2180 [29:50<5:15:22,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–Š         | 188/2180 [30:00<5:14:56,  9.49s/it]                                                    {'loss': 2.3222, 'learning_rate': 0.0009918048012265187, 'epoch': 0.09}
  9%|â–Š         | 188/2180 [30:00<5:14:56,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–Š         | 189/2180 [30:09<5:15:10,  9.50s/it]                                                    {'loss': 2.4716, 'learning_rate': 0.0009916702788994097, 'epoch': 0.09}
  9%|â–Š         | 189/2180 [30:09<5:15:10,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–Š         | 190/2180 [30:19<5:14:50,  9.49s/it]                                                    {'loss': 2.3127, 'learning_rate': 0.0009915346707377519, 'epoch': 0.09}
  9%|â–Š         | 190/2180 [30:19<5:14:50,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 191/2180 [30:28<5:15:16,  9.51s/it]                                                    {'loss': 2.2388, 'learning_rate': 0.0009913979770410305, 'epoch': 0.09}
  9%|â–‰         | 191/2180 [30:28<5:15:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 192/2180 [30:38<5:15:07,  9.51s/it]                                                    {'loss': 2.3222, 'learning_rate': 0.0009912601981111285, 'epoch': 0.09}
  9%|â–‰         | 192/2180 [30:38<5:15:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 193/2180 [30:47<5:15:03,  9.51s/it]                                                    {'loss': 2.3083, 'learning_rate': 0.0009911213342523248, 'epoch': 0.09}
  9%|â–‰         | 193/2180 [30:47<5:15:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 194/2180 [30:57<5:14:25,  9.50s/it]                                                    {'loss': 2.373, 'learning_rate': 0.000990981385771295, 'epoch': 0.09}
  9%|â–‰         | 194/2180 [30:57<5:14:25,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 195/2180 [31:06<5:13:59,  9.49s/it]                                                    {'loss': 2.3186, 'learning_rate': 0.00099084035297711, 'epoch': 0.09}
  9%|â–‰         | 195/2180 [31:06<5:13:59,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 196/2180 [31:16<5:13:52,  9.49s/it]                                                    {'loss': 2.3291, 'learning_rate': 0.000990698236181235, 'epoch': 0.09}
  9%|â–‰         | 196/2180 [31:16<5:13:52,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 197/2180 [31:25<5:14:53,  9.53s/it]                                                    {'loss': 2.2846, 'learning_rate': 0.0009905550356975293, 'epoch': 0.09}
  9%|â–‰         | 197/2180 [31:25<5:14:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  9%|â–‰         | 198/2180 [31:35<5:14:03,  9.51s/it]                                                    {'loss': 2.3034, 'learning_rate': 0.0009904107518422457, 'epoch': 0.09}
  9%|â–‰         | 198/2180 [31:35<5:14:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 199/2180 [31:44<5:13:27,  9.49s/it]                                                    {'loss': 2.229, 'learning_rate': 0.0009902653849340295, 'epoch': 0.09}
  9%|â–‰         | 199/2180 [31:44<5:13:27,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 200/2180 [31:54<5:13:26,  9.50s/it]                                                    {'loss': 2.3165, 'learning_rate': 0.0009901189352939177, 'epoch': 0.09}
  9%|â–‰         | 200/2180 [31:54<5:13:26,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 201/2180 [32:03<5:13:16,  9.50s/it]                                                    {'loss': 2.3558, 'learning_rate': 0.0009899714032453387, 'epoch': 0.09}
  9%|â–‰         | 201/2180 [32:03<5:13:16,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 202/2180 [32:13<5:13:17,  9.50s/it]                                                    {'loss': 2.2467, 'learning_rate': 0.000989822789114111, 'epoch': 0.09}
  9%|â–‰         | 202/2180 [32:13<5:13:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 203/2180 [32:22<5:12:52,  9.50s/it]                                                    {'loss': 2.2578, 'learning_rate': 0.0009896730932284434, 'epoch': 0.09}
  9%|â–‰         | 203/2180 [32:22<5:12:52,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  9%|â–‰         | 204/2180 [32:32<5:12:20,  9.48s/it]                                                    {'loss': 2.2182, 'learning_rate': 0.0009895223159189332, 'epoch': 0.09}
  9%|â–‰         | 204/2180 [32:32<5:12:20,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  9%|â–‰         | 205/2180 [32:41<5:12:26,  9.49s/it]                                                    {'loss': 2.3337, 'learning_rate': 0.0009893704575185663, 'epoch': 0.09}
  9%|â–‰         | 205/2180 [32:41<5:12:26,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  9%|â–‰         | 206/2180 [32:51<5:13:12,  9.52s/it]                                                    {'loss': 2.3595, 'learning_rate': 0.000989217518362716, 'epoch': 0.09}
  9%|â–‰         | 206/2180 [32:51<5:13:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

  9%|â–‰         | 207/2180 [33:00<5:12:52,  9.51s/it]                                                    {'loss': 2.2764, 'learning_rate': 0.0009890634987891425, 'epoch': 0.09}
  9%|â–‰         | 207/2180 [33:00<5:12:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 208/2180 [33:10<5:12:51,  9.52s/it]                                                    {'loss': 2.3085, 'learning_rate': 0.0009889083991379917, 'epoch': 0.1}
 10%|â–‰         | 208/2180 [33:10<5:12:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 10%|â–‰         | 209/2180 [33:19<5:12:22,  9.51s/it]                                                    {'loss': 2.2591, 'learning_rate': 0.0009887522197517954, 'epoch': 0.1}
 10%|â–‰         | 209/2180 [33:19<5:12:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 210/2180 [33:29<5:12:12,  9.51s/it]                                                    {'loss': 2.3317, 'learning_rate': 0.0009885949609754693, 'epoch': 0.1}
 10%|â–‰         | 210/2180 [33:29<5:12:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 211/2180 [33:38<5:12:01,  9.51s/it]                                                    {'loss': 2.346, 'learning_rate': 0.000988436623156314, 'epoch': 0.1}
 10%|â–‰         | 211/2180 [33:38<5:12:01,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 212/2180 [33:48<5:12:06,  9.52s/it]                                                    {'loss': 2.2947, 'learning_rate': 0.0009882772066440114, 'epoch': 0.1}
 10%|â–‰         | 212/2180 [33:48<5:12:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 213/2180 [33:58<5:12:03,  9.52s/it]                                                    {'loss': 2.3003, 'learning_rate': 0.0009881167117906276, 'epoch': 0.1}
 10%|â–‰         | 213/2180 [33:58<5:12:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 214/2180 [34:07<5:11:43,  9.51s/it]                                                    {'loss': 2.3236, 'learning_rate': 0.0009879551389506084, 'epoch': 0.1}
 10%|â–‰         | 214/2180 [34:07<5:11:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 215/2180 [34:17<5:11:48,  9.52s/it]                                                    {'loss': 2.2354, 'learning_rate': 0.0009877924884807814, 'epoch': 0.1}
 10%|â–‰         | 215/2180 [34:17<5:11:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 216/2180 [34:26<5:11:35,  9.52s/it]                                                    {'loss': 2.2834, 'learning_rate': 0.000987628760740354, 'epoch': 0.1}
 10%|â–‰         | 216/2180 [34:26<5:11:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–‰         | 217/2180 [34:36<5:12:11,  9.54s/it]                                                    {'loss': 2.2732, 'learning_rate': 0.0009874639560909118, 'epoch': 0.1}
 10%|â–‰         | 217/2180 [34:36<5:12:11,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 218/2180 [34:45<5:11:42,  9.53s/it]                                                    {'loss': 2.363, 'learning_rate': 0.0009872980748964202, 'epoch': 0.1}
 10%|â–ˆ         | 218/2180 [34:45<5:11:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 219/2180 [34:55<5:11:11,  9.52s/it]                                                    {'loss': 2.3312, 'learning_rate': 0.000987131117523221, 'epoch': 0.1}
 10%|â–ˆ         | 219/2180 [34:55<5:11:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 220/2180 [35:04<5:11:09,  9.53s/it]                                                    {'loss': 2.3113, 'learning_rate': 0.000986963084340033, 'epoch': 0.1}
 10%|â–ˆ         | 220/2180 [35:04<5:11:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 221/2180 [35:14<5:10:28,  9.51s/it]                                                    {'loss': 2.2534, 'learning_rate': 0.0009867939757179508, 'epoch': 0.1}
 10%|â–ˆ         | 221/2180 [35:14<5:10:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 222/2180 [35:23<5:09:45,  9.49s/it]                                                    {'loss': 2.2847, 'learning_rate': 0.0009866237920304443, 'epoch': 0.1}
 10%|â–ˆ         | 222/2180 [35:23<5:09:45,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 223/2180 [35:33<5:09:40,  9.49s/it]                                                    {'loss': 2.3045, 'learning_rate': 0.0009864525336533577, 'epoch': 0.1}
 10%|â–ˆ         | 223/2180 [35:33<5:09:40,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 10%|â–ˆ         | 224/2180 [35:42<5:09:59,  9.51s/it]                                                    {'loss': 2.317, 'learning_rate': 0.000986280200964908, 'epoch': 0.1}
 10%|â–ˆ         | 224/2180 [35:42<5:09:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 225/2180 [35:52<5:09:30,  9.50s/it]                                                    {'loss': 2.2654, 'learning_rate': 0.0009861067943456856, 'epoch': 0.1}
 10%|â–ˆ         | 225/2180 [35:52<5:09:30,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 226/2180 [36:01<5:09:29,  9.50s/it]                                                    {'loss': 2.2233, 'learning_rate': 0.000985932314178652, 'epoch': 0.1}
 10%|â–ˆ         | 226/2180 [36:01<5:09:29,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 227/2180 [36:11<5:09:07,  9.50s/it]                                                    {'loss': 2.3251, 'learning_rate': 0.00098575676084914, 'epoch': 0.1}
 10%|â–ˆ         | 227/2180 [36:11<5:09:07,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 10%|â–ˆ         | 228/2180 [36:20<5:09:13,  9.50s/it]                                                    {'loss': 2.2715, 'learning_rate': 0.0009855801347448518, 'epoch': 0.1}
 10%|â–ˆ         | 228/2180 [36:20<5:09:13,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 229/2180 [36:30<5:08:38,  9.49s/it]                                                    {'loss': 2.3411, 'learning_rate': 0.0009854024362558596, 'epoch': 0.11}
 11%|â–ˆ         | 229/2180 [36:30<5:08:38,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 230/2180 [36:39<5:09:09,  9.51s/it]                                                    {'loss': 2.2143, 'learning_rate': 0.0009852236657746035, 'epoch': 0.11}
 11%|â–ˆ         | 230/2180 [36:39<5:09:09,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 231/2180 [36:49<5:08:58,  9.51s/it]                                                    {'loss': 2.3823, 'learning_rate': 0.0009850438236958911, 'epoch': 0.11}
 11%|â–ˆ         | 231/2180 [36:49<5:08:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 232/2180 [36:58<5:09:12,  9.52s/it]                                                    {'loss': 2.3167, 'learning_rate': 0.0009848629104168966, 'epoch': 0.11}
 11%|â–ˆ         | 232/2180 [36:58<5:09:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 233/2180 [37:08<5:09:02,  9.52s/it]                                                    {'loss': 2.2234, 'learning_rate': 0.00098468092633716, 'epoch': 0.11}
 11%|â–ˆ         | 233/2180 [37:08<5:09:02,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 234/2180 [37:17<5:08:46,  9.52s/it]                                                    {'loss': 2.2487, 'learning_rate': 0.0009844978718585855, 'epoch': 0.11}
 11%|â–ˆ         | 234/2180 [37:17<5:08:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 235/2180 [37:27<5:08:43,  9.52s/it]                                                    {'loss': 2.2338, 'learning_rate': 0.0009843137473854423, 'epoch': 0.11}
 11%|â–ˆ         | 235/2180 [37:27<5:08:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 236/2180 [37:36<5:08:35,  9.52s/it]                                                    {'loss': 2.2986, 'learning_rate': 0.000984128553324362, 'epoch': 0.11}
 11%|â–ˆ         | 236/2180 [37:36<5:08:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 237/2180 [37:46<5:08:18,  9.52s/it]                                                    {'loss': 2.3087, 'learning_rate': 0.0009839422900843383, 'epoch': 0.11}
 11%|â–ˆ         | 237/2180 [37:46<5:08:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 238/2180 [37:55<5:08:26,  9.53s/it]                                                    {'loss': 2.2827, 'learning_rate': 0.0009837549580767261, 'epoch': 0.11}
 11%|â–ˆ         | 238/2180 [37:55<5:08:26,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 239/2180 [38:05<5:08:12,  9.53s/it]                                                    {'loss': 2.2633, 'learning_rate': 0.0009835665577152411, 'epoch': 0.11}
 11%|â–ˆ         | 239/2180 [38:05<5:08:12,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 240/2180 [38:14<5:08:00,  9.53s/it]                                                    {'loss': 2.2488, 'learning_rate': 0.000983377089415958, 'epoch': 0.11}
 11%|â–ˆ         | 240/2180 [38:14<5:08:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 241/2180 [38:24<5:07:45,  9.52s/it]                                                    {'loss': 2.1797, 'learning_rate': 0.0009831865535973102, 'epoch': 0.11}
 11%|â–ˆ         | 241/2180 [38:24<5:07:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 242/2180 [38:33<5:07:29,  9.52s/it]                                                    {'loss': 2.2915, 'learning_rate': 0.0009829949506800885, 'epoch': 0.11}
 11%|â–ˆ         | 242/2180 [38:33<5:07:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 243/2180 [38:43<5:07:51,  9.54s/it]                                                    {'loss': 2.2824, 'learning_rate': 0.0009828022810874405, 'epoch': 0.11}
 11%|â–ˆ         | 243/2180 [38:43<5:07:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 244/2180 [38:53<5:07:29,  9.53s/it]                                                    {'loss': 2.2881, 'learning_rate': 0.0009826085452448693, 'epoch': 0.11}
 11%|â–ˆ         | 244/2180 [38:53<5:07:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆ         | 245/2180 [39:02<5:07:09,  9.52s/it]                                                    {'loss': 2.3428, 'learning_rate': 0.000982413743580233, 'epoch': 0.11}
 11%|â–ˆ         | 245/2180 [39:02<5:07:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆâ–        | 246/2180 [39:12<5:06:46,  9.52s/it]                                                    {'loss': 2.2504, 'learning_rate': 0.0009822178765237436, 'epoch': 0.11}
 11%|â–ˆâ–        | 246/2180 [39:12<5:06:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆâ–        | 247/2180 [39:21<5:06:24,  9.51s/it]                                                    {'loss': 2.3142, 'learning_rate': 0.0009820209445079654, 'epoch': 0.11}
 11%|â–ˆâ–        | 247/2180 [39:21<5:06:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆâ–        | 248/2180 [39:31<5:05:55,  9.50s/it]                                                    {'loss': 2.2828, 'learning_rate': 0.0009818229479678158, 'epoch': 0.11}
 11%|â–ˆâ–        | 248/2180 [39:31<5:05:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆâ–        | 249/2180 [39:40<5:05:46,  9.50s/it]                                                    {'loss': 2.2718, 'learning_rate': 0.0009816238873405615, 'epoch': 0.11}
 11%|â–ˆâ–        | 249/2180 [39:40<5:05:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 11%|â–ˆâ–        | 250/2180 [39:50<5:06:09,  9.52s/it]                                                    {'loss': 2.2057, 'learning_rate': 0.0009814237630658207, 'epoch': 0.11}
 11%|â–ˆâ–        | 250/2180 [39:50<5:06:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 251/2180 [39:59<5:06:29,  9.53s/it]                                                    {'loss': 2.2562, 'learning_rate': 0.00098122257558556, 'epoch': 0.12}
 12%|â–ˆâ–        | 251/2180 [39:59<5:06:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 252/2180 [40:09<5:06:18,  9.53s/it]                                                    {'loss': 2.1994, 'learning_rate': 0.0009810203253440937, 'epoch': 0.12}
 12%|â–ˆâ–        | 252/2180 [40:09<5:06:18,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 253/2180 [40:18<5:06:13,  9.53s/it]                                                    {'loss': 2.2029, 'learning_rate': 0.0009808170127880837, 'epoch': 0.12}
 12%|â–ˆâ–        | 253/2180 [40:18<5:06:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 254/2180 [40:28<5:06:20,  9.54s/it]                                                    {'loss': 2.1905, 'learning_rate': 0.000980612638366538, 'epoch': 0.12}
 12%|â–ˆâ–        | 254/2180 [40:28<5:06:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 255/2180 [40:37<5:05:55,  9.54s/it]                                                    {'loss': 2.2654, 'learning_rate': 0.0009804072025308096, 'epoch': 0.12}
 12%|â–ˆâ–        | 255/2180 [40:37<5:05:55,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 12%|â–ˆâ–        | 256/2180 [40:47<5:05:35,  9.53s/it]                                                    {'loss': 2.2332, 'learning_rate': 0.000980200705734595, 'epoch': 0.12}
 12%|â–ˆâ–        | 256/2180 [40:47<5:05:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 257/2180 [40:56<5:04:55,  9.51s/it]                                                    {'loss': 2.2773, 'learning_rate': 0.0009799931484339344, 'epoch': 0.12}
 12%|â–ˆâ–        | 257/2180 [40:56<5:04:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 258/2180 [41:06<5:04:44,  9.51s/it]                                                    {'loss': 2.1872, 'learning_rate': 0.0009797845310872103, 'epoch': 0.12}
 12%|â–ˆâ–        | 258/2180 [41:06<5:04:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 259/2180 [41:15<5:04:43,  9.52s/it]                                                    {'loss': 2.2596, 'learning_rate': 0.0009795748541551457, 'epoch': 0.12}
 12%|â–ˆâ–        | 259/2180 [41:15<5:04:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 260/2180 [41:25<5:04:54,  9.53s/it]                                                    {'loss': 2.1879, 'learning_rate': 0.000979364118100804, 'epoch': 0.12}
 12%|â–ˆâ–        | 260/2180 [41:25<5:04:54,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 12%|â–ˆâ–        | 261/2180 [41:34<5:04:23,  9.52s/it]                                                    {'loss': 2.3208, 'learning_rate': 0.0009791523233895875, 'epoch': 0.12}
 12%|â–ˆâ–        | 261/2180 [41:34<5:04:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 12%|â–ˆâ–        | 262/2180 [41:44<5:03:50,  9.51s/it]                                                    {'loss': 2.2213, 'learning_rate': 0.0009789394704892364, 'epoch': 0.12}
 12%|â–ˆâ–        | 262/2180 [41:44<5:03:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 263/2180 [41:53<5:04:10,  9.52s/it]                                                    {'loss': 2.2562, 'learning_rate': 0.0009787255598698282, 'epoch': 0.12}
 12%|â–ˆâ–        | 263/2180 [41:53<5:04:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 264/2180 [42:03<5:04:21,  9.53s/it]                                                    {'loss': 2.3706, 'learning_rate': 0.0009785105920037758, 'epoch': 0.12}
 12%|â–ˆâ–        | 264/2180 [42:03<5:04:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 265/2180 [42:12<5:03:45,  9.52s/it]                                                    {'loss': 2.2899, 'learning_rate': 0.0009782945673658275, 'epoch': 0.12}
 12%|â–ˆâ–        | 265/2180 [42:12<5:03:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


 12%|â–ˆâ–        | 266/2180 [42:22<5:03:21,  9.51s/it]                                                    {'loss': 2.2557, 'learning_rate': 0.0009780774864330654, 'epoch': 0.12}
 12%|â–ˆâ–        | 266/2180 [42:22<5:03:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 267/2180 [42:31<5:03:20,  9.51s/it]                                                    {'loss': 2.209, 'learning_rate': 0.000977859349684904, 'epoch': 0.12}
 12%|â–ˆâ–        | 267/2180 [42:31<5:03:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 268/2180 [42:41<5:02:45,  9.50s/it]                                                    {'loss': 2.2634, 'learning_rate': 0.00097764015760309, 'epoch': 0.12}
 12%|â–ˆâ–        | 268/2180 [42:41<5:02:45,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 269/2180 [42:50<5:02:17,  9.49s/it]                                                    {'loss': 2.2467, 'learning_rate': 0.0009774199106717004, 'epoch': 0.12}
 12%|â–ˆâ–        | 269/2180 [42:50<5:02:17,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 270/2180 [43:00<5:02:30,  9.50s/it]                                                    {'loss': 2.2131, 'learning_rate': 0.0009771986093771417, 'epoch': 0.12}
 12%|â–ˆâ–        | 270/2180 [43:00<5:02:30,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 271/2180 [43:09<5:01:53,  9.49s/it]                                                    {'loss': 2.2107, 'learning_rate': 0.0009769762542081496, 'epoch': 0.12}
 12%|â–ˆâ–        | 271/2180 [43:09<5:01:53,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 12%|â–ˆâ–        | 272/2180 [43:19<5:01:48,  9.49s/it]                                                    {'loss': 2.252, 'learning_rate': 0.000976752845655786, 'epoch': 0.12}
 12%|â–ˆâ–        | 272/2180 [43:19<5:01:48,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 273/2180 [43:28<5:02:10,  9.51s/it]                                                    {'loss': 2.1871, 'learning_rate': 0.0009765283842134411, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 273/2180 [43:28<5:02:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 274/2180 [43:38<5:01:49,  9.50s/it]                                                    {'loss': 2.2927, 'learning_rate': 0.0009763028703768282, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 274/2180 [43:38<5:01:49,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 275/2180 [43:48<5:02:31,  9.53s/it]                                                    {'loss': 2.2009, 'learning_rate': 0.0009760763046439862, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 275/2180 [43:48<5:02:31,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 276/2180 [43:57<5:02:00,  9.52s/it]                                                    {'loss': 2.2494, 'learning_rate': 0.0009758486875152766, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 276/2180 [43:57<5:02:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 277/2180 [44:07<5:02:07,  9.53s/it]                                                    {'loss': 2.2581, 'learning_rate': 0.0009756200194933829, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 277/2180 [44:07<5:02:07,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 278/2180 [44:16<5:01:41,  9.52s/it]                                                    {'loss': 2.2991, 'learning_rate': 0.0009753903010833094, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 278/2180 [44:16<5:01:41,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 279/2180 [44:26<5:01:22,  9.51s/it]                                                    {'loss': 2.2653, 'learning_rate': 0.0009751595327923803, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 279/2180 [44:26<5:01:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 280/2180 [44:35<5:00:58,  9.50s/it]                                                    {'loss': 2.2555, 'learning_rate': 0.0009749277151302382, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 280/2180 [44:35<5:00:58,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 281/2180 [44:45<5:02:27,  9.56s/it]                                                    {'loss': 2.2363, 'learning_rate': 0.0009746948486088435, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 281/2180 [44:45<5:02:27,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 282/2180 [44:54<5:01:44,  9.54s/it]                                                    {'loss': 2.1792, 'learning_rate': 0.0009744609337424727, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 282/2180 [44:54<5:01:44,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 283/2180 [45:04<5:01:20,  9.53s/it]                                                    {'loss': 2.2013, 'learning_rate': 0.0009742259710477177, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 283/2180 [45:04<5:01:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 284/2180 [45:13<5:01:12,  9.53s/it]                                                    {'loss': 2.2093, 'learning_rate': 0.0009739899610434841, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 284/2180 [45:13<5:01:12,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 285/2180 [45:23<5:00:53,  9.53s/it]                                                    {'loss': 2.1851, 'learning_rate': 0.0009737529042509913, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 285/2180 [45:23<5:00:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 286/2180 [45:32<5:00:47,  9.53s/it]                                                    {'loss': 2.2724, 'learning_rate': 0.0009735148011937693, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 286/2180 [45:32<5:00:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 287/2180 [45:42<5:00:19,  9.52s/it]                                                    {'loss': 2.2784, 'learning_rate': 0.00097327565239766, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 287/2180 [45:42<5:00:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 13%|â–ˆâ–Ž        | 288/2180 [45:51<5:00:12,  9.52s/it]                                                    {'loss': 2.2811, 'learning_rate': 0.0009730354583908136, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 288/2180 [45:51<5:00:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 13%|â–ˆâ–Ž        | 289/2180 [46:01<5:00:08,  9.52s/it]                                                    {'loss': 2.1689, 'learning_rate': 0.0009727942197036895, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 289/2180 [46:01<5:00:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 290/2180 [46:10<4:59:55,  9.52s/it]                                                    {'loss': 2.2463, 'learning_rate': 0.0009725519368690539, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 290/2180 [46:10<4:59:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 291/2180 [46:20<4:59:59,  9.53s/it]                                                    {'loss': 2.2193, 'learning_rate': 0.0009723086104219787, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 291/2180 [46:20<4:59:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 292/2180 [46:29<4:59:22,  9.51s/it]                                                    {'loss': 2.2101, 'learning_rate': 0.0009720642408998409, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 292/2180 [46:29<4:59:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 293/2180 [46:39<4:59:28,  9.52s/it]                                                    {'loss': 2.1481, 'learning_rate': 0.0009718188288423211, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 293/2180 [46:39<4:59:28,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 13%|â–ˆâ–Ž        | 294/2180 [46:48<4:59:28,  9.53s/it]                                                    {'loss': 2.23, 'learning_rate': 0.0009715723747914022, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 294/2180 [46:49<4:59:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–Ž        | 295/2180 [46:58<4:59:22,  9.53s/it]                                                    {'loss': 2.2204, 'learning_rate': 0.0009713248792913685, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 295/2180 [46:58<4:59:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–Ž        | 296/2180 [47:08<4:59:03,  9.52s/it]                                                    {'loss': 2.2175, 'learning_rate': 0.0009710763428888037, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 296/2180 [47:08<4:59:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–Ž        | 297/2180 [47:17<4:58:37,  9.52s/it]                                                    {'loss': 2.2643, 'learning_rate': 0.0009708267661325909, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 297/2180 [47:17<4:58:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–Ž        | 298/2180 [47:27<4:58:15,  9.51s/it]                                                    {'loss': 2.2487, 'learning_rate': 0.0009705761495739107, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 298/2180 [47:27<4:58:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–Ž        | 299/2180 [47:36<4:58:15,  9.51s/it]                                                    {'loss': 2.225, 'learning_rate': 0.0009703244937662399, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 299/2180 [47:36<4:58:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 300/2180 [47:46<4:57:44,  9.50s/it]                                                    {'loss': 2.1856, 'learning_rate': 0.0009700717992653505, 'epoch': 0.14}
 14%|â–ˆâ–        | 300/2180 [47:46<4:57:44,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 301/2180 [47:55<4:57:44,  9.51s/it]                                                    {'loss': 2.1732, 'learning_rate': 0.0009698180666293083, 'epoch': 0.14}
 14%|â–ˆâ–        | 301/2180 [47:55<4:57:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 302/2180 [48:05<4:57:35,  9.51s/it]                                                    {'loss': 2.2168, 'learning_rate': 0.000969563296418472, 'epoch': 0.14}
 14%|â–ˆâ–        | 302/2180 [48:05<4:57:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 303/2180 [48:14<4:57:48,  9.52s/it]                                                    {'loss': 2.1803, 'learning_rate': 0.0009693074891954914, 'epoch': 0.14}
 14%|â–ˆâ–        | 303/2180 [48:14<4:57:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 304/2180 [48:24<4:57:05,  9.50s/it]                                                    {'loss': 2.204, 'learning_rate': 0.0009690506455253072, 'epoch': 0.14}
 14%|â–ˆâ–        | 304/2180 [48:24<4:57:05,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 305/2180 [48:33<4:56:58,  9.50s/it]                                                    {'loss': 2.2508, 'learning_rate': 0.0009687927659751481, 'epoch': 0.14}
 14%|â–ˆâ–        | 305/2180 [48:33<4:56:58,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 306/2180 [48:43<4:56:44,  9.50s/it]                                                    {'loss': 2.1446, 'learning_rate': 0.0009685338511145312, 'epoch': 0.14}
 14%|â–ˆâ–        | 306/2180 [48:43<4:56:44,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 307/2180 [48:52<4:56:53,  9.51s/it]                                                    {'loss': 2.2337, 'learning_rate': 0.0009682739015152598, 'epoch': 0.14}
 14%|â–ˆâ–        | 307/2180 [48:52<4:56:53,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 308/2180 [49:02<4:56:39,  9.51s/it]                                                    {'loss': 2.2405, 'learning_rate': 0.0009680129177514226, 'epoch': 0.14}
 14%|â–ˆâ–        | 308/2180 [49:02<4:56:39,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 309/2180 [49:11<4:56:30,  9.51s/it]                                                    {'loss': 2.2212, 'learning_rate': 0.0009677509003993915, 'epoch': 0.14}
 14%|â–ˆâ–        | 309/2180 [49:11<4:56:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 310/2180 [49:21<4:56:38,  9.52s/it]                                                    {'loss': 2.2307, 'learning_rate': 0.0009674878500378221, 'epoch': 0.14}
 14%|â–ˆâ–        | 310/2180 [49:21<4:56:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 311/2180 [49:30<4:56:26,  9.52s/it]                                                    {'loss': 2.1798, 'learning_rate': 0.0009672237672476505, 'epoch': 0.14}
 14%|â–ˆâ–        | 311/2180 [49:30<4:56:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 312/2180 [49:40<4:57:05,  9.54s/it]                                                    {'loss': 2.1861, 'learning_rate': 0.0009669586526120935, 'epoch': 0.14}
 14%|â–ˆâ–        | 312/2180 [49:40<4:57:05,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 313/2180 [49:49<4:57:30,  9.56s/it]                                                    {'loss': 2.2084, 'learning_rate': 0.0009666925067166459, 'epoch': 0.14}
 14%|â–ˆâ–        | 313/2180 [49:49<4:57:30,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 314/2180 [49:59<4:57:03,  9.55s/it]                                                    {'loss': 2.2058, 'learning_rate': 0.000966425330149081, 'epoch': 0.14}
 14%|â–ˆâ–        | 314/2180 [49:59<4:57:03,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 315/2180 [50:08<4:56:27,  9.54s/it]                                                    {'loss': 2.1791, 'learning_rate': 0.0009661571234994475, 'epoch': 0.14}
 14%|â–ˆâ–        | 315/2180 [50:08<4:56:27,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 14%|â–ˆâ–        | 316/2180 [50:18<4:56:30,  9.54s/it]                                                    {'loss': 2.1487, 'learning_rate': 0.0009658878873600691, 'epoch': 0.14}
 14%|â–ˆâ–        | 316/2180 [50:18<4:56:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 317/2180 [50:28<4:56:13,  9.54s/it]                                                    {'loss': 2.1989, 'learning_rate': 0.0009656176223255438, 'epoch': 0.15}
 15%|â–ˆâ–        | 317/2180 [50:28<4:56:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 15%|â–ˆâ–        | 318/2180 [50:37<4:55:48,  9.53s/it]                                                    {'loss': 2.2512, 'learning_rate': 0.000965346328992741, 'epoch': 0.15}
 15%|â–ˆâ–        | 318/2180 [50:37<4:55:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 319/2180 [50:47<4:55:27,  9.53s/it]                                                    {'loss': 2.2682, 'learning_rate': 0.0009650740079608014, 'epoch': 0.15}
 15%|â–ˆâ–        | 319/2180 [50:47<4:55:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 320/2180 [50:56<4:55:03,  9.52s/it]                                                    {'loss': 2.2006, 'learning_rate': 0.0009648006598311353, 'epoch': 0.15}
 15%|â–ˆâ–        | 320/2180 [50:56<4:55:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 321/2180 [51:06<4:54:36,  9.51s/it]                                                    {'loss': 2.1061, 'learning_rate': 0.0009645262852074214, 'epoch': 0.15}
 15%|â–ˆâ–        | 321/2180 [51:06<4:54:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 322/2180 [51:15<4:54:27,  9.51s/it]                                                    {'loss': 2.1666, 'learning_rate': 0.0009642508846956053, 'epoch': 0.15}
 15%|â–ˆâ–        | 322/2180 [51:15<4:54:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 323/2180 [51:25<4:54:31,  9.52s/it]                                                    {'loss': 2.2072, 'learning_rate': 0.0009639744589038983, 'epoch': 0.15}
 15%|â–ˆâ–        | 323/2180 [51:25<4:54:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 324/2180 [51:34<4:54:34,  9.52s/it]                                                    {'loss': 2.1772, 'learning_rate': 0.0009636970084427759, 'epoch': 0.15}
 15%|â–ˆâ–        | 324/2180 [51:34<4:54:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 325/2180 [51:44<4:54:27,  9.52s/it]                                                    {'loss': 2.2777, 'learning_rate': 0.0009634185339249766, 'epoch': 0.15}
 15%|â–ˆâ–        | 325/2180 [51:44<4:54:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–        | 326/2180 [51:53<4:54:03,  9.52s/it]                                                    {'loss': 2.2298, 'learning_rate': 0.0009631390359655003, 'epoch': 0.15}
 15%|â–ˆâ–        | 326/2180 [51:53<4:54:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 327/2180 [52:03<4:53:50,  9.51s/it]                                                    {'loss': 2.22, 'learning_rate': 0.0009628585151816074, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 327/2180 [52:03<4:53:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 328/2180 [52:12<4:53:50,  9.52s/it]                                                    {'loss': 2.2798, 'learning_rate': 0.0009625769721928172, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 328/2180 [52:12<4:53:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 329/2180 [52:22<4:55:06,  9.57s/it]                                                    {'loss': 2.1786, 'learning_rate': 0.0009622944076209061, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 329/2180 [52:22<4:55:06,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 330/2180 [52:31<4:54:01,  9.54s/it]                                                    {'loss': 2.2729, 'learning_rate': 0.0009620108220899071, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 330/2180 [52:31<4:54:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 331/2180 [52:41<4:53:20,  9.52s/it]                                                    {'loss': 2.2877, 'learning_rate': 0.0009617262162261075, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 331/2180 [52:41<4:53:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 332/2180 [52:50<4:53:12,  9.52s/it]                                                    {'loss': 2.2639, 'learning_rate': 0.0009614405906580486, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 332/2180 [52:50<4:53:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 333/2180 [53:00<4:53:01,  9.52s/it]                                                    {'loss': 2.2048, 'learning_rate': 0.000961153946016523, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 333/2180 [53:00<4:53:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 334/2180 [53:09<4:52:29,  9.51s/it]                                                    {'loss': 2.2027, 'learning_rate': 0.000960866282934574, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 334/2180 [53:09<4:52:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 335/2180 [53:19<4:53:12,  9.54s/it]                                                    {'loss': 2.2591, 'learning_rate': 0.0009605776020474945, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 335/2180 [53:19<4:53:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 336/2180 [53:28<4:52:34,  9.52s/it]                                                    {'loss': 2.1779, 'learning_rate': 0.0009602879039928249, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 336/2180 [53:28<4:52:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 15%|â–ˆâ–Œ        | 337/2180 [53:38<4:52:08,  9.51s/it]                                                    {'loss': 2.2264, 'learning_rate': 0.0009599971894103521, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 337/2180 [53:38<4:52:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 338/2180 [53:47<4:52:52,  9.54s/it]                                                    {'loss': 2.1965, 'learning_rate': 0.0009597054589421077, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 338/2180 [53:47<4:52:52,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 339/2180 [53:57<4:52:33,  9.54s/it]                                                    {'loss': 2.1527, 'learning_rate': 0.0009594127132323669, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 339/2180 [53:57<4:52:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 340/2180 [54:06<4:52:01,  9.52s/it]                                                    {'loss': 2.2532, 'learning_rate': 0.0009591189529276474, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 340/2180 [54:07<4:52:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 341/2180 [54:16<4:51:11,  9.50s/it]                                                    {'loss': 2.1937, 'learning_rate': 0.0009588241786767072, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 341/2180 [54:16<4:51:11,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 342/2180 [54:25<4:51:03,  9.50s/it]                                                    {'loss': 2.1878, 'learning_rate': 0.0009585283911305436, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 342/2180 [54:25<4:51:03,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 343/2180 [54:35<4:51:12,  9.51s/it]                                                    {'loss': 2.2411, 'learning_rate': 0.000958231590942392, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 343/2180 [54:35<4:51:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 344/2180 [54:45<4:51:19,  9.52s/it]                                                    {'loss': 2.1314, 'learning_rate': 0.0009579337787677238, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 344/2180 [54:45<4:51:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 345/2180 [54:54<4:51:34,  9.53s/it]                                                    {'loss': 2.2856, 'learning_rate': 0.0009576349552642456, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 345/2180 [54:54<4:51:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 346/2180 [55:04<4:51:21,  9.53s/it]                                                    {'loss': 2.2181, 'learning_rate': 0.0009573351210918975, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 346/2180 [55:04<4:51:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 347/2180 [55:13<4:52:25,  9.57s/it]                                                    {'loss': 2.2759, 'learning_rate': 0.0009570342769128514, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 347/2180 [55:13<4:52:25,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 348/2180 [55:23<4:51:21,  9.54s/it]                                                    {'loss': 2.1972, 'learning_rate': 0.0009567324233915099, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 348/2180 [55:23<4:51:21,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 349/2180 [55:32<4:51:10,  9.54s/it]                                                    {'loss': 2.2482, 'learning_rate': 0.0009564295611945047, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 349/2180 [55:32<4:51:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 350/2180 [55:42<4:50:36,  9.53s/it]                                                    {'loss': 2.1874, 'learning_rate': 0.000956125690990695, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 350/2180 [55:42<4:50:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 351/2180 [55:51<4:50:50,  9.54s/it]                                                    {'loss': 2.1875, 'learning_rate': 0.0009558208134511665, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 351/2180 [55:51<4:50:50,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 352/2180 [56:01<4:50:20,  9.53s/it]                                                    {'loss': 2.2241, 'learning_rate': 0.0009555149292492289, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 352/2180 [56:01<4:50:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 353/2180 [56:10<4:49:46,  9.52s/it]                                                    {'loss': 2.2086, 'learning_rate': 0.0009552080390604159, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 353/2180 [56:10<4:49:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–Œ        | 354/2180 [56:20<4:49:46,  9.52s/it]                                                    {'loss': 2.2647, 'learning_rate': 0.0009549001435624823, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 354/2180 [56:20<4:49:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–‹        | 355/2180 [56:29<4:50:01,  9.54s/it]                                                    {'loss': 2.2194, 'learning_rate': 0.0009545912434354029, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 355/2180 [56:29<4:50:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–‹        | 356/2180 [56:39<4:50:34,  9.56s/it]                                                    {'loss': 2.1992, 'learning_rate': 0.0009542813393613721, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 356/2180 [56:39<4:50:34,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–‹        | 357/2180 [56:49<4:50:02,  9.55s/it]                                                    {'loss': 2.0941, 'learning_rate': 0.0009539704320248006, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 357/2180 [56:49<4:50:02,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–‹        | 358/2180 [56:58<4:49:27,  9.53s/it]                                                    {'loss': 2.221, 'learning_rate': 0.0009536585221123151, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 358/2180 [56:58<4:49:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 16%|â–ˆâ–‹        | 359/2180 [57:08<4:49:17,  9.53s/it]                                                    {'loss': 2.1891, 'learning_rate': 0.0009533456103127565, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 359/2180 [57:08<4:49:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 360/2180 [57:17<4:48:49,  9.52s/it]                                                    {'loss': 2.226, 'learning_rate': 0.000953031697317178, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 360/2180 [57:17<4:48:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 361/2180 [57:27<4:48:47,  9.53s/it]                                                    {'loss': 2.1524, 'learning_rate': 0.0009527167838188445, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 361/2180 [57:27<4:48:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 362/2180 [57:36<4:48:42,  9.53s/it]                                                    {'loss': 2.1778, 'learning_rate': 0.0009524008705132299, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 362/2180 [57:36<4:48:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 363/2180 [57:46<4:48:41,  9.53s/it]                                                    {'loss': 2.2022, 'learning_rate': 0.0009520839580980166, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 363/2180 [57:46<4:48:41,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 364/2180 [57:55<4:48:10,  9.52s/it]                                                    {'loss': 2.1682, 'learning_rate': 0.0009517660472730929, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 364/2180 [57:55<4:48:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 365/2180 [58:05<4:47:53,  9.52s/it]                                                    {'loss': 2.2101, 'learning_rate': 0.0009514471387405526, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 365/2180 [58:05<4:47:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 366/2180 [58:14<4:47:44,  9.52s/it]                                                    {'loss': 2.2273, 'learning_rate': 0.0009511272332046926, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 366/2180 [58:14<4:47:44,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 17%|â–ˆâ–‹        | 367/2180 [58:24<4:47:24,  9.51s/it]                                                    {'loss': 2.2393, 'learning_rate': 0.0009508063313720119, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 367/2180 [58:24<4:47:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 17%|â–ˆâ–‹        | 368/2180 [58:33<4:47:12,  9.51s/it]                                                    {'loss': 2.1402, 'learning_rate': 0.0009504844339512095, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 368/2180 [58:33<4:47:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 17%|â–ˆâ–‹        | 369/2180 [58:43<4:46:57,  9.51s/it]                                                    {'loss': 2.2532, 'learning_rate': 0.0009501615416531835, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 369/2180 [58:43<4:46:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 370/2180 [58:52<4:46:49,  9.51s/it]                                                    {'loss': 2.1674, 'learning_rate': 0.0009498376551910285, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 370/2180 [58:52<4:46:49,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 371/2180 [59:02<4:46:56,  9.52s/it]                                                    {'loss': 2.1765, 'learning_rate': 0.0009495127752800352, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 371/2180 [59:02<4:46:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 17%|â–ˆâ–‹        | 372/2180 [59:11<4:46:40,  9.51s/it]                                                    {'loss': 2.1565, 'learning_rate': 0.0009491869026376882, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 372/2180 [59:11<4:46:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 373/2180 [59:21<4:46:23,  9.51s/it]                                                    {'loss': 2.1658, 'learning_rate': 0.0009488600379836648, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 373/2180 [59:21<4:46:23,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 374/2180 [59:30<4:45:55,  9.50s/it]                                                    {'loss': 2.2084, 'learning_rate': 0.0009485321820398321, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 374/2180 [59:30<4:45:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 375/2180 [59:40<4:45:48,  9.50s/it]                                                    {'loss': 2.3177, 'learning_rate': 0.0009482033355302475, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 375/2180 [59:40<4:45:48,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 376/2180 [59:49<4:45:28,  9.49s/it]                                                    {'loss': 2.1594, 'learning_rate': 0.0009478734991811556, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 376/2180 [59:49<4:45:28,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 377/2180 [59:59<4:45:40,  9.51s/it]                                                    {'loss': 2.1814, 'learning_rate': 0.0009475426737209871, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 377/2180 [59:59<4:45:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 378/2180 [1:00:08<4:45:35,  9.51s/it]                                                      {'loss': 2.2075, 'learning_rate': 0.000947210859880357, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 378/2180 [1:00:08<4:45:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 379/2180 [1:00:18<4:45:43,  9.52s/it]                                                      {'loss': 2.2396, 'learning_rate': 0.0009468780583920631, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 379/2180 [1:00:18<4:45:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 17%|â–ˆâ–‹        | 380/2180 [1:00:27<4:45:48,  9.53s/it]                                                      {'loss': 2.1608, 'learning_rate': 0.0009465442699910846, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 380/2180 [1:00:27<4:45:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 17%|â–ˆâ–‹        | 381/2180 [1:00:37<4:45:49,  9.53s/it]                                                      {'loss': 2.1313, 'learning_rate': 0.0009462094954145801, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 381/2180 [1:00:37<4:45:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 382/2180 [1:00:46<4:45:28,  9.53s/it]                                                      {'loss': 2.148, 'learning_rate': 0.0009458737354018859, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 382/2180 [1:00:46<4:45:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 383/2180 [1:00:56<4:45:19,  9.53s/it]                                                      {'loss': 2.2765, 'learning_rate': 0.000945536990694515, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 383/2180 [1:00:56<4:45:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 384/2180 [1:01:05<4:45:01,  9.52s/it]                                                      {'loss': 2.2638, 'learning_rate': 0.0009451992620361551, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 384/2180 [1:01:06<4:45:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 385/2180 [1:01:15<4:44:37,  9.51s/it]                                                      {'loss': 2.1994, 'learning_rate': 0.0009448605501726664, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 385/2180 [1:01:15<4:44:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 386/2180 [1:01:24<4:44:22,  9.51s/it]                                                      {'loss': 2.1676, 'learning_rate': 0.000944520855852081, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 386/2180 [1:01:25<4:44:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 387/2180 [1:01:34<4:43:58,  9.50s/it]                                                      {'loss': 2.1001, 'learning_rate': 0.0009441801798246002, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 387/2180 [1:01:34<4:43:58,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 388/2180 [1:01:44<4:44:13,  9.52s/it]                                                      {'loss': 2.1873, 'learning_rate': 0.0009438385228425939, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 388/2180 [1:01:44<4:44:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 389/2180 [1:01:53<4:44:07,  9.52s/it]                                                      {'loss': 2.2261, 'learning_rate': 0.0009434958856605982, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 389/2180 [1:01:53<4:44:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 390/2180 [1:02:03<4:43:26,  9.50s/it]                                                      {'loss': 2.168, 'learning_rate': 0.0009431522690353137, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 390/2180 [1:02:03<4:43:26,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 391/2180 [1:02:12<4:43:13,  9.50s/it]                                                      {'loss': 2.1404, 'learning_rate': 0.0009428076737256044, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 391/2180 [1:02:12<4:43:13,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 392/2180 [1:02:21<4:42:55,  9.49s/it]                                                      {'loss': 2.2391, 'learning_rate': 0.0009424621004924954, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 392/2180 [1:02:21<4:42:55,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 393/2180 [1:02:31<4:42:32,  9.49s/it]                                                      {'loss': 2.2873, 'learning_rate': 0.0009421155500991719, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 393/2180 [1:02:31<4:42:32,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 394/2180 [1:02:40<4:42:39,  9.50s/it]                                                      {'loss': 2.1093, 'learning_rate': 0.0009417680233109767, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 394/2180 [1:02:40<4:42:39,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 395/2180 [1:02:50<4:42:36,  9.50s/it]                                                      {'loss': 2.2304, 'learning_rate': 0.000941419520895409, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 395/2180 [1:02:50<4:42:36,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 396/2180 [1:02:59<4:42:12,  9.49s/it]                                                      {'loss': 2.1923, 'learning_rate': 0.0009410700436221229, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 396/2180 [1:02:59<4:42:12,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 397/2180 [1:03:09<4:42:29,  9.51s/it]                                                      {'loss': 2.1331, 'learning_rate': 0.0009407195922629252, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 397/2180 [1:03:09<4:42:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 398/2180 [1:03:18<4:42:00,  9.50s/it]                                                      {'loss': 2.2562, 'learning_rate': 0.000940368167591774, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 398/2180 [1:03:18<4:42:00,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])



 18%|â–ˆâ–Š        | 399/2180 [1:03:28<4:41:33,  9.49s/it]                                                      {'loss': 2.1526, 'learning_rate': 0.0009400157703847769, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 399/2180 [1:03:28<4:41:33,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 400/2180 [1:03:38<4:42:30,  9.52s/it]                                                      {'loss': 2.1826, 'learning_rate': 0.0009396624014201895, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 400/2180 [1:03:38<4:42:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 401/2180 [1:03:47<4:42:08,  9.52s/it]                                                      {'loss': 2.2988, 'learning_rate': 0.000939308061478413, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 401/2180 [1:03:47<4:42:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 402/2180 [1:03:57<4:42:22,  9.53s/it]                                                      {'loss': 2.1746, 'learning_rate': 0.0009389527513419935, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 402/2180 [1:03:57<4:42:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 18%|â–ˆâ–Š        | 403/2180 [1:04:06<4:42:13,  9.53s/it]                                                      {'loss': 2.1488, 'learning_rate': 0.0009385964717956195, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 403/2180 [1:04:06<4:42:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–Š        | 404/2180 [1:04:16<4:41:39,  9.52s/it]                                                      {'loss': 2.1773, 'learning_rate': 0.0009382392236261201, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 404/2180 [1:04:16<4:41:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–Š        | 405/2180 [1:04:25<4:41:19,  9.51s/it]                                                      {'loss': 2.2296, 'learning_rate': 0.0009378810076224644, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 405/2180 [1:04:25<4:41:19,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–Š        | 406/2180 [1:04:35<4:41:07,  9.51s/it]                                                      {'loss': 2.162, 'learning_rate': 0.0009375218245757582, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 406/2180 [1:04:35<4:41:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–Š        | 407/2180 [1:04:44<4:40:54,  9.51s/it]                                                      {'loss': 2.1379, 'learning_rate': 0.0009371616752792432, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 407/2180 [1:04:44<4:40:54,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–Š        | 408/2180 [1:04:54<4:40:48,  9.51s/it]                                                      {'loss': 2.2599, 'learning_rate': 0.0009368005605282949, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 408/2180 [1:04:54<4:40:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 409/2180 [1:05:03<4:40:37,  9.51s/it]                                                      {'loss': 2.2324, 'learning_rate': 0.0009364384811204212, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 409/2180 [1:05:03<4:40:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 410/2180 [1:05:13<4:40:26,  9.51s/it]                                                      {'loss': 2.154, 'learning_rate': 0.00093607543785526, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 410/2180 [1:05:13<4:40:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 411/2180 [1:05:22<4:40:25,  9.51s/it]                                                      {'loss': 2.1679, 'learning_rate': 0.0009357114315345787, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 411/2180 [1:05:22<4:40:25,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 412/2180 [1:05:32<4:40:34,  9.52s/it]                                                      {'loss': 2.1662, 'learning_rate': 0.0009353464629622705, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 412/2180 [1:05:32<4:40:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 413/2180 [1:05:41<4:40:07,  9.51s/it]                                                      {'loss': 2.2053, 'learning_rate': 0.0009349805329443544, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 413/2180 [1:05:41<4:40:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 414/2180 [1:05:51<4:39:55,  9.51s/it]                                                      {'loss': 2.2087, 'learning_rate': 0.0009346136422889724, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 414/2180 [1:05:51<4:39:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 415/2180 [1:06:00<4:40:24,  9.53s/it]                                                      {'loss': 2.1643, 'learning_rate': 0.0009342457918063882, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 415/2180 [1:06:00<4:40:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 416/2180 [1:06:10<4:39:48,  9.52s/it]                                                      {'loss': 2.1984, 'learning_rate': 0.0009338769823089853, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 416/2180 [1:06:10<4:39:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 417/2180 [1:06:19<4:39:31,  9.51s/it]                                                      {'loss': 2.2962, 'learning_rate': 0.0009335072146112648, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 417/2180 [1:06:19<4:39:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 19%|â–ˆâ–‰        | 418/2180 [1:06:29<4:38:57,  9.50s/it]                                                      {'loss': 2.118, 'learning_rate': 0.0009331364895298444, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 418/2180 [1:06:29<4:38:57,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 419/2180 [1:06:38<4:38:35,  9.49s/it]                                                      {'loss': 2.2186, 'learning_rate': 0.0009327648078834559, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 419/2180 [1:06:38<4:38:35,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 420/2180 [1:06:48<4:38:33,  9.50s/it]                                                      {'loss': 2.1963, 'learning_rate': 0.0009323921704929434, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 420/2180 [1:06:48<4:38:33,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 421/2180 [1:06:57<4:38:07,  9.49s/it]                                                      {'loss': 2.08, 'learning_rate': 0.0009320185781812623, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 421/2180 [1:06:57<4:38:07,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 422/2180 [1:07:07<4:38:25,  9.50s/it]                                                      {'loss': 2.1344, 'learning_rate': 0.0009316440317734762, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 422/2180 [1:07:07<4:38:25,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 423/2180 [1:07:16<4:38:23,  9.51s/it]                                                      {'loss': 2.2379, 'learning_rate': 0.0009312685320967565, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 423/2180 [1:07:16<4:38:23,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 424/2180 [1:07:26<4:38:46,  9.53s/it]                                                      {'loss': 2.1417, 'learning_rate': 0.0009308920799803793, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 424/2180 [1:07:26<4:38:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 19%|â–ˆâ–‰        | 425/2180 [1:07:35<4:38:08,  9.51s/it]                                                      {'loss': 2.2057, 'learning_rate': 0.0009305146762557246, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 425/2180 [1:07:35<4:38:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 426/2180 [1:07:45<4:37:51,  9.50s/it]                                                      {'loss': 2.1745, 'learning_rate': 0.0009301363217562736, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 426/2180 [1:07:45<4:37:51,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 427/2180 [1:07:54<4:38:19,  9.53s/it]                                                      {'loss': 2.1613, 'learning_rate': 0.0009297570173176074, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 427/2180 [1:07:54<4:38:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 428/2180 [1:08:04<4:37:56,  9.52s/it]                                                      {'loss': 2.1658, 'learning_rate': 0.000929376763777405, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 428/2180 [1:08:04<4:37:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 429/2180 [1:08:13<4:37:26,  9.51s/it]                                                      {'loss': 2.196, 'learning_rate': 0.0009289955619754413, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 429/2180 [1:08:13<4:37:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 430/2180 [1:08:23<4:37:30,  9.51s/it]                                                      {'loss': 2.2183, 'learning_rate': 0.0009286134127535859, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 430/2180 [1:08:23<4:37:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 431/2180 [1:08:32<4:37:31,  9.52s/it]                                                      {'loss': 2.244, 'learning_rate': 0.0009282303169558, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 431/2180 [1:08:32<4:37:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 432/2180 [1:08:42<4:37:37,  9.53s/it]                                                      {'loss': 2.1755, 'learning_rate': 0.0009278462754281359, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 432/2180 [1:08:42<4:37:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 20%|â–ˆâ–‰        | 433/2180 [1:08:51<4:37:16,  9.52s/it]                                                      {'loss': 2.1884, 'learning_rate': 0.0009274612890187342, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 433/2180 [1:08:51<4:37:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 434/2180 [1:09:01<4:36:57,  9.52s/it]                                                      {'loss': 2.1589, 'learning_rate': 0.0009270753585778222, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 434/2180 [1:09:01<4:36:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–‰        | 435/2180 [1:09:10<4:36:50,  9.52s/it]                                                      {'loss': 2.1718, 'learning_rate': 0.0009266884849577124, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 435/2180 [1:09:10<4:36:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 436/2180 [1:09:20<4:36:34,  9.52s/it]                                                      {'loss': 2.1482, 'learning_rate': 0.0009263006690127998, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 436/2180 [1:09:20<4:36:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 437/2180 [1:09:29<4:36:00,  9.50s/it]                                                      {'loss': 2.1611, 'learning_rate': 0.0009259119115995609, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 437/2180 [1:09:29<4:36:00,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 438/2180 [1:09:39<4:35:54,  9.50s/it]                                                      {'loss': 2.1474, 'learning_rate': 0.0009255222135765511, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 438/2180 [1:09:39<4:35:54,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 20%|â–ˆâ–ˆ        | 439/2180 [1:09:48<4:35:47,  9.50s/it]                                                      {'loss': 2.2418, 'learning_rate': 0.0009251315758044032, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 439/2180 [1:09:48<4:35:47,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 440/2180 [1:09:58<4:35:20,  9.49s/it]                                                      {'loss': 2.1327, 'learning_rate': 0.0009247399991458255, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 440/2180 [1:09:58<4:35:20,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 441/2180 [1:10:07<4:35:16,  9.50s/it]                                                      {'loss': 2.2115, 'learning_rate': 0.0009243474844655996, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 441/2180 [1:10:07<4:35:16,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 442/2180 [1:10:17<4:35:01,  9.49s/it]                                                      {'loss': 2.1577, 'learning_rate': 0.0009239540326305791, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 442/2180 [1:10:17<4:35:01,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 443/2180 [1:10:26<4:35:18,  9.51s/it]                                                      {'loss': 2.2164, 'learning_rate': 0.0009235596445096864, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 443/2180 [1:10:26<4:35:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 444/2180 [1:10:36<4:35:04,  9.51s/it]                                                      {'loss': 2.172, 'learning_rate': 0.0009231643209739127, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 444/2180 [1:10:36<4:35:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 445/2180 [1:10:45<4:34:39,  9.50s/it]                                                      {'loss': 2.216, 'learning_rate': 0.0009227680628963145, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 445/2180 [1:10:45<4:34:39,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 20%|â–ˆâ–ˆ        | 446/2180 [1:10:55<4:34:32,  9.50s/it]                                                      {'loss': 2.2074, 'learning_rate': 0.000922370871152012, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 446/2180 [1:10:55<4:34:32,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 447/2180 [1:11:04<4:34:13,  9.49s/it]                                                      {'loss': 2.2182, 'learning_rate': 0.0009219727466181877, 'epoch': 0.2}
 21%|â–ˆâ–ˆ        | 447/2180 [1:11:04<4:34:13,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 21%|â–ˆâ–ˆ        | 448/2180 [1:11:14<4:34:21,  9.50s/it]                                                      {'loss': 2.1836, 'learning_rate': 0.0009215736901740841, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 448/2180 [1:11:14<4:34:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 449/2180 [1:11:24<4:34:40,  9.52s/it]                                                      {'loss': 2.0843, 'learning_rate': 0.0009211737027010016, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 449/2180 [1:11:24<4:34:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 450/2180 [1:11:33<4:34:22,  9.52s/it]                                                      {'loss': 2.0698, 'learning_rate': 0.0009207727850822971, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 450/2180 [1:11:33<4:34:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 451/2180 [1:11:43<4:35:05,  9.55s/it]                                                      {'loss': 2.2164, 'learning_rate': 0.0009203709382033814, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 451/2180 [1:11:43<4:35:05,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 452/2180 [1:11:52<4:34:38,  9.54s/it]                                                      {'loss': 2.209, 'learning_rate': 0.0009199681629517173, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 452/2180 [1:11:52<4:34:38,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 453/2180 [1:12:02<4:34:38,  9.54s/it]                                                      {'loss': 2.1647, 'learning_rate': 0.0009195644602168184, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 453/2180 [1:12:02<4:34:38,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 454/2180 [1:12:11<4:34:22,  9.54s/it]                                                      {'loss': 2.1952, 'learning_rate': 0.0009191598308902464, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 454/2180 [1:12:11<4:34:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 455/2180 [1:12:21<4:33:50,  9.52s/it]                                                      {'loss': 2.1087, 'learning_rate': 0.0009187542758656091, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 455/2180 [1:12:21<4:33:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 456/2180 [1:12:30<4:33:29,  9.52s/it]                                                      {'loss': 2.0993, 'learning_rate': 0.0009183477960385591, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 456/2180 [1:12:30<4:33:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 457/2180 [1:12:40<4:34:52,  9.57s/it]                                                      {'loss': 2.188, 'learning_rate': 0.0009179403923067912, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 457/2180 [1:12:40<4:34:52,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 458/2180 [1:12:49<4:33:56,  9.54s/it]                                                      {'loss': 2.174, 'learning_rate': 0.0009175320655700406, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 458/2180 [1:12:49<4:33:56,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 459/2180 [1:12:59<4:34:14,  9.56s/it]                                                      {'loss': 2.185, 'learning_rate': 0.0009171228167300805, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 459/2180 [1:12:59<4:34:14,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 460/2180 [1:13:09<4:33:40,  9.55s/it]                                                      {'loss': 2.1327, 'learning_rate': 0.0009167126466907215, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 460/2180 [1:13:09<4:33:40,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 461/2180 [1:13:18<4:33:14,  9.54s/it]                                                      {'loss': 2.2642, 'learning_rate': 0.0009163015563578074, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 461/2180 [1:13:18<4:33:14,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 462/2180 [1:13:28<4:32:37,  9.52s/it]                                                      {'loss': 2.0935, 'learning_rate': 0.0009158895466392158, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 462/2180 [1:13:28<4:32:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆ        | 463/2180 [1:13:37<4:32:33,  9.52s/it]                                                      {'loss': 2.1171, 'learning_rate': 0.0009154766184448535, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 463/2180 [1:13:37<4:32:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆâ–       | 464/2180 [1:13:47<4:31:48,  9.50s/it]                                                      {'loss': 2.1826, 'learning_rate': 0.0009150627726866568, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 464/2180 [1:13:47<4:31:48,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆâ–       | 465/2180 [1:13:56<4:31:27,  9.50s/it]                                                      {'loss': 2.1805, 'learning_rate': 0.000914648010278587, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 465/2180 [1:13:56<4:31:27,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆâ–       | 466/2180 [1:14:06<4:31:24,  9.50s/it]                                                      {'loss': 2.1577, 'learning_rate': 0.0009142323321366315, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 466/2180 [1:14:06<4:31:24,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆâ–       | 467/2180 [1:14:15<4:31:06,  9.50s/it]                                                      {'loss': 2.1581, 'learning_rate': 0.0009138157391787986, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 467/2180 [1:14:15<4:31:06,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 21%|â–ˆâ–ˆâ–       | 468/2180 [1:14:25<4:31:06,  9.50s/it]                                                      {'loss': 2.0682, 'learning_rate': 0.0009133982323251177, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 468/2180 [1:14:25<4:31:06,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 469/2180 [1:14:34<4:30:43,  9.49s/it]                                                      {'loss': 2.2439, 'learning_rate': 0.0009129798124976365, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 469/2180 [1:14:34<4:30:43,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 470/2180 [1:14:44<4:30:41,  9.50s/it]                                                      {'loss': 2.1901, 'learning_rate': 0.0009125604806204187, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 470/2180 [1:14:44<4:30:41,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 471/2180 [1:14:53<4:31:14,  9.52s/it]                                                      {'loss': 2.1227, 'learning_rate': 0.0009121402376195421, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 471/2180 [1:14:53<4:31:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 472/2180 [1:15:03<4:31:56,  9.55s/it]                                                      {'loss': 2.0928, 'learning_rate': 0.0009117190844230972, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 472/2180 [1:15:03<4:31:56,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 473/2180 [1:15:12<4:32:09,  9.57s/it]                                                      {'loss': 2.1484, 'learning_rate': 0.0009112970219611841, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 473/2180 [1:15:12<4:32:09,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 474/2180 [1:15:22<4:31:25,  9.55s/it]                                                      {'loss': 2.2253, 'learning_rate': 0.0009108740511659115, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 474/2180 [1:15:22<4:31:25,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 475/2180 [1:15:31<4:30:40,  9.53s/it]                                                      {'loss': 2.1232, 'learning_rate': 0.0009104501729713935, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 475/2180 [1:15:31<4:30:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 476/2180 [1:15:41<4:29:56,  9.51s/it]                                                      {'loss': 2.2341, 'learning_rate': 0.0009100253883137488, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 476/2180 [1:15:41<4:29:56,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 477/2180 [1:15:50<4:29:46,  9.50s/it]                                                      {'loss': 2.173, 'learning_rate': 0.0009095996981310974, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 477/2180 [1:15:50<4:29:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 478/2180 [1:16:00<4:29:56,  9.52s/it]                                                      {'loss': 2.221, 'learning_rate': 0.0009091731033635596, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 478/2180 [1:16:00<4:29:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 479/2180 [1:16:09<4:29:25,  9.50s/it]                                                      {'loss': 2.128, 'learning_rate': 0.0009087456049532529, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 479/2180 [1:16:09<4:29:25,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 480/2180 [1:16:19<4:29:36,  9.52s/it]                                                      {'loss': 2.1498, 'learning_rate': 0.0009083172038442914, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 480/2180 [1:16:19<4:29:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 481/2180 [1:16:28<4:29:26,  9.52s/it]                                                      {'loss': 2.0859, 'learning_rate': 0.0009078879009827817, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 481/2180 [1:16:28<4:29:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 482/2180 [1:16:38<4:29:18,  9.52s/it]                                                      {'loss': 2.1335, 'learning_rate': 0.0009074576973168223, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 482/2180 [1:16:38<4:29:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 483/2180 [1:16:47<4:28:53,  9.51s/it]                                                      {'loss': 2.2322, 'learning_rate': 0.0009070265937965015, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 483/2180 [1:16:47<4:28:53,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 22%|â–ˆâ–ˆâ–       | 484/2180 [1:16:57<4:28:44,  9.51s/it]                                                      {'loss': 2.0892, 'learning_rate': 0.0009065945913738942, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 484/2180 [1:16:57<4:28:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 485/2180 [1:17:06<4:28:21,  9.50s/it]                                                      {'loss': 2.1373, 'learning_rate': 0.0009061616910030609, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 485/2180 [1:17:06<4:28:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 486/2180 [1:17:16<4:28:56,  9.53s/it]                                                      {'loss': 2.0922, 'learning_rate': 0.0009057278936400453, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 486/2180 [1:17:16<4:28:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 487/2180 [1:17:25<4:28:40,  9.52s/it]                                                      {'loss': 2.278, 'learning_rate': 0.0009052932002428715, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 487/2180 [1:17:25<4:28:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 488/2180 [1:17:35<4:28:14,  9.51s/it]                                                      {'loss': 2.1943, 'learning_rate': 0.0009048576117715435, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 488/2180 [1:17:35<4:28:14,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 489/2180 [1:17:44<4:27:45,  9.50s/it]                                                      {'loss': 2.1139, 'learning_rate': 0.0009044211291880407, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 489/2180 [1:17:44<4:27:45,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 22%|â–ˆâ–ˆâ–       | 490/2180 [1:17:54<4:27:39,  9.50s/it]                                                      {'loss': 2.108, 'learning_rate': 0.000903983753456318, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 490/2180 [1:17:54<4:27:39,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 23%|â–ˆâ–ˆâ–Ž       | 491/2180 [1:18:03<4:27:30,  9.50s/it]                                                      {'loss': 2.1561, 'learning_rate': 0.0009035454855423026, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 491/2180 [1:18:03<4:27:30,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 492/2180 [1:18:13<4:27:12,  9.50s/it]                                                      {'loss': 2.2262, 'learning_rate': 0.0009031063264138922, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 492/2180 [1:18:13<4:27:12,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 493/2180 [1:18:22<4:27:13,  9.50s/it]                                                      {'loss': 2.157, 'learning_rate': 0.0009026662770409522, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 493/2180 [1:18:22<4:27:13,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 494/2180 [1:18:32<4:26:49,  9.50s/it]                                                      {'loss': 2.2897, 'learning_rate': 0.0009022253383953147, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 494/2180 [1:18:32<4:26:49,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 23%|â–ˆâ–ˆâ–Ž       | 495/2180 [1:18:41<4:26:46,  9.50s/it]                                                      {'loss': 2.1728, 'learning_rate': 0.0009017835114507753, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 495/2180 [1:18:41<4:26:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 496/2180 [1:18:51<4:26:52,  9.51s/it]                                                      {'loss': 2.1022, 'learning_rate': 0.0009013407971830914, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 496/2180 [1:18:51<4:26:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 497/2180 [1:19:00<4:26:50,  9.51s/it]                                                      {'loss': 2.1505, 'learning_rate': 0.0009008971965699801, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 497/2180 [1:19:00<4:26:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 498/2180 [1:19:10<4:26:38,  9.51s/it]                                                      {'loss': 2.1369, 'learning_rate': 0.0009004527105911163, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 498/2180 [1:19:10<4:26:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 499/2180 [1:19:19<4:26:32,  9.51s/it]                                                      {'loss': 2.0818, 'learning_rate': 0.0009000073402281295, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 499/2180 [1:19:19<4:26:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 500/2180 [1:19:29<4:26:28,  9.52s/it]                                                      {'loss': 2.1782, 'learning_rate': 0.0008995610864646028, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 500/2180 [1:19:29<4:26:28,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 501/2180 [1:19:39<4:26:35,  9.53s/it]                                                      {'loss': 2.0862, 'learning_rate': 0.0008991139502860703, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 501/2180 [1:19:39<4:26:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 502/2180 [1:19:48<4:26:14,  9.52s/it]                                                      {'loss': 2.1624, 'learning_rate': 0.0008986659326800146, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 502/2180 [1:19:48<4:26:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 503/2180 [1:19:58<4:25:51,  9.51s/it]                                                      {'loss': 2.1695, 'learning_rate': 0.0008982170346358651, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 503/2180 [1:19:58<4:25:51,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 504/2180 [1:20:07<4:25:35,  9.51s/it]                                                      {'loss': 2.1094, 'learning_rate': 0.0008977672571449956, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 504/2180 [1:20:07<4:25:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 505/2180 [1:20:17<4:26:16,  9.54s/it]                                                      {'loss': 2.1505, 'learning_rate': 0.0008973166012007217, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 505/2180 [1:20:17<4:26:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 506/2180 [1:20:26<4:26:10,  9.54s/it]                                                      {'loss': 2.1582, 'learning_rate': 0.0008968650677982998, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 506/2180 [1:20:26<4:26:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 507/2180 [1:20:36<4:25:40,  9.53s/it]                                                      {'loss': 2.0808, 'learning_rate': 0.0008964126579349236, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 507/2180 [1:20:36<4:25:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 508/2180 [1:20:45<4:25:36,  9.53s/it]                                                      {'loss': 2.201, 'learning_rate': 0.0008959593726097226, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 508/2180 [1:20:45<4:25:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 509/2180 [1:20:55<4:25:03,  9.52s/it]                                                      {'loss': 2.1612, 'learning_rate': 0.0008955052128237596, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 509/2180 [1:20:55<4:25:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 510/2180 [1:21:04<4:25:02,  9.52s/it]                                                      {'loss': 2.1552, 'learning_rate': 0.0008950501795800288, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 510/2180 [1:21:04<4:25:02,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 511/2180 [1:21:14<4:25:17,  9.54s/it]                                                      {'loss': 2.1118, 'learning_rate': 0.0008945942738834532, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 511/2180 [1:21:14<4:25:17,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 23%|â–ˆâ–ˆâ–Ž       | 512/2180 [1:21:23<4:25:15,  9.54s/it]                                                      {'loss': 2.1611, 'learning_rate': 0.0008941374967408826, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 512/2180 [1:21:23<4:25:15,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–Ž       | 513/2180 [1:21:33<4:25:24,  9.55s/it]                                                      {'loss': 2.1099, 'learning_rate': 0.0008936798491610916, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 513/2180 [1:21:33<4:25:24,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–Ž       | 514/2180 [1:21:42<4:24:55,  9.54s/it]                                                      {'loss': 2.2299, 'learning_rate': 0.0008932213321547768, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 514/2180 [1:21:42<4:24:55,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–Ž       | 515/2180 [1:21:52<4:24:29,  9.53s/it]                                                      {'loss': 2.1898, 'learning_rate': 0.0008927619467345554, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 515/2180 [1:21:52<4:24:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–Ž       | 516/2180 [1:22:01<4:24:06,  9.52s/it]                                                      {'loss': 2.0631, 'learning_rate': 0.0008923016939149615, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 516/2180 [1:22:01<4:24:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 24%|â–ˆâ–ˆâ–Ž       | 517/2180 [1:22:11<4:23:40,  9.51s/it]                                                      {'loss': 2.1871, 'learning_rate': 0.0008918405747124458, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 517/2180 [1:22:11<4:23:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 518/2180 [1:22:20<4:23:33,  9.51s/it]                                                      {'loss': 2.1529, 'learning_rate': 0.0008913785901453721, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 518/2180 [1:22:20<4:23:33,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 24%|â–ˆâ–ˆâ–       | 519/2180 [1:22:30<4:23:27,  9.52s/it]                                                      {'loss': 2.231, 'learning_rate': 0.000890915741234015, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 519/2180 [1:22:30<4:23:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 24%|â–ˆâ–ˆâ–       | 520/2180 [1:22:40<4:23:16,  9.52s/it]                                                      {'loss': 2.2367, 'learning_rate': 0.0008904520290005582, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 520/2180 [1:22:40<4:23:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 521/2180 [1:22:49<4:22:43,  9.50s/it]                                                      {'loss': 2.1196, 'learning_rate': 0.000889987454469092, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 521/2180 [1:22:49<4:22:43,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 522/2180 [1:22:59<4:22:52,  9.51s/it]                                                      {'loss': 2.2127, 'learning_rate': 0.0008895220186656111, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 522/2180 [1:22:59<4:22:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 523/2180 [1:23:08<4:23:01,  9.52s/it]                                                      {'loss': 2.2314, 'learning_rate': 0.0008890557226180122, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 523/2180 [1:23:08<4:23:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 524/2180 [1:23:18<4:22:43,  9.52s/it]                                                      {'loss': 2.0565, 'learning_rate': 0.0008885885673560921, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 524/2180 [1:23:18<4:22:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 525/2180 [1:23:27<4:22:24,  9.51s/it]                                                      {'loss': 2.1901, 'learning_rate': 0.0008881205539115444, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 525/2180 [1:23:27<4:22:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 526/2180 [1:23:37<4:22:19,  9.52s/it]                                                      {'loss': 2.1746, 'learning_rate': 0.0008876516833179589, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 526/2180 [1:23:37<4:22:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 527/2180 [1:23:46<4:22:05,  9.51s/it]                                                      {'loss': 2.1183, 'learning_rate': 0.0008871819566108177, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 527/2180 [1:23:46<4:22:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 528/2180 [1:23:56<4:21:57,  9.51s/it]                                                      {'loss': 2.1873, 'learning_rate': 0.000886711374827494, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 528/2180 [1:23:56<4:21:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 529/2180 [1:24:05<4:21:36,  9.51s/it]                                                      {'loss': 2.2072, 'learning_rate': 0.0008862399390072491, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 529/2180 [1:24:05<4:21:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 530/2180 [1:24:15<4:21:40,  9.52s/it]                                                      {'loss': 2.1268, 'learning_rate': 0.0008857676501912305, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 530/2180 [1:24:15<4:21:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 531/2180 [1:24:24<4:21:21,  9.51s/it]                                                      {'loss': 2.1247, 'learning_rate': 0.0008852945094224697, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 531/2180 [1:24:24<4:21:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 532/2180 [1:24:34<4:21:21,  9.52s/it]                                                      {'loss': 2.1709, 'learning_rate': 0.0008848205177458795, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 532/2180 [1:24:34<4:21:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 533/2180 [1:24:43<4:21:12,  9.52s/it]                                                      {'loss': 2.0959, 'learning_rate': 0.0008843456762082518, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 533/2180 [1:24:43<4:21:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 24%|â–ˆâ–ˆâ–       | 534/2180 [1:24:53<4:20:56,  9.51s/it]                                                      {'loss': 2.1062, 'learning_rate': 0.0008838699858582557, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 534/2180 [1:24:53<4:20:56,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 535/2180 [1:25:02<4:20:57,  9.52s/it]                                                      {'loss': 2.2219, 'learning_rate': 0.0008833934477464347, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 535/2180 [1:25:02<4:20:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 25%|â–ˆâ–ˆâ–       | 536/2180 [1:25:12<4:20:44,  9.52s/it]                                                      {'loss': 2.1304, 'learning_rate': 0.0008829160629252045, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 536/2180 [1:25:12<4:20:44,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 537/2180 [1:25:21<4:20:34,  9.52s/it]                                                      {'loss': 2.1529, 'learning_rate': 0.0008824378324488509, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 537/2180 [1:25:21<4:20:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 538/2180 [1:25:31<4:20:11,  9.51s/it]                                                      {'loss': 2.1894, 'learning_rate': 0.0008819587573735268, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 538/2180 [1:25:31<4:20:11,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 539/2180 [1:25:40<4:20:04,  9.51s/it]                                                      {'loss': 2.1989, 'learning_rate': 0.0008814788387572513, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 539/2180 [1:25:40<4:20:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 540/2180 [1:25:50<4:20:16,  9.52s/it]                                                      {'loss': 2.1713, 'learning_rate': 0.0008809980776599053, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 540/2180 [1:25:50<4:20:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 541/2180 [1:25:59<4:20:01,  9.52s/it]                                                      {'loss': 2.139, 'learning_rate': 0.0008805164751432312, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 541/2180 [1:25:59<4:20:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 542/2180 [1:26:09<4:19:48,  9.52s/it]                                                      {'loss': 2.1599, 'learning_rate': 0.0008800340322708292, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 542/2180 [1:26:09<4:19:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 543/2180 [1:26:18<4:19:32,  9.51s/it]                                                      {'loss': 2.1018, 'learning_rate': 0.0008795507501081555, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 543/2180 [1:26:18<4:19:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–       | 544/2180 [1:26:28<4:19:02,  9.50s/it]                                                      {'loss': 2.1182, 'learning_rate': 0.0008790666297225196, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 544/2180 [1:26:28<4:19:02,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 545/2180 [1:26:37<4:18:55,  9.50s/it]                                                      {'loss': 2.0736, 'learning_rate': 0.0008785816721830829, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 545/2180 [1:26:37<4:18:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 546/2180 [1:26:47<4:18:43,  9.50s/it]                                                      {'loss': 2.1097, 'learning_rate': 0.0008780958785608546, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 546/2180 [1:26:47<4:18:43,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 547/2180 [1:26:56<4:18:40,  9.50s/it]                                                      {'loss': 2.1353, 'learning_rate': 0.0008776092499286912, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 547/2180 [1:26:56<4:18:40,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 548/2180 [1:27:06<4:18:33,  9.51s/it]                                                      {'loss': 2.0365, 'learning_rate': 0.0008771217873612929, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 548/2180 [1:27:06<4:18:33,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 549/2180 [1:27:15<4:18:21,  9.50s/it]                                                      {'loss': 2.1575, 'learning_rate': 0.0008766334919352017, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 549/2180 [1:27:15<4:18:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 550/2180 [1:27:25<4:19:07,  9.54s/it]                                                      {'loss': 2.1865, 'learning_rate': 0.0008761443647287987, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 550/2180 [1:27:25<4:19:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 551/2180 [1:27:34<4:18:50,  9.53s/it]                                                      {'loss': 2.2082, 'learning_rate': 0.0008756544068223026, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 551/2180 [1:27:34<4:18:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 552/2180 [1:27:44<4:18:23,  9.52s/it]                                                      {'loss': 2.2313, 'learning_rate': 0.0008751636192977659, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 552/2180 [1:27:44<4:18:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 553/2180 [1:27:53<4:17:57,  9.51s/it]                                                      {'loss': 2.0795, 'learning_rate': 0.0008746720032390737, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 553/2180 [1:27:53<4:17:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 554/2180 [1:28:03<4:17:44,  9.51s/it]                                                      {'loss': 2.1147, 'learning_rate': 0.0008741795597319408, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 554/2180 [1:28:03<4:17:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 25%|â–ˆâ–ˆâ–Œ       | 555/2180 [1:28:12<4:17:14,  9.50s/it]                                                      {'loss': 2.107, 'learning_rate': 0.0008736862898639095, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 555/2180 [1:28:12<4:17:14,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 556/2180 [1:28:22<4:17:25,  9.51s/it]                                                      {'loss': 2.2021, 'learning_rate': 0.0008731921947243468, 'epoch': 0.25}
 26%|â–ˆâ–ˆâ–Œ       | 556/2180 [1:28:22<4:17:25,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 557/2180 [1:28:31<4:17:22,  9.51s/it]                                                      {'loss': 2.1711, 'learning_rate': 0.0008726972754044427, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 557/2180 [1:28:31<4:17:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 558/2180 [1:28:41<4:17:32,  9.53s/it]                                                      {'loss': 2.2229, 'learning_rate': 0.0008722015329972069, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 558/2180 [1:28:41<4:17:32,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 559/2180 [1:28:51<4:17:21,  9.53s/it]                                                      {'loss': 2.1959, 'learning_rate': 0.0008717049685974672, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 559/2180 [1:28:51<4:17:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 560/2180 [1:29:00<4:17:06,  9.52s/it]                                                      {'loss': 2.2528, 'learning_rate': 0.0008712075833018665, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 560/2180 [1:29:00<4:17:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 561/2180 [1:29:10<4:16:30,  9.51s/it]                                                      {'loss': 2.0879, 'learning_rate': 0.0008707093782088608, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 561/2180 [1:29:10<4:16:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 562/2180 [1:29:19<4:16:38,  9.52s/it]                                                      {'loss': 2.1723, 'learning_rate': 0.0008702103544187167, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 562/2180 [1:29:19<4:16:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 563/2180 [1:29:29<4:16:33,  9.52s/it]                                                      {'loss': 2.1443, 'learning_rate': 0.0008697105130335085, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 563/2180 [1:29:29<4:16:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 26%|â–ˆâ–ˆâ–Œ       | 564/2180 [1:29:38<4:16:38,  9.53s/it]                                                      {'loss': 2.1904, 'learning_rate': 0.0008692098551571164, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 564/2180 [1:29:38<4:16:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 565/2180 [1:29:48<4:16:10,  9.52s/it]                                                      {'loss': 2.1771, 'learning_rate': 0.0008687083818952235, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 565/2180 [1:29:48<4:16:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 566/2180 [1:29:57<4:16:11,  9.52s/it]                                                      {'loss': 2.1288, 'learning_rate': 0.0008682060943553143, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 566/2180 [1:29:57<4:16:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 567/2180 [1:30:07<4:15:56,  9.52s/it]                                                      {'loss': 2.1914, 'learning_rate': 0.0008677029936466707, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 567/2180 [1:30:07<4:15:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 568/2180 [1:30:16<4:15:39,  9.52s/it]                                                      {'loss': 2.2923, 'learning_rate': 0.0008671990808803711, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 568/2180 [1:30:16<4:15:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 569/2180 [1:30:26<4:15:28,  9.51s/it]                                                      {'loss': 2.1395, 'learning_rate': 0.0008666943571692871, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 569/2180 [1:30:26<4:15:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 570/2180 [1:30:35<4:15:06,  9.51s/it]                                                      {'loss': 2.1636, 'learning_rate': 0.0008661888236280813, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 570/2180 [1:30:35<4:15:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 571/2180 [1:30:45<4:15:12,  9.52s/it]                                                      {'loss': 2.1318, 'learning_rate': 0.0008656824813732045, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 571/2180 [1:30:45<4:15:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–Œ       | 572/2180 [1:30:54<4:14:55,  9.51s/it]                                                      {'loss': 2.1646, 'learning_rate': 0.000865175331522894, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 572/2180 [1:30:54<4:14:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–‹       | 573/2180 [1:31:04<4:14:58,  9.52s/it]                                                      {'loss': 2.1783, 'learning_rate': 0.0008646673751971703, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 573/2180 [1:31:04<4:14:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 26%|â–ˆâ–ˆâ–‹       | 574/2180 [1:31:13<4:14:43,  9.52s/it]                                                      {'loss': 2.0999, 'learning_rate': 0.000864158613517835, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 574/2180 [1:31:13<4:14:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–‹       | 575/2180 [1:31:23<4:14:29,  9.51s/it]                                                      {'loss': 2.1944, 'learning_rate': 0.0008636490476084681, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 575/2180 [1:31:23<4:14:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–‹       | 576/2180 [1:31:32<4:14:30,  9.52s/it]                                                      {'loss': 2.1591, 'learning_rate': 0.0008631386785944264, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 576/2180 [1:31:32<4:14:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 26%|â–ˆâ–ˆâ–‹       | 577/2180 [1:31:42<4:14:38,  9.53s/it]                                                      {'loss': 2.1696, 'learning_rate': 0.0008626275076028397, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 577/2180 [1:31:42<4:14:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 578/2180 [1:31:51<4:14:35,  9.54s/it]                                                      {'loss': 2.1468, 'learning_rate': 0.0008621155357626091, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 578/2180 [1:31:51<4:14:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 579/2180 [1:32:01<4:14:51,  9.55s/it]                                                      {'loss': 2.1608, 'learning_rate': 0.0008616027642044042, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 579/2180 [1:32:01<4:14:51,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 580/2180 [1:32:11<4:14:32,  9.55s/it]                                                      {'loss': 2.1174, 'learning_rate': 0.000861089194060661, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 580/2180 [1:32:11<4:14:32,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 581/2180 [1:32:20<4:14:17,  9.54s/it]                                                      {'loss': 2.1324, 'learning_rate': 0.000860574826465579, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 581/2180 [1:32:20<4:14:17,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 582/2180 [1:32:30<4:14:12,  9.54s/it]                                                      {'loss': 2.2347, 'learning_rate': 0.0008600596625551191, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 582/2180 [1:32:30<4:14:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 27%|â–ˆâ–ˆâ–‹       | 583/2180 [1:32:39<4:14:15,  9.55s/it]                                                      {'loss': 2.0946, 'learning_rate': 0.0008595437034670006, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 583/2180 [1:32:39<4:14:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 27%|â–ˆâ–ˆâ–‹       | 584/2180 [1:32:49<4:14:26,  9.57s/it]                                                      {'loss': 2.1399, 'learning_rate': 0.0008590269503406985, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 584/2180 [1:32:49<4:14:26,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 585/2180 [1:32:58<4:14:03,  9.56s/it]                                                      {'loss': 2.0959, 'learning_rate': 0.0008585094043174423, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 585/2180 [1:32:58<4:14:03,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 586/2180 [1:33:08<4:13:14,  9.53s/it]                                                      {'loss': 2.142, 'learning_rate': 0.0008579910665402118, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 586/2180 [1:33:08<4:13:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 27%|â–ˆâ–ˆâ–‹       | 587/2180 [1:33:17<4:12:52,  9.52s/it]                                                      {'loss': 2.1267, 'learning_rate': 0.000857471938153736, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 587/2180 [1:33:17<4:12:52,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 588/2180 [1:33:27<4:12:30,  9.52s/it]                                                      {'loss': 2.0578, 'learning_rate': 0.0008569520203044892, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 588/2180 [1:33:27<4:12:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 589/2180 [1:33:36<4:12:54,  9.54s/it]                                                      {'loss': 2.0684, 'learning_rate': 0.0008564313141406901, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 589/2180 [1:33:36<4:12:54,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 590/2180 [1:33:46<4:12:48,  9.54s/it]                                                      {'loss': 2.077, 'learning_rate': 0.0008559098208122973, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 590/2180 [1:33:46<4:12:48,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 591/2180 [1:33:55<4:12:13,  9.52s/it]                                                      {'loss': 2.1604, 'learning_rate': 0.0008553875414710089, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 591/2180 [1:33:55<4:12:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 592/2180 [1:34:05<4:11:53,  9.52s/it]                                                      {'loss': 2.1131, 'learning_rate': 0.0008548644772702579, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 592/2180 [1:34:05<4:11:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 593/2180 [1:34:14<4:11:38,  9.51s/it]                                                      {'loss': 2.1663, 'learning_rate': 0.0008543406293652116, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 593/2180 [1:34:14<4:11:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 594/2180 [1:34:24<4:11:27,  9.51s/it]                                                      {'loss': 2.1307, 'learning_rate': 0.0008538159989127671, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 594/2180 [1:34:24<4:11:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 595/2180 [1:34:33<4:11:19,  9.51s/it]                                                      {'loss': 2.1523, 'learning_rate': 0.0008532905870715505, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 595/2180 [1:34:33<4:11:19,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 596/2180 [1:34:43<4:11:04,  9.51s/it]                                                      {'loss': 2.1343, 'learning_rate': 0.0008527643950019131, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 596/2180 [1:34:43<4:11:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 597/2180 [1:34:53<4:11:36,  9.54s/it]                                                      {'loss': 2.1217, 'learning_rate': 0.0008522374238659296, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 597/2180 [1:34:53<4:11:36,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 598/2180 [1:35:02<4:11:59,  9.56s/it]                                                      {'loss': 2.2414, 'learning_rate': 0.0008517096748273951, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 598/2180 [1:35:02<4:11:59,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 27%|â–ˆâ–ˆâ–‹       | 599/2180 [1:35:12<4:11:43,  9.55s/it]                                                      {'loss': 2.204, 'learning_rate': 0.0008511811490518227, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 599/2180 [1:35:12<4:11:43,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 600/2180 [1:35:21<4:11:21,  9.55s/it]                                                      {'loss': 2.2132, 'learning_rate': 0.0008506518477064405, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 600/2180 [1:35:21<4:11:21,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 601/2180 [1:35:31<4:10:46,  9.53s/it]                                                      {'loss': 2.1907, 'learning_rate': 0.0008501217719601903, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 601/2180 [1:35:31<4:10:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 602/2180 [1:35:40<4:10:21,  9.52s/it]                                                      {'loss': 2.1808, 'learning_rate': 0.0008495909229837233, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 602/2180 [1:35:40<4:10:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 603/2180 [1:35:50<4:09:57,  9.51s/it]                                                      {'loss': 2.0737, 'learning_rate': 0.000849059301949399, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 603/2180 [1:35:50<4:09:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 604/2180 [1:35:59<4:10:33,  9.54s/it]                                                      {'loss': 2.0992, 'learning_rate': 0.0008485269100312812, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 604/2180 [1:35:59<4:10:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 605/2180 [1:36:09<4:09:58,  9.52s/it]                                                      {'loss': 2.1824, 'learning_rate': 0.0008479937484051368, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 605/2180 [1:36:09<4:09:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 606/2180 [1:36:18<4:09:51,  9.52s/it]                                                      {'loss': 2.1534, 'learning_rate': 0.0008474598182484323, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 606/2180 [1:36:18<4:09:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 607/2180 [1:36:28<4:09:16,  9.51s/it]                                                      {'loss': 2.1375, 'learning_rate': 0.0008469251207403317, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 607/2180 [1:36:28<4:09:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 608/2180 [1:36:37<4:09:29,  9.52s/it]                                                      {'loss': 2.1701, 'learning_rate': 0.0008463896570616934, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 608/2180 [1:36:37<4:09:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 609/2180 [1:36:47<4:09:10,  9.52s/it]                                                      {'loss': 2.1357, 'learning_rate': 0.0008458534283950678, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 609/2180 [1:36:47<4:09:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 610/2180 [1:36:56<4:09:07,  9.52s/it]                                                      {'loss': 2.233, 'learning_rate': 0.0008453164359246952, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 610/2180 [1:36:56<4:09:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 611/2180 [1:37:06<4:09:57,  9.56s/it]                                                      {'loss': 2.1291, 'learning_rate': 0.0008447786808365022, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 611/2180 [1:37:06<4:09:57,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 612/2180 [1:37:16<4:09:28,  9.55s/it]                                                      {'loss': 2.1562, 'learning_rate': 0.0008442401643181, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 612/2180 [1:37:16<4:09:28,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 613/2180 [1:37:25<4:09:22,  9.55s/it]                                                      {'loss': 2.1501, 'learning_rate': 0.0008437008875587811, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 613/2180 [1:37:25<4:09:22,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 614/2180 [1:37:35<4:08:43,  9.53s/it]                                                      {'loss': 2.1711, 'learning_rate': 0.0008431608517495171, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 614/2180 [1:37:35<4:08:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 615/2180 [1:37:44<4:08:28,  9.53s/it]                                                      {'loss': 2.0745, 'learning_rate': 0.0008426200580829561, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 615/2180 [1:37:44<4:08:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 616/2180 [1:37:54<4:08:17,  9.53s/it]                                                      {'loss': 2.1879, 'learning_rate': 0.0008420785077534195, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 616/2180 [1:37:54<4:08:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 617/2180 [1:38:03<4:07:59,  9.52s/it]                                                      {'loss': 2.0578, 'learning_rate': 0.0008415362019569001, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 617/2180 [1:38:03<4:07:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 618/2180 [1:38:13<4:07:46,  9.52s/it]                                                      {'loss': 2.1894, 'learning_rate': 0.0008409931418910591, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 618/2180 [1:38:13<4:07:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 619/2180 [1:38:22<4:07:45,  9.52s/it]                                                      {'loss': 2.1512, 'learning_rate': 0.0008404493287552232, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 619/2180 [1:38:22<4:07:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 620/2180 [1:38:32<4:07:44,  9.53s/it]                                                      {'loss': 2.1393, 'learning_rate': 0.0008399047637503825, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 620/2180 [1:38:32<4:07:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 28%|â–ˆâ–ˆâ–Š       | 621/2180 [1:38:41<4:07:51,  9.54s/it]                                                      {'loss': 2.1654, 'learning_rate': 0.0008393594480791875, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 621/2180 [1:38:41<4:07:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–Š       | 622/2180 [1:38:51<4:07:39,  9.54s/it]                                                      {'loss': 2.1576, 'learning_rate': 0.0008388133829459463, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 622/2180 [1:38:51<4:07:39,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–Š       | 623/2180 [1:39:00<4:07:28,  9.54s/it]                                                      {'loss': 2.1244, 'learning_rate': 0.0008382665695566227, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 623/2180 [1:39:00<4:07:28,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–Š       | 624/2180 [1:39:10<4:07:13,  9.53s/it]                                                      {'loss': 2.1252, 'learning_rate': 0.0008377190091188324, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 624/2180 [1:39:10<4:07:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–Š       | 625/2180 [1:39:19<4:07:10,  9.54s/it]                                                      {'loss': 2.1963, 'learning_rate': 0.0008371707028418413, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 625/2180 [1:39:19<4:07:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–Š       | 626/2180 [1:39:29<4:08:09,  9.58s/it]                                                      {'loss': 2.1774, 'learning_rate': 0.0008366216519365621, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 626/2180 [1:39:29<4:08:09,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 627/2180 [1:39:39<4:07:31,  9.56s/it]                                                      {'loss': 2.1435, 'learning_rate': 0.0008360718576155525, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 627/2180 [1:39:39<4:07:31,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 628/2180 [1:39:48<4:07:11,  9.56s/it]                                                      {'loss': 2.1669, 'learning_rate': 0.0008355213210930118, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 628/2180 [1:39:48<4:07:11,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 629/2180 [1:39:58<4:07:29,  9.57s/it]                                                      {'loss': 2.1648, 'learning_rate': 0.0008349700435847778, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 629/2180 [1:39:58<4:07:29,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 630/2180 [1:40:07<4:06:45,  9.55s/it]                                                      {'loss': 2.1633, 'learning_rate': 0.0008344180263083256, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 630/2180 [1:40:07<4:06:45,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 631/2180 [1:40:17<4:06:21,  9.54s/it]                                                      {'loss': 2.1842, 'learning_rate': 0.000833865270482764, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 631/2180 [1:40:17<4:06:21,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 29%|â–ˆâ–ˆâ–‰       | 632/2180 [1:40:26<4:06:08,  9.54s/it]                                                      {'loss': 2.094, 'learning_rate': 0.0008333117773288324, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 632/2180 [1:40:26<4:06:08,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 633/2180 [1:40:36<4:05:47,  9.53s/it]                                                      {'loss': 2.0765, 'learning_rate': 0.0008327575480688985, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 633/2180 [1:40:36<4:05:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 634/2180 [1:40:45<4:05:43,  9.54s/it]                                                      {'loss': 2.2107, 'learning_rate': 0.000832202583926956, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 634/2180 [1:40:45<4:05:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 635/2180 [1:40:55<4:05:40,  9.54s/it]                                                      {'loss': 2.1778, 'learning_rate': 0.0008316468861286217, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 635/2180 [1:40:55<4:05:40,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 636/2180 [1:41:05<4:05:29,  9.54s/it]                                                      {'loss': 2.1585, 'learning_rate': 0.0008310904559011323, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 636/2180 [1:41:05<4:05:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 637/2180 [1:41:14<4:05:47,  9.56s/it]                                                      {'loss': 2.2048, 'learning_rate': 0.0008305332944733419, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 637/2180 [1:41:14<4:05:47,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 638/2180 [1:41:24<4:05:21,  9.55s/it]                                                      {'loss': 2.1134, 'learning_rate': 0.0008299754030757202, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 638/2180 [1:41:24<4:05:21,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 639/2180 [1:41:33<4:04:54,  9.54s/it]                                                      {'loss': 2.1021, 'learning_rate': 0.0008294167829403481, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 639/2180 [1:41:33<4:04:54,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 640/2180 [1:41:43<4:04:29,  9.53s/it]                                                      {'loss': 2.199, 'learning_rate': 0.0008288574353009164, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 640/2180 [1:41:43<4:04:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 641/2180 [1:41:52<4:04:21,  9.53s/it]                                                      {'loss': 2.1123, 'learning_rate': 0.0008282973613927225, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 641/2180 [1:41:52<4:04:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 642/2180 [1:42:02<4:03:47,  9.51s/it]                                                      {'loss': 2.0875, 'learning_rate': 0.0008277365624526675, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 642/2180 [1:42:02<4:03:47,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 29%|â–ˆâ–ˆâ–‰       | 643/2180 [1:42:11<4:03:31,  9.51s/it]                                                      {'loss': 2.1802, 'learning_rate': 0.0008271750397192541, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 643/2180 [1:42:11<4:03:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 30%|â–ˆâ–ˆâ–‰       | 644/2180 [1:42:21<4:04:15,  9.54s/it]                                                      {'loss': 1.9935, 'learning_rate': 0.0008266127944325832, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 644/2180 [1:42:21<4:04:15,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 645/2180 [1:42:30<4:04:00,  9.54s/it]                                                      {'loss': 2.1781, 'learning_rate': 0.0008260498278343513, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 645/2180 [1:42:30<4:04:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 646/2180 [1:42:40<4:03:44,  9.53s/it]                                                      {'loss': 2.1364, 'learning_rate': 0.0008254861411678485, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 646/2180 [1:42:40<4:03:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 647/2180 [1:42:49<4:03:22,  9.53s/it]                                                      {'loss': 2.1758, 'learning_rate': 0.0008249217356779544, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 647/2180 [1:42:49<4:03:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 648/2180 [1:42:59<4:03:05,  9.52s/it]                                                      {'loss': 2.1352, 'learning_rate': 0.0008243566126111363, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 648/2180 [1:42:59<4:03:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 649/2180 [1:43:08<4:03:08,  9.53s/it]                                                      {'loss': 2.1712, 'learning_rate': 0.0008237907732154466, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 649/2180 [1:43:08<4:03:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 650/2180 [1:43:18<4:02:44,  9.52s/it]                                                      {'loss': 2.0923, 'learning_rate': 0.0008232242187405194, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 650/2180 [1:43:18<4:02:44,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 651/2180 [1:43:27<4:02:50,  9.53s/it]                                                      {'loss': 2.169, 'learning_rate': 0.000822656950437568, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 651/2180 [1:43:27<4:02:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 652/2180 [1:43:37<4:02:25,  9.52s/it]                                                      {'loss': 2.0622, 'learning_rate': 0.0008220889695593823, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 652/2180 [1:43:37<4:02:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–‰       | 653/2180 [1:43:47<4:02:33,  9.53s/it]                                                      {'loss': 2.1173, 'learning_rate': 0.0008215202773603259, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 653/2180 [1:43:47<4:02:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 654/2180 [1:43:56<4:02:19,  9.53s/it]                                                      {'loss': 2.0988, 'learning_rate': 0.0008209508750963328, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 654/2180 [1:43:56<4:02:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 655/2180 [1:44:06<4:02:17,  9.53s/it]                                                      {'loss': 2.1551, 'learning_rate': 0.0008203807640249062, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 655/2180 [1:44:06<4:02:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 656/2180 [1:44:15<4:01:38,  9.51s/it]                                                      {'loss': 2.1579, 'learning_rate': 0.0008198099454051136, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 656/2180 [1:44:15<4:01:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 30%|â–ˆâ–ˆâ–ˆ       | 657/2180 [1:44:25<4:01:28,  9.51s/it]                                                      {'loss': 2.151, 'learning_rate': 0.0008192384204975857, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 657/2180 [1:44:25<4:01:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 658/2180 [1:44:34<4:01:15,  9.51s/it]                                                      {'loss': 2.1542, 'learning_rate': 0.000818666190564513, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 658/2180 [1:44:34<4:01:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 659/2180 [1:44:44<4:01:02,  9.51s/it]                                                      {'loss': 2.0749, 'learning_rate': 0.0008180932568696426, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 659/2180 [1:44:44<4:01:02,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 660/2180 [1:44:53<4:00:45,  9.50s/it]                                                      {'loss': 2.0738, 'learning_rate': 0.0008175196206782764, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 660/2180 [1:44:53<4:00:45,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 661/2180 [1:45:03<4:00:38,  9.51s/it]                                                      {'loss': 2.2055, 'learning_rate': 0.0008169452832572675, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 661/2180 [1:45:03<4:00:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 30%|â–ˆâ–ˆâ–ˆ       | 662/2180 [1:45:12<4:00:47,  9.52s/it]                                                      {'loss': 2.1715, 'learning_rate': 0.0008163702458750173, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 662/2180 [1:45:12<4:00:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 663/2180 [1:45:22<4:01:28,  9.55s/it]                                                      {'loss': 2.0689, 'learning_rate': 0.0008157945098014734, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 663/2180 [1:45:22<4:01:28,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 30%|â–ˆâ–ˆâ–ˆ       | 664/2180 [1:45:31<4:01:15,  9.55s/it]                                                      {'loss': 2.0782, 'learning_rate': 0.0008152180763081267, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 664/2180 [1:45:31<4:01:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 665/2180 [1:45:41<4:00:41,  9.53s/it]                                                      {'loss': 2.0755, 'learning_rate': 0.0008146409466680076, 'epoch': 0.3}
 31%|â–ˆâ–ˆâ–ˆ       | 665/2180 [1:45:41<4:00:41,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 666/2180 [1:45:50<4:01:11,  9.56s/it]                                                      {'loss': 2.1257, 'learning_rate': 0.0008140631221556845, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 666/2180 [1:45:50<4:01:11,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 667/2180 [1:46:00<4:01:14,  9.57s/it]                                                      {'loss': 2.1717, 'learning_rate': 0.0008134846040472599, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 667/2180 [1:46:00<4:01:14,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 31%|â–ˆâ–ˆâ–ˆ       | 668/2180 [1:46:09<4:00:41,  9.55s/it]                                                      {'loss': 2.1568, 'learning_rate': 0.0008129053936203688, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 668/2180 [1:46:10<4:00:41,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 669/2180 [1:46:19<4:00:22,  9.55s/it]                                                      {'loss': 2.1308, 'learning_rate': 0.0008123254921541745, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 669/2180 [1:46:19<4:00:22,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 670/2180 [1:46:29<3:59:50,  9.53s/it]                                                      {'loss': 2.0164, 'learning_rate': 0.0008117449009293668, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 670/2180 [1:46:29<3:59:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 31%|â–ˆâ–ˆâ–ˆ       | 671/2180 [1:46:38<3:59:44,  9.53s/it]                                                      {'loss': 2.1169, 'learning_rate': 0.0008111636212281586, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 671/2180 [1:46:38<3:59:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 672/2180 [1:46:48<3:59:13,  9.52s/it]                                                      {'loss': 2.2199, 'learning_rate': 0.0008105816543342833, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 672/2180 [1:46:48<3:59:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 673/2180 [1:46:57<3:59:21,  9.53s/it]                                                      {'loss': 2.1375, 'learning_rate': 0.0008099990015329919, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 673/2180 [1:46:57<3:59:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 674/2180 [1:47:07<3:59:36,  9.55s/it]                                                      {'loss': 2.1228, 'learning_rate': 0.0008094156641110504, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 674/2180 [1:47:07<3:59:36,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 675/2180 [1:47:16<3:59:02,  9.53s/it]                                                      {'loss': 2.2114, 'learning_rate': 0.0008088316433567369, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 675/2180 [1:47:16<3:59:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 676/2180 [1:47:26<3:58:51,  9.53s/it]                                                      {'loss': 2.1669, 'learning_rate': 0.0008082469405598378, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 676/2180 [1:47:26<3:58:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 677/2180 [1:47:35<3:58:34,  9.52s/it]                                                      {'loss': 2.1747, 'learning_rate': 0.0008076615570116468, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 677/2180 [1:47:35<3:58:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 678/2180 [1:47:45<3:58:17,  9.52s/it]                                                      {'loss': 2.0752, 'learning_rate': 0.0008070754940049603, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 678/2180 [1:47:45<3:58:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 679/2180 [1:47:54<3:58:00,  9.51s/it]                                                      {'loss': 2.1982, 'learning_rate': 0.0008064887528340756, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 679/2180 [1:47:54<3:58:00,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 680/2180 [1:48:04<3:57:49,  9.51s/it]                                                      {'loss': 2.1585, 'learning_rate': 0.0008059013347947874, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 680/2180 [1:48:04<3:57:49,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆ       | 681/2180 [1:48:13<3:58:24,  9.54s/it]                                                      {'loss': 2.2161, 'learning_rate': 0.0008053132411843857, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 681/2180 [1:48:13<3:58:24,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆâ–      | 682/2180 [1:48:23<3:57:50,  9.53s/it]                                                      {'loss': 2.1096, 'learning_rate': 0.0008047244733016521, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 682/2180 [1:48:23<3:57:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆâ–      | 683/2180 [1:48:32<3:57:48,  9.53s/it]                                                      {'loss': 2.1031, 'learning_rate': 0.0008041350324468573, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 683/2180 [1:48:32<3:57:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 31%|â–ˆâ–ˆâ–ˆâ–      | 684/2180 [1:48:42<3:57:33,  9.53s/it]                                                      {'loss': 2.1676, 'learning_rate': 0.0008035449199217583, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 684/2180 [1:48:42<3:57:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆâ–      | 685/2180 [1:48:51<3:57:17,  9.52s/it]                                                      {'loss': 2.0562, 'learning_rate': 0.0008029541370295957, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 685/2180 [1:48:51<3:57:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 31%|â–ˆâ–ˆâ–ˆâ–      | 686/2180 [1:49:01<3:57:17,  9.53s/it]                                                      {'loss': 2.0718, 'learning_rate': 0.0008023626850750903, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 686/2180 [1:49:01<3:57:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 687/2180 [1:49:10<3:57:10,  9.53s/it]                                                      {'loss': 2.2265, 'learning_rate': 0.0008017705653644406, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 687/2180 [1:49:11<3:57:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 688/2180 [1:49:20<3:57:35,  9.55s/it]                                                      {'loss': 2.1356, 'learning_rate': 0.0008011777792053195, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 688/2180 [1:49:20<3:57:35,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 689/2180 [1:49:30<3:57:10,  9.54s/it]                                                      {'loss': 2.0612, 'learning_rate': 0.0008005843279068725, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 689/2180 [1:49:30<3:57:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 690/2180 [1:49:39<3:57:15,  9.55s/it]                                                      {'loss': 2.2361, 'learning_rate': 0.000799990212779713, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 690/2180 [1:49:39<3:57:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 691/2180 [1:49:49<3:56:53,  9.55s/it]                                                      {'loss': 2.1625, 'learning_rate': 0.0007993954351359214, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 691/2180 [1:49:49<3:56:53,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 692/2180 [1:49:58<3:56:29,  9.54s/it]                                                      {'loss': 2.1143, 'learning_rate': 0.0007987999962890406, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 692/2180 [1:49:58<3:56:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 693/2180 [1:50:08<3:56:16,  9.53s/it]                                                      {'loss': 2.0988, 'learning_rate': 0.0007982038975540742, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 693/2180 [1:50:08<3:56:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 694/2180 [1:50:17<3:56:01,  9.53s/it]                                                      {'loss': 2.211, 'learning_rate': 0.0007976071402474826, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 694/2180 [1:50:17<3:56:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 695/2180 [1:50:27<3:55:37,  9.52s/it]                                                      {'loss': 2.1979, 'learning_rate': 0.0007970097256871811, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 695/2180 [1:50:27<3:55:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 696/2180 [1:50:36<3:55:29,  9.52s/it]                                                      {'loss': 2.1539, 'learning_rate': 0.0007964116551925364, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 696/2180 [1:50:36<3:55:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 697/2180 [1:50:46<3:55:16,  9.52s/it]                                                      {'loss': 2.0889, 'learning_rate': 0.0007958129300843637, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 697/2180 [1:50:46<3:55:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 698/2180 [1:50:55<3:54:57,  9.51s/it]                                                      {'loss': 2.1186, 'learning_rate': 0.0007952135516849239, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 698/2180 [1:50:55<3:54:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 699/2180 [1:51:05<3:54:32,  9.50s/it]                                                      {'loss': 2.2076, 'learning_rate': 0.0007946135213179207, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 699/2180 [1:51:05<3:54:32,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 700/2180 [1:51:14<3:54:42,  9.52s/it]                                                      {'loss': 2.207, 'learning_rate': 0.0007940128403084977, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 700/2180 [1:51:14<3:54:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 701/2180 [1:51:24<3:54:53,  9.53s/it]                                                      {'loss': 2.1587, 'learning_rate': 0.0007934115099832355, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 701/2180 [1:51:24<3:54:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 702/2180 [1:51:33<3:54:49,  9.53s/it]                                                      {'loss': 2.1496, 'learning_rate': 0.0007928095316701483, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 702/2180 [1:51:33<3:54:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 703/2180 [1:51:43<3:54:47,  9.54s/it]                                                      {'loss': 2.028, 'learning_rate': 0.0007922069066986819, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 703/2180 [1:51:43<3:54:47,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 704/2180 [1:51:53<3:54:25,  9.53s/it]                                                      {'loss': 2.1897, 'learning_rate': 0.0007916036363997097, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 704/2180 [1:51:53<3:54:25,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 705/2180 [1:52:02<3:53:55,  9.52s/it]                                                      {'loss': 2.0437, 'learning_rate': 0.0007909997221055308, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 705/2180 [1:52:02<3:53:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 706/2180 [1:52:12<3:54:31,  9.55s/it]                                                      {'loss': 2.0348, 'learning_rate': 0.0007903951651498658, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 706/2180 [1:52:12<3:54:31,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 707/2180 [1:52:21<3:54:12,  9.54s/it]                                                      {'loss': 2.1491, 'learning_rate': 0.0007897899668678557, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 707/2180 [1:52:21<3:54:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 32%|â–ˆâ–ˆâ–ˆâ–      | 708/2180 [1:52:31<3:53:58,  9.54s/it]                                                      {'loss': 2.1458, 'learning_rate': 0.0007891841285960566, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 708/2180 [1:52:31<3:53:58,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 709/2180 [1:52:40<3:53:18,  9.52s/it]                                                      {'loss': 2.1499, 'learning_rate': 0.0007885776516724388, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 709/2180 [1:52:40<3:53:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 710/2180 [1:52:50<3:52:56,  9.51s/it]                                                      {'loss': 2.2, 'learning_rate': 0.0007879705374363831, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 710/2180 [1:52:50<3:52:56,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 711/2180 [1:52:59<3:52:38,  9.50s/it]                                                      {'loss': 2.1719, 'learning_rate': 0.000787362787228677, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 711/2180 [1:52:59<3:52:38,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 712/2180 [1:53:09<3:52:31,  9.50s/it]                                                      {'loss': 2.1227, 'learning_rate': 0.0007867544023915134, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 712/2180 [1:53:09<3:52:31,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 713/2180 [1:53:18<3:52:17,  9.50s/it]                                                      {'loss': 2.1263, 'learning_rate': 0.0007861453842684861, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 713/2180 [1:53:18<3:52:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 714/2180 [1:53:28<3:52:09,  9.50s/it]                                                      {'loss': 2.0472, 'learning_rate': 0.0007855357342045882, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 714/2180 [1:53:28<3:52:09,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 715/2180 [1:53:37<3:52:03,  9.50s/it]                                                      {'loss': 2.2293, 'learning_rate': 0.0007849254535462074, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 715/2180 [1:53:37<3:52:03,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 716/2180 [1:53:47<3:52:08,  9.51s/it]                                                      {'loss': 2.0907, 'learning_rate': 0.0007843145436411252, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 716/2180 [1:53:47<3:52:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 717/2180 [1:53:56<3:52:10,  9.52s/it]                                                      {'loss': 2.0802, 'learning_rate': 0.0007837030058385117, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 717/2180 [1:53:56<3:52:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 718/2180 [1:54:06<3:51:58,  9.52s/it]                                                      {'loss': 2.1952, 'learning_rate': 0.0007830908414889246, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 718/2180 [1:54:06<3:51:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 719/2180 [1:54:15<3:51:41,  9.51s/it]                                                      {'loss': 2.1246, 'learning_rate': 0.0007824780519443046, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 719/2180 [1:54:15<3:51:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 720/2180 [1:54:25<3:52:01,  9.54s/it]                                                      {'loss': 2.096, 'learning_rate': 0.0007818646385579735, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 720/2180 [1:54:25<3:52:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 721/2180 [1:54:34<3:51:36,  9.52s/it]                                                      {'loss': 2.0553, 'learning_rate': 0.0007812506026846307, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 721/2180 [1:54:34<3:51:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 722/2180 [1:54:44<3:51:18,  9.52s/it]                                                      {'loss': 2.1866, 'learning_rate': 0.0007806359456803504, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 722/2180 [1:54:44<3:51:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2180 [1:54:53<3:51:01,  9.51s/it]                                                      {'loss': 2.1229, 'learning_rate': 0.0007800206689025785, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2180 [1:54:53<3:51:01,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2180 [1:55:03<3:51:01,  9.52s/it]                                                      {'loss': 2.093, 'learning_rate': 0.0007794047737101297, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2180 [1:55:03<3:51:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2180 [1:55:12<3:51:08,  9.53s/it]                                                      {'loss': 2.0594, 'learning_rate': 0.0007787882614631843, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2180 [1:55:12<3:51:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2180 [1:55:22<3:50:49,  9.53s/it]                                                      {'loss': 2.155, 'learning_rate': 0.0007781711335232856, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2180 [1:55:22<3:50:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2180 [1:55:31<3:50:21,  9.51s/it]                                                      {'loss': 2.1284, 'learning_rate': 0.0007775533912533363, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2180 [1:55:31<3:50:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2180 [1:55:41<3:50:12,  9.51s/it]                                                      {'loss': 2.1956, 'learning_rate': 0.0007769350360175962, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2180 [1:55:41<3:50:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2180 [1:55:50<3:49:55,  9.51s/it]                                                      {'loss': 2.1418, 'learning_rate': 0.0007763160691816784, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2180 [1:55:50<3:49:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2180 [1:56:00<3:49:50,  9.51s/it]                                                      {'loss': 2.1133, 'learning_rate': 0.000775696492112547, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2180 [1:56:00<3:49:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2180 [1:56:09<3:49:51,  9.52s/it]                                                      {'loss': 2.1798, 'learning_rate': 0.0007750763061785137, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2180 [1:56:09<3:49:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2180 [1:56:19<3:49:21,  9.50s/it]                                                      {'loss': 2.0695, 'learning_rate': 0.000774455512749235, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2180 [1:56:19<3:49:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2180 [1:56:29<3:49:55,  9.53s/it]                                                      {'loss': 2.1237, 'learning_rate': 0.0007738341131957085, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2180 [1:56:29<3:49:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2180 [1:56:38<3:49:33,  9.53s/it]                                                      {'loss': 2.0951, 'learning_rate': 0.000773212108890271, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2180 [1:56:38<3:49:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2180 [1:56:48<3:49:14,  9.52s/it]                                                      {'loss': 2.1523, 'learning_rate': 0.0007725895012065947, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2180 [1:56:48<3:49:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 736/2180 [1:56:57<3:49:14,  9.53s/it]                                                      {'loss': 2.0643, 'learning_rate': 0.0007719662915196844, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 736/2180 [1:56:57<3:49:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 737/2180 [1:57:07<3:49:19,  9.54s/it]                                                      {'loss': 2.0939, 'learning_rate': 0.0007713424812058736, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 737/2180 [1:57:07<3:49:19,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 738/2180 [1:57:16<3:49:10,  9.54s/it]                                                      {'loss': 2.1295, 'learning_rate': 0.0007707180716428237, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 738/2180 [1:57:16<3:49:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 739/2180 [1:57:26<3:49:35,  9.56s/it]                                                      {'loss': 2.1106, 'learning_rate': 0.0007700930642095184, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 739/2180 [1:57:26<3:49:35,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 740/2180 [1:57:35<3:49:08,  9.55s/it]                                                      {'loss': 2.1737, 'learning_rate': 0.0007694674602862621, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 740/2180 [1:57:35<3:49:08,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 741/2180 [1:57:45<3:48:56,  9.55s/it]                                                      {'loss': 2.234, 'learning_rate': 0.0007688412612546769, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 741/2180 [1:57:45<3:48:56,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 742/2180 [1:57:54<3:48:21,  9.53s/it]                                                      {'loss': 2.1302, 'learning_rate': 0.0007682144684976983, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 742/2180 [1:57:54<3:48:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 743/2180 [1:58:04<3:48:12,  9.53s/it]                                                      {'loss': 2.1527, 'learning_rate': 0.0007675870833995739, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 743/2180 [1:58:04<3:48:12,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 744/2180 [1:58:13<3:48:00,  9.53s/it]                                                      {'loss': 2.1449, 'learning_rate': 0.0007669591073458592, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 744/2180 [1:58:13<3:48:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 745/2180 [1:58:23<3:47:36,  9.52s/it]                                                      {'loss': 2.1642, 'learning_rate': 0.0007663305417234146, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 745/2180 [1:58:23<3:47:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 746/2180 [1:58:32<3:47:28,  9.52s/it]                                                      {'loss': 2.1421, 'learning_rate': 0.0007657013879204022, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 746/2180 [1:58:32<3:47:28,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 747/2180 [1:58:42<3:47:16,  9.52s/it]                                                      {'loss': 2.0449, 'learning_rate': 0.0007650716473262842, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 747/2180 [1:58:42<3:47:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 748/2180 [1:58:51<3:46:59,  9.51s/it]                                                      {'loss': 2.129, 'learning_rate': 0.0007644413213318177, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 748/2180 [1:58:51<3:46:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 749/2180 [1:59:01<3:47:01,  9.52s/it]                                                      {'loss': 2.1391, 'learning_rate': 0.0007638104113290531, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 749/2180 [1:59:01<3:47:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 750/2180 [1:59:11<3:47:07,  9.53s/it]                                                      {'loss': 2.0965, 'learning_rate': 0.0007631789187113303, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 750/2180 [1:59:11<3:47:07,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 751/2180 [1:59:20<3:46:56,  9.53s/it]                                                      {'loss': 2.0956, 'learning_rate': 0.000762546844873276, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 751/2180 [1:59:20<3:46:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 34%|â–ˆâ–ˆâ–ˆâ–      | 752/2180 [1:59:30<3:46:26,  9.51s/it]                                                      {'loss': 2.1585, 'learning_rate': 0.0007619141912108007, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 752/2180 [1:59:30<3:46:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

 35%|â–ˆâ–ˆâ–ˆâ–      | 753/2180 [1:59:39<3:46:25,  9.52s/it]                                                      {'loss': 2.0752, 'learning_rate': 0.000761280959121095, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 753/2180 [1:59:39<3:46:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 754/2180 [1:59:49<3:46:17,  9.52s/it]                                                      {'loss': 2.1692, 'learning_rate': 0.0007606471500026273, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 754/2180 [1:59:49<3:46:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 755/2180 [1:59:58<3:46:09,  9.52s/it]                                                      {'loss': 2.1321, 'learning_rate': 0.0007600127652551401, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 755/2180 [1:59:58<3:46:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 756/2180 [2:00:08<3:45:48,  9.51s/it]                                                      {'loss': 2.1179, 'learning_rate': 0.0007593778062796472, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 756/2180 [2:00:08<3:45:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 757/2180 [2:00:17<3:46:00,  9.53s/it]                                                      {'loss': 2.1093, 'learning_rate': 0.000758742274478431, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 757/2180 [2:00:17<3:46:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 758/2180 [2:00:27<3:45:48,  9.53s/it]                                                      {'loss': 2.1817, 'learning_rate': 0.0007581061712550381, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 758/2180 [2:00:27<3:45:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 759/2180 [2:00:36<3:46:25,  9.56s/it]                                                      {'loss': 2.1457, 'learning_rate': 0.0007574694980142779, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 759/2180 [2:00:36<3:46:25,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 760/2180 [2:00:46<3:46:01,  9.55s/it]                                                      {'loss': 2.153, 'learning_rate': 0.0007568322561622183, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 760/2180 [2:00:46<3:46:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 761/2180 [2:00:55<3:45:40,  9.54s/it]                                                      {'loss': 2.1289, 'learning_rate': 0.0007561944471061826, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 761/2180 [2:00:55<3:45:40,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–      | 762/2180 [2:01:05<3:45:10,  9.53s/it]                                                      {'loss': 2.1972, 'learning_rate': 0.0007555560722547475, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 762/2180 [2:01:05<3:45:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 763/2180 [2:01:14<3:44:58,  9.53s/it]                                                      {'loss': 2.0419, 'learning_rate': 0.0007549171330177387, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 763/2180 [2:01:14<3:44:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 764/2180 [2:01:24<3:44:56,  9.53s/it]                                                      {'loss': 2.206, 'learning_rate': 0.0007542776308062285, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 764/2180 [2:01:24<3:44:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 765/2180 [2:01:33<3:44:29,  9.52s/it]                                                      {'loss': 2.1572, 'learning_rate': 0.0007536375670325325, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 765/2180 [2:01:33<3:44:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 766/2180 [2:01:43<3:44:26,  9.52s/it]                                                      {'loss': 2.0863, 'learning_rate': 0.0007529969431102063, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 766/2180 [2:01:43<3:44:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 767/2180 [2:01:53<3:44:26,  9.53s/it]                                                      {'loss': 2.1293, 'learning_rate': 0.000752355760454043, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 767/2180 [2:01:53<3:44:26,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 768/2180 [2:02:02<3:44:07,  9.52s/it]                                                      {'loss': 2.116, 'learning_rate': 0.0007517140204800693, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 768/2180 [2:02:02<3:44:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 769/2180 [2:02:12<3:43:56,  9.52s/it]                                                      {'loss': 2.0821, 'learning_rate': 0.0007510717246055425, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 769/2180 [2:02:12<3:43:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 770/2180 [2:02:21<3:43:47,  9.52s/it]                                                      {'loss': 2.1334, 'learning_rate': 0.0007504288742489482, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 770/2180 [2:02:21<3:43:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 771/2180 [2:02:31<3:43:31,  9.52s/it]                                                      {'loss': 2.145, 'learning_rate': 0.0007497854708299963, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 771/2180 [2:02:31<3:43:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 772/2180 [2:02:40<3:43:16,  9.51s/it]                                                      {'loss': 2.1205, 'learning_rate': 0.0007491415157696178, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 772/2180 [2:02:40<3:43:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 773/2180 [2:02:50<3:43:11,  9.52s/it]                                                      {'loss': 2.1418, 'learning_rate': 0.0007484970104899623, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 773/2180 [2:02:50<3:43:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 774/2180 [2:02:59<3:43:14,  9.53s/it]                                                      {'loss': 2.1854, 'learning_rate': 0.0007478519564143945, 'epoch': 0.35}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 774/2180 [2:02:59<3:43:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 775/2180 [2:03:09<3:42:45,  9.51s/it]                                                      {'loss': 2.1579, 'learning_rate': 0.000747206354967491, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 775/2180 [2:03:09<3:42:45,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 776/2180 [2:03:18<3:42:33,  9.51s/it]                                                      {'loss': 2.2265, 'learning_rate': 0.0007465602075750373, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 776/2180 [2:03:18<3:42:33,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 777/2180 [2:03:28<3:42:26,  9.51s/it]                                                      {'loss': 2.1144, 'learning_rate': 0.0007459135156640247, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 777/2180 [2:03:28<3:42:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 778/2180 [2:03:37<3:42:17,  9.51s/it]                                                      {'loss': 2.1166, 'learning_rate': 0.0007452662806626468, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 778/2180 [2:03:37<3:42:17,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2180 [2:03:47<3:42:08,  9.51s/it]                                                      {'loss': 2.1088, 'learning_rate': 0.0007446185040002967, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2180 [2:03:47<3:42:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2180 [2:03:56<3:42:18,  9.53s/it]                                                      {'loss': 2.196, 'learning_rate': 0.0007439701871075642, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2180 [2:03:56<3:42:18,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2180 [2:04:06<3:42:01,  9.52s/it]                                                      {'loss': 2.1452, 'learning_rate': 0.0007433213314162313, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2180 [2:04:06<3:42:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2180 [2:04:15<3:41:58,  9.53s/it]                                                      {'loss': 2.1283, 'learning_rate': 0.0007426719383592705, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2180 [2:04:15<3:41:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2180 [2:04:25<3:41:46,  9.53s/it]                                                      {'loss': 2.1612, 'learning_rate': 0.000742022009370841, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2180 [2:04:25<3:41:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2180 [2:04:34<3:41:29,  9.52s/it]                                                      {'loss': 2.1279, 'learning_rate': 0.0007413715458862855, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2180 [2:04:34<3:41:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2180 [2:04:44<3:41:15,  9.52s/it]                                                      {'loss': 2.1119, 'learning_rate': 0.0007407205493421272, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2180 [2:04:44<3:41:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2180 [2:04:53<3:41:04,  9.52s/it]                                                      {'loss': 2.1114, 'learning_rate': 0.0007400690211760661, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2180 [2:04:53<3:41:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2180 [2:05:03<3:41:05,  9.52s/it]                                                      {'loss': 2.0728, 'learning_rate': 0.0007394169628269771, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2180 [2:05:03<3:41:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2180 [2:05:12<3:40:59,  9.53s/it]                                                      {'loss': 2.1117, 'learning_rate': 0.0007387643757349051, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2180 [2:05:12<3:40:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2180 [2:05:22<3:40:55,  9.53s/it]                                                      {'loss': 2.1717, 'learning_rate': 0.0007381112613410635, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2180 [2:05:22<3:40:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2180 [2:05:32<3:43:33,  9.65s/it]                                                      {'loss': 2.0562, 'learning_rate': 0.0007374576210878298, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2180 [2:05:32<3:43:33,  9.65s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 791/2180 [2:05:41<3:42:34,  9.61s/it]                                                      {'loss': 2.1488, 'learning_rate': 0.0007368034564187425, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 791/2180 [2:05:41<3:42:34,  9.61s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 792/2180 [2:05:51<3:41:53,  9.59s/it]                                                      {'loss': 2.1069, 'learning_rate': 0.0007361487687784989, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 792/2180 [2:05:51<3:41:53,  9.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 793/2180 [2:06:00<3:40:59,  9.56s/it]                                                      {'loss': 2.0659, 'learning_rate': 0.0007354935596129513, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 793/2180 [2:06:00<3:40:59,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 794/2180 [2:06:10<3:40:10,  9.53s/it]                                                      {'loss': 2.152, 'learning_rate': 0.000734837830369103, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 794/2180 [2:06:10<3:40:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 795/2180 [2:06:19<3:40:02,  9.53s/it]                                                      {'loss': 2.1052, 'learning_rate': 0.0007341815824951066, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 795/2180 [2:06:19<3:40:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 796/2180 [2:06:29<3:40:02,  9.54s/it]                                                      {'loss': 2.1851, 'learning_rate': 0.0007335248174402597, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 796/2180 [2:06:29<3:40:02,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 797/2180 [2:06:39<3:39:53,  9.54s/it]                                                      {'loss': 2.1374, 'learning_rate': 0.0007328675366550023, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 797/2180 [2:06:39<3:39:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 798/2180 [2:06:48<3:40:07,  9.56s/it]                                                      {'loss': 2.0541, 'learning_rate': 0.0007322097415909134, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 798/2180 [2:06:48<3:40:07,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 799/2180 [2:06:58<3:40:20,  9.57s/it]                                                      {'loss': 2.1512, 'learning_rate': 0.0007315514337007071, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 799/2180 [2:06:58<3:40:20,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 800/2180 [2:07:07<3:39:44,  9.55s/it]                                                      {'loss': 2.0625, 'learning_rate': 0.0007308926144382312, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 800/2180 [2:07:07<3:39:44,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 801/2180 [2:07:17<3:39:16,  9.54s/it]                                                      {'loss': 2.1045, 'learning_rate': 0.0007302332852584619, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 801/2180 [2:07:17<3:39:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 802/2180 [2:07:26<3:38:40,  9.52s/it]                                                      {'loss': 2.2109, 'learning_rate': 0.0007295734476175018, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 802/2180 [2:07:26<3:38:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 803/2180 [2:07:36<3:38:31,  9.52s/it]                                                      {'loss': 2.1137, 'learning_rate': 0.0007289131029725768, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 803/2180 [2:07:36<3:38:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 804/2180 [2:07:45<3:38:29,  9.53s/it]                                                      {'loss': 2.1699, 'learning_rate': 0.0007282522527820319, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 804/2180 [2:07:45<3:38:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 805/2180 [2:07:55<3:38:09,  9.52s/it]                                                      {'loss': 2.1265, 'learning_rate': 0.000727590898505329, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 805/2180 [2:07:55<3:38:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 806/2180 [2:08:04<3:38:08,  9.53s/it]                                                      {'loss': 1.9838, 'learning_rate': 0.0007269290416030429, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 806/2180 [2:08:04<3:38:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2180 [2:08:14<3:37:39,  9.51s/it]                                                      {'loss': 2.0597, 'learning_rate': 0.000726266683536859, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2180 [2:08:14<3:37:39,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2180 [2:08:23<3:37:33,  9.51s/it]                                                      {'loss': 2.1215, 'learning_rate': 0.0007256038257695687, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2180 [2:08:23<3:37:33,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2180 [2:08:33<3:37:18,  9.51s/it]                                                      {'loss': 2.1366, 'learning_rate': 0.0007249404697650678, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2180 [2:08:33<3:37:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2180 [2:08:42<3:37:10,  9.51s/it]                                                      {'loss': 2.1414, 'learning_rate': 0.0007242766169883518, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2180 [2:08:42<3:37:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2180 [2:08:52<3:37:17,  9.52s/it]                                                      {'loss': 2.2032, 'learning_rate': 0.0007236122689055138, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2180 [2:08:52<3:37:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2180 [2:09:01<3:37:02,  9.52s/it]                                                      {'loss': 2.1332, 'learning_rate': 0.0007229474269837401, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2180 [2:09:01<3:37:02,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2180 [2:09:11<3:36:38,  9.51s/it]                                                      {'loss': 2.0658, 'learning_rate': 0.0007222820926913085, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2180 [2:09:11<3:36:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2180 [2:09:20<3:36:38,  9.52s/it]                                                      {'loss': 2.1167, 'learning_rate': 0.0007216162674975833, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2180 [2:09:20<3:36:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2180 [2:09:30<3:36:11,  9.50s/it]                                                      {'loss': 2.1454, 'learning_rate': 0.0007209499528730138, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2180 [2:09:30<3:36:11,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2180 [2:09:39<3:36:06,  9.51s/it]                                                      {'loss': 2.0466, 'learning_rate': 0.0007202831502891294, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2180 [2:09:39<3:36:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2180 [2:09:49<3:36:06,  9.51s/it]                                                      {'loss': 2.1445, 'learning_rate': 0.0007196158612185375, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2180 [2:09:49<3:36:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 818/2180 [2:09:58<3:35:46,  9.51s/it]                                                      {'loss': 2.0353, 'learning_rate': 0.0007189480871349201, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 818/2180 [2:09:58<3:35:46,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 819/2180 [2:10:08<3:35:27,  9.50s/it]                                                      {'loss': 2.0258, 'learning_rate': 0.0007182798295130299, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 819/2180 [2:10:08<3:35:27,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 820/2180 [2:10:17<3:35:18,  9.50s/it]                                                      {'loss': 2.0153, 'learning_rate': 0.0007176110898286878, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 820/2180 [2:10:17<3:35:18,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 821/2180 [2:10:27<3:35:17,  9.50s/it]                                                      {'loss': 2.0496, 'learning_rate': 0.0007169418695587791, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 821/2180 [2:10:27<3:35:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 822/2180 [2:10:36<3:34:55,  9.50s/it]                                                      {'loss': 2.1328, 'learning_rate': 0.0007162721701812506, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 822/2180 [2:10:36<3:34:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 823/2180 [2:10:46<3:34:53,  9.50s/it]                                                      {'loss': 2.0161, 'learning_rate': 0.0007156019931751072, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 823/2180 [2:10:46<3:34:53,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 824/2180 [2:10:55<3:34:51,  9.51s/it]                                                      {'loss': 2.0936, 'learning_rate': 0.0007149313400204082, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 824/2180 [2:10:55<3:34:51,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 825/2180 [2:11:05<3:34:56,  9.52s/it]                                                      {'loss': 2.0651, 'learning_rate': 0.0007142602121982653, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 825/2180 [2:11:05<3:34:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 826/2180 [2:11:15<3:34:51,  9.52s/it]                                                      {'loss': 2.1565, 'learning_rate': 0.0007135886111908379, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 826/2180 [2:11:15<3:34:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 827/2180 [2:11:24<3:34:36,  9.52s/it]                                                      {'loss': 2.1284, 'learning_rate': 0.0007129165384813303, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 827/2180 [2:11:24<3:34:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 828/2180 [2:11:34<3:34:59,  9.54s/it]                                                      {'loss': 2.1465, 'learning_rate': 0.0007122439955539888, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 828/2180 [2:11:34<3:34:59,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 829/2180 [2:11:43<3:34:42,  9.54s/it]                                                      {'loss': 2.1428, 'learning_rate': 0.0007115709838940983, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 829/2180 [2:11:43<3:34:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 830/2180 [2:11:53<3:35:01,  9.56s/it]                                                      {'loss': 2.1363, 'learning_rate': 0.0007108975049879785, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 830/2180 [2:11:53<3:35:01,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 831/2180 [2:12:02<3:34:19,  9.53s/it]                                                      {'loss': 2.0487, 'learning_rate': 0.0007102235603229814, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 831/2180 [2:12:02<3:34:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 832/2180 [2:12:12<3:33:48,  9.52s/it]                                                      {'loss': 2.0713, 'learning_rate': 0.000709549151387487, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 832/2180 [2:12:12<3:33:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 833/2180 [2:12:21<3:33:37,  9.52s/it]                                                      {'loss': 2.1148, 'learning_rate': 0.0007088742796709013, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 833/2180 [2:12:21<3:33:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2180 [2:12:31<3:33:31,  9.52s/it]                                                      {'loss': 2.1339, 'learning_rate': 0.000708198946663652, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2180 [2:12:31<3:33:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2180 [2:12:40<3:33:19,  9.52s/it]                                                      {'loss': 2.1228, 'learning_rate': 0.0007075231538571856, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2180 [2:12:40<3:33:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2180 [2:12:50<3:33:02,  9.51s/it]                                                      {'loss': 2.1944, 'learning_rate': 0.0007068469027439641, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2180 [2:12:50<3:33:02,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2180 [2:12:59<3:32:56,  9.51s/it]                                                      {'loss': 2.1702, 'learning_rate': 0.0007061701948174613, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2180 [2:12:59<3:32:56,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2180 [2:13:09<3:32:50,  9.52s/it]                                                      {'loss': 2.0725, 'learning_rate': 0.0007054930315721606, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2180 [2:13:09<3:32:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2180 [2:13:18<3:32:40,  9.52s/it]                                                      {'loss': 2.0967, 'learning_rate': 0.0007048154145035501, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2180 [2:13:18<3:32:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2180 [2:13:28<3:32:28,  9.51s/it]                                                      {'loss': 2.1299, 'learning_rate': 0.0007041373451081207, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2180 [2:13:28<3:32:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2180 [2:13:37<3:32:15,  9.51s/it]                                                      {'loss': 2.1854, 'learning_rate': 0.0007034588248833621, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2180 [2:13:37<3:32:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2180 [2:13:47<3:32:07,  9.51s/it]                                                      {'loss': 2.092, 'learning_rate': 0.0007027798553277595, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2180 [2:13:47<3:32:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2180 [2:13:56<3:31:49,  9.51s/it]                                                      {'loss': 2.0854, 'learning_rate': 0.0007021004379407909, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2180 [2:13:56<3:31:49,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 39%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2180 [2:14:06<3:32:12,  9.53s/it]                                                      {'loss': 2.1743, 'learning_rate': 0.0007014205742229227, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2180 [2:14:06<3:32:12,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 845/2180 [2:14:15<3:31:49,  9.52s/it]                                                      {'loss': 2.1275, 'learning_rate': 0.0007007402656756072, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 845/2180 [2:14:15<3:31:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 846/2180 [2:14:25<3:31:22,  9.51s/it]                                                      {'loss': 2.1171, 'learning_rate': 0.0007000595138012797, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 846/2180 [2:14:25<3:31:22,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 847/2180 [2:14:34<3:31:25,  9.52s/it]                                                      {'loss': 2.0529, 'learning_rate': 0.0006993783201033535, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 847/2180 [2:14:34<3:31:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 848/2180 [2:14:44<3:31:30,  9.53s/it]                                                      {'loss': 2.1379, 'learning_rate': 0.0006986966860862182, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 848/2180 [2:14:44<3:31:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 849/2180 [2:14:53<3:31:00,  9.51s/it]                                                      {'loss': 2.0314, 'learning_rate': 0.000698014613255236, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 849/2180 [2:14:53<3:31:00,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 850/2180 [2:15:03<3:30:48,  9.51s/it]                                                      {'loss': 2.1524, 'learning_rate': 0.0006973321031167382, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 850/2180 [2:15:03<3:30:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 851/2180 [2:15:12<3:30:44,  9.51s/it]                                                      {'loss': 2.1427, 'learning_rate': 0.0006966491571780216, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 851/2180 [2:15:12<3:30:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 852/2180 [2:15:22<3:30:19,  9.50s/it]                                                      {'loss': 2.0414, 'learning_rate': 0.0006959657769473453, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 852/2180 [2:15:22<3:30:19,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 853/2180 [2:15:32<3:30:52,  9.53s/it]                                                      {'loss': 2.1354, 'learning_rate': 0.000695281963933928, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 853/2180 [2:15:32<3:30:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 854/2180 [2:15:41<3:30:24,  9.52s/it]                                                      {'loss': 2.0611, 'learning_rate': 0.0006945977196479438, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 854/2180 [2:15:41<3:30:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 855/2180 [2:15:51<3:30:06,  9.51s/it]                                                      {'loss': 2.1279, 'learning_rate': 0.0006939130456005196, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 855/2180 [2:15:51<3:30:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 856/2180 [2:16:00<3:30:09,  9.52s/it]                                                      {'loss': 2.1257, 'learning_rate': 0.0006932279433037311, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 856/2180 [2:16:00<3:30:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 857/2180 [2:16:10<3:29:38,  9.51s/it]                                                      {'loss': 2.1224, 'learning_rate': 0.0006925424142705997, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 857/2180 [2:16:10<3:29:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 858/2180 [2:16:19<3:29:27,  9.51s/it]                                                      {'loss': 2.0958, 'learning_rate': 0.0006918564600150896, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 858/2180 [2:16:19<3:29:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 859/2180 [2:16:29<3:29:18,  9.51s/it]                                                      {'loss': 2.0841, 'learning_rate': 0.0006911700820521042, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 859/2180 [2:16:29<3:29:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 860/2180 [2:16:38<3:29:25,  9.52s/it]                                                      {'loss': 2.0027, 'learning_rate': 0.0006904832818974818, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 860/2180 [2:16:38<3:29:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 861/2180 [2:16:48<3:29:13,  9.52s/it]                                                      {'loss': 2.1048, 'learning_rate': 0.0006897960610679939, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 861/2180 [2:16:48<3:29:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2180 [2:16:57<3:28:58,  9.51s/it]                                                      {'loss': 1.9898, 'learning_rate': 0.0006891084210813407, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2180 [2:16:57<3:28:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2180 [2:17:07<3:28:56,  9.52s/it]                                                      {'loss': 2.0197, 'learning_rate': 0.0006884203634561483, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2180 [2:17:07<3:28:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2180 [2:17:16<3:29:01,  9.53s/it]                                                      {'loss': 2.0863, 'learning_rate': 0.0006877318897119651, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2180 [2:17:16<3:29:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2180 [2:17:26<3:29:02,  9.54s/it]                                                      {'loss': 2.1893, 'learning_rate': 0.0006870430013692579, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2180 [2:17:26<3:29:02,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2180 [2:17:35<3:28:41,  9.53s/it]                                                      {'loss': 2.0635, 'learning_rate': 0.0006863536999494101, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2180 [2:17:35<3:28:41,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2180 [2:17:45<3:28:41,  9.54s/it]                                                      {'loss': 2.1518, 'learning_rate': 0.0006856639869747167, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2180 [2:17:45<3:28:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2180 [2:17:54<3:28:23,  9.53s/it]                                                      {'loss': 2.1078, 'learning_rate': 0.0006849738639683818, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2180 [2:17:54<3:28:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2180 [2:18:04<3:28:18,  9.53s/it]                                                      {'loss': 2.0978, 'learning_rate': 0.000684283332454515, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2180 [2:18:04<3:28:18,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2180 [2:18:13<3:28:07,  9.53s/it]                                                      {'loss': 2.1197, 'learning_rate': 0.0006835923939581281, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2180 [2:18:13<3:28:07,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2180 [2:18:23<3:27:36,  9.52s/it]                                                      {'loss': 2.054, 'learning_rate': 0.0006829010500051318, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2180 [2:18:23<3:27:36,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 872/2180 [2:18:32<3:27:20,  9.51s/it]                                                      {'loss': 2.0488, 'learning_rate': 0.0006822093021223321, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 872/2180 [2:18:32<3:27:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 873/2180 [2:18:42<3:28:00,  9.55s/it]                                                      {'loss': 2.0787, 'learning_rate': 0.0006815171518374268, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 873/2180 [2:18:42<3:28:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 874/2180 [2:18:52<3:27:35,  9.54s/it]                                                      {'loss': 2.1611, 'learning_rate': 0.0006808246006790031, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 874/2180 [2:18:52<3:27:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 875/2180 [2:19:01<3:27:09,  9.52s/it]                                                      {'loss': 2.1533, 'learning_rate': 0.0006801316501765329, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 875/2180 [2:19:01<3:27:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 876/2180 [2:19:11<3:26:41,  9.51s/it]                                                      {'loss': 2.0924, 'learning_rate': 0.0006794383018603704, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 876/2180 [2:19:11<3:26:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 877/2180 [2:19:20<3:26:39,  9.52s/it]                                                      {'loss': 2.1701, 'learning_rate': 0.0006787445572617481, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 877/2180 [2:19:20<3:26:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 878/2180 [2:19:30<3:26:35,  9.52s/it]                                                      {'loss': 2.0986, 'learning_rate': 0.0006780504179127734, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 878/2180 [2:19:30<3:26:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 879/2180 [2:19:39<3:27:18,  9.56s/it]                                                      {'loss': 2.1092, 'learning_rate': 0.0006773558853464265, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 879/2180 [2:19:39<3:27:18,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 880/2180 [2:19:49<3:27:21,  9.57s/it]                                                      {'loss': 2.1, 'learning_rate': 0.000676660961096555, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 880/2180 [2:19:49<3:27:21,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 881/2180 [2:19:58<3:26:37,  9.54s/it]                                                      {'loss': 2.0864, 'learning_rate': 0.000675965646697872, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 881/2180 [2:19:58<3:26:37,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 882/2180 [2:20:08<3:26:16,  9.53s/it]                                                      {'loss': 2.0612, 'learning_rate': 0.0006752699436859519, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 882/2180 [2:20:08<3:26:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 883/2180 [2:20:17<3:25:53,  9.52s/it]                                                      {'loss': 2.0308, 'learning_rate': 0.0006745738535972279, 'epoch': 0.4}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 883/2180 [2:20:17<3:25:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 884/2180 [2:20:27<3:25:52,  9.53s/it]                                                      {'loss': 2.1525, 'learning_rate': 0.0006738773779689874, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 884/2180 [2:20:27<3:25:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 885/2180 [2:20:36<3:25:30,  9.52s/it]                                                      {'loss': 2.1072, 'learning_rate': 0.0006731805183393696, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 885/2180 [2:20:36<3:25:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 886/2180 [2:20:46<3:25:08,  9.51s/it]                                                      {'loss': 2.1728, 'learning_rate': 0.0006724832762473618, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 886/2180 [2:20:46<3:25:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 887/2180 [2:20:55<3:24:58,  9.51s/it]                                                      {'loss': 2.0959, 'learning_rate': 0.0006717856532327956, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 887/2180 [2:20:55<3:24:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 888/2180 [2:21:05<3:24:55,  9.52s/it]                                                      {'loss': 2.0859, 'learning_rate': 0.0006710876508363444, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 888/2180 [2:21:05<3:24:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 889/2180 [2:21:14<3:24:40,  9.51s/it]                                                      {'loss': 2.0772, 'learning_rate': 0.0006703892705995189, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 889/2180 [2:21:14<3:24:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2180 [2:21:24<3:24:54,  9.53s/it]                                                      {'loss': 2.1664, 'learning_rate': 0.0006696905140646647, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2180 [2:21:24<3:24:54,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2180 [2:21:34<3:24:39,  9.53s/it]                                                      {'loss': 2.1288, 'learning_rate': 0.0006689913827749581, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2180 [2:21:34<3:24:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2180 [2:21:43<3:24:27,  9.52s/it]                                                      {'loss': 2.0984, 'learning_rate': 0.0006682918782744032, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2180 [2:21:43<3:24:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2180 [2:21:53<3:24:01,  9.51s/it]                                                      {'loss': 2.1507, 'learning_rate': 0.0006675920021078282, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2180 [2:21:53<3:24:01,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2180 [2:22:02<3:23:55,  9.51s/it]                                                      {'loss': 2.1041, 'learning_rate': 0.0006668917558208823, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2180 [2:22:02<3:23:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2180 [2:22:12<3:23:43,  9.51s/it]                                                      {'loss': 2.1412, 'learning_rate': 0.0006661911409600321, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2180 [2:22:12<3:23:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2180 [2:22:21<3:24:02,  9.53s/it]                                                      {'loss': 2.1525, 'learning_rate': 0.0006654901590725577, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2180 [2:22:21<3:24:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2180 [2:22:31<3:23:55,  9.54s/it]                                                      {'loss': 2.0752, 'learning_rate': 0.0006647888117065507, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2180 [2:22:31<3:23:55,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2180 [2:22:40<3:23:34,  9.53s/it]                                                      {'loss': 2.1301, 'learning_rate': 0.0006640871004109086, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2180 [2:22:40<3:23:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2180 [2:22:50<3:23:38,  9.54s/it]                                                      {'loss': 2.1611, 'learning_rate': 0.000663385026735334, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2180 [2:22:50<3:23:38,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 900/2180 [2:22:59<3:24:08,  9.57s/it]                                                      {'loss': 2.1886, 'learning_rate': 0.0006626825922303285, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 900/2180 [2:22:59<3:24:08,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 901/2180 [2:23:09<3:23:30,  9.55s/it]                                                      {'loss': 2.0312, 'learning_rate': 0.0006619797984471915, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 901/2180 [2:23:09<3:23:30,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 902/2180 [2:23:18<3:23:10,  9.54s/it]                                                      {'loss': 2.0567, 'learning_rate': 0.0006612766469380158, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 902/2180 [2:23:18<3:23:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 903/2180 [2:23:28<3:22:57,  9.54s/it]                                                      {'loss': 2.0823, 'learning_rate': 0.0006605731392556833, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 903/2180 [2:23:28<3:22:57,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 904/2180 [2:23:37<3:22:46,  9.54s/it]                                                      {'loss': 2.1442, 'learning_rate': 0.0006598692769538637, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 904/2180 [2:23:37<3:22:46,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 905/2180 [2:23:47<3:23:20,  9.57s/it]                                                      {'loss': 2.0225, 'learning_rate': 0.0006591650615870091, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 905/2180 [2:23:47<3:23:20,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 906/2180 [2:23:57<3:23:16,  9.57s/it]                                                      {'loss': 2.0138, 'learning_rate': 0.0006584604947103514, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 906/2180 [2:23:57<3:23:16,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 907/2180 [2:24:06<3:22:35,  9.55s/it]                                                      {'loss': 2.1234, 'learning_rate': 0.0006577555778798993, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 907/2180 [2:24:06<3:22:35,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 908/2180 [2:24:16<3:21:56,  9.53s/it]                                                      {'loss': 2.1329, 'learning_rate': 0.0006570503126524336, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 908/2180 [2:24:16<3:21:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 909/2180 [2:24:25<3:21:37,  9.52s/it]                                                      {'loss': 2.1218, 'learning_rate': 0.0006563447005855054, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 909/2180 [2:24:25<3:21:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 910/2180 [2:24:35<3:21:06,  9.50s/it]                                                      {'loss': 2.1058, 'learning_rate': 0.000655638743237431, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 910/2180 [2:24:35<3:21:06,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 911/2180 [2:24:44<3:20:54,  9.50s/it]                                                      {'loss': 2.1458, 'learning_rate': 0.0006549324421672894, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 911/2180 [2:24:44<3:20:54,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 912/2180 [2:24:54<3:21:07,  9.52s/it]                                                      {'loss': 1.9899, 'learning_rate': 0.0006542257989349194, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 912/2180 [2:24:54<3:21:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 913/2180 [2:25:03<3:20:50,  9.51s/it]                                                      {'loss': 2.0335, 'learning_rate': 0.0006535188151009142, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 913/2180 [2:25:03<3:20:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 914/2180 [2:25:13<3:20:40,  9.51s/it]                                                      {'loss': 2.1153, 'learning_rate': 0.0006528114922266204, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 914/2180 [2:25:13<3:20:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 915/2180 [2:25:22<3:20:15,  9.50s/it]                                                      {'loss': 2.138, 'learning_rate': 0.0006521038318741327, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 915/2180 [2:25:22<3:20:15,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 916/2180 [2:25:32<3:20:06,  9.50s/it]                                                      {'loss': 2.1054, 'learning_rate': 0.0006513958356062912, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 916/2180 [2:25:32<3:20:06,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 917/2180 [2:25:41<3:19:55,  9.50s/it]                                                      {'loss': 2.1983, 'learning_rate': 0.0006506875049866781, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 917/2180 [2:25:41<3:19:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2180 [2:25:51<3:20:00,  9.51s/it]                                                      {'loss': 2.2278, 'learning_rate': 0.0006499788415796137, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2180 [2:25:51<3:20:00,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2180 [2:26:00<3:20:11,  9.53s/it]                                                      {'loss': 2.0483, 'learning_rate': 0.0006492698469501532, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2180 [2:26:00<3:20:11,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2180 [2:26:10<3:19:55,  9.52s/it]                                                      {'loss': 2.1424, 'learning_rate': 0.0006485605226640837, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2180 [2:26:10<3:19:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2180 [2:26:19<3:19:59,  9.53s/it]                                                      {'loss': 2.154, 'learning_rate': 0.00064785087028792, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2180 [2:26:19<3:19:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2180 [2:26:29<3:19:39,  9.52s/it]                                                      {'loss': 2.082, 'learning_rate': 0.0006471408913889019, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2180 [2:26:29<3:19:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2180 [2:26:38<3:20:17,  9.56s/it]                                                      {'loss': 2.124, 'learning_rate': 0.0006464305875349892, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2180 [2:26:38<3:20:17,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2180 [2:26:48<3:19:48,  9.54s/it]                                                      {'loss': 2.1321, 'learning_rate': 0.000645719960294861, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2180 [2:26:48<3:19:48,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2180 [2:26:57<3:19:29,  9.54s/it]                                                      {'loss': 2.091, 'learning_rate': 0.0006450090112379092, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2180 [2:26:57<3:19:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2180 [2:27:07<3:19:09,  9.53s/it]                                                      {'loss': 2.1895, 'learning_rate': 0.0006442977419342371, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2180 [2:27:07<3:19:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 927/2180 [2:27:16<3:18:48,  9.52s/it]                                                      {'loss': 2.0943, 'learning_rate': 0.000643586153954655, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 927/2180 [2:27:16<3:18:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 928/2180 [2:27:26<3:18:38,  9.52s/it]                                                      {'loss': 2.1022, 'learning_rate': 0.0006428742488706772, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 928/2180 [2:27:26<3:18:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 929/2180 [2:27:36<3:18:26,  9.52s/it]                                                      {'loss': 2.133, 'learning_rate': 0.0006421620282545182, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 929/2180 [2:27:36<3:18:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 930/2180 [2:27:45<3:18:09,  9.51s/it]                                                      {'loss': 2.0535, 'learning_rate': 0.0006414494936790892, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 930/2180 [2:27:45<3:18:09,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 931/2180 [2:27:55<3:17:58,  9.51s/it]                                                      {'loss': 1.9746, 'learning_rate': 0.0006407366467179951, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 931/2180 [2:27:55<3:17:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 932/2180 [2:28:04<3:17:57,  9.52s/it]                                                      {'loss': 2.1227, 'learning_rate': 0.0006400234889455301, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 932/2180 [2:28:04<3:17:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 933/2180 [2:28:14<3:17:43,  9.51s/it]                                                      {'loss': 2.0918, 'learning_rate': 0.0006393100219366755, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 933/2180 [2:28:14<3:17:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 934/2180 [2:28:23<3:17:43,  9.52s/it]                                                      {'loss': 2.0763, 'learning_rate': 0.0006385962472670953, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 934/2180 [2:28:23<3:17:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 935/2180 [2:28:33<3:18:01,  9.54s/it]                                                      {'loss': 2.2107, 'learning_rate': 0.0006378821665131328, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 935/2180 [2:28:33<3:18:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 936/2180 [2:28:42<3:17:43,  9.54s/it]                                                      {'loss': 2.1336, 'learning_rate': 0.0006371677812518072, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 936/2180 [2:28:42<3:17:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 937/2180 [2:28:52<3:17:06,  9.51s/it]                                                      {'loss': 2.0856, 'learning_rate': 0.0006364530930608107, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 937/2180 [2:28:52<3:17:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 938/2180 [2:29:01<3:17:16,  9.53s/it]                                                      {'loss': 2.0317, 'learning_rate': 0.0006357381035185038, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 938/2180 [2:29:01<3:17:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 939/2180 [2:29:11<3:17:10,  9.53s/it]                                                      {'loss': 2.0554, 'learning_rate': 0.0006350228142039131, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 939/2180 [2:29:11<3:17:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 940/2180 [2:29:20<3:16:46,  9.52s/it]                                                      {'loss': 2.1045, 'learning_rate': 0.000634307226696727, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 940/2180 [2:29:20<3:16:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 941/2180 [2:29:30<3:16:57,  9.54s/it]                                                      {'loss': 2.07, 'learning_rate': 0.0006335913425772926, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 941/2180 [2:29:30<3:16:57,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 942/2180 [2:29:39<3:16:31,  9.52s/it]                                                      {'loss': 2.1314, 'learning_rate': 0.0006328751634266117, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 942/2180 [2:29:39<3:16:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 943/2180 [2:29:49<3:16:04,  9.51s/it]                                                      {'loss': 2.0243, 'learning_rate': 0.0006321586908263382, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 943/2180 [2:29:49<3:16:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 944/2180 [2:29:58<3:15:49,  9.51s/it]                                                      {'loss': 2.1683, 'learning_rate': 0.0006314419263587732, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 944/2180 [2:29:58<3:15:49,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 945/2180 [2:30:08<3:15:23,  9.49s/it]                                                      {'loss': 2.0496, 'learning_rate': 0.0006307248716068637, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 945/2180 [2:30:08<3:15:23,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2180 [2:30:17<3:15:16,  9.49s/it]                                                      {'loss': 2.0883, 'learning_rate': 0.0006300075281541964, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2180 [2:30:17<3:15:16,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2180 [2:30:27<3:15:20,  9.51s/it]                                                      {'loss': 2.0802, 'learning_rate': 0.0006292898975849966, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2180 [2:30:27<3:15:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2180 [2:30:36<3:14:52,  9.49s/it]                                                      {'loss': 2.0319, 'learning_rate': 0.000628571981484123, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2180 [2:30:36<3:14:52,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2180 [2:30:46<3:14:55,  9.50s/it]                                                      {'loss': 2.0773, 'learning_rate': 0.0006278537814370654, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2180 [2:30:46<3:14:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2180 [2:30:55<3:15:02,  9.51s/it]                                                      {'loss': 2.1221, 'learning_rate': 0.0006271352990299406, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2180 [2:30:55<3:15:02,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2180 [2:31:05<3:15:00,  9.52s/it]                                                      {'loss': 2.1083, 'learning_rate': 0.0006264165358494885, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2180 [2:31:05<3:15:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2180 [2:31:14<3:14:38,  9.51s/it]                                                      {'loss': 2.1742, 'learning_rate': 0.0006256974934830694, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2180 [2:31:14<3:14:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2180 [2:31:24<3:14:30,  9.51s/it]                                                      {'loss': 2.0914, 'learning_rate': 0.0006249781735186606, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2180 [2:31:24<3:14:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 954/2180 [2:31:33<3:14:29,  9.52s/it]                                                      {'loss': 2.0678, 'learning_rate': 0.0006242585775448518, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 954/2180 [2:31:33<3:14:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 955/2180 [2:31:43<3:14:17,  9.52s/it]                                                      {'loss': 2.0417, 'learning_rate': 0.0006235387071508427, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 955/2180 [2:31:43<3:14:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 956/2180 [2:31:52<3:14:14,  9.52s/it]                                                      {'loss': 2.1696, 'learning_rate': 0.0006228185639264384, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 956/2180 [2:31:52<3:14:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 957/2180 [2:32:02<3:14:30,  9.54s/it]                                                      {'loss': 2.1057, 'learning_rate': 0.0006220981494620475, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 957/2180 [2:32:02<3:14:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 958/2180 [2:32:12<3:13:57,  9.52s/it]                                                      {'loss': 2.1625, 'learning_rate': 0.000621377465348677, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 958/2180 [2:32:12<3:13:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 959/2180 [2:32:21<3:13:27,  9.51s/it]                                                      {'loss': 2.0963, 'learning_rate': 0.0006206565131779293, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 959/2180 [2:32:21<3:13:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 960/2180 [2:32:30<3:13:13,  9.50s/it]                                                      {'loss': 2.1605, 'learning_rate': 0.0006199352945419994, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 960/2180 [2:32:30<3:13:13,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 961/2180 [2:32:40<3:12:51,  9.49s/it]                                                      {'loss': 2.0746, 'learning_rate': 0.00061921381103367, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 961/2180 [2:32:40<3:12:51,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 962/2180 [2:32:49<3:12:49,  9.50s/it]                                                      {'loss': 2.1602, 'learning_rate': 0.0006184920642463094, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 962/2180 [2:32:49<3:12:49,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 963/2180 [2:32:59<3:12:34,  9.49s/it]                                                      {'loss': 2.1877, 'learning_rate': 0.0006177700557738672, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 963/2180 [2:32:59<3:12:34,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 964/2180 [2:33:09<3:12:47,  9.51s/it]                                                      {'loss': 1.9643, 'learning_rate': 0.0006170477872108706, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 964/2180 [2:33:09<3:12:47,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 965/2180 [2:33:18<3:12:37,  9.51s/it]                                                      {'loss': 2.1692, 'learning_rate': 0.0006163252601524216, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 965/2180 [2:33:18<3:12:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 966/2180 [2:33:28<3:12:24,  9.51s/it]                                                      {'loss': 2.1057, 'learning_rate': 0.0006156024761941925, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 966/2180 [2:33:28<3:12:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 967/2180 [2:33:37<3:12:14,  9.51s/it]                                                      {'loss': 2.1196, 'learning_rate': 0.000614879436932424, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 967/2180 [2:33:37<3:12:14,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 968/2180 [2:33:47<3:12:06,  9.51s/it]                                                      {'loss': 2.1841, 'learning_rate': 0.0006141561439639196, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 968/2180 [2:33:47<3:12:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 969/2180 [2:33:56<3:12:37,  9.54s/it]                                                      {'loss': 2.1055, 'learning_rate': 0.0006134325988860433, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 969/2180 [2:33:56<3:12:37,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 970/2180 [2:34:06<3:12:03,  9.52s/it]                                                      {'loss': 2.1791, 'learning_rate': 0.0006127088032967165, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 970/2180 [2:34:06<3:12:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 971/2180 [2:34:15<3:11:34,  9.51s/it]                                                      {'loss': 2.0964, 'learning_rate': 0.0006119847587944131, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 971/2180 [2:34:15<3:11:34,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 972/2180 [2:34:25<3:11:27,  9.51s/it]                                                      {'loss': 2.0696, 'learning_rate': 0.0006112604669781572, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 972/2180 [2:34:25<3:11:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2180 [2:34:34<3:11:34,  9.52s/it]                                                      {'loss': 2.1021, 'learning_rate': 0.0006105359294475188, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2180 [2:34:34<3:11:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2180 [2:34:44<3:11:24,  9.52s/it]                                                      {'loss': 2.1149, 'learning_rate': 0.0006098111478026107, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2180 [2:34:44<3:11:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2180 [2:34:53<3:11:29,  9.53s/it]                                                      {'loss': 2.0944, 'learning_rate': 0.0006090861236440848, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2180 [2:34:53<3:11:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2180 [2:35:03<3:11:01,  9.52s/it]                                                      {'loss': 2.0567, 'learning_rate': 0.0006083608585731282, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2180 [2:35:03<3:11:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2180 [2:35:12<3:10:59,  9.53s/it]                                                      {'loss': 2.0796, 'learning_rate': 0.0006076353541914609, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2180 [2:35:12<3:10:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2180 [2:35:22<3:10:43,  9.52s/it]                                                      {'loss': 2.0841, 'learning_rate': 0.0006069096121013307, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2180 [2:35:22<3:10:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2180 [2:35:31<3:10:31,  9.52s/it]                                                      {'loss': 2.0126, 'learning_rate': 0.0006061836339055105, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2180 [2:35:31<3:10:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2180 [2:35:41<3:10:19,  9.52s/it]                                                      {'loss': 2.0563, 'learning_rate': 0.0006054574212072948, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2180 [2:35:41<3:10:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 981/2180 [2:35:50<3:10:17,  9.52s/it]                                                      {'loss': 2.118, 'learning_rate': 0.0006047309756104958, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 981/2180 [2:35:50<3:10:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 982/2180 [2:36:00<3:10:20,  9.53s/it]                                                      {'loss': 2.0503, 'learning_rate': 0.00060400429871944, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 982/2180 [2:36:00<3:10:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 983/2180 [2:36:09<3:09:53,  9.52s/it]                                                      {'loss': 2.1683, 'learning_rate': 0.0006032773921389654, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 983/2180 [2:36:09<3:09:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 984/2180 [2:36:19<3:09:51,  9.52s/it]                                                      {'loss': 2.1537, 'learning_rate': 0.0006025502574744162, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 984/2180 [2:36:19<3:09:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 985/2180 [2:36:28<3:09:34,  9.52s/it]                                                      {'loss': 2.0404, 'learning_rate': 0.000601822896331641, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 985/2180 [2:36:28<3:09:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 986/2180 [2:36:38<3:09:13,  9.51s/it]                                                      {'loss': 2.0275, 'learning_rate': 0.0006010953103169883, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 986/2180 [2:36:38<3:09:13,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 987/2180 [2:36:47<3:09:17,  9.52s/it]                                                      {'loss': 2.1156, 'learning_rate': 0.0006003675010373034, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 987/2180 [2:36:47<3:09:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 988/2180 [2:36:57<3:08:59,  9.51s/it]                                                      {'loss': 2.0803, 'learning_rate': 0.0005996394700999246, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 988/2180 [2:36:57<3:08:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 989/2180 [2:37:06<3:08:36,  9.50s/it]                                                      {'loss': 2.2024, 'learning_rate': 0.0005989112191126794, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 989/2180 [2:37:06<3:08:36,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 990/2180 [2:37:16<3:08:33,  9.51s/it]                                                      {'loss': 2.1768, 'learning_rate': 0.0005981827496838822, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 990/2180 [2:37:16<3:08:33,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 991/2180 [2:37:25<3:08:25,  9.51s/it]                                                      {'loss': 2.0952, 'learning_rate': 0.0005974540634223286, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 991/2180 [2:37:25<3:08:25,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 992/2180 [2:37:35<3:08:01,  9.50s/it]                                                      {'loss': 2.1302, 'learning_rate': 0.0005967251619372939, 'epoch': 0.45}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 992/2180 [2:37:35<3:08:01,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 993/2180 [2:37:44<3:07:49,  9.49s/it]                                                      {'loss': 2.0743, 'learning_rate': 0.0005959960468385284, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 993/2180 [2:37:44<3:07:49,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 994/2180 [2:37:54<3:07:32,  9.49s/it]                                                      {'loss': 2.0522, 'learning_rate': 0.0005952667197362542, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 994/2180 [2:37:54<3:07:32,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 995/2180 [2:38:03<3:07:18,  9.48s/it]                                                      {'loss': 2.089, 'learning_rate': 0.0005945371822411621, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 995/2180 [2:38:03<3:07:18,  9.48s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 996/2180 [2:38:13<3:07:33,  9.50s/it]                                                      {'loss': 2.079, 'learning_rate': 0.0005938074359644063, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 996/2180 [2:38:13<3:07:33,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 997/2180 [2:38:22<3:07:37,  9.52s/it]                                                      {'loss': 2.1109, 'learning_rate': 0.0005930774825176034, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 997/2180 [2:38:22<3:07:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 998/2180 [2:38:32<3:07:19,  9.51s/it]                                                      {'loss': 1.9867, 'learning_rate': 0.0005923473235128268, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 998/2180 [2:38:32<3:07:19,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 999/2180 [2:38:42<3:07:17,  9.52s/it]                                                      {'loss': 2.0936, 'learning_rate': 0.0005916169605626042, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 999/2180 [2:38:42<3:07:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1000/2180 [2:38:51<3:06:55,  9.50s/it]                                                       {'loss': 2.0746, 'learning_rate': 0.0005908863952799134, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1000/2180 [2:38:51<3:06:55,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2180 [2:39:00<3:06:38,  9.50s/it]                                                       {'loss': 2.1461, 'learning_rate': 0.0005901556292781793, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2180 [2:39:00<3:06:38,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2180 [2:39:10<3:06:42,  9.51s/it]                                                       {'loss': 2.0973, 'learning_rate': 0.0005894246641712698, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2180 [2:39:10<3:06:42,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2180 [2:39:19<3:06:19,  9.50s/it]                                                       {'loss': 2.1537, 'learning_rate': 0.0005886935015734931, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2180 [2:39:19<3:06:19,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2180 [2:39:29<3:06:12,  9.50s/it]                                                       {'loss': 2.0533, 'learning_rate': 0.0005879621430995928, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2180 [2:39:29<3:06:12,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2180 [2:39:39<3:06:11,  9.51s/it]                                                       {'loss': 2.0773, 'learning_rate': 0.0005872305903647455, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2180 [2:39:39<3:06:11,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2180 [2:39:48<3:06:21,  9.52s/it]                                                       {'loss': 2.1441, 'learning_rate': 0.0005864988449845569, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2180 [2:39:48<3:06:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2180 [2:39:58<3:05:55,  9.51s/it]                                                       {'loss': 2.0552, 'learning_rate': 0.0005857669085750578, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2180 [2:39:58<3:05:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2180 [2:40:07<3:05:52,  9.52s/it]                                                       {'loss': 2.052, 'learning_rate': 0.0005850347827527013, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2180 [2:40:07<3:05:52,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1009/2180 [2:40:17<3:05:58,  9.53s/it]                                                       {'loss': 2.108, 'learning_rate': 0.0005843024691343584, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1009/2180 [2:40:17<3:05:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1010/2180 [2:40:26<3:05:39,  9.52s/it]                                                       {'loss': 2.1855, 'learning_rate': 0.000583569969337315, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1010/2180 [2:40:26<3:05:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1011/2180 [2:40:36<3:05:40,  9.53s/it]                                                       {'loss': 2.1014, 'learning_rate': 0.0005828372849792686, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1011/2180 [2:40:36<3:05:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1012/2180 [2:40:45<3:05:28,  9.53s/it]                                                       {'loss': 2.1766, 'learning_rate': 0.0005821044176783234, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1012/2180 [2:40:45<3:05:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1013/2180 [2:40:55<3:05:05,  9.52s/it]                                                       {'loss': 2.136, 'learning_rate': 0.0005813713690529886, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1013/2180 [2:40:55<3:05:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1014/2180 [2:41:04<3:05:05,  9.52s/it]                                                       {'loss': 2.1044, 'learning_rate': 0.0005806381407221729, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1014/2180 [2:41:04<3:05:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1015/2180 [2:41:14<3:04:37,  9.51s/it]                                                       {'loss': 2.0775, 'learning_rate': 0.0005799047343051826, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1015/2180 [2:41:14<3:04:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1016/2180 [2:41:23<3:04:16,  9.50s/it]                                                       {'loss': 2.1035, 'learning_rate': 0.0005791711514217171, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1016/2180 [2:41:23<3:04:16,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1017/2180 [2:41:33<3:04:05,  9.50s/it]                                                       {'loss': 2.0707, 'learning_rate': 0.0005784373936918654, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1017/2180 [2:41:33<3:04:05,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1018/2180 [2:41:42<3:04:07,  9.51s/it]                                                       {'loss': 2.0905, 'learning_rate': 0.0005777034627361025, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1018/2180 [2:41:42<3:04:07,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1019/2180 [2:41:52<3:03:51,  9.50s/it]                                                       {'loss': 2.1236, 'learning_rate': 0.0005769693601752864, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1019/2180 [2:41:52<3:03:51,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1020/2180 [2:42:01<3:03:35,  9.50s/it]                                                       {'loss': 2.0642, 'learning_rate': 0.0005762350876306537, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1020/2180 [2:42:01<3:03:35,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1021/2180 [2:42:11<3:03:28,  9.50s/it]                                                       {'loss': 2.1562, 'learning_rate': 0.0005755006467238168, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1021/2180 [2:42:11<3:03:28,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1022/2180 [2:42:20<3:03:46,  9.52s/it]                                                       {'loss': 2.1102, 'learning_rate': 0.0005747660390767593, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1022/2180 [2:42:20<3:03:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1023/2180 [2:42:30<3:03:38,  9.52s/it]                                                       {'loss': 2.1289, 'learning_rate': 0.0005740312663118338, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1023/2180 [2:42:30<3:03:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1024/2180 [2:42:39<3:03:23,  9.52s/it]                                                       {'loss': 2.0341, 'learning_rate': 0.0005732963300517568, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1024/2180 [2:42:39<3:03:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1025/2180 [2:42:49<3:03:02,  9.51s/it]                                                       {'loss': 2.0959, 'learning_rate': 0.0005725612319196064, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1025/2180 [2:42:49<3:03:02,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1026/2180 [2:42:58<3:03:14,  9.53s/it]                                                       {'loss': 2.1581, 'learning_rate': 0.000571825973538818, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1026/2180 [2:42:58<3:03:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1027/2180 [2:43:08<3:02:53,  9.52s/it]                                                       {'loss': 2.0917, 'learning_rate': 0.0005710905565331811, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1027/2180 [2:43:08<3:02:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1028/2180 [2:43:17<3:02:52,  9.52s/it]                                                       {'loss': 2.1147, 'learning_rate': 0.0005703549825268353, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1028/2180 [2:43:17<3:02:52,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2180 [2:43:27<3:02:48,  9.53s/it]                                                       {'loss': 2.0343, 'learning_rate': 0.0005696192531442667, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2180 [2:43:27<3:02:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2180 [2:43:36<3:02:36,  9.53s/it]                                                       {'loss': 2.1729, 'learning_rate': 0.000568883370010305, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2180 [2:43:36<3:02:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2180 [2:43:46<3:02:21,  9.52s/it]                                                       {'loss': 2.0487, 'learning_rate': 0.0005681473347501192, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2180 [2:43:46<3:02:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2180 [2:43:56<3:02:27,  9.54s/it]                                                       {'loss': 2.0387, 'learning_rate': 0.0005674111489892144, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2180 [2:43:56<3:02:27,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2180 [2:44:05<3:02:31,  9.55s/it]                                                       {'loss': 2.0308, 'learning_rate': 0.0005666748143534282, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2180 [2:44:05<3:02:31,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2180 [2:44:15<3:02:48,  9.57s/it]                                                       {'loss': 2.1344, 'learning_rate': 0.0005659383324689266, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2180 [2:44:15<3:02:48,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2180 [2:44:24<3:02:18,  9.55s/it]                                                       {'loss': 2.1026, 'learning_rate': 0.0005652017049622007, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2180 [2:44:24<3:02:18,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1036/2180 [2:44:34<3:02:01,  9.55s/it]                                                       {'loss': 2.078, 'learning_rate': 0.0005644649334600641, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1036/2180 [2:44:34<3:02:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1037/2180 [2:44:43<3:01:34,  9.53s/it]                                                       {'loss': 2.1197, 'learning_rate': 0.0005637280195896474, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1037/2180 [2:44:43<3:01:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1038/2180 [2:44:53<3:01:14,  9.52s/it]                                                       {'loss': 2.0729, 'learning_rate': 0.0005629909649783961, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1038/2180 [2:44:53<3:01:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1039/2180 [2:45:02<3:00:57,  9.52s/it]                                                       {'loss': 2.1259, 'learning_rate': 0.0005622537712540664, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1039/2180 [2:45:02<3:00:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1040/2180 [2:45:12<3:00:47,  9.52s/it]                                                       {'loss': 2.028, 'learning_rate': 0.0005615164400447218, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1040/2180 [2:45:12<3:00:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1041/2180 [2:45:21<3:00:39,  9.52s/it]                                                       {'loss': 2.1318, 'learning_rate': 0.0005607789729787294, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1041/2180 [2:45:21<3:00:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1042/2180 [2:45:31<3:00:26,  9.51s/it]                                                       {'loss': 2.1106, 'learning_rate': 0.0005600413716847564, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1042/2180 [2:45:31<3:00:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1043/2180 [2:45:40<3:00:12,  9.51s/it]                                                       {'loss': 2.158, 'learning_rate': 0.000559303637791766, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1043/2180 [2:45:40<3:00:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1044/2180 [2:45:50<3:00:03,  9.51s/it]                                                       {'loss': 2.0525, 'learning_rate': 0.0005585657729290151, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1044/2180 [2:45:50<3:00:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1045/2180 [2:45:59<3:00:02,  9.52s/it]                                                       {'loss': 2.1079, 'learning_rate': 0.000557827778726049, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1045/2180 [2:45:59<3:00:02,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1046/2180 [2:46:09<3:00:20,  9.54s/it]                                                       {'loss': 2.0873, 'learning_rate': 0.0005570896568126993, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1046/2180 [2:46:09<3:00:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1047/2180 [2:46:18<3:00:01,  9.53s/it]                                                       {'loss': 2.0571, 'learning_rate': 0.0005563514088190788, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1047/2180 [2:46:18<3:00:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1048/2180 [2:46:28<3:00:00,  9.54s/it]                                                       {'loss': 2.1214, 'learning_rate': 0.0005556130363755798, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1048/2180 [2:46:28<3:00:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1049/2180 [2:46:38<2:59:47,  9.54s/it]                                                       {'loss': 2.0204, 'learning_rate': 0.0005548745411128688, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1049/2180 [2:46:38<2:59:47,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1050/2180 [2:46:47<2:59:35,  9.54s/it]                                                       {'loss': 2.1439, 'learning_rate': 0.0005541359246618835, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1050/2180 [2:46:47<2:59:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1051/2180 [2:46:57<2:59:21,  9.53s/it]                                                       {'loss': 2.1252, 'learning_rate': 0.0005533971886538293, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1051/2180 [2:46:57<2:59:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1052/2180 [2:47:06<2:59:06,  9.53s/it]                                                       {'loss': 2.0934, 'learning_rate': 0.000552658334720176, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1052/2180 [2:47:06<2:59:06,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1053/2180 [2:47:16<2:58:55,  9.53s/it]                                                       {'loss': 2.0821, 'learning_rate': 0.0005519193644926535, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1053/2180 [2:47:16<2:58:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1054/2180 [2:47:25<2:58:30,  9.51s/it]                                                       {'loss': 2.1176, 'learning_rate': 0.0005511802796032485, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1054/2180 [2:47:25<2:58:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1055/2180 [2:47:35<2:58:33,  9.52s/it]                                                       {'loss': 2.0933, 'learning_rate': 0.0005504410816842009, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1055/2180 [2:47:35<2:58:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1056/2180 [2:47:44<2:58:34,  9.53s/it]                                                       {'loss': 2.0799, 'learning_rate': 0.0005497017723680009, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1056/2180 [2:47:44<2:58:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2180 [2:47:54<2:58:25,  9.53s/it]                                                       {'loss': 2.0922, 'learning_rate': 0.0005489623532873836, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2180 [2:47:54<2:58:25,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2180 [2:48:03<2:58:20,  9.54s/it]                                                       {'loss': 2.092, 'learning_rate': 0.0005482228260753273, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2180 [2:48:03<2:58:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2180 [2:48:13<2:58:49,  9.57s/it]                                                       {'loss': 2.1303, 'learning_rate': 0.0005474831923650488, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2180 [2:48:13<2:58:49,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2180 [2:48:22<2:58:21,  9.55s/it]                                                       {'loss': 2.0598, 'learning_rate': 0.00054674345379, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2180 [2:48:23<2:58:21,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2180 [2:48:32<2:57:59,  9.54s/it]                                                       {'loss': 2.1062, 'learning_rate': 0.000546003611983865, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2180 [2:48:32<2:57:59,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2180 [2:48:42<2:57:41,  9.54s/it]                                                       {'loss': 2.1212, 'learning_rate': 0.0005452636685805552, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2180 [2:48:42<2:57:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1063/2180 [2:48:51<2:57:24,  9.53s/it]                                                       {'loss': 2.017, 'learning_rate': 0.0005445236252142066, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1063/2180 [2:48:51<2:57:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1064/2180 [2:49:01<2:57:24,  9.54s/it]                                                       {'loss': 2.0692, 'learning_rate': 0.000543783483519176, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1064/2180 [2:49:01<2:57:24,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1065/2180 [2:49:10<2:57:10,  9.53s/it]                                                       {'loss': 2.1467, 'learning_rate': 0.0005430432451300374, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1065/2180 [2:49:10<2:57:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1066/2180 [2:49:20<2:56:52,  9.53s/it]                                                       {'loss': 2.081, 'learning_rate': 0.0005423029116815781, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1066/2180 [2:49:20<2:56:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1067/2180 [2:49:29<2:56:29,  9.51s/it]                                                       {'loss': 2.0645, 'learning_rate': 0.0005415624848087959, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1067/2180 [2:49:29<2:56:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1068/2180 [2:49:39<2:56:25,  9.52s/it]                                                       {'loss': 2.1485, 'learning_rate': 0.000540821966146894, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1068/2180 [2:49:39<2:56:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1069/2180 [2:49:48<2:56:04,  9.51s/it]                                                       {'loss': 2.0915, 'learning_rate': 0.0005400813573312793, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1069/2180 [2:49:48<2:56:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1070/2180 [2:49:58<2:55:55,  9.51s/it]                                                       {'loss': 2.04, 'learning_rate': 0.0005393406599975572, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1070/2180 [2:49:58<2:55:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1071/2180 [2:50:07<2:56:09,  9.53s/it]                                                       {'loss': 2.1319, 'learning_rate': 0.0005385998757815287, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1071/2180 [2:50:07<2:56:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1072/2180 [2:50:17<2:55:59,  9.53s/it]                                                       {'loss': 2.0607, 'learning_rate': 0.0005378590063191867, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1072/2180 [2:50:17<2:55:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1073/2180 [2:50:26<2:55:45,  9.53s/it]                                                       {'loss': 2.0373, 'learning_rate': 0.0005371180532467124, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1073/2180 [2:50:26<2:55:45,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1074/2180 [2:50:36<2:55:43,  9.53s/it]                                                       {'loss': 2.0733, 'learning_rate': 0.000536377018200472, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1074/2180 [2:50:36<2:55:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1075/2180 [2:50:45<2:55:15,  9.52s/it]                                                       {'loss': 2.1016, 'learning_rate': 0.0005356359028170118, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1075/2180 [2:50:45<2:55:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1076/2180 [2:50:55<2:54:55,  9.51s/it]                                                       {'loss': 2.1207, 'learning_rate': 0.0005348947087330564, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1076/2180 [2:50:55<2:54:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1077/2180 [2:51:04<2:55:04,  9.52s/it]                                                       {'loss': 2.1549, 'learning_rate': 0.0005341534375855037, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1077/2180 [2:51:04<2:55:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1078/2180 [2:51:14<2:54:36,  9.51s/it]                                                       {'loss': 2.0201, 'learning_rate': 0.0005334120910114222, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1078/2180 [2:51:14<2:54:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1079/2180 [2:51:23<2:54:26,  9.51s/it]                                                       {'loss': 2.1078, 'learning_rate': 0.0005326706706480467, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1079/2180 [2:51:23<2:54:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1080/2180 [2:51:33<2:54:15,  9.50s/it]                                                       {'loss': 2.0834, 'learning_rate': 0.0005319291781327749, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1080/2180 [2:51:33<2:54:15,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1081/2180 [2:51:42<2:54:08,  9.51s/it]                                                       {'loss': 2.0966, 'learning_rate': 0.0005311876151031642, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1081/2180 [2:51:42<2:54:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1082/2180 [2:51:52<2:53:52,  9.50s/it]                                                       {'loss': 2.0147, 'learning_rate': 0.0005304459831969274, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1082/2180 [2:51:52<2:53:52,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1083/2180 [2:52:01<2:53:57,  9.51s/it]                                                       {'loss': 2.1085, 'learning_rate': 0.0005297042840519294, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1083/2180 [2:52:01<2:53:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1084/2180 [2:52:11<2:53:47,  9.51s/it]                                                       {'loss': 2.0973, 'learning_rate': 0.0005289625193061838, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1084/2180 [2:52:11<2:53:47,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2180 [2:52:20<2:53:24,  9.50s/it]                                                       {'loss': 2.069, 'learning_rate': 0.0005282206905978489, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2180 [2:52:20<2:53:24,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2180 [2:52:30<2:53:11,  9.50s/it]                                                       {'loss': 2.1409, 'learning_rate': 0.0005274787995652246, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2180 [2:52:30<2:53:11,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2180 [2:52:39<2:53:03,  9.50s/it]                                                       {'loss': 2.0731, 'learning_rate': 0.000526736847846748, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2180 [2:52:39<2:53:03,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2180 [2:52:49<2:52:45,  9.49s/it]                                                       {'loss': 2.0515, 'learning_rate': 0.0005259948370809901, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2180 [2:52:49<2:52:45,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2180 [2:52:58<2:52:40,  9.50s/it]                                                       {'loss': 2.0546, 'learning_rate': 0.0005252527689066533, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2180 [2:52:58<2:52:40,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1090/2180 [2:53:08<2:52:41,  9.51s/it]                                                       {'loss': 2.1222, 'learning_rate': 0.0005245106449625654, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1090/2180 [2:53:08<2:52:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1091/2180 [2:53:17<2:52:35,  9.51s/it]                                                       {'loss': 2.0858, 'learning_rate': 0.0005237684668876785, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1091/2180 [2:53:17<2:52:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1092/2180 [2:53:27<2:52:30,  9.51s/it]                                                       {'loss': 2.0264, 'learning_rate': 0.0005230262363210637, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1092/2180 [2:53:27<2:52:30,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1093/2180 [2:53:36<2:52:27,  9.52s/it]                                                       {'loss': 2.0375, 'learning_rate': 0.0005222839549019079, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1093/2180 [2:53:36<2:52:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1094/2180 [2:53:46<2:52:42,  9.54s/it]                                                       {'loss': 2.0323, 'learning_rate': 0.0005215416242695108, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1094/2180 [2:53:46<2:52:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1095/2180 [2:53:56<2:52:15,  9.53s/it]                                                       {'loss': 2.0185, 'learning_rate': 0.0005207992460632804, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1095/2180 [2:53:56<2:52:15,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1096/2180 [2:54:05<2:51:56,  9.52s/it]                                                       {'loss': 2.1148, 'learning_rate': 0.0005200568219227299, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1096/2180 [2:54:05<2:51:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1097/2180 [2:54:15<2:51:43,  9.51s/it]                                                       {'loss': 2.1761, 'learning_rate': 0.000519314353487474, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1097/2180 [2:54:15<2:51:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1098/2180 [2:54:24<2:51:26,  9.51s/it]                                                       {'loss': 2.0882, 'learning_rate': 0.0005185718423972251, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1098/2180 [2:54:24<2:51:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1099/2180 [2:54:34<2:51:46,  9.53s/it]                                                       {'loss': 2.0603, 'learning_rate': 0.0005178292902917898, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1099/2180 [2:54:34<2:51:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1100/2180 [2:54:43<2:51:42,  9.54s/it]                                                       {'loss': 2.0185, 'learning_rate': 0.0005170866988110656, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1100/2180 [2:54:43<2:51:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1101/2180 [2:54:53<2:51:59,  9.56s/it]                                                       {'loss': 2.1158, 'learning_rate': 0.0005163440695950362, 'epoch': 0.5}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1101/2180 [2:54:53<2:51:59,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1102/2180 [2:55:02<2:51:31,  9.55s/it]                                                       {'loss': 2.0421, 'learning_rate': 0.0005156014042837695, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1102/2180 [2:55:02<2:51:31,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1103/2180 [2:55:12<2:50:59,  9.53s/it]                                                       {'loss': 2.078, 'learning_rate': 0.0005148587045174128, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1103/2180 [2:55:12<2:50:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1104/2180 [2:55:21<2:50:40,  9.52s/it]                                                       {'loss': 2.0793, 'learning_rate': 0.0005141159719361891, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1104/2180 [2:55:21<2:50:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1105/2180 [2:55:31<2:50:31,  9.52s/it]                                                       {'loss': 2.0182, 'learning_rate': 0.0005133732081803945, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1105/2180 [2:55:31<2:50:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1106/2180 [2:55:40<2:50:23,  9.52s/it]                                                       {'loss': 2.0746, 'learning_rate': 0.0005126304148903936, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1106/2180 [2:55:40<2:50:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1107/2180 [2:55:50<2:50:13,  9.52s/it]                                                       {'loss': 2.0913, 'learning_rate': 0.0005118875937066161, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1107/2180 [2:55:50<2:50:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1108/2180 [2:55:59<2:50:06,  9.52s/it]                                                       {'loss': 2.0279, 'learning_rate': 0.0005111447462695537, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1108/2180 [2:55:59<2:50:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1109/2180 [2:56:09<2:49:59,  9.52s/it]                                                       {'loss': 2.0572, 'learning_rate': 0.0005104018742197557, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1109/2180 [2:56:09<2:49:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1110/2180 [2:56:18<2:49:58,  9.53s/it]                                                       {'loss': 2.0266, 'learning_rate': 0.0005096589791978261, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1110/2180 [2:56:18<2:49:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1111/2180 [2:56:28<2:49:35,  9.52s/it]                                                       {'loss': 2.0881, 'learning_rate': 0.0005089160628444192, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1111/2180 [2:56:28<2:49:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2180 [2:56:37<2:49:30,  9.52s/it]                                                       {'loss': 2.0525, 'learning_rate': 0.0005081731268002371, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2180 [2:56:37<2:49:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2180 [2:56:47<2:49:32,  9.53s/it]                                                       {'loss': 2.1226, 'learning_rate': 0.0005074301727060243, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2180 [2:56:47<2:49:32,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2180 [2:56:56<2:49:03,  9.52s/it]                                                       {'loss': 2.1283, 'learning_rate': 0.0005066872022025663, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2180 [2:56:56<2:49:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2180 [2:57:06<2:48:42,  9.50s/it]                                                       {'loss': 2.064, 'learning_rate': 0.0005059442169306844, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2180 [2:57:06<2:48:42,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2180 [2:57:16<2:48:47,  9.52s/it]                                                       {'loss': 2.0023, 'learning_rate': 0.0005052012185312321, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2180 [2:57:16<2:48:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2180 [2:57:25<2:48:47,  9.53s/it]                                                       {'loss': 2.0728, 'learning_rate': 0.0005044582086450925, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2180 [2:57:25<2:48:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1118/2180 [2:57:35<2:48:34,  9.52s/it]                                                       {'loss': 2.0366, 'learning_rate': 0.0005037151889131737, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1118/2180 [2:57:35<2:48:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1119/2180 [2:57:44<2:48:33,  9.53s/it]                                                       {'loss': 2.0851, 'learning_rate': 0.0005029721609764059, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1119/2180 [2:57:44<2:48:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1120/2180 [2:57:54<2:48:32,  9.54s/it]                                                       {'loss': 2.1281, 'learning_rate': 0.000502229126475737, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1120/2180 [2:57:54<2:48:32,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1121/2180 [2:58:03<2:48:01,  9.52s/it]                                                       {'loss': 2.0707, 'learning_rate': 0.0005014860870521293, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1121/2180 [2:58:03<2:48:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1122/2180 [2:58:13<2:47:49,  9.52s/it]                                                       {'loss': 2.0446, 'learning_rate': 0.0005007430443465569, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1122/2180 [2:58:13<2:47:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1123/2180 [2:58:22<2:47:38,  9.52s/it]                                                       {'loss': 2.0675, 'learning_rate': 0.0005, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1123/2180 [2:58:22<2:47:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1124/2180 [2:58:32<2:47:27,  9.51s/it]                                                       {'loss': 2.0926, 'learning_rate': 0.0004992569556534432, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1124/2180 [2:58:32<2:47:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1125/2180 [2:58:41<2:47:29,  9.53s/it]                                                       {'loss': 2.0493, 'learning_rate': 0.0004985139129478707, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1125/2180 [2:58:41<2:47:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1126/2180 [2:58:51<2:47:16,  9.52s/it]                                                       {'loss': 2.1133, 'learning_rate': 0.0004977708735242633, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1126/2180 [2:58:51<2:47:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1127/2180 [2:59:00<2:47:04,  9.52s/it]                                                       {'loss': 2.0778, 'learning_rate': 0.0004970278390235942, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1127/2180 [2:59:00<2:47:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1128/2180 [2:59:10<2:46:50,  9.52s/it]                                                       {'loss': 2.0192, 'learning_rate': 0.0004962848110868262, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1128/2180 [2:59:10<2:46:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1129/2180 [2:59:19<2:46:37,  9.51s/it]                                                       {'loss': 2.0841, 'learning_rate': 0.0004955417913549074, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1129/2180 [2:59:19<2:46:37,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1130/2180 [2:59:29<2:46:17,  9.50s/it]                                                       {'loss': 2.0826, 'learning_rate': 0.0004947987814687679, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1130/2180 [2:59:29<2:46:17,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1131/2180 [2:59:38<2:46:06,  9.50s/it]                                                       {'loss': 2.1, 'learning_rate': 0.0004940557830693157, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1131/2180 [2:59:38<2:46:06,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1132/2180 [2:59:48<2:46:06,  9.51s/it]                                                       {'loss': 2.1268, 'learning_rate': 0.0004933127977974338, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1132/2180 [2:59:48<2:46:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1133/2180 [2:59:57<2:46:08,  9.52s/it]                                                       {'loss': 2.0512, 'learning_rate': 0.0004925698272939757, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1133/2180 [2:59:57<2:46:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1134/2180 [3:00:07<2:45:53,  9.52s/it]                                                       {'loss': 2.0393, 'learning_rate': 0.0004918268731997632, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1134/2180 [3:00:07<2:45:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1135/2180 [3:00:16<2:45:42,  9.51s/it]                                                       {'loss': 2.1326, 'learning_rate': 0.0004910839371555809, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1135/2180 [3:00:16<2:45:42,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1136/2180 [3:00:26<2:45:32,  9.51s/it]                                                       {'loss': 2.124, 'learning_rate': 0.0004903410208021739, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1136/2180 [3:00:26<2:45:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1137/2180 [3:00:35<2:45:10,  9.50s/it]                                                       {'loss': 2.1033, 'learning_rate': 0.0004895981257802443, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1137/2180 [3:00:35<2:45:10,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1138/2180 [3:00:45<2:45:00,  9.50s/it]                                                       {'loss': 2.1282, 'learning_rate': 0.0004888552537304463, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1138/2180 [3:00:45<2:45:00,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1139/2180 [3:00:54<2:44:40,  9.49s/it]                                                       {'loss': 2.0758, 'learning_rate': 0.00048811240629338394, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1139/2180 [3:00:54<2:44:40,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2180 [3:01:04<2:44:48,  9.51s/it]                                                       {'loss': 2.1195, 'learning_rate': 0.00048736958510960663, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2180 [3:01:04<2:44:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2180 [3:01:13<2:44:40,  9.51s/it]                                                       {'loss': 1.996, 'learning_rate': 0.00048662679181960564, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2180 [3:01:13<2:44:40,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2180 [3:01:23<2:44:32,  9.51s/it]                                                       {'loss': 2.0748, 'learning_rate': 0.00048588402806381094, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2180 [3:01:23<2:44:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2180 [3:01:32<2:44:32,  9.52s/it]                                                       {'loss': 2.0994, 'learning_rate': 0.0004851412954825874, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2180 [3:01:32<2:44:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2180 [3:01:42<2:44:22,  9.52s/it]                                                       {'loss': 2.0562, 'learning_rate': 0.00048439859571623034, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2180 [3:01:42<2:44:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1145/2180 [3:01:51<2:44:19,  9.53s/it]                                                       {'loss': 2.0809, 'learning_rate': 0.00048365593040496373, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1145/2180 [3:01:51<2:44:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1146/2180 [3:02:01<2:43:53,  9.51s/it]                                                       {'loss': 2.0903, 'learning_rate': 0.00048291330118893443, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1146/2180 [3:02:01<2:43:53,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1147/2180 [3:02:10<2:43:46,  9.51s/it]                                                       {'loss': 2.0814, 'learning_rate': 0.0004821707097082102, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1147/2180 [3:02:10<2:43:46,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1148/2180 [3:02:20<2:43:24,  9.50s/it]                                                       {'loss': 2.1236, 'learning_rate': 0.0004814281576027749, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1148/2180 [3:02:20<2:43:24,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1149/2180 [3:02:30<2:43:32,  9.52s/it]                                                       {'loss': 2.0499, 'learning_rate': 0.000480685646512526, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1149/2180 [3:02:30<2:43:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1150/2180 [3:02:39<2:43:45,  9.54s/it]                                                       {'loss': 2.0555, 'learning_rate': 0.00047994317807727025, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1150/2180 [3:02:39<2:43:45,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1151/2180 [3:02:49<2:43:24,  9.53s/it]                                                       {'loss': 2.0696, 'learning_rate': 0.00047920075393671974, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1151/2180 [3:02:49<2:43:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1152/2180 [3:02:58<2:43:09,  9.52s/it]                                                       {'loss': 2.1498, 'learning_rate': 0.0004784583757304893, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1152/2180 [3:02:58<2:43:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1153/2180 [3:03:08<2:42:59,  9.52s/it]                                                       {'loss': 2.1476, 'learning_rate': 0.00047771604509809214, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1153/2180 [3:03:08<2:42:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1154/2180 [3:03:17<2:43:02,  9.53s/it]                                                       {'loss': 2.1062, 'learning_rate': 0.0004769737636789364, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1154/2180 [3:03:17<2:43:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1155/2180 [3:03:27<2:42:37,  9.52s/it]                                                       {'loss': 2.121, 'learning_rate': 0.00047623153311232157, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1155/2180 [3:03:27<2:42:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1156/2180 [3:03:36<2:42:35,  9.53s/it]                                                       {'loss': 2.0753, 'learning_rate': 0.0004754893550374346, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1156/2180 [3:03:36<2:42:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1157/2180 [3:03:46<2:42:25,  9.53s/it]                                                       {'loss': 2.0394, 'learning_rate': 0.00047474723109334685, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1157/2180 [3:03:46<2:42:25,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1158/2180 [3:03:55<2:42:12,  9.52s/it]                                                       {'loss': 2.0268, 'learning_rate': 0.00047400516291900993, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1158/2180 [3:03:55<2:42:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1159/2180 [3:04:05<2:42:01,  9.52s/it]                                                       {'loss': 2.0625, 'learning_rate': 0.0004732631521532522, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1159/2180 [3:04:05<2:42:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1160/2180 [3:04:14<2:41:43,  9.51s/it]                                                       {'loss': 2.0361, 'learning_rate': 0.0004725212004347755, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1160/2180 [3:04:14<2:41:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1161/2180 [3:04:24<2:41:49,  9.53s/it]                                                       {'loss': 2.0236, 'learning_rate': 0.00047177930940215095, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1161/2180 [3:04:24<2:41:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1162/2180 [3:04:34<2:42:29,  9.58s/it]                                                       {'loss': 1.9899, 'learning_rate': 0.00047103748069381624, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1162/2180 [3:04:34<2:42:29,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1163/2180 [3:04:43<2:42:10,  9.57s/it]                                                       {'loss': 2.0429, 'learning_rate': 0.0004702957159480707, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1163/2180 [3:04:43<2:42:10,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1164/2180 [3:04:53<2:41:41,  9.55s/it]                                                       {'loss': 2.0812, 'learning_rate': 0.00046955401680307267, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1164/2180 [3:04:53<2:41:41,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1165/2180 [3:05:02<2:41:26,  9.54s/it]                                                       {'loss': 2.0703, 'learning_rate': 0.0004688123848968359, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1165/2180 [3:05:02<2:41:26,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1166/2180 [3:05:12<2:41:12,  9.54s/it]                                                       {'loss': 2.0516, 'learning_rate': 0.00046807082186722516, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1166/2180 [3:05:12<2:41:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1167/2180 [3:05:21<2:40:46,  9.52s/it]                                                       {'loss': 2.0198, 'learning_rate': 0.0004673293293519535, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1167/2180 [3:05:21<2:40:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2180 [3:05:31<2:40:33,  9.52s/it]                                                       {'loss': 2.1122, 'learning_rate': 0.00046658790898857806, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2180 [3:05:31<2:40:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2180 [3:05:40<2:40:21,  9.52s/it]                                                       {'loss': 2.0359, 'learning_rate': 0.0004658465624144963, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2180 [3:05:40<2:40:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2180 [3:05:50<2:40:19,  9.52s/it]                                                       {'loss': 2.1006, 'learning_rate': 0.0004651052912669438, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2180 [3:05:50<2:40:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2180 [3:05:59<2:40:16,  9.53s/it]                                                       {'loss': 2.1937, 'learning_rate': 0.0004643640971829883, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2180 [3:05:59<2:40:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2180 [3:06:09<2:40:20,  9.54s/it]                                                       {'loss': 1.9634, 'learning_rate': 0.0004636229817995281, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2180 [3:06:09<2:40:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2180 [3:06:18<2:39:59,  9.53s/it]                                                       {'loss': 1.9831, 'learning_rate': 0.0004628819467532876, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2180 [3:06:18<2:39:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1174/2180 [3:06:28<2:40:03,  9.55s/it]                                                       {'loss': 2.0731, 'learning_rate': 0.00046214099368081335, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1174/2180 [3:06:28<2:40:03,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1175/2180 [3:06:37<2:39:47,  9.54s/it]                                                       {'loss': 2.0436, 'learning_rate': 0.0004614001242184714, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1175/2180 [3:06:37<2:39:47,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1176/2180 [3:06:47<2:39:34,  9.54s/it]                                                       {'loss': 2.0525, 'learning_rate': 0.000460659340002443, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1176/2180 [3:06:47<2:39:34,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1177/2180 [3:06:56<2:39:26,  9.54s/it]                                                       {'loss': 2.0637, 'learning_rate': 0.00045991864266872073, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1177/2180 [3:06:56<2:39:26,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1178/2180 [3:07:06<2:39:18,  9.54s/it]                                                       {'loss': 2.1067, 'learning_rate': 0.00045917803385310595, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1178/2180 [3:07:06<2:39:18,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1179/2180 [3:07:16<2:39:47,  9.58s/it]                                                       {'loss': 2.0623, 'learning_rate': 0.00045843751519120417, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1179/2180 [3:07:16<2:39:47,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2180 [3:07:25<2:39:09,  9.55s/it]                                                       {'loss': 2.1361, 'learning_rate': 0.00045769708831842193, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2180 [3:07:25<2:39:09,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1181/2180 [3:07:35<2:39:23,  9.57s/it]                                                       {'loss': 2.0568, 'learning_rate': 0.00045695675486996266, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1181/2180 [3:07:35<2:39:23,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1182/2180 [3:07:44<2:39:05,  9.56s/it]                                                       {'loss': 2.0721, 'learning_rate': 0.00045621651648082405, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1182/2180 [3:07:44<2:39:05,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1183/2180 [3:07:54<2:38:38,  9.55s/it]                                                       {'loss': 2.0263, 'learning_rate': 0.00045547637478579356, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1183/2180 [3:07:54<2:38:38,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1184/2180 [3:08:03<2:38:25,  9.54s/it]                                                       {'loss': 2.0391, 'learning_rate': 0.0004547363314194449, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1184/2180 [3:08:03<2:38:25,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1185/2180 [3:08:13<2:38:04,  9.53s/it]                                                       {'loss': 2.1198, 'learning_rate': 0.000453996388016135, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1185/2180 [3:08:13<2:38:04,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1186/2180 [3:08:22<2:38:07,  9.55s/it]                                                       {'loss': 2.0694, 'learning_rate': 0.0004532565462099999, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1186/2180 [3:08:22<2:38:07,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1187/2180 [3:08:32<2:37:58,  9.55s/it]                                                       {'loss': 2.0717, 'learning_rate': 0.0004525168076349513, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1187/2180 [3:08:32<2:37:58,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1188/2180 [3:08:42<2:37:35,  9.53s/it]                                                       {'loss': 2.1024, 'learning_rate': 0.0004517771739246729, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1188/2180 [3:08:42<2:37:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1189/2180 [3:08:51<2:37:19,  9.53s/it]                                                       {'loss': 2.1038, 'learning_rate': 0.0004510376467126165, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1189/2180 [3:08:51<2:37:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1190/2180 [3:09:01<2:37:19,  9.53s/it]                                                       {'loss': 2.1017, 'learning_rate': 0.0004502982276319992, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1190/2180 [3:09:01<2:37:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1191/2180 [3:09:10<2:36:53,  9.52s/it]                                                       {'loss': 2.1111, 'learning_rate': 0.0004495589183157991, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1191/2180 [3:09:10<2:36:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1192/2180 [3:09:20<2:36:45,  9.52s/it]                                                       {'loss': 2.0206, 'learning_rate': 0.0004488197203967517, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1192/2180 [3:09:20<2:36:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1193/2180 [3:09:29<2:37:01,  9.55s/it]                                                       {'loss': 2.1106, 'learning_rate': 0.0004480806355073467, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1193/2180 [3:09:29<2:37:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1194/2180 [3:09:39<2:36:43,  9.54s/it]                                                       {'loss': 2.0542, 'learning_rate': 0.000447341665279824, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1194/2180 [3:09:39<2:36:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1195/2180 [3:09:48<2:36:36,  9.54s/it]                                                       {'loss': 2.0884, 'learning_rate': 0.0004466028113461708, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1195/2180 [3:09:48<2:36:36,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2180 [3:09:58<2:36:29,  9.54s/it]                                                       {'loss': 2.1972, 'learning_rate': 0.0004458640753381167, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2180 [3:09:58<2:36:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2180 [3:10:07<2:36:14,  9.54s/it]                                                       {'loss': 2.0523, 'learning_rate': 0.0004451254588871313, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2180 [3:10:07<2:36:14,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2180 [3:10:17<2:36:12,  9.54s/it]                                                       {'loss': 2.0787, 'learning_rate': 0.0004443869636244203, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2180 [3:10:17<2:36:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1199/2180 [3:10:26<2:36:09,  9.55s/it]                                                       {'loss': 2.0659, 'learning_rate': 0.0004436485911809212, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1199/2180 [3:10:26<2:36:09,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1200/2180 [3:10:36<2:35:59,  9.55s/it]                                                       {'loss': 2.0941, 'learning_rate': 0.00044291034318730087, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1200/2180 [3:10:36<2:35:59,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1201/2180 [3:10:45<2:35:28,  9.53s/it]                                                       {'loss': 2.1291, 'learning_rate': 0.0004421722212739511, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1201/2180 [3:10:45<2:35:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1202/2180 [3:10:55<2:35:14,  9.52s/it]                                                       {'loss': 2.0607, 'learning_rate': 0.0004414342270709848, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1202/2180 [3:10:55<2:35:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1203/2180 [3:11:05<2:34:59,  9.52s/it]                                                       {'loss': 2.0648, 'learning_rate': 0.00044069636220823397, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1203/2180 [3:11:05<2:34:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1204/2180 [3:11:14<2:34:50,  9.52s/it]                                                       {'loss': 2.0801, 'learning_rate': 0.0004399586283152437, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1204/2180 [3:11:14<2:34:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1205/2180 [3:11:24<2:34:41,  9.52s/it]                                                       {'loss': 2.1323, 'learning_rate': 0.0004392210270212706, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1205/2180 [3:11:24<2:34:41,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1206/2180 [3:11:33<2:34:29,  9.52s/it]                                                       {'loss': 2.0406, 'learning_rate': 0.00043848355995527825, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1206/2180 [3:11:33<2:34:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1207/2180 [3:11:43<2:34:45,  9.54s/it]                                                       {'loss': 2.0472, 'learning_rate': 0.00043774622874593374, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1207/2180 [3:11:43<2:34:45,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1208/2180 [3:11:52<2:34:20,  9.53s/it]                                                       {'loss': 2.1591, 'learning_rate': 0.000437009035021604, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1208/2180 [3:11:52<2:34:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1209/2180 [3:12:02<2:34:32,  9.55s/it]                                                       {'loss': 2.0854, 'learning_rate': 0.00043627198041035274, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1209/2180 [3:12:02<2:34:32,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1210/2180 [3:12:11<2:34:05,  9.53s/it]                                                       {'loss': 2.0352, 'learning_rate': 0.00043553506653993597, 'epoch': 0.55}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1210/2180 [3:12:11<2:34:05,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1211/2180 [3:12:21<2:33:46,  9.52s/it]                                                       {'loss': 2.0848, 'learning_rate': 0.0004347982950377992, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1211/2180 [3:12:21<2:33:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1212/2180 [3:12:30<2:33:40,  9.53s/it]                                                       {'loss': 2.1188, 'learning_rate': 0.0004340616675310735, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1212/2180 [3:12:30<2:33:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1213/2180 [3:12:40<2:33:24,  9.52s/it]                                                       {'loss': 2.034, 'learning_rate': 0.00043332518564657193, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1213/2180 [3:12:40<2:33:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1214/2180 [3:12:49<2:33:10,  9.51s/it]                                                       {'loss': 2.0419, 'learning_rate': 0.0004325888510107856, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1214/2180 [3:12:49<2:33:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1215/2180 [3:12:59<2:33:28,  9.54s/it]                                                       {'loss': 2.1148, 'learning_rate': 0.0004318526652498809, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1215/2180 [3:12:59<2:33:28,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1216/2180 [3:13:08<2:33:16,  9.54s/it]                                                       {'loss': 2.1051, 'learning_rate': 0.00043111662998969523, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1216/2180 [3:13:08<2:33:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1217/2180 [3:13:18<2:32:50,  9.52s/it]                                                       {'loss': 2.1263, 'learning_rate': 0.0004303807468557335, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1217/2180 [3:13:18<2:32:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1218/2180 [3:13:27<2:32:43,  9.53s/it]                                                       {'loss': 2.0642, 'learning_rate': 0.0004296450174731648, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1218/2180 [3:13:27<2:32:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1219/2180 [3:13:37<2:32:32,  9.52s/it]                                                       {'loss': 2.0741, 'learning_rate': 0.0004289094434668188, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1219/2180 [3:13:37<2:32:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1220/2180 [3:13:46<2:32:21,  9.52s/it]                                                       {'loss': 2.0547, 'learning_rate': 0.00042817402646118185, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1220/2180 [3:13:46<2:32:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1221/2180 [3:13:56<2:32:11,  9.52s/it]                                                       {'loss': 2.0245, 'learning_rate': 0.0004274387680803936, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1221/2180 [3:13:56<2:32:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1222/2180 [3:14:06<2:32:13,  9.53s/it]                                                       {'loss': 1.9671, 'learning_rate': 0.00042670366994824327, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1222/2180 [3:14:06<2:32:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1223/2180 [3:14:15<2:31:47,  9.52s/it]                                                       {'loss': 2.1396, 'learning_rate': 0.0004259687336881663, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1223/2180 [3:14:15<2:31:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2180 [3:14:25<2:31:53,  9.53s/it]                                                       {'loss': 2.0514, 'learning_rate': 0.0004252339609232408, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2180 [3:14:25<2:31:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2180 [3:14:34<2:31:47,  9.54s/it]                                                       {'loss': 2.0278, 'learning_rate': 0.0004244993532761834, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2180 [3:14:34<2:31:47,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2180 [3:14:44<2:31:22,  9.52s/it]                                                       {'loss': 1.9997, 'learning_rate': 0.00042376491236934634, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2180 [3:14:44<2:31:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1227/2180 [3:14:53<2:31:24,  9.53s/it]                                                       {'loss': 2.1111, 'learning_rate': 0.0004230306398247136, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1227/2180 [3:14:53<2:31:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1228/2180 [3:15:03<2:31:06,  9.52s/it]                                                       {'loss': 2.0955, 'learning_rate': 0.0004222965372638976, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1228/2180 [3:15:03<2:31:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1229/2180 [3:15:12<2:30:55,  9.52s/it]                                                       {'loss': 2.1138, 'learning_rate': 0.0004215626063081348, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1229/2180 [3:15:12<2:30:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1230/2180 [3:15:22<2:30:32,  9.51s/it]                                                       {'loss': 2.0432, 'learning_rate': 0.000420828848578283, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1230/2180 [3:15:22<2:30:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1231/2180 [3:15:31<2:30:29,  9.51s/it]                                                       {'loss': 2.0432, 'learning_rate': 0.0004200952656948175, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1231/2180 [3:15:31<2:30:29,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1232/2180 [3:15:41<2:30:21,  9.52s/it]                                                       {'loss': 2.128, 'learning_rate': 0.0004193618592778272, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1232/2180 [3:15:41<2:30:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1233/2180 [3:15:50<2:30:19,  9.52s/it]                                                       {'loss': 2.1089, 'learning_rate': 0.0004186286309470116, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1233/2180 [3:15:50<2:30:19,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1234/2180 [3:16:00<2:30:16,  9.53s/it]                                                       {'loss': 2.1318, 'learning_rate': 0.0004178955823216767, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1234/2180 [3:16:00<2:30:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1235/2180 [3:16:09<2:30:09,  9.53s/it]                                                       {'loss': 2.0609, 'learning_rate': 0.00041716271502073137, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1235/2180 [3:16:09<2:30:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1236/2180 [3:16:19<2:30:06,  9.54s/it]                                                       {'loss': 2.0911, 'learning_rate': 0.000416430030662685, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1236/2180 [3:16:19<2:30:06,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1237/2180 [3:16:28<2:29:51,  9.54s/it]                                                       {'loss': 2.0772, 'learning_rate': 0.00041569753086564173, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1237/2180 [3:16:28<2:29:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1238/2180 [3:16:38<2:29:36,  9.53s/it]                                                       {'loss': 2.0962, 'learning_rate': 0.0004149652172472988, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1238/2180 [3:16:38<2:29:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1239/2180 [3:16:47<2:29:22,  9.52s/it]                                                       {'loss': 2.0209, 'learning_rate': 0.00041423309142494234, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1239/2180 [3:16:47<2:29:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1240/2180 [3:16:57<2:29:10,  9.52s/it]                                                       {'loss': 2.1672, 'learning_rate': 0.0004135011550154433, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1240/2180 [3:16:57<2:29:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1241/2180 [3:17:07<2:29:13,  9.54s/it]                                                       {'loss': 2.0642, 'learning_rate': 0.0004127694096352546, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1241/2180 [3:17:07<2:29:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1242/2180 [3:17:16<2:29:04,  9.54s/it]                                                       {'loss': 2.0299, 'learning_rate': 0.00041203785690040743, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1242/2180 [3:17:16<2:29:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1243/2180 [3:17:26<2:28:51,  9.53s/it]                                                       {'loss': 2.1183, 'learning_rate': 0.00041130649842650694, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1243/2180 [3:17:26<2:28:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1244/2180 [3:17:35<2:28:58,  9.55s/it]                                                       {'loss': 2.1483, 'learning_rate': 0.00041057533582873016, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1244/2180 [3:17:35<2:28:58,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1245/2180 [3:17:45<2:29:03,  9.57s/it]                                                       {'loss': 2.0725, 'learning_rate': 0.0004098443707218208, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1245/2180 [3:17:45<2:29:03,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1246/2180 [3:17:54<2:28:37,  9.55s/it]                                                       {'loss': 1.9955, 'learning_rate': 0.00040911360472008673, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1246/2180 [3:17:54<2:28:37,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1247/2180 [3:18:04<2:28:22,  9.54s/it]                                                       {'loss': 2.1376, 'learning_rate': 0.0004083830394373959, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1247/2180 [3:18:04<2:28:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1248/2180 [3:18:13<2:28:03,  9.53s/it]                                                       {'loss': 2.1049, 'learning_rate': 0.00040765267648717324, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1248/2180 [3:18:13<2:28:03,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1249/2180 [3:18:23<2:27:47,  9.52s/it]                                                       {'loss': 2.0756, 'learning_rate': 0.00040692251748239677, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1249/2180 [3:18:23<2:27:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1250/2180 [3:18:32<2:27:29,  9.52s/it]                                                       {'loss': 2.1022, 'learning_rate': 0.00040619256403559383, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1250/2180 [3:18:32<2:27:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2180 [3:18:42<2:27:15,  9.51s/it]                                                       {'loss': 2.1431, 'learning_rate': 0.000405462817758838, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2180 [3:18:42<2:27:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2180 [3:18:51<2:27:06,  9.51s/it]                                                       {'loss': 2.0038, 'learning_rate': 0.0004047332802637457, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2180 [3:18:51<2:27:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2180 [3:19:01<2:26:46,  9.50s/it]                                                       {'loss': 2.0067, 'learning_rate': 0.00040400395316147157, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2180 [3:19:01<2:26:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1254/2180 [3:19:10<2:26:43,  9.51s/it]                                                       {'loss': 1.992, 'learning_rate': 0.00040327483806270627, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1254/2180 [3:19:10<2:26:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1255/2180 [3:19:20<2:26:32,  9.51s/it]                                                       {'loss': 2.1293, 'learning_rate': 0.0004025459365776715, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1255/2180 [3:19:20<2:26:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1256/2180 [3:19:29<2:26:21,  9.50s/it]                                                       {'loss': 2.0981, 'learning_rate': 0.00040181725031611794, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1256/2180 [3:19:29<2:26:21,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1257/2180 [3:19:39<2:26:07,  9.50s/it]                                                       {'loss': 2.0905, 'learning_rate': 0.0004010887808873206, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1257/2180 [3:19:39<2:26:07,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1258/2180 [3:19:48<2:26:34,  9.54s/it]                                                       {'loss': 2.0155, 'learning_rate': 0.00040036052990007553, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1258/2180 [3:19:48<2:26:34,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1259/2180 [3:19:58<2:26:17,  9.53s/it]                                                       {'loss': 2.121, 'learning_rate': 0.0003996324989626967, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1259/2180 [3:19:58<2:26:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1260/2180 [3:20:08<2:26:08,  9.53s/it]                                                       {'loss': 2.0229, 'learning_rate': 0.00039890468968301166, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1260/2180 [3:20:08<2:26:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1261/2180 [3:20:17<2:25:55,  9.53s/it]                                                       {'loss': 2.1167, 'learning_rate': 0.0003981771036683591, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1261/2180 [3:20:17<2:25:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1262/2180 [3:20:27<2:26:06,  9.55s/it]                                                       {'loss': 1.9951, 'learning_rate': 0.00039744974252558385, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1262/2180 [3:20:27<2:26:06,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1263/2180 [3:20:36<2:25:44,  9.54s/it]                                                       {'loss': 2.0375, 'learning_rate': 0.00039672260786103463, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1263/2180 [3:20:36<2:25:44,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1264/2180 [3:20:46<2:25:18,  9.52s/it]                                                       {'loss': 2.0385, 'learning_rate': 0.00039599570128055994, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1264/2180 [3:20:46<2:25:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1265/2180 [3:20:55<2:25:17,  9.53s/it]                                                       {'loss': 2.1365, 'learning_rate': 0.0003952690243895044, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1265/2180 [3:20:55<2:25:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1266/2180 [3:21:05<2:25:12,  9.53s/it]                                                       {'loss': 2.058, 'learning_rate': 0.0003945425787927054, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1266/2180 [3:21:05<2:25:12,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1267/2180 [3:21:14<2:24:56,  9.53s/it]                                                       {'loss': 2.1009, 'learning_rate': 0.00039381636609448975, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1267/2180 [3:21:14<2:24:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1268/2180 [3:21:24<2:24:36,  9.51s/it]                                                       {'loss': 2.1001, 'learning_rate': 0.0003930903878986693, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1268/2180 [3:21:24<2:24:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1269/2180 [3:21:33<2:24:30,  9.52s/it]                                                       {'loss': 2.0469, 'learning_rate': 0.00039236464580853916, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1269/2180 [3:21:33<2:24:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1270/2180 [3:21:43<2:24:21,  9.52s/it]                                                       {'loss': 2.1398, 'learning_rate': 0.0003916391414268718, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1270/2180 [3:21:43<2:24:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1271/2180 [3:21:52<2:24:09,  9.52s/it]                                                       {'loss': 2.1174, 'learning_rate': 0.00039091387635591536, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1271/2180 [3:21:52<2:24:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1272/2180 [3:22:02<2:24:06,  9.52s/it]                                                       {'loss': 2.0823, 'learning_rate': 0.0003901888521973894, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1272/2180 [3:22:02<2:24:06,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1273/2180 [3:22:11<2:24:04,  9.53s/it]                                                       {'loss': 2.0696, 'learning_rate': 0.0003894640705524813, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1273/2180 [3:22:11<2:24:04,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1274/2180 [3:22:21<2:23:51,  9.53s/it]                                                       {'loss': 2.0519, 'learning_rate': 0.00038873953302184284, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1274/2180 [3:22:21<2:23:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1275/2180 [3:22:30<2:23:38,  9.52s/it]                                                       {'loss': 2.0677, 'learning_rate': 0.000388015241205587, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1275/2180 [3:22:30<2:23:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1276/2180 [3:22:40<2:23:30,  9.53s/it]                                                       {'loss': 2.1135, 'learning_rate': 0.00038729119670328355, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1276/2180 [3:22:40<2:23:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1277/2180 [3:22:49<2:23:22,  9.53s/it]                                                       {'loss': 2.006, 'learning_rate': 0.00038656740111395665, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1277/2180 [3:22:49<2:23:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1278/2180 [3:22:59<2:23:17,  9.53s/it]                                                       {'loss': 2.1906, 'learning_rate': 0.00038584385603608053, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1278/2180 [3:22:59<2:23:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2180 [3:23:09<2:23:10,  9.53s/it]                                                       {'loss': 2.1223, 'learning_rate': 0.00038512056306757615, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2180 [3:23:09<2:23:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2180 [3:23:18<2:22:52,  9.53s/it]                                                       {'loss': 2.1787, 'learning_rate': 0.0003843975238058075, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2180 [3:23:18<2:22:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1281/2180 [3:23:28<2:22:40,  9.52s/it]                                                       {'loss': 2.047, 'learning_rate': 0.00038367473984757863, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1281/2180 [3:23:28<2:22:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1282/2180 [3:23:37<2:22:38,  9.53s/it]                                                       {'loss': 2.0061, 'learning_rate': 0.0003829522127891296, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1282/2180 [3:23:37<2:22:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1283/2180 [3:23:47<2:22:31,  9.53s/it]                                                       {'loss': 2.1179, 'learning_rate': 0.0003822299442261329, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1283/2180 [3:23:47<2:22:31,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1284/2180 [3:23:56<2:22:12,  9.52s/it]                                                       {'loss': 2.0458, 'learning_rate': 0.00038150793575369063, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1284/2180 [3:23:56<2:22:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1285/2180 [3:24:06<2:22:09,  9.53s/it]                                                       {'loss': 2.0311, 'learning_rate': 0.0003807861889663299, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1285/2180 [3:24:06<2:22:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1286/2180 [3:24:15<2:21:56,  9.53s/it]                                                       {'loss': 2.1174, 'learning_rate': 0.0003800647054580006, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1286/2180 [3:24:15<2:21:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1287/2180 [3:24:25<2:21:59,  9.54s/it]                                                       {'loss': 2.0442, 'learning_rate': 0.00037934348682207064, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1287/2180 [3:24:25<2:21:59,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1288/2180 [3:24:34<2:21:38,  9.53s/it]                                                       {'loss': 2.1592, 'learning_rate': 0.00037862253465132306, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1288/2180 [3:24:34<2:21:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1289/2180 [3:24:44<2:21:29,  9.53s/it]                                                       {'loss': 1.9881, 'learning_rate': 0.00037790185053795245, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1289/2180 [3:24:44<2:21:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1290/2180 [3:24:53<2:21:14,  9.52s/it]                                                       {'loss': 1.9665, 'learning_rate': 0.0003771814360735616, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1290/2180 [3:24:53<2:21:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1291/2180 [3:25:03<2:21:07,  9.52s/it]                                                       {'loss': 2.1322, 'learning_rate': 0.00037646129284915755, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1291/2180 [3:25:03<2:21:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1292/2180 [3:25:12<2:20:51,  9.52s/it]                                                       {'loss': 2.0697, 'learning_rate': 0.00037574142245514825, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1292/2180 [3:25:12<2:20:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1293/2180 [3:25:22<2:20:49,  9.53s/it]                                                       {'loss': 2.1529, 'learning_rate': 0.0003750218264813393, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1293/2180 [3:25:22<2:20:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1294/2180 [3:25:31<2:20:26,  9.51s/it]                                                       {'loss': 1.9615, 'learning_rate': 0.0003743025065169305, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1294/2180 [3:25:31<2:20:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1295/2180 [3:25:41<2:20:45,  9.54s/it]                                                       {'loss': 2.1143, 'learning_rate': 0.0003735834641505116, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1295/2180 [3:25:41<2:20:45,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1296/2180 [3:25:50<2:20:18,  9.52s/it]                                                       {'loss': 2.0334, 'learning_rate': 0.00037286470097005954, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1296/2180 [3:25:50<2:20:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1297/2180 [3:26:00<2:20:13,  9.53s/it]                                                       {'loss': 2.0399, 'learning_rate': 0.0003721462185629347, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1297/2180 [3:26:00<2:20:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1298/2180 [3:26:10<2:20:01,  9.53s/it]                                                       {'loss': 2.13, 'learning_rate': 0.00037142801851587707, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1298/2180 [3:26:10<2:20:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1299/2180 [3:26:19<2:19:51,  9.52s/it]                                                       {'loss': 2.0756, 'learning_rate': 0.00037071010241500357, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1299/2180 [3:26:19<2:19:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1300/2180 [3:26:29<2:19:43,  9.53s/it]                                                       {'loss': 2.0552, 'learning_rate': 0.00036999247184580383, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1300/2180 [3:26:29<2:19:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1301/2180 [3:26:38<2:19:41,  9.53s/it]                                                       {'loss': 2.0373, 'learning_rate': 0.00036927512839313636, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1301/2180 [3:26:38<2:19:41,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1302/2180 [3:26:48<2:19:35,  9.54s/it]                                                       {'loss': 2.0303, 'learning_rate': 0.0003685580736412268, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1302/2180 [3:26:48<2:19:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1303/2180 [3:26:57<2:19:19,  9.53s/it]                                                       {'loss': 2.0132, 'learning_rate': 0.000367841309173662, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1303/2180 [3:26:57<2:19:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1304/2180 [3:27:07<2:19:01,  9.52s/it]                                                       {'loss': 2.1434, 'learning_rate': 0.0003671248365733883, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1304/2180 [3:27:07<2:19:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1305/2180 [3:27:16<2:18:46,  9.52s/it]                                                       {'loss': 2.0464, 'learning_rate': 0.0003664086574227075, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1305/2180 [3:27:16<2:18:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1306/2180 [3:27:26<2:19:00,  9.54s/it]                                                       {'loss': 2.1027, 'learning_rate': 0.000365692773303273, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1306/2180 [3:27:26<2:19:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2180 [3:27:35<2:18:39,  9.53s/it]                                                       {'loss': 2.1208, 'learning_rate': 0.00036497718579608696, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2180 [3:27:35<2:18:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1308/2180 [3:27:45<2:18:33,  9.53s/it]                                                       {'loss': 2.0138, 'learning_rate': 0.0003642618964814964, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1308/2180 [3:27:45<2:18:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1309/2180 [3:27:54<2:18:19,  9.53s/it]                                                       {'loss': 2.1118, 'learning_rate': 0.00036354690693918946, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1309/2180 [3:27:54<2:18:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1310/2180 [3:28:04<2:17:57,  9.51s/it]                                                       {'loss': 1.9859, 'learning_rate': 0.00036283221874819284, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1310/2180 [3:28:04<2:17:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1311/2180 [3:28:13<2:17:51,  9.52s/it]                                                       {'loss': 2.1169, 'learning_rate': 0.0003621178334868672, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1311/2180 [3:28:13<2:17:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1312/2180 [3:28:23<2:17:39,  9.52s/it]                                                       {'loss': 2.1064, 'learning_rate': 0.00036140375273290476, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1312/2180 [3:28:23<2:17:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1313/2180 [3:28:32<2:17:24,  9.51s/it]                                                       {'loss': 2.061, 'learning_rate': 0.0003606899780633245, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1313/2180 [3:28:32<2:17:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1314/2180 [3:28:42<2:17:19,  9.51s/it]                                                       {'loss': 2.0817, 'learning_rate': 0.0003599765110544699, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1314/2180 [3:28:42<2:17:19,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1315/2180 [3:28:51<2:17:18,  9.52s/it]                                                       {'loss': 2.0676, 'learning_rate': 0.0003592633532820052, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1315/2180 [3:28:51<2:17:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1316/2180 [3:29:01<2:16:54,  9.51s/it]                                                       {'loss': 2.0244, 'learning_rate': 0.0003585505063209109, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1316/2180 [3:29:01<2:16:54,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1317/2180 [3:29:10<2:16:51,  9.52s/it]                                                       {'loss': 2.1504, 'learning_rate': 0.00035783797174548194, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1317/2180 [3:29:10<2:16:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1318/2180 [3:29:20<2:16:36,  9.51s/it]                                                       {'loss': 2.0714, 'learning_rate': 0.00035712575112932277, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1318/2180 [3:29:20<2:16:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1319/2180 [3:29:29<2:16:23,  9.50s/it]                                                       {'loss': 2.0485, 'learning_rate': 0.000356413846045345, 'epoch': 0.6}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1319/2180 [3:29:29<2:16:23,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1320/2180 [3:29:39<2:16:33,  9.53s/it]                                                       {'loss': 1.9941, 'learning_rate': 0.000355702258065763, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1320/2180 [3:29:39<2:16:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1321/2180 [3:29:49<2:16:27,  9.53s/it]                                                       {'loss': 2.0717, 'learning_rate': 0.0003549909887620909, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1321/2180 [3:29:49<2:16:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1322/2180 [3:29:58<2:16:32,  9.55s/it]                                                       {'loss': 1.9713, 'learning_rate': 0.00035428003970513914, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1322/2180 [3:29:58<2:16:32,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1323/2180 [3:30:08<2:16:26,  9.55s/it]                                                       {'loss': 2.0527, 'learning_rate': 0.00035356941246501085, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1323/2180 [3:30:08<2:16:26,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1324/2180 [3:30:17<2:16:01,  9.53s/it]                                                       {'loss': 1.9983, 'learning_rate': 0.0003528591086110984, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1324/2180 [3:30:17<2:16:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1325/2180 [3:30:27<2:15:50,  9.53s/it]                                                       {'loss': 2.0869, 'learning_rate': 0.00035214912971208, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1325/2180 [3:30:27<2:15:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1326/2180 [3:30:36<2:15:38,  9.53s/it]                                                       {'loss': 2.0515, 'learning_rate': 0.0003514394773359163, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1326/2180 [3:30:36<2:15:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1327/2180 [3:30:46<2:15:33,  9.53s/it]                                                       {'loss': 2.1501, 'learning_rate': 0.0003507301530498469, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1327/2180 [3:30:46<2:15:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1328/2180 [3:30:55<2:15:18,  9.53s/it]                                                       {'loss': 2.0726, 'learning_rate': 0.00035002115842038646, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1328/2180 [3:30:55<2:15:18,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1329/2180 [3:31:05<2:15:04,  9.52s/it]                                                       {'loss': 2.0748, 'learning_rate': 0.00034931249501332195, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1329/2180 [3:31:05<2:15:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1330/2180 [3:31:14<2:14:53,  9.52s/it]                                                       {'loss': 2.1291, 'learning_rate': 0.00034860416439370885, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1330/2180 [3:31:14<2:14:53,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1331/2180 [3:31:24<2:14:39,  9.52s/it]                                                       {'loss': 2.1286, 'learning_rate': 0.0003478961681258674, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1331/2180 [3:31:24<2:14:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1332/2180 [3:31:33<2:14:29,  9.52s/it]                                                       {'loss': 2.0387, 'learning_rate': 0.0003471885077733796, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1332/2180 [3:31:33<2:14:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1333/2180 [3:31:43<2:14:21,  9.52s/it]                                                       {'loss': 2.091, 'learning_rate': 0.0003464811848990859, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1333/2180 [3:31:43<2:14:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1334/2180 [3:31:52<2:14:09,  9.51s/it]                                                       {'loss': 2.0741, 'learning_rate': 0.00034577420106508063, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1334/2180 [3:31:52<2:14:09,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2180 [3:32:02<2:14:41,  9.56s/it]                                                       {'loss': 1.9913, 'learning_rate': 0.0003450675578327105, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2180 [3:32:02<2:14:41,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1336/2180 [3:32:12<2:14:23,  9.55s/it]                                                       {'loss': 2.0756, 'learning_rate': 0.000344361256762569, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1336/2180 [3:32:12<2:14:23,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1337/2180 [3:32:21<2:13:53,  9.53s/it]                                                       {'loss': 1.974, 'learning_rate': 0.00034365529941449456, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1337/2180 [3:32:21<2:13:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1338/2180 [3:32:31<2:13:43,  9.53s/it]                                                       {'loss': 2.0745, 'learning_rate': 0.0003429496873475664, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1338/2180 [3:32:31<2:13:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1339/2180 [3:32:40<2:13:29,  9.52s/it]                                                       {'loss': 2.0479, 'learning_rate': 0.0003422444221201009, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1339/2180 [3:32:40<2:13:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1340/2180 [3:32:50<2:13:27,  9.53s/it]                                                       {'loss': 2.0835, 'learning_rate': 0.0003415395052896487, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1340/2180 [3:32:50<2:13:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1341/2180 [3:32:59<2:13:11,  9.52s/it]                                                       {'loss': 1.9995, 'learning_rate': 0.0003408349384129912, 'epoch': 0.61}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1341/2180 [3:32:59<2:13:11,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1342/2180 [3:33:09<2:12:58,  9.52s/it]                                                       {'loss': 2.094, 'learning_rate': 0.00034013072304613643, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1342/2180 [3:33:09<2:12:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1343/2180 [3:33:18<2:12:54,  9.53s/it]                                                       {'loss': 2.0613, 'learning_rate': 0.00033942686074431674, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1343/2180 [3:33:18<2:12:54,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1344/2180 [3:33:28<2:12:31,  9.51s/it]                                                       {'loss': 2.1155, 'learning_rate': 0.0003387233530619843, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1344/2180 [3:33:28<2:12:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1345/2180 [3:33:37<2:12:14,  9.50s/it]                                                       {'loss': 2.0518, 'learning_rate': 0.0003380202015528084, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1345/2180 [3:33:37<2:12:14,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1346/2180 [3:33:47<2:12:14,  9.51s/it]                                                       {'loss': 2.0249, 'learning_rate': 0.0003373174077696715, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1346/2180 [3:33:47<2:12:14,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1347/2180 [3:33:56<2:12:16,  9.53s/it]                                                       {'loss': 2.0619, 'learning_rate': 0.0003366149732646661, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1347/2180 [3:33:56<2:12:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1348/2180 [3:34:06<2:12:00,  9.52s/it]                                                       {'loss': 2.0837, 'learning_rate': 0.00033591289958909143, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1348/2180 [3:34:06<2:12:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1349/2180 [3:34:15<2:11:49,  9.52s/it]                                                       {'loss': 2.0126, 'learning_rate': 0.00033521118829344954, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1349/2180 [3:34:15<2:11:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1350/2180 [3:34:25<2:11:45,  9.53s/it]                                                       {'loss': 2.0249, 'learning_rate': 0.0003345098409274423, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1350/2180 [3:34:25<2:11:45,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1351/2180 [3:34:34<2:11:36,  9.53s/it]                                                       {'loss': 2.0992, 'learning_rate': 0.00033380885903996796, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1351/2180 [3:34:34<2:11:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1352/2180 [3:34:44<2:11:22,  9.52s/it]                                                       {'loss': 2.0915, 'learning_rate': 0.00033310824417911766, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1352/2180 [3:34:44<2:11:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1353/2180 [3:34:53<2:11:05,  9.51s/it]                                                       {'loss': 2.0241, 'learning_rate': 0.00033240799789217184, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1353/2180 [3:34:53<2:11:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1354/2180 [3:35:03<2:11:07,  9.52s/it]                                                       {'loss': 2.0417, 'learning_rate': 0.00033170812172559694, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1354/2180 [3:35:03<2:11:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1355/2180 [3:35:12<2:11:01,  9.53s/it]                                                       {'loss': 2.0746, 'learning_rate': 0.000331008617225042, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1355/2180 [3:35:12<2:11:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1356/2180 [3:35:22<2:10:49,  9.53s/it]                                                       {'loss': 2.1443, 'learning_rate': 0.0003303094859353355, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1356/2180 [3:35:22<2:10:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1357/2180 [3:35:32<2:10:59,  9.55s/it]                                                       {'loss': 2.012, 'learning_rate': 0.0003296107294004812, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1357/2180 [3:35:32<2:10:59,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1358/2180 [3:35:41<2:10:47,  9.55s/it]                                                       {'loss': 2.0004, 'learning_rate': 0.0003289123491636559, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1358/2180 [3:35:41<2:10:47,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1359/2180 [3:35:51<2:10:26,  9.53s/it]                                                       {'loss': 2.0094, 'learning_rate': 0.00032821434676720443, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1359/2180 [3:35:51<2:10:26,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1360/2180 [3:36:00<2:10:08,  9.52s/it]                                                       {'loss': 2.0286, 'learning_rate': 0.00032751672375263836, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1360/2180 [3:36:00<2:10:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1361/2180 [3:36:10<2:10:20,  9.55s/it]                                                       {'loss': 2.0786, 'learning_rate': 0.0003268194816606305, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1361/2180 [3:36:10<2:10:20,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1362/2180 [3:36:19<2:10:05,  9.54s/it]                                                       {'loss': 2.0885, 'learning_rate': 0.00032612262203101267, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1362/2180 [3:36:19<2:10:05,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1363/2180 [3:36:29<2:09:44,  9.53s/it]                                                       {'loss': 2.1065, 'learning_rate': 0.00032542614640277225, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1363/2180 [3:36:29<2:09:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1364/2180 [3:36:38<2:09:33,  9.53s/it]                                                       {'loss': 2.0601, 'learning_rate': 0.0003247300563140481, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1364/2180 [3:36:38<2:09:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1365/2180 [3:36:48<2:09:25,  9.53s/it]                                                       {'loss': 2.0559, 'learning_rate': 0.00032403435330212807, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1365/2180 [3:36:48<2:09:25,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1366/2180 [3:36:57<2:09:15,  9.53s/it]                                                       {'loss': 2.2471, 'learning_rate': 0.00032333903890344515, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1366/2180 [3:36:57<2:09:15,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1367/2180 [3:37:07<2:09:00,  9.52s/it]                                                       {'loss': 2.0633, 'learning_rate': 0.00032264411465357333, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1367/2180 [3:37:07<2:09:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1368/2180 [3:37:16<2:08:48,  9.52s/it]                                                       {'loss': 2.0472, 'learning_rate': 0.00032194958208722654, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1368/2180 [3:37:16<2:08:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1369/2180 [3:37:26<2:08:43,  9.52s/it]                                                       {'loss': 1.9827, 'learning_rate': 0.00032125544273825204, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1369/2180 [3:37:26<2:08:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1370/2180 [3:37:35<2:08:35,  9.53s/it]                                                       {'loss': 2.0125, 'learning_rate': 0.0003205616981396297, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1370/2180 [3:37:35<2:08:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1371/2180 [3:37:45<2:08:27,  9.53s/it]                                                       {'loss': 1.9701, 'learning_rate': 0.00031986834982346713, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1371/2180 [3:37:45<2:08:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1372/2180 [3:37:54<2:08:12,  9.52s/it]                                                       {'loss': 2.136, 'learning_rate': 0.00031917539932099694, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1372/2180 [3:37:54<2:08:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1373/2180 [3:38:04<2:07:59,  9.52s/it]                                                       {'loss': 2.0977, 'learning_rate': 0.00031848284816257336, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1373/2180 [3:38:04<2:07:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1374/2180 [3:38:13<2:07:45,  9.51s/it]                                                       {'loss': 2.0056, 'learning_rate': 0.0003177906978776682, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1374/2180 [3:38:13<2:07:45,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1375/2180 [3:38:23<2:07:39,  9.52s/it]                                                       {'loss': 2.0575, 'learning_rate': 0.0003170989499948683, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1375/2180 [3:38:23<2:07:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1376/2180 [3:38:33<2:07:27,  9.51s/it]                                                       {'loss': 2.0874, 'learning_rate': 0.0003164076060418719, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1376/2180 [3:38:33<2:07:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1377/2180 [3:38:42<2:07:18,  9.51s/it]                                                       {'loss': 2.112, 'learning_rate': 0.000315716667545485, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1377/2180 [3:38:42<2:07:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1378/2180 [3:38:52<2:07:05,  9.51s/it]                                                       {'loss': 2.0374, 'learning_rate': 0.00031502613603161836, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1378/2180 [3:38:52<2:07:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1379/2180 [3:39:01<2:07:04,  9.52s/it]                                                       {'loss': 2.0926, 'learning_rate': 0.00031433601302528335, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1379/2180 [3:39:01<2:07:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1380/2180 [3:39:11<2:06:47,  9.51s/it]                                                       {'loss': 2.0889, 'learning_rate': 0.00031364630005058995, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1380/2180 [3:39:11<2:06:47,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1381/2180 [3:39:20<2:06:39,  9.51s/it]                                                       {'loss': 2.0445, 'learning_rate': 0.0003129569986307422, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1381/2180 [3:39:20<2:06:39,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1382/2180 [3:39:30<2:06:42,  9.53s/it]                                                       {'loss': 2.1126, 'learning_rate': 0.00031226811028803515, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1382/2180 [3:39:30<2:06:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1383/2180 [3:39:39<2:06:31,  9.53s/it]                                                       {'loss': 2.0716, 'learning_rate': 0.00031157963654385173, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1383/2180 [3:39:39<2:06:31,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1384/2180 [3:39:49<2:06:18,  9.52s/it]                                                       {'loss': 2.0375, 'learning_rate': 0.0003108915789186592, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1384/2180 [3:39:49<2:06:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1385/2180 [3:39:58<2:05:57,  9.51s/it]                                                       {'loss': 1.9458, 'learning_rate': 0.00031020393893200604, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1385/2180 [3:39:58<2:05:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1386/2180 [3:40:08<2:05:48,  9.51s/it]                                                       {'loss': 2.08, 'learning_rate': 0.00030951671810251823, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1386/2180 [3:40:08<2:05:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1387/2180 [3:40:17<2:05:33,  9.50s/it]                                                       {'loss': 2.1253, 'learning_rate': 0.0003088299179478959, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1387/2180 [3:40:17<2:05:33,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1388/2180 [3:40:27<2:05:31,  9.51s/it]                                                       {'loss': 2.0385, 'learning_rate': 0.0003081435399849104, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1388/2180 [3:40:27<2:05:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1389/2180 [3:40:36<2:05:26,  9.52s/it]                                                       {'loss': 2.0106, 'learning_rate': 0.0003074575857294004, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1389/2180 [3:40:36<2:05:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2180 [3:40:46<2:05:17,  9.52s/it]                                                       {'loss': 2.1228, 'learning_rate': 0.0003067720566962691, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2180 [3:40:46<2:05:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2180 [3:40:55<2:05:01,  9.51s/it]                                                       {'loss': 2.0909, 'learning_rate': 0.0003060869543994806, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2180 [3:40:55<2:05:01,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2180 [3:41:05<2:04:45,  9.50s/it]                                                       {'loss': 2.0418, 'learning_rate': 0.0003054022803520562, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2180 [3:41:05<2:04:45,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2180 [3:41:14<2:04:44,  9.51s/it]                                                       {'loss': 2.0629, 'learning_rate': 0.0003047180360660721, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2180 [3:41:14<2:04:44,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2180 [3:41:24<2:04:46,  9.53s/it]                                                       {'loss': 2.047, 'learning_rate': 0.00030403422305265475, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2180 [3:41:24<2:04:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2180 [3:41:33<2:04:42,  9.53s/it]                                                       {'loss': 2.1503, 'learning_rate': 0.0003033508428219785, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2180 [3:41:33<2:04:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2180 [3:41:43<2:04:29,  9.53s/it]                                                       {'loss': 2.0138, 'learning_rate': 0.00030266789688326184, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2180 [3:41:43<2:04:29,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1397/2180 [3:41:52<2:04:22,  9.53s/it]                                                       {'loss': 2.0695, 'learning_rate': 0.00030198538674476393, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1397/2180 [3:41:52<2:04:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1398/2180 [3:42:02<2:03:57,  9.51s/it]                                                       {'loss': 2.071, 'learning_rate': 0.00030130331391378185, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1398/2180 [3:42:02<2:03:57,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1399/2180 [3:42:11<2:04:00,  9.53s/it]                                                       {'loss': 2.0582, 'learning_rate': 0.0003006216798966468, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1399/2180 [3:42:11<2:04:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2180 [3:42:21<2:03:49,  9.52s/it]                                                       {'loss': 2.0687, 'learning_rate': 0.00029994048619872034, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2180 [3:42:21<2:03:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1401/2180 [3:42:30<2:03:32,  9.51s/it]                                                       {'loss': 1.9993, 'learning_rate': 0.0002992597343243927, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1401/2180 [3:42:30<2:03:32,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1402/2180 [3:42:40<2:03:27,  9.52s/it]                                                       {'loss': 2.0938, 'learning_rate': 0.0002985794257770773, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1402/2180 [3:42:40<2:03:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1403/2180 [3:42:49<2:03:22,  9.53s/it]                                                       {'loss': 2.0457, 'learning_rate': 0.0002978995620592092, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1403/2180 [3:42:49<2:03:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1404/2180 [3:42:59<2:03:22,  9.54s/it]                                                       {'loss': 2.0632, 'learning_rate': 0.0002972201446722405, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1404/2180 [3:42:59<2:03:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1405/2180 [3:43:09<2:03:16,  9.54s/it]                                                       {'loss': 2.0125, 'learning_rate': 0.00029654117511663803, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1405/2180 [3:43:09<2:03:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1406/2180 [3:43:18<2:02:55,  9.53s/it]                                                       {'loss': 2.0942, 'learning_rate': 0.0002958626548918795, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1406/2180 [3:43:18<2:02:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1407/2180 [3:43:28<2:02:40,  9.52s/it]                                                       {'loss': 2.0668, 'learning_rate': 0.00029518458549645014, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1407/2180 [3:43:28<2:02:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1408/2180 [3:43:37<2:02:29,  9.52s/it]                                                       {'loss': 2.0598, 'learning_rate': 0.00029450696842783954, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1408/2180 [3:43:37<2:02:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1409/2180 [3:43:47<2:02:35,  9.54s/it]                                                       {'loss': 2.0083, 'learning_rate': 0.00029382980518253865, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1409/2180 [3:43:47<2:02:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1410/2180 [3:43:56<2:02:20,  9.53s/it]                                                       {'loss': 2.0257, 'learning_rate': 0.00029315309725603595, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1410/2180 [3:43:56<2:02:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1411/2180 [3:44:06<2:02:05,  9.53s/it]                                                       {'loss': 2.1002, 'learning_rate': 0.00029247684614281446, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1411/2180 [3:44:06<2:02:05,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1412/2180 [3:44:15<2:01:52,  9.52s/it]                                                       {'loss': 2.0868, 'learning_rate': 0.0002918010533363481, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1412/2180 [3:44:15<2:01:52,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1413/2180 [3:44:25<2:01:38,  9.52s/it]                                                       {'loss': 1.9895, 'learning_rate': 0.0002911257203290987, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1413/2180 [3:44:25<2:01:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1414/2180 [3:44:34<2:01:43,  9.53s/it]                                                       {'loss': 2.0896, 'learning_rate': 0.00029045084861251314, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1414/2180 [3:44:34<2:01:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1415/2180 [3:44:44<2:01:34,  9.54s/it]                                                       {'loss': 2.0552, 'learning_rate': 0.00028977643967701897, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1415/2180 [3:44:44<2:01:34,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1416/2180 [3:44:53<2:01:12,  9.52s/it]                                                       {'loss': 1.9911, 'learning_rate': 0.00028910249501202156, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1416/2180 [3:44:53<2:01:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1417/2180 [3:45:03<2:00:58,  9.51s/it]                                                       {'loss': 2.0811, 'learning_rate': 0.00028842901610590165, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1417/2180 [3:45:03<2:00:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1418/2180 [3:45:12<2:00:43,  9.51s/it]                                                       {'loss': 2.0388, 'learning_rate': 0.00028775600444601123, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1418/2180 [3:45:12<2:00:43,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1419/2180 [3:45:22<2:01:09,  9.55s/it]                                                       {'loss': 2.0407, 'learning_rate': 0.00028708346151866973, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1419/2180 [3:45:22<2:01:09,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1420/2180 [3:45:31<2:00:42,  9.53s/it]                                                       {'loss': 2.0422, 'learning_rate': 0.0002864113888091622, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1420/2180 [3:45:31<2:00:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1421/2180 [3:45:41<2:00:27,  9.52s/it]                                                       {'loss': 2.077, 'learning_rate': 0.0002857397878017348, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1421/2180 [3:45:41<2:00:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1422/2180 [3:45:50<2:00:14,  9.52s/it]                                                       {'loss': 2.1827, 'learning_rate': 0.00028506865997959173, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1422/2180 [3:45:50<2:00:14,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1423/2180 [3:46:00<2:00:09,  9.52s/it]                                                       {'loss': 2.0989, 'learning_rate': 0.000284398006824893, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1423/2180 [3:46:00<2:00:09,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1424/2180 [3:46:10<1:59:52,  9.51s/it]                                                       {'loss': 2.0317, 'learning_rate': 0.00028372782981874963, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1424/2180 [3:46:10<1:59:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1425/2180 [3:46:19<1:59:34,  9.50s/it]                                                       {'loss': 2.1007, 'learning_rate': 0.00028305813044122096, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1425/2180 [3:46:19<1:59:34,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1426/2180 [3:46:28<1:59:20,  9.50s/it]                                                       {'loss': 2.1049, 'learning_rate': 0.0002823889101713122, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1426/2180 [3:46:28<1:59:20,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1427/2180 [3:46:38<1:59:21,  9.51s/it]                                                       {'loss': 1.9999, 'learning_rate': 0.0002817201704869701, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1427/2180 [3:46:38<1:59:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1428/2180 [3:46:48<1:59:10,  9.51s/it]                                                       {'loss': 2.0505, 'learning_rate': 0.00028105191286508, 'epoch': 0.65}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1428/2180 [3:46:48<1:59:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1429/2180 [3:46:57<1:59:00,  9.51s/it]                                                       {'loss': 2.0491, 'learning_rate': 0.00028038413878146245, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1429/2180 [3:46:57<1:59:00,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1430/2180 [3:47:07<1:58:59,  9.52s/it]                                                       {'loss': 1.9621, 'learning_rate': 0.00027971684971087073, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1430/2180 [3:47:07<1:58:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1431/2180 [3:47:16<1:58:50,  9.52s/it]                                                       {'loss': 2.059, 'learning_rate': 0.00027905004712698643, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1431/2180 [3:47:16<1:58:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1432/2180 [3:47:26<1:58:50,  9.53s/it]                                                       {'loss': 1.9891, 'learning_rate': 0.0002783837325024167, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1432/2180 [3:47:26<1:58:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1433/2180 [3:47:35<1:58:42,  9.53s/it]                                                       {'loss': 2.0156, 'learning_rate': 0.00027771790730869153, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1433/2180 [3:47:35<1:58:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1434/2180 [3:47:45<1:58:59,  9.57s/it]                                                       {'loss': 2.0985, 'learning_rate': 0.0002770525730162599, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1434/2180 [3:47:45<1:58:59,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1435/2180 [3:47:54<1:58:34,  9.55s/it]                                                       {'loss': 2.0168, 'learning_rate': 0.00027638773109448645, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1435/2180 [3:47:54<1:58:34,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1436/2180 [3:48:04<1:58:31,  9.56s/it]                                                       {'loss': 2.0328, 'learning_rate': 0.00027572338301164824, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1436/2180 [3:48:04<1:58:31,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1437/2180 [3:48:13<1:58:10,  9.54s/it]                                                       {'loss': 2.1622, 'learning_rate': 0.0002750595302349324, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1437/2180 [3:48:13<1:58:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1438/2180 [3:48:23<1:57:45,  9.52s/it]                                                       {'loss': 2.0053, 'learning_rate': 0.00027439617423043145, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1438/2180 [3:48:23<1:57:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1439/2180 [3:48:32<1:57:34,  9.52s/it]                                                       {'loss': 2.0437, 'learning_rate': 0.00027373331646314114, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1439/2180 [3:48:32<1:57:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1440/2180 [3:48:42<1:57:24,  9.52s/it]                                                       {'loss': 2.0325, 'learning_rate': 0.0002730709583969572, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1440/2180 [3:48:42<1:57:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1441/2180 [3:48:52<1:57:26,  9.54s/it]                                                       {'loss': 2.0951, 'learning_rate': 0.0002724091014946711, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1441/2180 [3:48:52<1:57:26,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1442/2180 [3:49:01<1:57:20,  9.54s/it]                                                       {'loss': 1.9878, 'learning_rate': 0.00027174774721796824, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1442/2180 [3:49:01<1:57:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1443/2180 [3:49:11<1:57:12,  9.54s/it]                                                       {'loss': 2.0497, 'learning_rate': 0.0002710868970274232, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1443/2180 [3:49:11<1:57:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1444/2180 [3:49:20<1:57:00,  9.54s/it]                                                       {'loss': 2.0359, 'learning_rate': 0.0002704265523824982, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1444/2180 [3:49:20<1:57:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1445/2180 [3:49:30<1:56:38,  9.52s/it]                                                       {'loss': 2.0259, 'learning_rate': 0.00026976671474153826, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1445/2180 [3:49:30<1:56:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1446/2180 [3:49:39<1:56:35,  9.53s/it]                                                       {'loss': 2.0918, 'learning_rate': 0.00026910738556176886, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1446/2180 [3:49:39<1:56:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1447/2180 [3:49:49<1:56:20,  9.52s/it]                                                       {'loss': 2.0476, 'learning_rate': 0.0002684485662992929, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1447/2180 [3:49:49<1:56:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1448/2180 [3:49:58<1:56:04,  9.51s/it]                                                       {'loss': 2.0793, 'learning_rate': 0.0002677902584090869, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1448/2180 [3:49:58<1:56:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1449/2180 [3:50:08<1:55:46,  9.50s/it]                                                       {'loss': 2.0622, 'learning_rate': 0.00026713246334499774, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1449/2180 [3:50:08<1:55:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1450/2180 [3:50:17<1:56:01,  9.54s/it]                                                       {'loss': 2.1086, 'learning_rate': 0.00026647518255974023, 'epoch': 0.66}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1450/2180 [3:50:17<1:56:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])


Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1451/2180 [3:50:27<1:55:50,  9.53s/it]                                                       {'loss': 2.109, 'learning_rate': 0.0002658184175048934, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1451/2180 [3:50:27<1:55:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1452/2180 [3:50:36<1:55:27,  9.52s/it]                                                       {'loss': 2.0986, 'learning_rate': 0.00026516216963089694, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1452/2180 [3:50:36<1:55:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1453/2180 [3:50:46<1:55:12,  9.51s/it]                                                       {'loss': 2.1133, 'learning_rate': 0.0002645064403870488, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1453/2180 [3:50:46<1:55:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1454/2180 [3:50:55<1:54:59,  9.50s/it]                                                       {'loss': 2.0622, 'learning_rate': 0.0002638512312215011, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1454/2180 [3:50:55<1:54:59,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1455/2180 [3:51:05<1:54:49,  9.50s/it]                                                       {'loss': 1.9741, 'learning_rate': 0.0002631965435812575, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1455/2180 [3:51:05<1:54:49,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1456/2180 [3:51:14<1:54:39,  9.50s/it]                                                       {'loss': 2.0589, 'learning_rate': 0.00026254237891217046, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1456/2180 [3:51:14<1:54:39,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1457/2180 [3:51:24<1:54:45,  9.52s/it]                                                       {'loss': 2.0062, 'learning_rate': 0.0002618887386589367, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1457/2180 [3:51:24<1:54:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1458/2180 [3:51:33<1:54:39,  9.53s/it]                                                       {'loss': 2.0311, 'learning_rate': 0.0002612356242650949, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1458/2180 [3:51:33<1:54:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1459/2180 [3:51:43<1:54:26,  9.52s/it]                                                       {'loss': 2.0593, 'learning_rate': 0.0002605830371730229, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1459/2180 [3:51:43<1:54:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1460/2180 [3:51:52<1:54:16,  9.52s/it]                                                       {'loss': 2.1084, 'learning_rate': 0.0002599309788239339, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1460/2180 [3:51:52<1:54:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1461/2180 [3:52:02<1:54:10,  9.53s/it]                                                       {'loss': 2.0568, 'learning_rate': 0.00025927945065787306, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1461/2180 [3:52:02<1:54:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1462/2180 [3:52:11<1:54:07,  9.54s/it]                                                       {'loss': 1.9874, 'learning_rate': 0.0002586284541137145, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1462/2180 [3:52:11<1:54:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1463/2180 [3:52:21<1:53:52,  9.53s/it]                                                       {'loss': 2.0299, 'learning_rate': 0.00025797799062915905, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1463/2180 [3:52:21<1:53:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1464/2180 [3:52:30<1:53:34,  9.52s/it]                                                       {'loss': 2.0398, 'learning_rate': 0.00025732806164072966, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1464/2180 [3:52:31<1:53:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1465/2180 [3:52:40<1:53:31,  9.53s/it]                                                       {'loss': 2.0225, 'learning_rate': 0.00025667866858376874, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1465/2180 [3:52:40<1:53:31,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1466/2180 [3:52:50<1:53:39,  9.55s/it]                                                       {'loss': 2.0409, 'learning_rate': 0.0002560298128924358, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1466/2180 [3:52:50<1:53:39,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1467/2180 [3:52:59<1:53:16,  9.53s/it]                                                       {'loss': 2.0358, 'learning_rate': 0.0002553814959997032, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1467/2180 [3:52:59<1:53:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1468/2180 [3:53:09<1:53:03,  9.53s/it]                                                       {'loss': 2.0819, 'learning_rate': 0.00025473371933735334, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1468/2180 [3:53:09<1:53:03,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1469/2180 [3:53:18<1:52:55,  9.53s/it]                                                       {'loss': 2.0685, 'learning_rate': 0.00025408648433597534, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1469/2180 [3:53:18<1:52:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1470/2180 [3:53:28<1:52:45,  9.53s/it]                                                       {'loss': 2.004, 'learning_rate': 0.00025343979242496283, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1470/2180 [3:53:28<1:52:45,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1471/2180 [3:53:37<1:52:40,  9.54s/it]                                                       {'loss': 2.0868, 'learning_rate': 0.00025279364503250925, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1471/2180 [3:53:37<1:52:40,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1472/2180 [3:53:47<1:52:31,  9.54s/it]                                                       {'loss': 2.0396, 'learning_rate': 0.0002521480435856056, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1472/2180 [3:53:47<1:52:31,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1473/2180 [3:53:56<1:52:16,  9.53s/it]                                                       {'loss': 2.1217, 'learning_rate': 0.0002515029895100378, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1473/2180 [3:53:56<1:52:16,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1474/2180 [3:54:06<1:52:00,  9.52s/it]                                                       {'loss': 2.0472, 'learning_rate': 0.0002508584842303822, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1474/2180 [3:54:06<1:52:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1475/2180 [3:54:15<1:51:46,  9.51s/it]                                                       {'loss': 2.1178, 'learning_rate': 0.0002502145291700038, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1475/2180 [3:54:15<1:51:46,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1476/2180 [3:54:25<1:51:36,  9.51s/it]                                                       {'loss': 1.9981, 'learning_rate': 0.0002495711257510517, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1476/2180 [3:54:25<1:51:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1477/2180 [3:54:34<1:51:20,  9.50s/it]                                                       {'loss': 2.145, 'learning_rate': 0.0002489282753944575, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1477/2180 [3:54:34<1:51:20,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1478/2180 [3:54:44<1:51:09,  9.50s/it]                                                       {'loss': 2.0599, 'learning_rate': 0.00024828597951993093, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1478/2180 [3:54:44<1:51:09,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1479/2180 [3:54:53<1:51:00,  9.50s/it]                                                       {'loss': 2.087, 'learning_rate': 0.00024764423954595706, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1479/2180 [3:54:53<1:51:00,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1480/2180 [3:55:03<1:50:56,  9.51s/it]                                                       {'loss': 1.9715, 'learning_rate': 0.0002470030568897938, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1480/2180 [3:55:03<1:50:56,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1481/2180 [3:55:12<1:50:41,  9.50s/it]                                                       {'loss': 2.0195, 'learning_rate': 0.00024636243296746773, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1481/2180 [3:55:12<1:50:41,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1482/2180 [3:55:22<1:50:33,  9.50s/it]                                                       {'loss': 1.9678, 'learning_rate': 0.0002457223691937716, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1482/2180 [3:55:22<1:50:33,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1483/2180 [3:55:31<1:50:26,  9.51s/it]                                                       {'loss': 2.0653, 'learning_rate': 0.0002450828669822613, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1483/2180 [3:55:31<1:50:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1484/2180 [3:55:41<1:50:20,  9.51s/it]                                                       {'loss': 2.0425, 'learning_rate': 0.00024444392774525253, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1484/2180 [3:55:41<1:50:20,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1485/2180 [3:55:50<1:50:35,  9.55s/it]                                                       {'loss': 1.9468, 'learning_rate': 0.00024380555289381733, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1485/2180 [3:55:50<1:50:35,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1486/2180 [3:56:00<1:50:14,  9.53s/it]                                                       {'loss': 2.1626, 'learning_rate': 0.00024316774383778184, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1486/2180 [3:56:00<1:50:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1487/2180 [3:56:09<1:49:56,  9.52s/it]                                                       {'loss': 2.0272, 'learning_rate': 0.0002425305019857222, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1487/2180 [3:56:09<1:49:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1488/2180 [3:56:19<1:49:48,  9.52s/it]                                                       {'loss': 2.1104, 'learning_rate': 0.00024189382874496184, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1488/2180 [3:56:19<1:49:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1489/2180 [3:56:29<1:49:38,  9.52s/it]                                                       {'loss': 2.0709, 'learning_rate': 0.00024125772552156916, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1489/2180 [3:56:29<1:49:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1490/2180 [3:56:38<1:49:27,  9.52s/it]                                                       {'loss': 2.056, 'learning_rate': 0.00024062219372035292, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1490/2180 [3:56:38<1:49:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1491/2180 [3:56:48<1:49:12,  9.51s/it]                                                       {'loss': 2.051, 'learning_rate': 0.00023998723474486007, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1491/2180 [3:56:48<1:49:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1492/2180 [3:56:57<1:49:03,  9.51s/it]                                                       {'loss': 2.0475, 'learning_rate': 0.00023935284999737272, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1492/2180 [3:56:57<1:49:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1493/2180 [3:57:07<1:48:47,  9.50s/it]                                                       {'loss': 2.1152, 'learning_rate': 0.00023871904087890505, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1493/2180 [3:57:07<1:48:47,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1494/2180 [3:57:16<1:48:33,  9.49s/it]                                                       {'loss': 2.0331, 'learning_rate': 0.00023808580878919945, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1494/2180 [3:57:16<1:48:33,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1495/2180 [3:57:26<1:48:28,  9.50s/it]                                                       {'loss': 2.0822, 'learning_rate': 0.00023745315512672398, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1495/2180 [3:57:26<1:48:28,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1496/2180 [3:57:35<1:48:12,  9.49s/it]                                                       {'loss': 2.0889, 'learning_rate': 0.0002368210812886698, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1496/2180 [3:57:35<1:48:12,  9.49s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1497/2180 [3:57:45<1:48:08,  9.50s/it]                                                       {'loss': 1.9605, 'learning_rate': 0.0002361895886709471, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1497/2180 [3:57:45<1:48:08,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1498/2180 [3:57:54<1:48:03,  9.51s/it]                                                       {'loss': 2.0293, 'learning_rate': 0.0002355586786681823, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1498/2180 [3:57:54<1:48:03,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1499/2180 [3:58:04<1:48:01,  9.52s/it]                                                       {'loss': 2.0042, 'learning_rate': 0.00023492835267371575, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1499/2180 [3:58:04<1:48:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1500/2180 [3:58:13<1:48:04,  9.54s/it]                                                       {'loss': 2.0706, 'learning_rate': 0.0002342986120795978, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1500/2180 [3:58:13<1:48:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1501/2180 [3:58:23<1:47:51,  9.53s/it]                                                       {'loss': 2.017, 'learning_rate': 0.0002336694582765857, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1501/2180 [3:58:23<1:47:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1502/2180 [3:58:32<1:47:35,  9.52s/it]                                                       {'loss': 2.0723, 'learning_rate': 0.00023304089265414085, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1502/2180 [3:58:32<1:47:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1503/2180 [3:58:42<1:47:28,  9.53s/it]                                                       {'loss': 2.071, 'learning_rate': 0.00023241291660042613, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1503/2180 [3:58:42<1:47:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1504/2180 [3:58:51<1:47:17,  9.52s/it]                                                       {'loss': 2.0294, 'learning_rate': 0.00023178553150230186, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1504/2180 [3:58:51<1:47:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1505/2180 [3:59:01<1:47:08,  9.52s/it]                                                       {'loss': 2.1441, 'learning_rate': 0.00023115873874532324, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1505/2180 [3:59:01<1:47:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1506/2180 [3:59:10<1:46:55,  9.52s/it]                                                       {'loss': 2.0489, 'learning_rate': 0.00023053253971373796, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1506/2180 [3:59:10<1:46:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1507/2180 [3:59:20<1:46:59,  9.54s/it]                                                       {'loss': 2.0014, 'learning_rate': 0.00022990693579048166, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1507/2180 [3:59:20<1:46:59,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1508/2180 [3:59:29<1:47:00,  9.55s/it]                                                       {'loss': 1.9976, 'learning_rate': 0.00022928192835717644, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1508/2180 [3:59:29<1:47:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1509/2180 [3:59:39<1:46:50,  9.55s/it]                                                       {'loss': 2.1324, 'learning_rate': 0.00022865751879412634, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1509/2180 [3:59:39<1:46:50,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1510/2180 [3:59:48<1:46:32,  9.54s/it]                                                       {'loss': 2.0407, 'learning_rate': 0.00022803370848031585, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1510/2180 [3:59:48<1:46:32,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1511/2180 [3:59:58<1:46:29,  9.55s/it]                                                       {'loss': 2.0448, 'learning_rate': 0.00022741049879340542, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1511/2180 [3:59:58<1:46:29,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1512/2180 [4:00:08<1:46:10,  9.54s/it]                                                       {'loss': 2.0577, 'learning_rate': 0.00022678789110972897, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1512/2180 [4:00:08<1:46:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1513/2180 [4:00:17<1:45:51,  9.52s/it]                                                       {'loss': 2.08, 'learning_rate': 0.00022616588680429155, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1513/2180 [4:00:17<1:45:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1514/2180 [4:00:27<1:45:48,  9.53s/it]                                                       {'loss': 1.97, 'learning_rate': 0.00022554448725076526, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1514/2180 [4:00:27<1:45:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1515/2180 [4:00:36<1:45:36,  9.53s/it]                                                       {'loss': 1.9995, 'learning_rate': 0.0002249236938214863, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1515/2180 [4:00:36<1:45:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1516/2180 [4:00:46<1:45:16,  9.51s/it]                                                       {'loss': 2.0968, 'learning_rate': 0.00022430350788745296, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1516/2180 [4:00:46<1:45:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1517/2180 [4:00:55<1:45:05,  9.51s/it]                                                       {'loss': 2.0445, 'learning_rate': 0.00022368393081832166, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1517/2180 [4:00:55<1:45:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1518/2180 [4:01:05<1:44:58,  9.51s/it]                                                       {'loss': 2.0641, 'learning_rate': 0.00022306496398240383, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1518/2180 [4:01:05<1:44:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1519/2180 [4:01:14<1:44:55,  9.52s/it]                                                       {'loss': 2.0228, 'learning_rate': 0.00022244660874666373, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1519/2180 [4:01:14<1:44:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1520/2180 [4:01:24<1:44:42,  9.52s/it]                                                       {'loss': 2.1085, 'learning_rate': 0.00022182886647671452, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1520/2180 [4:01:24<1:44:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1521/2180 [4:01:33<1:44:26,  9.51s/it]                                                       {'loss': 2.0894, 'learning_rate': 0.0002212117385368157, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1521/2180 [4:01:33<1:44:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1522/2180 [4:01:43<1:44:46,  9.55s/it]                                                       {'loss': 2.0127, 'learning_rate': 0.00022059522628987038, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1522/2180 [4:01:43<1:44:46,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1523/2180 [4:01:52<1:44:31,  9.55s/it]                                                       {'loss': 2.0477, 'learning_rate': 0.00021997933109742162, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1523/2180 [4:01:52<1:44:31,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1524/2180 [4:02:02<1:44:14,  9.53s/it]                                                       {'loss': 2.0829, 'learning_rate': 0.00021936405431964969, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1524/2180 [4:02:02<1:44:14,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1525/2180 [4:02:11<1:43:58,  9.52s/it]                                                       {'loss': 2.0277, 'learning_rate': 0.00021874939731536926, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1525/2180 [4:02:11<1:43:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1526/2180 [4:02:21<1:43:51,  9.53s/it]                                                       {'loss': 2.0391, 'learning_rate': 0.00021813536144202656, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1526/2180 [4:02:21<1:43:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1527/2180 [4:02:30<1:43:42,  9.53s/it]                                                       {'loss': 1.9551, 'learning_rate': 0.00021752194805569553, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1527/2180 [4:02:30<1:43:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1528/2180 [4:02:40<1:43:33,  9.53s/it]                                                       {'loss': 1.9749, 'learning_rate': 0.0002169091585110754, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1528/2180 [4:02:40<1:43:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1529/2180 [4:02:49<1:43:23,  9.53s/it]                                                       {'loss': 2.0906, 'learning_rate': 0.00021629699416148828, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1529/2180 [4:02:50<1:43:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1530/2180 [4:02:59<1:43:21,  9.54s/it]                                                       {'loss': 2.0451, 'learning_rate': 0.000215685456358875, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1530/2180 [4:02:59<1:43:21,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1531/2180 [4:03:09<1:43:15,  9.55s/it]                                                       {'loss': 2.034, 'learning_rate': 0.00021507454645379258, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1531/2180 [4:03:09<1:43:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1532/2180 [4:03:18<1:42:56,  9.53s/it]                                                       {'loss': 2.1215, 'learning_rate': 0.00021446426579541184, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1532/2180 [4:03:18<1:42:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1533/2180 [4:03:28<1:42:42,  9.52s/it]                                                       {'loss': 2.1049, 'learning_rate': 0.00021385461573151387, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1533/2180 [4:03:28<1:42:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1534/2180 [4:03:37<1:42:39,  9.53s/it]                                                       {'loss': 2.1549, 'learning_rate': 0.00021324559760848677, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1534/2180 [4:03:37<1:42:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1535/2180 [4:03:47<1:42:20,  9.52s/it]                                                       {'loss': 2.1101, 'learning_rate': 0.00021263721277132303, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1535/2180 [4:03:47<1:42:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1536/2180 [4:03:56<1:42:10,  9.52s/it]                                                       {'loss': 2.0702, 'learning_rate': 0.0002120294625636171, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1536/2180 [4:03:56<1:42:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1537/2180 [4:04:06<1:42:20,  9.55s/it]                                                       {'loss': 1.9394, 'learning_rate': 0.0002114223483275613, 'epoch': 0.7}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1537/2180 [4:04:06<1:42:20,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1538/2180 [4:04:15<1:42:04,  9.54s/it]                                                       {'loss': 2.0182, 'learning_rate': 0.0002108158714039435, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1538/2180 [4:04:15<1:42:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1539/2180 [4:04:25<1:42:02,  9.55s/it]                                                       {'loss': 2.0457, 'learning_rate': 0.00021021003313214455, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1539/2180 [4:04:25<1:42:02,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1540/2180 [4:04:34<1:41:53,  9.55s/it]                                                       {'loss': 2.0389, 'learning_rate': 0.00020960483485013432, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1540/2180 [4:04:34<1:41:53,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1541/2180 [4:04:44<1:41:53,  9.57s/it]                                                       {'loss': 2.0838, 'learning_rate': 0.0002090002778944694, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1541/2180 [4:04:44<1:41:53,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1542/2180 [4:04:54<1:41:37,  9.56s/it]                                                       {'loss': 2.0268, 'learning_rate': 0.00020839636360029025, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1542/2180 [4:04:54<1:41:37,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1543/2180 [4:05:03<1:41:15,  9.54s/it]                                                       {'loss': 2.0661, 'learning_rate': 0.00020779309330131818, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1543/2180 [4:05:03<1:41:15,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1544/2180 [4:05:13<1:41:00,  9.53s/it]                                                       {'loss': 2.0816, 'learning_rate': 0.00020719046832985184, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1544/2180 [4:05:13<1:41:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1545/2180 [4:05:22<1:40:50,  9.53s/it]                                                       {'loss': 2.1039, 'learning_rate': 0.0002065884900167646, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1545/2180 [4:05:22<1:40:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1546/2180 [4:05:32<1:40:37,  9.52s/it]                                                       {'loss': 1.999, 'learning_rate': 0.0002059871596915024, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1546/2180 [4:05:32<1:40:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1547/2180 [4:05:41<1:40:33,  9.53s/it]                                                       {'loss': 2.0024, 'learning_rate': 0.0002053864786820795, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1547/2180 [4:05:41<1:40:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1548/2180 [4:05:51<1:40:29,  9.54s/it]                                                       {'loss': 2.0733, 'learning_rate': 0.00020478644831507627, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1548/2180 [4:05:51<1:40:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1549/2180 [4:06:00<1:40:32,  9.56s/it]                                                       {'loss': 2.0524, 'learning_rate': 0.00020418706991563634, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1549/2180 [4:06:00<1:40:32,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1550/2180 [4:06:10<1:40:19,  9.56s/it]                                                       {'loss': 2.057, 'learning_rate': 0.00020358834480746363, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1550/2180 [4:06:10<1:40:19,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1551/2180 [4:06:19<1:40:03,  9.55s/it]                                                       {'loss': 2.0686, 'learning_rate': 0.0002029902743128188, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1551/2180 [4:06:19<1:40:03,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1552/2180 [4:06:29<1:39:51,  9.54s/it]                                                       {'loss': 2.0634, 'learning_rate': 0.0002023928597525174, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1552/2180 [4:06:29<1:39:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1553/2180 [4:06:38<1:39:36,  9.53s/it]                                                       {'loss': 2.0871, 'learning_rate': 0.00020179610244592595, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1553/2180 [4:06:38<1:39:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1554/2180 [4:06:48<1:39:24,  9.53s/it]                                                       {'loss': 2.0974, 'learning_rate': 0.00020120000371095937, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1554/2180 [4:06:48<1:39:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1555/2180 [4:06:58<1:39:21,  9.54s/it]                                                       {'loss': 2.0354, 'learning_rate': 0.0002006045648640787, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1555/2180 [4:06:58<1:39:21,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1556/2180 [4:07:07<1:39:17,  9.55s/it]                                                       {'loss': 2.037, 'learning_rate': 0.00020000978722028713, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1556/2180 [4:07:07<1:39:17,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1557/2180 [4:07:17<1:39:10,  9.55s/it]                                                       {'loss': 2.0698, 'learning_rate': 0.00019941567209312767, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1557/2180 [4:07:17<1:39:10,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1558/2180 [4:07:26<1:38:57,  9.55s/it]                                                       {'loss': 2.0858, 'learning_rate': 0.00019882222079468036, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1558/2180 [4:07:26<1:38:57,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1559/2180 [4:07:36<1:38:43,  9.54s/it]                                                       {'loss': 2.0023, 'learning_rate': 0.0001982294346355595, 'epoch': 0.71}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1559/2180 [4:07:36<1:38:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1560/2180 [4:07:45<1:38:33,  9.54s/it]                                                       {'loss': 2.1374, 'learning_rate': 0.00019763731492490976, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1560/2180 [4:07:45<1:38:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1561/2180 [4:07:55<1:38:42,  9.57s/it]                                                       {'loss': 2.0155, 'learning_rate': 0.00019704586297040422, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1561/2180 [4:07:55<1:38:42,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1562/2180 [4:08:04<1:38:29,  9.56s/it]                                                       {'loss': 2.0416, 'learning_rate': 0.0001964550800782417, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1562/2180 [4:08:04<1:38:29,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1563/2180 [4:08:14<1:38:16,  9.56s/it]                                                       {'loss': 2.0735, 'learning_rate': 0.00019586496755314288, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1563/2180 [4:08:14<1:38:16,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1564/2180 [4:08:24<1:38:00,  9.55s/it]                                                       {'loss': 2.0608, 'learning_rate': 0.00019527552669834798, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1564/2180 [4:08:24<1:38:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1565/2180 [4:08:33<1:37:42,  9.53s/it]                                                       {'loss': 2.086, 'learning_rate': 0.0001946867588156142, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1565/2180 [4:08:33<1:37:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1566/2180 [4:08:43<1:37:30,  9.53s/it]                                                       {'loss': 2.0571, 'learning_rate': 0.00019409866520521258, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1566/2180 [4:08:43<1:37:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1567/2180 [4:08:52<1:37:39,  9.56s/it]                                                       {'loss': 1.9465, 'learning_rate': 0.00019351124716592455, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1567/2180 [4:08:52<1:37:39,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1568/2180 [4:09:02<1:37:20,  9.54s/it]                                                       {'loss': 2.0731, 'learning_rate': 0.0001929245059950397, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1568/2180 [4:09:02<1:37:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1569/2180 [4:09:11<1:37:10,  9.54s/it]                                                       {'loss': 2.143, 'learning_rate': 0.0001923384429883533, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1569/2180 [4:09:11<1:37:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1570/2180 [4:09:21<1:37:01,  9.54s/it]                                                       {'loss': 2.0333, 'learning_rate': 0.00019175305944016237, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1570/2180 [4:09:21<1:37:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1571/2180 [4:09:30<1:36:46,  9.53s/it]                                                       {'loss': 2.0805, 'learning_rate': 0.00019116835664326326, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1571/2180 [4:09:30<1:36:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1572/2180 [4:09:40<1:36:35,  9.53s/it]                                                       {'loss': 2.0607, 'learning_rate': 0.0001905843358889497, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1572/2180 [4:09:40<1:36:35,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1573/2180 [4:09:49<1:36:23,  9.53s/it]                                                       {'loss': 2.0296, 'learning_rate': 0.00019000099846700836, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1573/2180 [4:09:49<1:36:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1574/2180 [4:09:59<1:36:23,  9.54s/it]                                                       {'loss': 2.0667, 'learning_rate': 0.00018941834566571692, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1574/2180 [4:09:59<1:36:23,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1575/2180 [4:10:08<1:36:04,  9.53s/it]                                                       {'loss': 2.0785, 'learning_rate': 0.00018883637877184145, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1575/2180 [4:10:08<1:36:04,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1576/2180 [4:10:18<1:35:47,  9.52s/it]                                                       {'loss': 2.1349, 'learning_rate': 0.00018825509907063325, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1576/2180 [4:10:18<1:35:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1577/2180 [4:10:27<1:35:44,  9.53s/it]                                                       {'loss': 2.1144, 'learning_rate': 0.00018767450784582557, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1577/2180 [4:10:27<1:35:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1578/2180 [4:10:37<1:35:41,  9.54s/it]                                                       {'loss': 2.0891, 'learning_rate': 0.00018709460637963122, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1578/2180 [4:10:37<1:35:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1579/2180 [4:10:46<1:35:23,  9.52s/it]                                                       {'loss': 2.0457, 'learning_rate': 0.00018651539595274013, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1579/2180 [4:10:46<1:35:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1580/2180 [4:10:56<1:35:13,  9.52s/it]                                                       {'loss': 2.0546, 'learning_rate': 0.00018593687784431578, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1580/2180 [4:10:56<1:35:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1581/2180 [4:11:06<1:35:05,  9.52s/it]                                                       {'loss': 1.9832, 'learning_rate': 0.00018535905333199248, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1581/2180 [4:11:06<1:35:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1582/2180 [4:11:15<1:34:57,  9.53s/it]                                                       {'loss': 2.0356, 'learning_rate': 0.0001847819236918733, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1582/2180 [4:11:15<1:34:57,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1583/2180 [4:11:25<1:34:59,  9.55s/it]                                                       {'loss': 2.0216, 'learning_rate': 0.00018420549019852655, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1583/2180 [4:11:25<1:34:59,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1584/2180 [4:11:34<1:34:43,  9.54s/it]                                                       {'loss': 2.0487, 'learning_rate': 0.00018362975412498266, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1584/2180 [4:11:34<1:34:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1585/2180 [4:11:44<1:34:36,  9.54s/it]                                                       {'loss': 2.0204, 'learning_rate': 0.00018305471674273261, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1585/2180 [4:11:44<1:34:36,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1586/2180 [4:11:53<1:34:23,  9.53s/it]                                                       {'loss': 2.1439, 'learning_rate': 0.0001824803793217237, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1586/2180 [4:11:53<1:34:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1587/2180 [4:12:03<1:34:13,  9.53s/it]                                                       {'loss': 2.0601, 'learning_rate': 0.00018190674313035737, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1587/2180 [4:12:03<1:34:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1588/2180 [4:12:12<1:34:11,  9.55s/it]                                                       {'loss': 2.0487, 'learning_rate': 0.00018133380943548716, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1588/2180 [4:12:12<1:34:11,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1589/2180 [4:12:22<1:33:58,  9.54s/it]                                                       {'loss': 2.0986, 'learning_rate': 0.00018076157950241452, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1589/2180 [4:12:22<1:33:58,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1590/2180 [4:12:31<1:33:39,  9.52s/it]                                                       {'loss': 2.0294, 'learning_rate': 0.00018019005459488652, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1590/2180 [4:12:31<1:33:39,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1591/2180 [4:12:41<1:33:39,  9.54s/it]                                                       {'loss': 1.9621, 'learning_rate': 0.00017961923597509388, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1591/2180 [4:12:41<1:33:39,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1592/2180 [4:12:50<1:33:27,  9.54s/it]                                                       {'loss': 2.0798, 'learning_rate': 0.00017904912490366722, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1592/2180 [4:12:50<1:33:27,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1593/2180 [4:13:00<1:33:22,  9.55s/it]                                                       {'loss': 2.0825, 'learning_rate': 0.00017847972263967433, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1593/2180 [4:13:00<1:33:22,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1594/2180 [4:13:10<1:33:16,  9.55s/it]                                                       {'loss': 2.0555, 'learning_rate': 0.0001779110304406177, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1594/2180 [4:13:10<1:33:16,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1595/2180 [4:13:19<1:33:01,  9.54s/it]                                                       {'loss': 1.9967, 'learning_rate': 0.000177343049562432, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1595/2180 [4:13:19<1:33:01,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1596/2180 [4:13:29<1:32:47,  9.53s/it]                                                       {'loss': 2.0294, 'learning_rate': 0.0001767757812594807, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1596/2180 [4:13:29<1:32:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1597/2180 [4:13:38<1:32:43,  9.54s/it]                                                       {'loss': 2.0856, 'learning_rate': 0.0001762092267845534, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1597/2180 [4:13:38<1:32:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1598/2180 [4:13:48<1:32:33,  9.54s/it]                                                       {'loss': 2.1065, 'learning_rate': 0.00017564338738886365, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1598/2180 [4:13:48<1:32:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1599/2180 [4:13:57<1:32:23,  9.54s/it]                                                       {'loss': 2.1188, 'learning_rate': 0.0001750782643220457, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1599/2180 [4:13:57<1:32:23,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1600/2180 [4:14:07<1:32:10,  9.54s/it]                                                       {'loss': 2.1475, 'learning_rate': 0.00017451385883215166, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1600/2180 [4:14:07<1:32:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1601/2180 [4:14:16<1:31:57,  9.53s/it]                                                       {'loss': 2.0306, 'learning_rate': 0.00017395017216564863, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1601/2180 [4:14:16<1:31:57,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1602/2180 [4:14:26<1:31:49,  9.53s/it]                                                       {'loss': 2.0289, 'learning_rate': 0.00017338720556741687, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1602/2180 [4:14:26<1:31:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1603/2180 [4:14:35<1:31:37,  9.53s/it]                                                       {'loss': 1.9912, 'learning_rate': 0.00017282496028074606, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1603/2180 [4:14:35<1:31:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1604/2180 [4:14:45<1:31:28,  9.53s/it]                                                       {'loss': 2.017, 'learning_rate': 0.00017226343754733254, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1604/2180 [4:14:45<1:31:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1605/2180 [4:14:54<1:31:19,  9.53s/it]                                                       {'loss': 1.9727, 'learning_rate': 0.00017170263860727769, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1605/2180 [4:14:54<1:31:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1606/2180 [4:15:04<1:31:08,  9.53s/it]                                                       {'loss': 2.0523, 'learning_rate': 0.0001711425646990838, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1606/2180 [4:15:04<1:31:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1607/2180 [4:15:13<1:30:56,  9.52s/it]                                                       {'loss': 1.948, 'learning_rate': 0.00017058321705965202, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1607/2180 [4:15:13<1:30:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2180 [4:15:23<1:30:49,  9.53s/it]                                                       {'loss': 2.1238, 'learning_rate': 0.0001700245969242798, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2180 [4:15:23<1:30:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2180 [4:15:33<1:30:43,  9.53s/it]                                                       {'loss': 2.0416, 'learning_rate': 0.00016946670552665804, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2180 [4:15:33<1:30:43,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2180 [4:15:42<1:30:31,  9.53s/it]                                                       {'loss': 2.0218, 'learning_rate': 0.00016890954409886795, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2180 [4:15:42<1:30:31,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2180 [4:15:52<1:30:28,  9.54s/it]                                                       {'loss': 2.0685, 'learning_rate': 0.00016835311387137836, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2180 [4:15:52<1:30:28,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2180 [4:16:01<1:30:29,  9.56s/it]                                                       {'loss': 2.1019, 'learning_rate': 0.0001677974160730441, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2180 [4:16:01<1:30:29,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1613/2180 [4:16:11<1:30:11,  9.54s/it]                                                       {'loss': 1.9268, 'learning_rate': 0.00016724245193110176, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1613/2180 [4:16:11<1:30:11,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1614/2180 [4:16:20<1:30:09,  9.56s/it]                                                       {'loss': 2.0057, 'learning_rate': 0.00016668822267116784, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1614/2180 [4:16:20<1:30:09,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1615/2180 [4:16:30<1:29:47,  9.54s/it]                                                       {'loss': 2.0844, 'learning_rate': 0.00016613472951723597, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1615/2180 [4:16:30<1:29:47,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1616/2180 [4:16:39<1:29:31,  9.52s/it]                                                       {'loss': 2.0938, 'learning_rate': 0.00016558197369167434, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1616/2180 [4:16:39<1:29:31,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1617/2180 [4:16:49<1:29:39,  9.56s/it]                                                       {'loss': 2.1472, 'learning_rate': 0.00016502995641522216, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1617/2180 [4:16:49<1:29:39,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1618/2180 [4:16:58<1:29:18,  9.53s/it]                                                       {'loss': 2.1472, 'learning_rate': 0.00016447867890698843, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1618/2180 [4:16:58<1:29:18,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1619/2180 [4:17:08<1:29:09,  9.54s/it]                                                       {'loss': 1.9679, 'learning_rate': 0.00016392814238444753, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1619/2180 [4:17:08<1:29:09,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2180 [4:17:17<1:28:51,  9.52s/it]                                                       {'loss': 2.0532, 'learning_rate': 0.00016337834806343782, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2180 [4:17:17<1:28:51,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1621/2180 [4:17:27<1:28:50,  9.54s/it]                                                       {'loss': 2.0411, 'learning_rate': 0.0001628292971581588, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1621/2180 [4:17:27<1:28:50,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1622/2180 [4:17:37<1:28:30,  9.52s/it]                                                       {'loss': 2.0993, 'learning_rate': 0.00016228099088116772, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1622/2180 [4:17:37<1:28:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1623/2180 [4:17:46<1:28:21,  9.52s/it]                                                       {'loss': 2.0898, 'learning_rate': 0.00016173343044337734, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1623/2180 [4:17:46<1:28:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1624/2180 [4:17:56<1:28:19,  9.53s/it]                                                       {'loss': 2.0011, 'learning_rate': 0.00016118661705405356, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1624/2180 [4:17:56<1:28:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1625/2180 [4:18:05<1:28:20,  9.55s/it]                                                       {'loss': 2.064, 'learning_rate': 0.00016064055192081255, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1625/2180 [4:18:05<1:28:20,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1626/2180 [4:18:15<1:28:04,  9.54s/it]                                                       {'loss': 2.0424, 'learning_rate': 0.00016009523624961757, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1626/2180 [4:18:15<1:28:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1627/2180 [4:18:24<1:27:47,  9.52s/it]                                                       {'loss': 2.0583, 'learning_rate': 0.00015955067124477678, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1627/2180 [4:18:24<1:27:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1628/2180 [4:18:34<1:27:28,  9.51s/it]                                                       {'loss': 2.0154, 'learning_rate': 0.000159006858108941, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1628/2180 [4:18:34<1:27:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1629/2180 [4:18:43<1:27:15,  9.50s/it]                                                       {'loss': 1.9937, 'learning_rate': 0.00015846379804310002, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1629/2180 [4:18:43<1:27:15,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1630/2180 [4:18:53<1:27:11,  9.51s/it]                                                       {'loss': 2.0028, 'learning_rate': 0.00015792149224658054, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1630/2180 [4:18:53<1:27:11,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1631/2180 [4:19:02<1:27:09,  9.53s/it]                                                       {'loss': 2.0157, 'learning_rate': 0.00015737994191704385, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1631/2180 [4:19:02<1:27:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1632/2180 [4:19:12<1:27:01,  9.53s/it]                                                       {'loss': 1.9834, 'learning_rate': 0.0001568391482504829, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1632/2180 [4:19:12<1:27:01,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1633/2180 [4:19:21<1:26:45,  9.52s/it]                                                       {'loss': 1.9893, 'learning_rate': 0.00015629911244121903, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1633/2180 [4:19:21<1:26:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1634/2180 [4:19:31<1:26:34,  9.51s/it]                                                       {'loss': 1.9714, 'learning_rate': 0.0001557598356819, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1634/2180 [4:19:31<1:26:34,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1635/2180 [4:19:40<1:26:24,  9.51s/it]                                                       {'loss': 2.0198, 'learning_rate': 0.00015522131916349786, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1635/2180 [4:19:40<1:26:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1636/2180 [4:19:50<1:26:10,  9.51s/it]                                                       {'loss': 2.0281, 'learning_rate': 0.00015468356407530493, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1636/2180 [4:19:50<1:26:10,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1637/2180 [4:19:59<1:26:07,  9.52s/it]                                                       {'loss': 1.9957, 'learning_rate': 0.00015414657160493217, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1637/2180 [4:19:59<1:26:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1638/2180 [4:20:09<1:25:58,  9.52s/it]                                                       {'loss': 2.0473, 'learning_rate': 0.00015361034293830673, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1638/2180 [4:20:09<1:25:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1639/2180 [4:20:18<1:25:45,  9.51s/it]                                                       {'loss': 2.003, 'learning_rate': 0.00015307487925966844, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1639/2180 [4:20:18<1:25:45,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1640/2180 [4:20:28<1:25:35,  9.51s/it]                                                       {'loss': 2.0752, 'learning_rate': 0.00015254018175156776, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1640/2180 [4:20:28<1:25:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1641/2180 [4:20:37<1:25:27,  9.51s/it]                                                       {'loss': 2.0188, 'learning_rate': 0.0001520062515948632, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1641/2180 [4:20:37<1:25:27,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1642/2180 [4:20:47<1:25:21,  9.52s/it]                                                       {'loss': 1.9584, 'learning_rate': 0.0001514730899687189, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1642/2180 [4:20:47<1:25:21,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1643/2180 [4:20:56<1:25:10,  9.52s/it]                                                       {'loss': 2.1157, 'learning_rate': 0.00015094069805060122, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1643/2180 [4:20:56<1:25:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1644/2180 [4:21:06<1:24:58,  9.51s/it]                                                       {'loss': 2.1105, 'learning_rate': 0.00015040907701627666, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1644/2180 [4:21:06<1:24:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1645/2180 [4:21:15<1:24:44,  9.50s/it]                                                       {'loss': 2.0903, 'learning_rate': 0.00014987822803980976, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1645/2180 [4:21:15<1:24:44,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1646/2180 [4:21:25<1:24:38,  9.51s/it]                                                       {'loss': 2.0217, 'learning_rate': 0.00014934815229355965, 'epoch': 0.75}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1646/2180 [4:21:25<1:24:38,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1647/2180 [4:21:34<1:24:31,  9.51s/it]                                                       {'loss': 1.9886, 'learning_rate': 0.00014881885094817748, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1647/2180 [4:21:34<1:24:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1648/2180 [4:21:44<1:24:23,  9.52s/it]                                                       {'loss': 2.0849, 'learning_rate': 0.00014829032517260488, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1648/2180 [4:21:44<1:24:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1649/2180 [4:21:54<1:24:23,  9.53s/it]                                                       {'loss': 2.0969, 'learning_rate': 0.0001477625761340704, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1649/2180 [4:21:54<1:24:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1650/2180 [4:22:03<1:24:09,  9.53s/it]                                                       {'loss': 2.045, 'learning_rate': 0.0001472356049980868, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1650/2180 [4:22:03<1:24:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1651/2180 [4:22:13<1:24:07,  9.54s/it]                                                       {'loss': 2.0194, 'learning_rate': 0.00014670941292844954, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1651/2180 [4:22:13<1:24:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1652/2180 [4:22:22<1:23:58,  9.54s/it]                                                       {'loss': 2.0085, 'learning_rate': 0.00014618400108723295, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1652/2180 [4:22:22<1:23:58,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1653/2180 [4:22:32<1:23:57,  9.56s/it]                                                       {'loss': 2.0434, 'learning_rate': 0.00014565937063478862, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1653/2180 [4:22:32<1:23:57,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1654/2180 [4:22:41<1:23:45,  9.55s/it]                                                       {'loss': 2.0208, 'learning_rate': 0.00014513552272974207, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1654/2180 [4:22:41<1:23:45,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1655/2180 [4:22:51<1:23:34,  9.55s/it]                                                       {'loss': 2.0154, 'learning_rate': 0.0001446124585289913, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1655/2180 [4:22:51<1:23:34,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1656/2180 [4:23:00<1:23:25,  9.55s/it]                                                       {'loss': 2.0056, 'learning_rate': 0.00014409017918770266, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1656/2180 [4:23:00<1:23:25,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1657/2180 [4:23:10<1:23:08,  9.54s/it]                                                       {'loss': 2.0904, 'learning_rate': 0.00014356868585930994, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1657/2180 [4:23:10<1:23:08,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1658/2180 [4:23:19<1:23:00,  9.54s/it]                                                       {'loss': 1.9943, 'learning_rate': 0.00014304797969551077, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1658/2180 [4:23:19<1:23:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1659/2180 [4:23:29<1:22:51,  9.54s/it]                                                       {'loss': 1.9999, 'learning_rate': 0.00014252806184626417, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1659/2180 [4:23:29<1:22:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1660/2180 [4:23:39<1:22:44,  9.55s/it]                                                       {'loss': 2.1095, 'learning_rate': 0.00014200893345978817, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1660/2180 [4:23:39<1:22:44,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1661/2180 [4:23:48<1:22:36,  9.55s/it]                                                       {'loss': 1.9803, 'learning_rate': 0.00014149059568255778, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1661/2180 [4:23:48<1:22:36,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1662/2180 [4:23:58<1:22:30,  9.56s/it]                                                       {'loss': 2.0464, 'learning_rate': 0.00014097304965930157, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1662/2180 [4:23:58<1:22:30,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1663/2180 [4:24:07<1:22:16,  9.55s/it]                                                       {'loss': 2.0025, 'learning_rate': 0.00014045629653299953, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1663/2180 [4:24:07<1:22:16,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1664/2180 [4:24:17<1:22:00,  9.54s/it]                                                       {'loss': 1.9706, 'learning_rate': 0.00013994033744488076, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1664/2180 [4:24:17<1:22:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1665/2180 [4:24:26<1:21:48,  9.53s/it]                                                       {'loss': 2.0195, 'learning_rate': 0.00013942517353442092, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1665/2180 [4:24:26<1:21:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1666/2180 [4:24:36<1:21:32,  9.52s/it]                                                       {'loss': 2.1286, 'learning_rate': 0.0001389108059393391, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1666/2180 [4:24:36<1:21:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1667/2180 [4:24:45<1:21:26,  9.53s/it]                                                       {'loss': 1.9897, 'learning_rate': 0.00013839723579559581, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1667/2180 [4:24:45<1:21:26,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1668/2180 [4:24:55<1:21:15,  9.52s/it]                                                       {'loss': 2.089, 'learning_rate': 0.00013788446423739103, 'epoch': 0.76}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1668/2180 [4:24:55<1:21:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1669/2180 [4:25:04<1:21:14,  9.54s/it]                                                       {'loss': 2.1009, 'learning_rate': 0.00013737249239716042, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1669/2180 [4:25:04<1:21:14,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1670/2180 [4:25:14<1:21:16,  9.56s/it]                                                       {'loss': 2.0204, 'learning_rate': 0.00013686132140557355, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1670/2180 [4:25:14<1:21:16,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1671/2180 [4:25:24<1:21:00,  9.55s/it]                                                       {'loss': 1.9293, 'learning_rate': 0.00013635095239153188, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1671/2180 [4:25:24<1:21:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1672/2180 [4:25:33<1:20:40,  9.53s/it]                                                       {'loss': 2.0418, 'learning_rate': 0.00013584138648216527, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1672/2180 [4:25:33<1:20:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1673/2180 [4:25:43<1:20:37,  9.54s/it]                                                       {'loss': 2.0397, 'learning_rate': 0.0001353326248028298, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1673/2180 [4:25:43<1:20:37,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1674/2180 [4:25:52<1:20:33,  9.55s/it]                                                       {'loss': 2.0132, 'learning_rate': 0.00013482466847710594, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1674/2180 [4:25:52<1:20:33,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1675/2180 [4:26:02<1:20:16,  9.54s/it]                                                       {'loss': 2.0885, 'learning_rate': 0.00013431751862679554, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1675/2180 [4:26:02<1:20:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1676/2180 [4:26:11<1:20:07,  9.54s/it]                                                       {'loss': 2.09, 'learning_rate': 0.00013381117637191887, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1676/2180 [4:26:11<1:20:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1677/2180 [4:26:21<1:19:49,  9.52s/it]                                                       {'loss': 2.0278, 'learning_rate': 0.00013330564283071293, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1677/2180 [4:26:21<1:19:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1678/2180 [4:26:30<1:19:53,  9.55s/it]                                                       {'loss': 2.0249, 'learning_rate': 0.000132800919119629, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1678/2180 [4:26:30<1:19:53,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1679/2180 [4:26:40<1:19:36,  9.53s/it]                                                       {'loss': 2.0844, 'learning_rate': 0.00013229700635332948, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1679/2180 [4:26:40<1:19:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1680/2180 [4:26:49<1:19:31,  9.54s/it]                                                       {'loss': 2.1189, 'learning_rate': 0.00013179390564468585, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1680/2180 [4:26:49<1:19:31,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1681/2180 [4:26:59<1:19:25,  9.55s/it]                                                       {'loss': 2.0727, 'learning_rate': 0.00013129161810477641, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1681/2180 [4:26:59<1:19:25,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1682/2180 [4:27:08<1:19:14,  9.55s/it]                                                       {'loss': 2.1127, 'learning_rate': 0.0001307901448428837, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1682/2180 [4:27:08<1:19:14,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1683/2180 [4:27:18<1:19:13,  9.56s/it]                                                       {'loss': 2.0479, 'learning_rate': 0.0001302894869664916, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1683/2180 [4:27:18<1:19:13,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1684/2180 [4:27:28<1:19:02,  9.56s/it]                                                       {'loss': 2.0204, 'learning_rate': 0.00012978964558128336, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1684/2180 [4:27:28<1:19:02,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1685/2180 [4:27:37<1:18:45,  9.55s/it]                                                       {'loss': 2.1008, 'learning_rate': 0.00012929062179113925, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1685/2180 [4:27:37<1:18:45,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1686/2180 [4:27:47<1:18:28,  9.53s/it]                                                       {'loss': 2.0846, 'learning_rate': 0.00012879241669813368, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1686/2180 [4:27:47<1:18:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1687/2180 [4:27:56<1:18:15,  9.52s/it]                                                       {'loss': 1.9755, 'learning_rate': 0.00012829503140253295, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1687/2180 [4:27:56<1:18:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1688/2180 [4:28:06<1:18:04,  9.52s/it]                                                       {'loss': 2.0348, 'learning_rate': 0.0001277984670027933, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1688/2180 [4:28:06<1:18:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1689/2180 [4:28:15<1:18:04,  9.54s/it]                                                       {'loss': 2.0593, 'learning_rate': 0.00012730272459555737, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1689/2180 [4:28:15<1:18:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1690/2180 [4:28:25<1:17:51,  9.53s/it]                                                       {'loss': 2.0294, 'learning_rate': 0.00012680780527565312, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1690/2180 [4:28:25<1:17:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1691/2180 [4:28:34<1:17:40,  9.53s/it]                                                       {'loss': 2.0561, 'learning_rate': 0.0001263137101360905, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1691/2180 [4:28:34<1:17:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1692/2180 [4:28:44<1:17:25,  9.52s/it]                                                       {'loss': 2.0587, 'learning_rate': 0.00012582044026805922, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1692/2180 [4:28:44<1:17:25,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1693/2180 [4:28:53<1:17:12,  9.51s/it]                                                       {'loss': 2.0135, 'learning_rate': 0.00012532799676092627, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1693/2180 [4:28:53<1:17:12,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1694/2180 [4:29:03<1:17:04,  9.51s/it]                                                       {'loss': 1.9892, 'learning_rate': 0.00012483638070223414, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1694/2180 [4:29:03<1:17:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1695/2180 [4:29:12<1:16:58,  9.52s/it]                                                       {'loss': 1.9936, 'learning_rate': 0.00012434559317769752, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1695/2180 [4:29:12<1:16:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1696/2180 [4:29:22<1:16:51,  9.53s/it]                                                       {'loss': 1.9983, 'learning_rate': 0.0001238556352712012, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1696/2180 [4:29:22<1:16:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1697/2180 [4:29:31<1:16:53,  9.55s/it]                                                       {'loss': 2.0317, 'learning_rate': 0.00012336650806479827, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1697/2180 [4:29:31<1:16:53,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1698/2180 [4:29:41<1:16:34,  9.53s/it]                                                       {'loss': 2.0489, 'learning_rate': 0.00012287821263870708, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1698/2180 [4:29:41<1:16:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1699/2180 [4:29:50<1:16:23,  9.53s/it]                                                       {'loss': 1.9565, 'learning_rate': 0.00012239075007130885, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1699/2180 [4:29:50<1:16:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1700/2180 [4:30:00<1:16:10,  9.52s/it]                                                       {'loss': 2.0361, 'learning_rate': 0.00012190412143914536, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1700/2180 [4:30:00<1:16:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1701/2180 [4:30:09<1:15:59,  9.52s/it]                                                       {'loss': 2.0891, 'learning_rate': 0.0001214183278169172, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1701/2180 [4:30:09<1:15:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1702/2180 [4:30:19<1:15:57,  9.54s/it]                                                       {'loss': 2.0943, 'learning_rate': 0.00012093337027748042, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1702/2180 [4:30:19<1:15:57,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1703/2180 [4:30:29<1:15:49,  9.54s/it]                                                       {'loss': 2.1345, 'learning_rate': 0.00012044924989184459, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1703/2180 [4:30:29<1:15:49,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1704/2180 [4:30:38<1:15:41,  9.54s/it]                                                       {'loss': 2.0965, 'learning_rate': 0.0001199659677291709, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1704/2180 [4:30:38<1:15:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1705/2180 [4:30:48<1:15:23,  9.52s/it]                                                       {'loss': 1.9718, 'learning_rate': 0.00011948352485676895, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1705/2180 [4:30:48<1:15:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1706/2180 [4:30:57<1:15:15,  9.53s/it]                                                       {'loss': 1.9862, 'learning_rate': 0.00011900192234009477, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1706/2180 [4:30:57<1:15:15,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1707/2180 [4:31:07<1:14:59,  9.51s/it]                                                       {'loss': 1.9705, 'learning_rate': 0.00011852116124274875, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1707/2180 [4:31:07<1:14:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1708/2180 [4:31:16<1:14:48,  9.51s/it]                                                       {'loss': 1.9611, 'learning_rate': 0.00011804124262647314, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1708/2180 [4:31:16<1:14:48,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1709/2180 [4:31:26<1:14:39,  9.51s/it]                                                       {'loss': 1.9582, 'learning_rate': 0.00011756216755114929, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1709/2180 [4:31:26<1:14:39,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1710/2180 [4:31:35<1:14:31,  9.51s/it]                                                       {'loss': 2.0675, 'learning_rate': 0.00011708393707479548, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1710/2180 [4:31:35<1:14:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1711/2180 [4:31:45<1:14:30,  9.53s/it]                                                       {'loss': 2.1271, 'learning_rate': 0.00011660655225356531, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1711/2180 [4:31:45<1:14:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1712/2180 [4:31:54<1:14:22,  9.54s/it]                                                       {'loss': 1.9843, 'learning_rate': 0.0001161300141417444, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1712/2180 [4:31:54<1:14:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1713/2180 [4:32:04<1:14:08,  9.53s/it]                                                       {'loss': 2.1364, 'learning_rate': 0.00011565432379174823, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1713/2180 [4:32:04<1:14:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1714/2180 [4:32:13<1:13:56,  9.52s/it]                                                       {'loss': 2.0622, 'learning_rate': 0.00011517948225412056, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1714/2180 [4:32:13<1:13:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1715/2180 [4:32:23<1:13:46,  9.52s/it]                                                       {'loss': 2.0753, 'learning_rate': 0.00011470549057753032, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1715/2180 [4:32:23<1:13:46,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1716/2180 [4:32:32<1:13:38,  9.52s/it]                                                       {'loss': 2.0639, 'learning_rate': 0.00011423234980876957, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1716/2180 [4:32:32<1:13:38,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1717/2180 [4:32:42<1:13:33,  9.53s/it]                                                       {'loss': 2.0491, 'learning_rate': 0.00011376006099275099, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1717/2180 [4:32:42<1:13:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1718/2180 [4:32:51<1:13:20,  9.52s/it]                                                       {'loss': 1.9406, 'learning_rate': 0.00011328862517250609, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1718/2180 [4:32:51<1:13:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1719/2180 [4:33:01<1:13:04,  9.51s/it]                                                       {'loss': 2.0584, 'learning_rate': 0.00011281804338918239, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1719/2180 [4:33:01<1:13:04,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1720/2180 [4:33:10<1:12:57,  9.52s/it]                                                       {'loss': 2.0703, 'learning_rate': 0.00011234831668204115, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1720/2180 [4:33:10<1:12:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1721/2180 [4:33:20<1:12:54,  9.53s/it]                                                       {'loss': 1.9286, 'learning_rate': 0.00011187944608845569, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1721/2180 [4:33:20<1:12:54,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1722/2180 [4:33:29<1:12:42,  9.52s/it]                                                       {'loss': 2.1188, 'learning_rate': 0.00011141143264390801, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1722/2180 [4:33:30<1:12:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1723/2180 [4:33:39<1:12:28,  9.51s/it]                                                       {'loss': 1.9951, 'learning_rate': 0.0001109442773819877, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1723/2180 [4:33:39<1:12:28,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1724/2180 [4:33:48<1:12:16,  9.51s/it]                                                       {'loss': 1.9891, 'learning_rate': 0.0001104779813343889, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1724/2180 [4:33:48<1:12:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1725/2180 [4:33:58<1:12:10,  9.52s/it]                                                       {'loss': 2.0296, 'learning_rate': 0.00011001254553090812, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1725/2180 [4:33:58<1:12:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1726/2180 [4:34:08<1:12:00,  9.52s/it]                                                       {'loss': 2.0691, 'learning_rate': 0.00010954797099944186, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1726/2180 [4:34:08<1:12:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1727/2180 [4:34:17<1:11:57,  9.53s/it]                                                       {'loss': 2.0739, 'learning_rate': 0.0001090842587659851, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1727/2180 [4:34:17<1:11:57,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1728/2180 [4:34:27<1:11:43,  9.52s/it]                                                       {'loss': 2.0929, 'learning_rate': 0.00010862140985462804, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1728/2180 [4:34:27<1:11:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1729/2180 [4:34:36<1:11:37,  9.53s/it]                                                       {'loss': 2.0219, 'learning_rate': 0.00010815942528755418, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1729/2180 [4:34:36<1:11:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1730/2180 [4:34:46<1:11:28,  9.53s/it]                                                       {'loss': 2.0236, 'learning_rate': 0.00010769830608503844, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1730/2180 [4:34:46<1:11:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1731/2180 [4:34:55<1:11:22,  9.54s/it]                                                       {'loss': 2.0268, 'learning_rate': 0.00010723805326544473, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1731/2180 [4:34:55<1:11:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1732/2180 [4:35:05<1:11:13,  9.54s/it]                                                       {'loss': 2.056, 'learning_rate': 0.00010677866784522316, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1732/2180 [4:35:05<1:11:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1733/2180 [4:35:14<1:10:59,  9.53s/it]                                                       {'loss': 2.0314, 'learning_rate': 0.00010632015083890839, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1733/2180 [4:35:14<1:10:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1734/2180 [4:35:24<1:10:53,  9.54s/it]                                                       {'loss': 2.0654, 'learning_rate': 0.00010586250325911745, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1734/2180 [4:35:24<1:10:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1735/2180 [4:35:33<1:10:43,  9.54s/it]                                                       {'loss': 2.0744, 'learning_rate': 0.00010540572611654697, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1735/2180 [4:35:33<1:10:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1736/2180 [4:35:43<1:10:37,  9.54s/it]                                                       {'loss': 2.0322, 'learning_rate': 0.00010494982041997126, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1736/2180 [4:35:43<1:10:37,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1737/2180 [4:35:52<1:10:30,  9.55s/it]                                                       {'loss': 1.9766, 'learning_rate': 0.0001044947871762405, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1737/2180 [4:35:53<1:10:30,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1738/2180 [4:36:02<1:10:21,  9.55s/it]                                                       {'loss': 1.997, 'learning_rate': 0.00010404062739027753, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1738/2180 [4:36:02<1:10:21,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1739/2180 [4:36:12<1:10:13,  9.55s/it]                                                       {'loss': 1.9764, 'learning_rate': 0.00010358734206507641, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1739/2180 [4:36:12<1:10:13,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1740/2180 [4:36:21<1:10:02,  9.55s/it]                                                       {'loss': 1.9666, 'learning_rate': 0.00010313493220170017, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1740/2180 [4:36:21<1:10:02,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1741/2180 [4:36:31<1:09:57,  9.56s/it]                                                       {'loss': 2.04, 'learning_rate': 0.00010268339879927836, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1741/2180 [4:36:31<1:09:57,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1742/2180 [4:36:40<1:09:49,  9.56s/it]                                                       {'loss': 1.946, 'learning_rate': 0.00010223274285500466, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1742/2180 [4:36:40<1:09:49,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1743/2180 [4:36:50<1:09:41,  9.57s/it]                                                       {'loss': 2.0851, 'learning_rate': 0.00010178296536413495, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1743/2180 [4:36:50<1:09:41,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1744/2180 [4:36:59<1:09:30,  9.57s/it]                                                       {'loss': 1.9833, 'learning_rate': 0.00010133406731998546, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1744/2180 [4:36:59<1:09:30,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1745/2180 [4:37:09<1:09:13,  9.55s/it]                                                       {'loss': 2.0654, 'learning_rate': 0.00010088604971392979, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1745/2180 [4:37:09<1:09:13,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1746/2180 [4:37:19<1:09:04,  9.55s/it]                                                       {'loss': 1.9845, 'learning_rate': 0.0001004389135353972, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1746/2180 [4:37:19<1:09:04,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1747/2180 [4:37:28<1:08:47,  9.53s/it]                                                       {'loss': 2.1088, 'learning_rate': 9.999265977187049e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1747/2180 [4:37:28<1:08:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1748/2180 [4:37:38<1:08:39,  9.53s/it]                                                       {'loss': 2.0663, 'learning_rate': 9.95472894088838e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1748/2180 [4:37:38<1:08:39,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1749/2180 [4:37:47<1:08:23,  9.52s/it]                                                       {'loss': 2.0528, 'learning_rate': 9.910280343001993e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1749/2180 [4:37:47<1:08:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1750/2180 [4:37:57<1:08:15,  9.53s/it]                                                       {'loss': 2.0062, 'learning_rate': 9.865920281690866e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1750/2180 [4:37:57<1:08:15,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1751/2180 [4:38:06<1:08:05,  9.52s/it]                                                       {'loss': 2.0248, 'learning_rate': 9.821648854922482e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1751/2180 [4:38:06<1:08:05,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1752/2180 [4:38:16<1:07:55,  9.52s/it]                                                       {'loss': 2.0276, 'learning_rate': 9.77746616046854e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1752/2180 [4:38:16<1:07:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1753/2180 [4:38:25<1:07:42,  9.51s/it]                                                       {'loss': 2.0821, 'learning_rate': 9.733372295904774e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1753/2180 [4:38:25<1:07:42,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1754/2180 [4:38:35<1:07:40,  9.53s/it]                                                       {'loss': 1.9441, 'learning_rate': 9.68936735861079e-05, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1754/2180 [4:38:35<1:07:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1755/2180 [4:38:44<1:07:26,  9.52s/it]                                                       {'loss': 2.0121, 'learning_rate': 9.645451445769737e-05, 'epoch': 0.8}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1755/2180 [4:38:44<1:07:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1756/2180 [4:38:54<1:07:15,  9.52s/it]                                                       {'loss': 2.1313, 'learning_rate': 9.601624654368196e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1756/2180 [4:38:54<1:07:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1757/2180 [4:39:03<1:07:04,  9.52s/it]                                                       {'loss': 2.1129, 'learning_rate': 9.557887081195938e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1757/2180 [4:39:03<1:07:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1758/2180 [4:39:13<1:06:57,  9.52s/it]                                                       {'loss': 2.0814, 'learning_rate': 9.514238822845667e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1758/2180 [4:39:13<1:06:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1759/2180 [4:39:22<1:06:55,  9.54s/it]                                                       {'loss': 1.9543, 'learning_rate': 9.470679975712837e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1759/2180 [4:39:22<1:06:55,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1760/2180 [4:39:32<1:06:50,  9.55s/it]                                                       {'loss': 2.0304, 'learning_rate': 9.427210635995481e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1760/2180 [4:39:32<1:06:50,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1761/2180 [4:39:41<1:06:38,  9.54s/it]                                                       {'loss': 1.9895, 'learning_rate': 9.383830899693923e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1761/2180 [4:39:41<1:06:38,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1762/2180 [4:39:51<1:06:41,  9.57s/it]                                                       {'loss': 2.0389, 'learning_rate': 9.340540862610591e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1762/2180 [4:39:51<1:06:41,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1763/2180 [4:40:01<1:06:26,  9.56s/it]                                                       {'loss': 2.0528, 'learning_rate': 9.297340620349854e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1763/2180 [4:40:01<1:06:26,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1764/2180 [4:40:10<1:06:12,  9.55s/it]                                                       {'loss': 2.0026, 'learning_rate': 9.25423026831777e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1764/2180 [4:40:10<1:06:12,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1765/2180 [4:40:20<1:05:56,  9.53s/it]                                                       {'loss': 2.0455, 'learning_rate': 9.211209901721846e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1765/2180 [4:40:20<1:05:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1766/2180 [4:40:29<1:05:51,  9.54s/it]                                                       {'loss': 1.9957, 'learning_rate': 9.168279615570863e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1766/2180 [4:40:29<1:05:51,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1767/2180 [4:40:39<1:05:49,  9.56s/it]                                                       {'loss': 2.1103, 'learning_rate': 9.125439504674699e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1767/2180 [4:40:39<1:05:49,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1768/2180 [4:40:48<1:05:36,  9.55s/it]                                                       {'loss': 2.0601, 'learning_rate': 9.082689663644057e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1768/2180 [4:40:48<1:05:36,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1769/2180 [4:40:58<1:05:24,  9.55s/it]                                                       {'loss': 2.0925, 'learning_rate': 9.040030186890264e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1769/2180 [4:40:58<1:05:24,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1770/2180 [4:41:07<1:05:11,  9.54s/it]                                                       {'loss': 1.9801, 'learning_rate': 8.997461168625138e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1770/2180 [4:41:07<1:05:11,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1771/2180 [4:41:17<1:05:06,  9.55s/it]                                                       {'loss': 1.9901, 'learning_rate': 8.954982702860664e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1771/2180 [4:41:17<1:05:06,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1772/2180 [4:41:26<1:04:47,  9.53s/it]                                                       {'loss': 2.0663, 'learning_rate': 8.912594883408865e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1772/2180 [4:41:26<1:04:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1773/2180 [4:41:36<1:04:32,  9.52s/it]                                                       {'loss': 2.005, 'learning_rate': 8.870297803881589e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1773/2180 [4:41:36<1:04:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1774/2180 [4:41:45<1:04:27,  9.53s/it]                                                       {'loss': 2.0571, 'learning_rate': 8.828091557690287e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1774/2180 [4:41:45<1:04:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1775/2180 [4:41:55<1:04:16,  9.52s/it]                                                       {'loss': 2.1584, 'learning_rate': 8.785976238045801e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1775/2180 [4:41:55<1:04:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1776/2180 [4:42:05<1:04:09,  9.53s/it]                                                       {'loss': 2.0223, 'learning_rate': 8.743951937958144e-05, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1776/2180 [4:42:05<1:04:09,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1777/2180 [4:42:14<1:03:57,  9.52s/it]                                                       {'loss': 1.9917, 'learning_rate': 8.702018750236357e-05, 'epoch': 0.81}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1777/2180 [4:42:14<1:03:57,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1778/2180 [4:42:24<1:03:48,  9.52s/it]                                                       {'loss': 2.0296, 'learning_rate': 8.660176767488237e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1778/2180 [4:42:24<1:03:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1779/2180 [4:42:33<1:03:46,  9.54s/it]                                                       {'loss': 2.001, 'learning_rate': 8.618426082120146e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1779/2180 [4:42:33<1:03:46,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1780/2180 [4:42:43<1:03:33,  9.53s/it]                                                       {'loss': 2.0489, 'learning_rate': 8.576766786336854e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1780/2180 [4:42:43<1:03:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1781/2180 [4:42:52<1:03:20,  9.52s/it]                                                       {'loss': 2.0686, 'learning_rate': 8.535198972141294e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1781/2180 [4:42:52<1:03:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1782/2180 [4:43:02<1:03:16,  9.54s/it]                                                       {'loss': 2.0388, 'learning_rate': 8.493722731334347e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1782/2180 [4:43:02<1:03:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1783/2180 [4:43:11<1:03:03,  9.53s/it]                                                       {'loss': 1.8974, 'learning_rate': 8.452338155514644e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1783/2180 [4:43:11<1:03:03,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1784/2180 [4:43:21<1:02:52,  9.53s/it]                                                       {'loss': 2.035, 'learning_rate': 8.411045336078426e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1784/2180 [4:43:21<1:02:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1785/2180 [4:43:30<1:02:41,  9.52s/it]                                                       {'loss': 2.0484, 'learning_rate': 8.369844364219264e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1785/2180 [4:43:30<1:02:41,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1786/2180 [4:43:40<1:02:38,  9.54s/it]                                                       {'loss': 2.0581, 'learning_rate': 8.328735330927873e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1786/2180 [4:43:40<1:02:38,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1787/2180 [4:43:49<1:02:33,  9.55s/it]                                                       {'loss': 2.0669, 'learning_rate': 8.287718326991961e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1787/2180 [4:43:49<1:02:33,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1788/2180 [4:43:59<1:02:13,  9.52s/it]                                                       {'loss': 1.9986, 'learning_rate': 8.246793442995954e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1788/2180 [4:43:59<1:02:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1789/2180 [4:44:08<1:01:59,  9.51s/it]                                                       {'loss': 2.0209, 'learning_rate': 8.205960769320875e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1789/2180 [4:44:08<1:01:59,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1790/2180 [4:44:18<1:01:46,  9.50s/it]                                                       {'loss': 2.0649, 'learning_rate': 8.165220396144085e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1790/2180 [4:44:18<1:01:46,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1791/2180 [4:44:27<1:01:35,  9.50s/it]                                                       {'loss': 2.0372, 'learning_rate': 8.12457241343909e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1791/2180 [4:44:27<1:01:35,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1792/2180 [4:44:37<1:01:31,  9.51s/it]                                                       {'loss': 2.047, 'learning_rate': 8.084016910975367e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1792/2180 [4:44:37<1:01:31,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1793/2180 [4:44:46<1:01:33,  9.54s/it]                                                       {'loss': 2.0505, 'learning_rate': 8.043553978318169e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1793/2180 [4:44:46<1:01:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1794/2180 [4:44:56<1:01:20,  9.53s/it]                                                       {'loss': 2.1672, 'learning_rate': 8.003183704828281e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1794/2180 [4:44:56<1:01:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1795/2180 [4:45:06<1:01:07,  9.53s/it]                                                       {'loss': 1.9511, 'learning_rate': 7.962906179661872e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1795/2180 [4:45:06<1:01:07,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1796/2180 [4:45:15<1:01:02,  9.54s/it]                                                       {'loss': 1.9958, 'learning_rate': 7.922721491770296e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1796/2180 [4:45:15<1:01:02,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1797/2180 [4:45:25<1:00:50,  9.53s/it]                                                       {'loss': 2.1236, 'learning_rate': 7.882629729899832e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1797/2180 [4:45:25<1:00:50,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1798/2180 [4:45:34<1:00:38,  9.53s/it]                                                       {'loss': 2.1221, 'learning_rate': 7.842630982591598e-05, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1798/2180 [4:45:34<1:00:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1799/2180 [4:45:44<1:00:27,  9.52s/it]                                                       {'loss': 2.0378, 'learning_rate': 7.802725338181232e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1799/2180 [4:45:44<1:00:27,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1800/2180 [4:45:53<1:00:15,  9.51s/it]                                                       {'loss': 1.9974, 'learning_rate': 7.762912884798812e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1800/2180 [4:45:53<1:00:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1801/2180 [4:46:03<1:00:07,  9.52s/it]                                                       {'loss': 1.99, 'learning_rate': 7.723193710368564e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1801/2180 [4:46:03<1:00:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1802/2180 [4:46:12<1:00:05,  9.54s/it]                                                       {'loss': 1.9929, 'learning_rate': 7.683567902608729e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1802/2180 [4:46:12<1:00:05,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1803/2180 [4:46:22<59:49,  9.52s/it]                                                       {'loss': 1.9564, 'learning_rate': 7.644035549031364e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1803/2180 [4:46:22<59:49,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1804/2180 [4:46:31<59:36,  9.51s/it]                                                     {'loss': 2.0877, 'learning_rate': 7.604596736942115e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1804/2180 [4:46:31<59:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1805/2180 [4:46:41<59:26,  9.51s/it]                                                     {'loss': 2.0426, 'learning_rate': 7.56525155344004e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1805/2180 [4:46:41<59:26,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1806/2180 [4:46:50<59:18,  9.51s/it]                                                     {'loss': 2.0416, 'learning_rate': 7.52600008541745e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1806/2180 [4:46:50<59:18,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1807/2180 [4:47:00<59:08,  9.51s/it]                                                     {'loss': 2.0265, 'learning_rate': 7.486842419559681e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1807/2180 [4:47:00<59:08,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1808/2180 [4:47:09<58:58,  9.51s/it]                                                     {'loss': 2.0615, 'learning_rate': 7.447778642344898e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1808/2180 [4:47:09<58:58,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1809/2180 [4:47:19<58:47,  9.51s/it]                                                     {'loss': 2.0275, 'learning_rate': 7.408808840043912e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1809/2180 [4:47:19<58:47,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1810/2180 [4:47:28<58:36,  9.50s/it]                                                     {'loss': 2.0982, 'learning_rate': 7.369933098720021e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1810/2180 [4:47:28<58:36,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1811/2180 [4:47:38<58:39,  9.54s/it]                                                     {'loss': 2.0389, 'learning_rate': 7.331151504228767e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1811/2180 [4:47:38<58:39,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1812/2180 [4:47:47<58:26,  9.53s/it]                                                     {'loss': 2.057, 'learning_rate': 7.292464142217775e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1812/2180 [4:47:47<58:26,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1813/2180 [4:47:57<58:23,  9.55s/it]                                                     {'loss': 2.069, 'learning_rate': 7.25387109812658e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1813/2180 [4:47:57<58:23,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1814/2180 [4:48:06<58:06,  9.53s/it]                                                     {'loss': 2.0307, 'learning_rate': 7.215372457186415e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1814/2180 [4:48:06<58:06,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1815/2180 [4:48:16<57:55,  9.52s/it]                                                     {'loss': 2.1339, 'learning_rate': 7.176968304420007e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1815/2180 [4:48:16<57:55,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1816/2180 [4:48:26<57:53,  9.54s/it]                                                     {'loss': 1.9973, 'learning_rate': 7.138658724641417e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1816/2180 [4:48:26<57:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1817/2180 [4:48:35<57:42,  9.54s/it]                                                     {'loss': 2.0243, 'learning_rate': 7.10044380245587e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1817/2180 [4:48:35<57:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1818/2180 [4:48:45<57:34,  9.54s/it]                                                     {'loss': 2.0468, 'learning_rate': 7.062323622259515e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1818/2180 [4:48:45<57:34,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1819/2180 [4:48:54<57:23,  9.54s/it]                                                     {'loss': 2.0169, 'learning_rate': 7.024298268239265e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1819/2180 [4:48:54<57:23,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1820/2180 [4:49:04<57:17,  9.55s/it]                                                     {'loss': 2.0454, 'learning_rate': 6.986367824372647e-05, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1820/2180 [4:49:04<57:17,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1821/2180 [4:49:13<57:03,  9.54s/it]                                                     {'loss': 2.0486, 'learning_rate': 6.948532374427541e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1821/2180 [4:49:13<57:03,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1822/2180 [4:49:23<56:48,  9.52s/it]                                                     {'loss': 2.0803, 'learning_rate': 6.910792001962063e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1822/2180 [4:49:23<56:48,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1823/2180 [4:49:32<56:36,  9.51s/it]                                                     {'loss': 2.0328, 'learning_rate': 6.873146790324358e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1823/2180 [4:49:32<56:36,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1824/2180 [4:49:42<56:24,  9.51s/it]                                                     {'loss': 2.0964, 'learning_rate': 6.83559682265239e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1824/2180 [4:49:42<56:24,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1825/2180 [4:49:51<56:16,  9.51s/it]                                                     {'loss': 1.9851, 'learning_rate': 6.798142181873784e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1825/2180 [4:49:51<56:16,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1826/2180 [4:50:01<56:06,  9.51s/it]                                                     {'loss': 1.9956, 'learning_rate': 6.760782950705662e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1826/2180 [4:50:01<56:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1827/2180 [4:50:10<55:55,  9.51s/it]                                                     {'loss': 2.0961, 'learning_rate': 6.723519211654422e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1827/2180 [4:50:10<55:55,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1828/2180 [4:50:20<55:46,  9.51s/it]                                                     {'loss': 2.0149, 'learning_rate': 6.686351047015554e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1828/2180 [4:50:20<55:46,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1829/2180 [4:50:29<55:42,  9.52s/it]                                                     {'loss': 2.0034, 'learning_rate': 6.649278538873515e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1829/2180 [4:50:29<55:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2180 [4:50:39<55:33,  9.52s/it]                                                     {'loss': 2.0496, 'learning_rate': 6.612301769101465e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2180 [4:50:39<55:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1831/2180 [4:50:48<55:24,  9.53s/it]                                                     {'loss': 2.0032, 'learning_rate': 6.575420819361177e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1831/2180 [4:50:48<55:24,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1832/2180 [4:50:58<55:12,  9.52s/it]                                                     {'loss': 2.0271, 'learning_rate': 6.538635771102757e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1832/2180 [4:50:58<55:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2180 [4:51:07<55:03,  9.52s/it]                                                     {'loss': 2.0845, 'learning_rate': 6.501946705564566e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2180 [4:51:07<55:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2180 [4:51:17<54:50,  9.51s/it]                                                     {'loss': 2.0736, 'learning_rate': 6.465353703772959e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2180 [4:51:17<54:50,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1835/2180 [4:51:26<54:41,  9.51s/it]                                                     {'loss': 2.0288, 'learning_rate': 6.428856846542136e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1835/2180 [4:51:26<54:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1836/2180 [4:51:36<54:42,  9.54s/it]                                                     {'loss': 1.9634, 'learning_rate': 6.392456214473996e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1836/2180 [4:51:36<54:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1837/2180 [4:51:46<54:44,  9.58s/it]                                                     {'loss': 2.077, 'learning_rate': 6.3561518879579e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1837/2180 [4:51:46<54:44,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1838/2180 [4:51:55<54:28,  9.56s/it]                                                     {'loss': 2.049, 'learning_rate': 6.31994394717052e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1838/2180 [4:51:55<54:28,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1839/2180 [4:52:05<54:13,  9.54s/it]                                                     {'loss': 2.118, 'learning_rate': 6.283832472075685e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1839/2180 [4:52:05<54:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2180 [4:52:14<54:00,  9.53s/it]                                                     {'loss': 1.9377, 'learning_rate': 6.247817542424178e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2180 [4:52:14<54:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1841/2180 [4:52:24<53:47,  9.52s/it]                                                     {'loss': 2.1097, 'learning_rate': 6.211899237753559e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1841/2180 [4:52:24<53:47,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1842/2180 [4:52:33<53:37,  9.52s/it]                                                     {'loss': 2.0826, 'learning_rate': 6.176077637387984e-05, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1842/2180 [4:52:33<53:37,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1843/2180 [4:52:43<53:28,  9.52s/it]                                                     {'loss': 2.0297, 'learning_rate': 6.140352820438066e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1843/2180 [4:52:43<53:28,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1844/2180 [4:52:52<53:22,  9.53s/it]                                                     {'loss': 2.042, 'learning_rate': 6.104724865800665e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1844/2180 [4:52:52<53:22,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1845/2180 [4:53:02<53:08,  9.52s/it]                                                     {'loss': 2.05, 'learning_rate': 6.069193852158711e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1845/2180 [4:53:02<53:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1846/2180 [4:53:11<53:03,  9.53s/it]                                                     {'loss': 2.1065, 'learning_rate': 6.0337598579810584e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1846/2180 [4:53:11<53:03,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1847/2180 [4:53:21<53:03,  9.56s/it]                                                     {'loss': 2.0179, 'learning_rate': 5.9984229615223096e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1847/2180 [4:53:21<53:03,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1848/2180 [4:53:31<52:55,  9.56s/it]                                                     {'loss': 2.002, 'learning_rate': 5.963183240822606e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1848/2180 [4:53:31<52:55,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1849/2180 [4:53:40<52:44,  9.56s/it]                                                     {'loss': 1.9457, 'learning_rate': 5.9280407737074825e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1849/2180 [4:53:40<52:44,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1850/2180 [4:53:50<52:33,  9.55s/it]                                                     {'loss': 2.0971, 'learning_rate': 5.8929956377877125e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1850/2180 [4:53:50<52:33,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1851/2180 [4:53:59<52:18,  9.54s/it]                                                     {'loss': 2.0208, 'learning_rate': 5.8580479104591075e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1851/2180 [4:53:59<52:18,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1852/2180 [4:54:09<52:05,  9.53s/it]                                                     {'loss': 2.0895, 'learning_rate': 5.823197668902341e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1852/2180 [4:54:09<52:05,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1853/2180 [4:54:18<51:56,  9.53s/it]                                                     {'loss': 1.9752, 'learning_rate': 5.78844499008282e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1853/2180 [4:54:18<51:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1854/2180 [4:54:28<51:49,  9.54s/it]                                                     {'loss': 2.0508, 'learning_rate': 5.753789950750454e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1854/2180 [4:54:28<51:49,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1855/2180 [4:54:37<51:37,  9.53s/it]                                                     {'loss': 1.919, 'learning_rate': 5.719232627439558e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1855/2180 [4:54:37<51:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1856/2180 [4:54:47<51:24,  9.52s/it]                                                     {'loss': 2.0187, 'learning_rate': 5.6847730964686315e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1856/2180 [4:54:47<51:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1857/2180 [4:54:56<51:15,  9.52s/it]                                                     {'loss': 2.0954, 'learning_rate': 5.650411433940189e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1857/2180 [4:54:56<51:15,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1858/2180 [4:55:06<51:18,  9.56s/it]                                                     {'loss': 2.037, 'learning_rate': 5.61614771574061e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1858/2180 [4:55:06<51:18,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1859/2180 [4:55:15<51:03,  9.54s/it]                                                     {'loss': 2.0311, 'learning_rate': 5.581982017539988e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1859/2180 [4:55:15<51:03,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1860/2180 [4:55:25<50:54,  9.55s/it]                                                     {'loss': 1.9238, 'learning_rate': 5.5479144147919216e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1860/2180 [4:55:25<50:54,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1861/2180 [4:55:34<50:44,  9.54s/it]                                                     {'loss': 1.9583, 'learning_rate': 5.51394498273336e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1861/2180 [4:55:34<50:44,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1862/2180 [4:55:44<50:32,  9.54s/it]                                                     {'loss': 1.9977, 'learning_rate': 5.480073796384494e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1862/2180 [4:55:44<50:32,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1863/2180 [4:55:53<50:18,  9.52s/it]                                                     {'loss': 2.0875, 'learning_rate': 5.446300930548492e-05, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1863/2180 [4:55:53<50:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1864/2180 [4:56:03<50:06,  9.51s/it]                                                     {'loss': 1.9643, 'learning_rate': 5.412626459811415e-05, 'epoch': 0.85}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1864/2180 [4:56:03<50:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1865/2180 [4:56:12<49:54,  9.51s/it]                                                     {'loss': 2.0203, 'learning_rate': 5.3790504585419954e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1865/2180 [4:56:12<49:54,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1866/2180 [4:56:22<49:50,  9.52s/it]                                                     {'loss': 2.0008, 'learning_rate': 5.345573000891541e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1866/2180 [4:56:22<49:50,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1867/2180 [4:56:31<49:35,  9.51s/it]                                                     {'loss': 2.0149, 'learning_rate': 5.312194160793693e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1867/2180 [4:56:32<49:35,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1868/2180 [4:56:41<49:29,  9.52s/it]                                                     {'loss': 1.9748, 'learning_rate': 5.278914011964303e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1868/2180 [4:56:41<49:29,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1869/2180 [4:56:51<49:15,  9.50s/it]                                                     {'loss': 2.1152, 'learning_rate': 5.2457326279013006e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1869/2180 [4:56:51<49:15,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1870/2180 [4:57:00<49:06,  9.51s/it]                                                     {'loss': 2.0164, 'learning_rate': 5.2126500818844514e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1870/2180 [4:57:00<49:06,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1871/2180 [4:57:10<49:06,  9.54s/it]                                                     {'loss': 1.9414, 'learning_rate': 5.1796664469752566e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1871/2180 [4:57:10<49:06,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1872/2180 [4:57:19<48:54,  9.53s/it]                                                     {'loss': 2.0537, 'learning_rate': 5.1467817960167975e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1872/2180 [4:57:19<48:54,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1873/2180 [4:57:29<48:43,  9.52s/it]                                                     {'loss': 2.0071, 'learning_rate': 5.113996201633536e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1873/2180 [4:57:29<48:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1874/2180 [4:57:38<48:34,  9.52s/it]                                                     {'loss': 2.0137, 'learning_rate': 5.0813097362311765e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1874/2180 [4:57:38<48:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1875/2180 [4:57:48<48:21,  9.51s/it]                                                     {'loss': 2.0219, 'learning_rate': 5.048722471996475e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1875/2180 [4:57:48<48:21,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1876/2180 [4:57:57<48:11,  9.51s/it]                                                     {'loss': 2.0409, 'learning_rate': 5.016234480897158e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1876/2180 [4:57:57<48:11,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1877/2180 [4:58:07<48:03,  9.52s/it]                                                     {'loss': 1.9875, 'learning_rate': 4.9838458346816664e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1877/2180 [4:58:07<48:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1878/2180 [4:58:16<47:52,  9.51s/it]                                                     {'loss': 2.0003, 'learning_rate': 4.9515566048790485e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1878/2180 [4:58:16<47:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1879/2180 [4:58:26<47:45,  9.52s/it]                                                     {'loss': 1.9697, 'learning_rate': 4.9193668627988074e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1879/2180 [4:58:26<47:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1880/2180 [4:58:35<47:42,  9.54s/it]                                                     {'loss': 2.0978, 'learning_rate': 4.887276679530744e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1880/2180 [4:58:35<47:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1881/2180 [4:58:45<47:36,  9.55s/it]                                                     {'loss': 2.0403, 'learning_rate': 4.855286125944752e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1881/2180 [4:58:45<47:36,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1882/2180 [4:58:54<47:24,  9.54s/it]                                                     {'loss': 1.9137, 'learning_rate': 4.8233952726907224e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1882/2180 [4:58:54<47:24,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1883/2180 [4:59:04<47:16,  9.55s/it]                                                     {'loss': 2.0723, 'learning_rate': 4.7916041901983565e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1883/2180 [4:59:04<47:16,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1884/2180 [4:59:14<47:08,  9.56s/it]                                                     {'loss': 2.012, 'learning_rate': 4.7599129486770145e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1884/2180 [4:59:14<47:08,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1885/2180 [4:59:23<47:01,  9.56s/it]                                                     {'loss': 2.1719, 'learning_rate': 4.728321618115555e-05, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1885/2180 [4:59:23<47:01,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1886/2180 [4:59:33<46:52,  9.57s/it]                                                     {'loss': 2.0692, 'learning_rate': 4.696830268282204e-05, 'epoch': 0.86}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1886/2180 [4:59:33<46:52,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1887/2180 [4:59:42<46:38,  9.55s/it]                                                     {'loss': 2.0695, 'learning_rate': 4.665438968724361e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1887/2180 [4:59:42<46:38,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1888/2180 [4:59:52<46:25,  9.54s/it]                                                     {'loss': 2.027, 'learning_rate': 4.634147788768489e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1888/2180 [4:59:52<46:25,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1889/2180 [5:00:01<46:10,  9.52s/it]                                                     {'loss': 2.0652, 'learning_rate': 4.6029567975199414e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1889/2180 [5:00:01<46:10,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1890/2180 [5:00:11<45:59,  9.52s/it]                                                     {'loss': 1.9595, 'learning_rate': 4.571866063862795e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1890/2180 [5:00:11<45:59,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1891/2180 [5:00:20<46:01,  9.55s/it]                                                     {'loss': 1.9962, 'learning_rate': 4.540875656459703e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1891/2180 [5:00:20<46:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1892/2180 [5:00:30<45:56,  9.57s/it]                                                     {'loss': 2.1152, 'learning_rate': 4.509985643751785e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1892/2180 [5:00:30<45:56,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1893/2180 [5:00:40<45:44,  9.56s/it]                                                     {'loss': 2.1397, 'learning_rate': 4.479196093958421e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1893/2180 [5:00:40<45:44,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1894/2180 [5:00:49<45:35,  9.56s/it]                                                     {'loss': 2.0193, 'learning_rate': 4.4485070750771187e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1894/2180 [5:00:49<45:35,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1895/2180 [5:00:59<45:20,  9.55s/it]                                                     {'loss': 1.9815, 'learning_rate': 4.417918654883363e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1895/2180 [5:00:59<45:20,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1896/2180 [5:01:08<45:04,  9.52s/it]                                                     {'loss': 1.9533, 'learning_rate': 4.3874309009305e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1896/2180 [5:01:08<45:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1897/2180 [5:01:18<44:56,  9.53s/it]                                                     {'loss': 2.0729, 'learning_rate': 4.357043880549538e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1897/2180 [5:01:18<44:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1898/2180 [5:01:27<44:47,  9.53s/it]                                                     {'loss': 2.0885, 'learning_rate': 4.326757660849012e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1898/2180 [5:01:27<44:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1899/2180 [5:01:37<44:40,  9.54s/it]                                                     {'loss': 2.0842, 'learning_rate': 4.2965723087148635e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1899/2180 [5:01:37<44:40,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1900/2180 [5:01:46<44:30,  9.54s/it]                                                     {'loss': 2.0165, 'learning_rate': 4.266487890810256e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1900/2180 [5:01:46<44:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1901/2180 [5:01:56<44:20,  9.54s/it]                                                     {'loss': 1.9739, 'learning_rate': 4.2365044735754365e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1901/2180 [5:01:56<44:20,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1902/2180 [5:02:05<44:10,  9.53s/it]                                                     {'loss': 2.147, 'learning_rate': 4.2066221232276266e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1902/2180 [5:02:05<44:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1903/2180 [5:02:15<44:03,  9.54s/it]                                                     {'loss': 2.1064, 'learning_rate': 4.176840905760815e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1903/2180 [5:02:15<44:03,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1904/2180 [5:02:24<43:57,  9.56s/it]                                                     {'loss': 1.9602, 'learning_rate': 4.1471608869456443e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1904/2180 [5:02:24<43:57,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1905/2180 [5:02:34<43:51,  9.57s/it]                                                     {'loss': 2.1126, 'learning_rate': 4.117582132329284e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1905/2180 [5:02:34<43:51,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1906/2180 [5:02:44<43:38,  9.56s/it]                                                     {'loss': 2.1547, 'learning_rate': 4.088104707235263e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1906/2180 [5:02:44<43:38,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1907/2180 [5:02:53<43:32,  9.57s/it]                                                     {'loss': 2.0502, 'learning_rate': 4.058728676763313e-05, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1907/2180 [5:02:53<43:32,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1908/2180 [5:03:03<43:17,  9.55s/it]                                                     {'loss': 2.1117, 'learning_rate': 4.0294541057892375e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1908/2180 [5:03:03<43:17,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1909/2180 [5:03:12<43:18,  9.59s/it]                                                     {'loss': 2.071, 'learning_rate': 4.000281058964794e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1909/2180 [5:03:12<43:18,  9.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1910/2180 [5:03:22<43:07,  9.58s/it]                                                     {'loss': 2.0069, 'learning_rate': 3.971209600717507e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1910/2180 [5:03:22<43:07,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1911/2180 [5:03:31<42:51,  9.56s/it]                                                     {'loss': 1.9449, 'learning_rate': 3.9422397952505465e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1911/2180 [5:03:31<42:51,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1912/2180 [5:03:41<42:39,  9.55s/it]                                                     {'loss': 2.0494, 'learning_rate': 3.913371706542596e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1912/2180 [5:03:41<42:39,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1913/2180 [5:03:51<42:36,  9.57s/it]                                                     {'loss': 2.1167, 'learning_rate': 3.884605398347707e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1913/2180 [5:03:51<42:36,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1914/2180 [5:04:00<42:27,  9.58s/it]                                                     {'loss': 2.0867, 'learning_rate': 3.8559409341951456e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1914/2180 [5:04:00<42:27,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1915/2180 [5:04:10<42:11,  9.55s/it]                                                     {'loss': 2.0202, 'learning_rate': 3.8273783773892404e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1915/2180 [5:04:10<42:11,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1916/2180 [5:04:19<42:00,  9.55s/it]                                                     {'loss': 2.0772, 'learning_rate': 3.798917791009293e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1916/2180 [5:04:19<42:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1917/2180 [5:04:29<41:51,  9.55s/it]                                                     {'loss': 2.1419, 'learning_rate': 3.770559237909393e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1917/2180 [5:04:29<41:51,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1918/2180 [5:04:38<41:53,  9.59s/it]                                                     {'loss': 1.9615, 'learning_rate': 3.742302780718288e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1918/2180 [5:04:38<41:53,  9.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1919/2180 [5:04:48<41:37,  9.57s/it]                                                     {'loss': 2.0961, 'learning_rate': 3.7141484818392635e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1919/2180 [5:04:48<41:37,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1920/2180 [5:04:57<41:23,  9.55s/it]                                                     {'loss': 1.981, 'learning_rate': 3.686096403449973e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1920/2180 [5:04:57<41:23,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1921/2180 [5:05:07<41:12,  9.55s/it]                                                     {'loss': 2.0684, 'learning_rate': 3.658146607502344e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1921/2180 [5:05:07<41:12,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1922/2180 [5:05:17<41:00,  9.54s/it]                                                     {'loss': 2.0305, 'learning_rate': 3.630299155722411e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1922/2180 [5:05:17<41:00,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1923/2180 [5:05:26<40:56,  9.56s/it]                                                     {'loss': 2.0629, 'learning_rate': 3.6025541096101676e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1923/2180 [5:05:26<40:56,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1924/2180 [5:05:36<40:48,  9.56s/it]                                                     {'loss': 2.0427, 'learning_rate': 3.574911530439473e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1924/2180 [5:05:36<40:48,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1925/2180 [5:05:45<40:33,  9.54s/it]                                                     {'loss': 2.1092, 'learning_rate': 3.5473714792578606e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1925/2180 [5:05:45<40:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1926/2180 [5:05:55<40:21,  9.53s/it]                                                     {'loss': 2.0066, 'learning_rate': 3.519934016886478e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1926/2180 [5:05:55<40:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1927/2180 [5:06:04<40:12,  9.54s/it]                                                     {'loss': 1.986, 'learning_rate': 3.4925992039198776e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1927/2180 [5:06:04<40:12,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1928/2180 [5:06:14<40:04,  9.54s/it]                                                     {'loss': 2.0214, 'learning_rate': 3.465367100725908e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1928/2180 [5:06:14<40:04,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1929/2180 [5:06:23<39:53,  9.53s/it]                                                     {'loss': 2.0422, 'learning_rate': 3.438237767445618e-05, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1929/2180 [5:06:23<39:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1930/2180 [5:06:33<39:42,  9.53s/it]                                                     {'loss': 2.1467, 'learning_rate': 3.4112112639930804e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1930/2180 [5:06:33<39:42,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1931/2180 [5:06:42<39:33,  9.53s/it]                                                     {'loss': 1.9869, 'learning_rate': 3.3842876500552564e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1931/2180 [5:06:42<39:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1932/2180 [5:06:52<39:20,  9.52s/it]                                                     {'loss': 2.0246, 'learning_rate': 3.357466985091906e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1932/2180 [5:06:52<39:20,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1933/2180 [5:07:01<39:13,  9.53s/it]                                                     {'loss': 2.0511, 'learning_rate': 3.330749328335414e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1933/2180 [5:07:01<39:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1934/2180 [5:07:11<39:03,  9.52s/it]                                                     {'loss': 2.0067, 'learning_rate': 3.304134738790659e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1934/2180 [5:07:11<39:03,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1935/2180 [5:07:21<38:55,  9.53s/it]                                                     {'loss': 1.9579, 'learning_rate': 3.277623275234953e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1935/2180 [5:07:21<38:55,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1936/2180 [5:07:30<38:46,  9.53s/it]                                                     {'loss': 2.0109, 'learning_rate': 3.2512149962177994e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1936/2180 [5:07:30<38:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1937/2180 [5:07:40<38:34,  9.53s/it]                                                     {'loss': 2.0693, 'learning_rate': 3.224909960060851e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1937/2180 [5:07:40<38:34,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1938/2180 [5:07:49<38:24,  9.52s/it]                                                     {'loss': 2.0719, 'learning_rate': 3.198708224857755e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1938/2180 [5:07:49<38:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1939/2180 [5:07:59<38:23,  9.56s/it]                                                     {'loss': 2.0656, 'learning_rate': 3.172609848474023e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1939/2180 [5:07:59<38:23,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1940/2180 [5:08:08<38:11,  9.55s/it]                                                     {'loss': 1.9936, 'learning_rate': 3.1466148885468895e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1940/2180 [5:08:08<38:11,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1941/2180 [5:08:18<38:05,  9.56s/it]                                                     {'loss': 1.9332, 'learning_rate': 3.120723402485198e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1941/2180 [5:08:18<38:05,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1942/2180 [5:08:27<37:52,  9.55s/it]                                                     {'loss': 1.9899, 'learning_rate': 3.094935447469294e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1942/2180 [5:08:27<37:52,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1943/2180 [5:08:37<37:43,  9.55s/it]                                                     {'loss': 2.0442, 'learning_rate': 3.069251080450863e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1943/2180 [5:08:37<37:43,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1944/2180 [5:08:47<37:38,  9.57s/it]                                                     {'loss': 2.0084, 'learning_rate': 3.0436703581528113e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1944/2180 [5:08:47<37:38,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1945/2180 [5:08:56<37:28,  9.57s/it]                                                     {'loss': 2.0194, 'learning_rate': 3.0181933370691694e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1945/2180 [5:08:56<37:28,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1946/2180 [5:09:06<37:13,  9.55s/it]                                                     {'loss': 2.0669, 'learning_rate': 2.9928200734649523e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1946/2180 [5:09:06<37:13,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1947/2180 [5:09:15<37:04,  9.55s/it]                                                     {'loss': 1.9115, 'learning_rate': 2.9675506233760142e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1947/2180 [5:09:15<37:04,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1948/2180 [5:09:25<36:49,  9.53s/it]                                                     {'loss': 2.016, 'learning_rate': 2.942385042608925e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1948/2180 [5:09:25<36:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1949/2180 [5:09:34<36:42,  9.54s/it]                                                     {'loss': 1.9654, 'learning_rate': 2.9173233867409054e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1949/2180 [5:09:34<36:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1950/2180 [5:09:44<36:47,  9.60s/it]                                                     {'loss': 1.9883, 'learning_rate': 2.892365711119638e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1950/2180 [5:09:44<36:47,  9.60s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1951/2180 [5:09:53<36:31,  9.57s/it]                                                     {'loss': 2.0828, 'learning_rate': 2.8675120708631596e-05, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1951/2180 [5:09:53<36:31,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1952/2180 [5:10:03<36:21,  9.57s/it]                                                     {'loss': 2.0212, 'learning_rate': 2.8427625208597764e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1952/2180 [5:10:03<36:21,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1953/2180 [5:10:12<36:06,  9.54s/it]                                                     {'loss': 1.9398, 'learning_rate': 2.8181171157678874e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1953/2180 [5:10:12<36:06,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1954/2180 [5:10:22<35:56,  9.54s/it]                                                     {'loss': 2.0918, 'learning_rate': 2.7935759100159053e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1954/2180 [5:10:22<35:56,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1955/2180 [5:10:31<35:44,  9.53s/it]                                                     {'loss': 2.0154, 'learning_rate': 2.7691389578021365e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1955/2180 [5:10:32<35:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1956/2180 [5:10:41<35:33,  9.52s/it]                                                     {'loss': 2.022, 'learning_rate': 2.7448063130946223e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1956/2180 [5:10:41<35:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1957/2180 [5:10:50<35:19,  9.50s/it]                                                     {'loss': 2.0046, 'learning_rate': 2.7205780296310544e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1957/2180 [5:10:50<35:19,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1958/2180 [5:11:00<35:10,  9.50s/it]                                                     {'loss': 2.1429, 'learning_rate': 2.6964541609186378e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1958/2180 [5:11:00<35:10,  9.50s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1959/2180 [5:11:10<35:05,  9.53s/it]                                                     {'loss': 2.0994, 'learning_rate': 2.6724347602340104e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1959/2180 [5:11:10<35:05,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1960/2180 [5:11:19<34:56,  9.53s/it]                                                     {'loss': 2.0503, 'learning_rate': 2.6485198806230682e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1960/2180 [5:11:19<34:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1961/2180 [5:11:29<34:46,  9.53s/it]                                                     {'loss': 2.055, 'learning_rate': 2.6247095749008797e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1961/2180 [5:11:29<34:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1962/2180 [5:11:38<34:35,  9.52s/it]                                                     {'loss': 2.0536, 'learning_rate': 2.6010038956515826e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1962/2180 [5:11:38<34:35,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1963/2180 [5:11:48<34:23,  9.51s/it]                                                     {'loss': 2.0471, 'learning_rate': 2.5774028952282423e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1963/2180 [5:11:48<34:23,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1964/2180 [5:11:57<34:16,  9.52s/it]                                                     {'loss': 2.0432, 'learning_rate': 2.5539066257527277e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1964/2180 [5:11:57<34:16,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1965/2180 [5:12:07<34:05,  9.51s/it]                                                     {'loss': 2.0237, 'learning_rate': 2.530515139115652e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1965/2180 [5:12:07<34:05,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1966/2180 [5:12:16<33:56,  9.52s/it]                                                     {'loss': 2.0349, 'learning_rate': 2.5072284869761874e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1966/2180 [5:12:16<33:56,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1967/2180 [5:12:26<33:55,  9.55s/it]                                                     {'loss': 1.9406, 'learning_rate': 2.4840467207619786e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1967/2180 [5:12:26<33:55,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1968/2180 [5:12:35<33:45,  9.55s/it]                                                     {'loss': 2.0463, 'learning_rate': 2.460969891669068e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1968/2180 [5:12:35<33:45,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1969/2180 [5:12:45<33:33,  9.54s/it]                                                     {'loss': 2.0032, 'learning_rate': 2.4379980506617272e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1969/2180 [5:12:45<33:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1970/2180 [5:12:54<33:23,  9.54s/it]                                                     {'loss': 1.9949, 'learning_rate': 2.4151312484723464e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1970/2180 [5:12:54<33:23,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1971/2180 [5:13:04<33:13,  9.54s/it]                                                     {'loss': 2.0122, 'learning_rate': 2.3923695356013798e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1971/2180 [5:13:04<33:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1972/2180 [5:13:13<33:02,  9.53s/it]                                                     {'loss': 1.9428, 'learning_rate': 2.3697129623171833e-05, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1972/2180 [5:13:13<33:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1973/2180 [5:13:23<32:52,  9.53s/it]                                                     {'loss': 2.0355, 'learning_rate': 2.3471615786559042e-05, 'epoch': 0.9}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1973/2180 [5:13:23<32:52,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1974/2180 [5:13:32<32:40,  9.52s/it]                                                     {'loss': 2.0826, 'learning_rate': 2.3247154344213818e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1974/2180 [5:13:32<32:40,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1975/2180 [5:13:42<32:33,  9.53s/it]                                                     {'loss': 2.1187, 'learning_rate': 2.3023745791850625e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1975/2180 [5:13:42<32:33,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1976/2180 [5:13:52<32:23,  9.53s/it]                                                     {'loss': 2.0786, 'learning_rate': 2.2801390622858354e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1976/2180 [5:13:52<32:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1977/2180 [5:14:01<32:13,  9.53s/it]                                                     {'loss': 2.1064, 'learning_rate': 2.2580089328299746e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1977/2180 [5:14:01<32:13,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1978/2180 [5:14:11<32:04,  9.53s/it]                                                     {'loss': 2.0181, 'learning_rate': 2.235984239690997e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1978/2180 [5:14:11<32:04,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1979/2180 [5:14:20<31:52,  9.51s/it]                                                     {'loss': 1.9584, 'learning_rate': 2.2140650315095934e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1979/2180 [5:14:20<31:52,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1980/2180 [5:14:30<31:43,  9.52s/it]                                                     {'loss': 2.0042, 'learning_rate': 2.192251356693459e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1980/2180 [5:14:30<31:43,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1981/2180 [5:14:39<31:36,  9.53s/it]                                                     {'loss': 2.0107, 'learning_rate': 2.170543263417246e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1981/2180 [5:14:39<31:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1982/2180 [5:14:49<31:31,  9.55s/it]                                                     {'loss': 2.0315, 'learning_rate': 2.1489407996224286e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1982/2180 [5:14:49<31:31,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1983/2180 [5:14:58<31:29,  9.59s/it]                                                     {'loss': 2.1082, 'learning_rate': 2.127444013017199e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1983/2180 [5:14:58<31:29,  9.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1984/2180 [5:15:08<31:17,  9.58s/it]                                                     {'loss': 1.9746, 'learning_rate': 2.1060529510763648e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1984/2180 [5:15:08<31:17,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1985/2180 [5:15:17<31:01,  9.55s/it]                                                     {'loss': 2.0243, 'learning_rate': 2.084767661041259e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1985/2180 [5:15:17<31:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1986/2180 [5:15:27<30:53,  9.55s/it]                                                     {'loss': 2.0256, 'learning_rate': 2.063588189919596e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1986/2180 [5:15:27<30:53,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1987/2180 [5:15:37<30:41,  9.54s/it]                                                     {'loss': 2.012, 'learning_rate': 2.0425145844854275e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1987/2180 [5:15:37<30:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1988/2180 [5:15:46<30:32,  9.55s/it]                                                     {'loss': 2.0603, 'learning_rate': 2.0215468912789693e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1988/2180 [5:15:46<30:32,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1989/2180 [5:15:56<30:22,  9.54s/it]                                                     {'loss': 2.0863, 'learning_rate': 2.0006851566065575e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1989/2180 [5:15:56<30:22,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1990/2180 [5:16:05<30:11,  9.53s/it]                                                     {'loss': 1.9793, 'learning_rate': 1.9799294265405166e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1990/2180 [5:16:05<30:11,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1991/2180 [5:16:15<30:03,  9.54s/it]                                                     {'loss': 2.0732, 'learning_rate': 1.9592797469190572e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1991/2180 [5:16:15<30:03,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1992/2180 [5:16:24<29:58,  9.57s/it]                                                     {'loss': 1.9632, 'learning_rate': 1.938736163346194e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1992/2180 [5:16:24<29:58,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1993/2180 [5:16:34<29:48,  9.56s/it]                                                     {'loss': 2.0738, 'learning_rate': 1.9182987211916246e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1993/2180 [5:16:34<29:48,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1994/2180 [5:16:43<29:37,  9.55s/it]                                                     {'loss': 2.0617, 'learning_rate': 1.8979674655906332e-05, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1994/2180 [5:16:43<29:37,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1995/2180 [5:16:53<29:31,  9.57s/it]                                                     {'loss': 2.0857, 'learning_rate': 1.8777424414440024e-05, 'epoch': 0.91}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1995/2180 [5:16:53<29:31,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1996/2180 [5:17:03<29:17,  9.55s/it]                                                     {'loss': 2.0663, 'learning_rate': 1.8576236934179202e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1996/2180 [5:17:03<29:17,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1997/2180 [5:17:12<29:08,  9.55s/it]                                                     {'loss': 1.9957, 'learning_rate': 1.8376112659438393e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1997/2180 [5:17:12<29:08,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1998/2180 [5:17:22<28:56,  9.54s/it]                                                     {'loss': 1.9964, 'learning_rate': 1.8177052032184282e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1998/2180 [5:17:22<28:56,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1999/2180 [5:17:31<28:44,  9.53s/it]                                                     {'loss': 2.0031, 'learning_rate': 1.7979055492034435e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1999/2180 [5:17:31<28:44,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2000/2180 [5:17:41<28:34,  9.52s/it]                                                     {'loss': 2.0121, 'learning_rate': 1.7782123476256407e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2000/2180 [5:17:41<28:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2001/2180 [5:17:50<28:24,  9.52s/it]                                                     {'loss': 1.9692, 'learning_rate': 1.7586256419766965e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2001/2180 [5:17:50<28:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2002/2180 [5:18:00<28:17,  9.53s/it]                                                     {'loss': 1.9387, 'learning_rate': 1.7391454755130766e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2002/2180 [5:18:00<28:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2003/2180 [5:18:09<28:08,  9.54s/it]                                                     {'loss': 2.0321, 'learning_rate': 1.7197718912559557e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2003/2180 [5:18:09<28:08,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2004/2180 [5:18:19<28:00,  9.55s/it]                                                     {'loss': 2.0159, 'learning_rate': 1.700504931991148e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2004/2180 [5:18:19<28:00,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2005/2180 [5:18:28<27:50,  9.54s/it]                                                     {'loss': 2.0051, 'learning_rate': 1.681344640268978e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2005/2180 [5:18:28<27:50,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2006/2180 [5:18:38<27:37,  9.53s/it]                                                     {'loss': 1.9923, 'learning_rate': 1.6622910584041974e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2006/2180 [5:18:38<27:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2007/2180 [5:18:47<27:26,  9.52s/it]                                                     {'loss': 2.0036, 'learning_rate': 1.6433442284758903e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2007/2180 [5:18:47<27:26,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2008/2180 [5:18:57<27:17,  9.52s/it]                                                     {'loss': 2.0479, 'learning_rate': 1.624504192327392e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2008/2180 [5:18:57<27:17,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2009/2180 [5:19:06<27:07,  9.52s/it]                                                     {'loss': 1.9294, 'learning_rate': 1.6057709915661856e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2009/2180 [5:19:06<27:07,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2010/2180 [5:19:16<27:08,  9.58s/it]                                                     {'loss': 2.0243, 'learning_rate': 1.5871446675638057e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2010/2180 [5:19:16<27:08,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2011/2180 [5:19:26<26:54,  9.55s/it]                                                     {'loss': 2.0387, 'learning_rate': 1.5686252614557638e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2011/2180 [5:19:26<26:54,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2012/2180 [5:19:35<26:44,  9.55s/it]                                                     {'loss': 1.8897, 'learning_rate': 1.5502128141414497e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2012/2180 [5:19:35<26:44,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2013/2180 [5:19:45<26:33,  9.54s/it]                                                     {'loss': 2.1023, 'learning_rate': 1.5319073662840188e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2013/2180 [5:19:45<26:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2014/2180 [5:19:54<26:25,  9.55s/it]                                                     {'loss': 1.9939, 'learning_rate': 1.5137089583103391e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2014/2180 [5:19:54<26:25,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2015/2180 [5:20:04<26:15,  9.55s/it]                                                     {'loss': 1.9954, 'learning_rate': 1.4956176304108893e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2015/2180 [5:20:04<26:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2016/2180 [5:20:13<26:03,  9.53s/it]                                                     {'loss': 2.015, 'learning_rate': 1.4776334225396481e-05, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2016/2180 [5:20:13<26:03,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2017/2180 [5:20:23<25:53,  9.53s/it]                                                     {'loss': 2.0413, 'learning_rate': 1.4597563744140397e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2017/2180 [5:20:23<25:53,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2018/2180 [5:20:32<25:41,  9.51s/it]                                                     {'loss': 2.1176, 'learning_rate': 1.4419865255148269e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2018/2180 [5:20:32<25:41,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2019/2180 [5:20:42<25:32,  9.52s/it]                                                     {'loss': 2.0906, 'learning_rate': 1.4243239150860122e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2019/2180 [5:20:42<25:32,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2020/2180 [5:20:51<25:23,  9.52s/it]                                                     {'loss': 2.0372, 'learning_rate': 1.4067685821347932e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2020/2180 [5:20:51<25:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2021/2180 [5:21:01<25:12,  9.52s/it]                                                     {'loss': 2.0163, 'learning_rate': 1.389320565431429e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2021/2180 [5:21:01<25:12,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2022/2180 [5:21:10<25:07,  9.54s/it]                                                     {'loss': 2.0289, 'learning_rate': 1.3719799035091851e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2022/2180 [5:21:10<25:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2023/2180 [5:21:20<24:56,  9.53s/it]                                                     {'loss': 1.9741, 'learning_rate': 1.3547466346642278e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2023/2180 [5:21:20<24:56,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2024/2180 [5:21:29<24:46,  9.53s/it]                                                     {'loss': 2.0182, 'learning_rate': 1.3376207969555577e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2024/2180 [5:21:29<24:46,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2025/2180 [5:21:39<24:37,  9.53s/it]                                                     {'loss': 2.0315, 'learning_rate': 1.32060242820492e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2025/2180 [5:21:39<24:37,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2026/2180 [5:21:49<24:27,  9.53s/it]                                                     {'loss': 2.0067, 'learning_rate': 1.3036915659967118e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2026/2180 [5:21:49<24:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2027/2180 [5:21:58<24:17,  9.53s/it]                                                     {'loss': 2.0279, 'learning_rate': 1.2868882476779087e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2027/2180 [5:21:58<24:17,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2028/2180 [5:22:08<24:12,  9.55s/it]                                                     {'loss': 1.9642, 'learning_rate': 1.2701925103579815e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2028/2180 [5:22:08<24:12,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2029/2180 [5:22:17<24:04,  9.57s/it]                                                     {'loss': 2.0092, 'learning_rate': 1.2536043909088191e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2029/2180 [5:22:17<24:04,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2030/2180 [5:22:27<23:51,  9.55s/it]                                                     {'loss': 1.9853, 'learning_rate': 1.2371239259646228e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2030/2180 [5:22:27<23:51,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2031/2180 [5:22:36<23:41,  9.54s/it]                                                     {'loss': 2.0094, 'learning_rate': 1.2207511519218672e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2031/2180 [5:22:36<23:41,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2032/2180 [5:22:46<23:30,  9.53s/it]                                                     {'loss': 1.9766, 'learning_rate': 1.2044861049391676e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2032/2180 [5:22:46<23:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2033/2180 [5:22:55<23:21,  9.54s/it]                                                     {'loss': 2.0695, 'learning_rate': 1.1883288209372512e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2033/2180 [5:22:55<23:21,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2034/2180 [5:23:05<23:14,  9.55s/it]                                                     {'loss': 2.0314, 'learning_rate': 1.1722793355988471e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2034/2180 [5:23:05<23:14,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2035/2180 [5:23:14<23:04,  9.55s/it]                                                     {'loss': 1.9968, 'learning_rate': 1.1563376843686135e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2035/2180 [5:23:14<23:04,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2036/2180 [5:23:24<22:54,  9.55s/it]                                                     {'loss': 1.9823, 'learning_rate': 1.140503902453055e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2036/2180 [5:23:24<22:54,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2037/2180 [5:23:34<22:46,  9.55s/it]                                                     {'loss': 2.0965, 'learning_rate': 1.1247780248204665e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2037/2180 [5:23:34<22:46,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2038/2180 [5:23:43<22:34,  9.54s/it]                                                     {'loss': 2.0027, 'learning_rate': 1.1091600862008333e-05, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2038/2180 [5:23:43<22:34,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2039/2180 [5:23:53<22:25,  9.54s/it]                                                     {'loss': 2.0296, 'learning_rate': 1.0936501210857652e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2039/2180 [5:23:53<22:25,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2040/2180 [5:24:02<22:16,  9.54s/it]                                                     {'loss': 1.9835, 'learning_rate': 1.0782481637284013e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2040/2180 [5:24:02<22:16,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2041/2180 [5:24:12<22:05,  9.54s/it]                                                     {'loss': 1.9954, 'learning_rate': 1.0629542481433663e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2041/2180 [5:24:12<22:05,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2042/2180 [5:24:21<21:56,  9.54s/it]                                                     {'loss': 1.9969, 'learning_rate': 1.0477684081066751e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2042/2180 [5:24:21<21:56,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2043/2180 [5:24:31<21:47,  9.55s/it]                                                     {'loss': 1.9731, 'learning_rate': 1.0326906771556566e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2043/2180 [5:24:31<21:47,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2044/2180 [5:24:40<21:38,  9.55s/it]                                                     {'loss': 2.0412, 'learning_rate': 1.017721088588891e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2044/2180 [5:24:40<21:38,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2045/2180 [5:24:50<21:29,  9.55s/it]                                                     {'loss': 2.0494, 'learning_rate': 1.0028596754661334e-05, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2045/2180 [5:24:50<21:29,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2046/2180 [5:24:59<21:19,  9.55s/it]                                                     {'loss': 2.0302, 'learning_rate': 9.881064706082298e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2046/2180 [5:24:59<21:19,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2047/2180 [5:25:09<21:09,  9.55s/it]                                                     {'loss': 2.0278, 'learning_rate': 9.734615065970454e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2047/2180 [5:25:09<21:09,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2048/2180 [5:25:19<20:58,  9.53s/it]                                                     {'loss': 1.9949, 'learning_rate': 9.58924815775425e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2048/2180 [5:25:19<20:58,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2049/2180 [5:25:28<20:50,  9.54s/it]                                                     {'loss': 2.1259, 'learning_rate': 9.444964302470715e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2049/2180 [5:25:28<20:50,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2180 [5:25:38<20:42,  9.55s/it]                                                     {'loss': 1.98, 'learning_rate': 9.301763818765018e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2180 [5:25:38<20:42,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2051/2180 [5:25:47<20:30,  9.54s/it]                                                     {'loss': 1.9481, 'learning_rate': 9.15964702288996e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2051/2180 [5:25:47<20:30,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2052/2180 [5:25:57<20:23,  9.56s/it]                                                     {'loss': 2.1141, 'learning_rate': 9.018614228704925e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2052/2180 [5:25:57<20:23,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2053/2180 [5:26:06<20:12,  9.55s/it]                                                     {'loss': 2.0069, 'learning_rate': 8.878665747675152e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2053/2180 [5:26:06<20:12,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2054/2180 [5:26:16<20:00,  9.53s/it]                                                     {'loss': 2.0896, 'learning_rate': 8.739801888871469e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2054/2180 [5:26:16<20:00,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2055/2180 [5:26:25<19:51,  9.53s/it]                                                     {'loss': 2.0399, 'learning_rate': 8.602022958969336e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2055/2180 [5:26:25<19:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2056/2180 [5:26:35<19:41,  9.53s/it]                                                     {'loss': 1.9931, 'learning_rate': 8.465329262248078e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2056/2180 [5:26:35<19:41,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2057/2180 [5:26:44<19:35,  9.56s/it]                                                     {'loss': 2.0569, 'learning_rate': 8.32972110059027e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2057/2180 [5:26:44<19:35,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2058/2180 [5:26:54<19:30,  9.59s/it]                                                     {'loss': 1.9943, 'learning_rate': 8.195198773481406e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2058/2180 [5:26:54<19:30,  9.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2059/2180 [5:27:04<19:18,  9.57s/it]                                                     {'loss': 2.0938, 'learning_rate': 8.061762578008613e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2059/2180 [5:27:04<19:18,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2180 [5:27:13<19:07,  9.56s/it]                                                     {'loss': 2.0207, 'learning_rate': 7.929412808860559e-06, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2180 [5:27:13<19:07,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2061/2180 [5:27:23<18:56,  9.55s/it]                                                     {'loss': 2.0619, 'learning_rate': 7.79814975832649e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2061/2180 [5:27:23<18:56,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2062/2180 [5:27:32<18:45,  9.54s/it]                                                     {'loss': 1.9702, 'learning_rate': 7.667973716295851e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2062/2180 [5:27:32<18:45,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2063/2180 [5:27:42<18:34,  9.52s/it]                                                     {'loss': 2.0533, 'learning_rate': 7.5388849702571205e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2063/2180 [5:27:42<18:34,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2064/2180 [5:27:51<18:24,  9.52s/it]                                                     {'loss': 1.9436, 'learning_rate': 7.4108838052979185e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2064/2180 [5:27:51<18:24,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2065/2180 [5:28:01<18:15,  9.53s/it]                                                     {'loss': 2.0005, 'learning_rate': 7.283970504103732e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2065/2180 [5:28:01<18:15,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2066/2180 [5:28:10<18:06,  9.53s/it]                                                     {'loss': 2.0542, 'learning_rate': 7.1581453469575785e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2066/2180 [5:28:10<18:06,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2067/2180 [5:28:20<18:00,  9.56s/it]                                                     {'loss': 1.9611, 'learning_rate': 7.033408611739456e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2067/2180 [5:28:20<18:00,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2068/2180 [5:28:30<17:51,  9.57s/it]                                                     {'loss': 2.0085, 'learning_rate': 6.909760573925561e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2068/2180 [5:28:30<17:51,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2069/2180 [5:28:39<17:39,  9.55s/it]                                                     {'loss': 1.9606, 'learning_rate': 6.787201506587626e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2069/2180 [5:28:39<17:39,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2070/2180 [5:28:49<17:30,  9.55s/it]                                                     {'loss': 1.9865, 'learning_rate': 6.66573168039264e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2070/2180 [5:28:49<17:30,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2071/2180 [5:28:58<17:19,  9.53s/it]                                                     {'loss': 2.1648, 'learning_rate': 6.545351363601959e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2071/2180 [5:28:58<17:19,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2072/2180 [5:29:08<17:10,  9.54s/it]                                                     {'loss': 2.0364, 'learning_rate': 6.426060822070812e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2072/2180 [5:29:08<17:10,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2073/2180 [5:29:17<16:59,  9.53s/it]                                                     {'loss': 2.0578, 'learning_rate': 6.3078603192475716e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2073/2180 [5:29:17<16:59,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2074/2180 [5:29:27<16:50,  9.54s/it]                                                     {'loss': 2.0375, 'learning_rate': 6.1907501161735934e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2074/2180 [5:29:27<16:50,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2075/2180 [5:29:36<16:40,  9.53s/it]                                                     {'loss': 2.0132, 'learning_rate': 6.074730471482049e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2075/2180 [5:29:36<16:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2076/2180 [5:29:46<16:30,  9.52s/it]                                                     {'loss': 2.0447, 'learning_rate': 5.959801641397755e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2076/2180 [5:29:46<16:30,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2077/2180 [5:29:55<16:21,  9.53s/it]                                                     {'loss': 2.0741, 'learning_rate': 5.845963879736627e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2077/2180 [5:29:55<16:21,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2078/2180 [5:30:05<16:11,  9.53s/it]                                                     {'loss': 2.0896, 'learning_rate': 5.733217437904892e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2078/2180 [5:30:05<16:11,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2079/2180 [5:30:14<16:01,  9.52s/it]                                                     {'loss': 1.9993, 'learning_rate': 5.621562564898597e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2079/2180 [5:30:14<16:01,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2080/2180 [5:30:24<15:53,  9.54s/it]                                                     {'loss': 1.9938, 'learning_rate': 5.51099950730316e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2080/2180 [5:30:24<15:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2081/2180 [5:30:33<15:42,  9.52s/it]                                                     {'loss': 2.0796, 'learning_rate': 5.401528509292763e-06, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2081/2180 [5:30:33<15:42,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2082/2180 [5:30:43<15:33,  9.52s/it]                                                     {'loss': 2.0655, 'learning_rate': 5.2931498126298495e-06, 'epoch': 0.95}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2082/2180 [5:30:43<15:33,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])



Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2083/2180 [5:30:52<15:23,  9.52s/it]                                                     {'loss': 2.0852, 'learning_rate': 5.1858636566645135e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2083/2180 [5:30:52<15:23,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2084/2180 [5:31:02<15:13,  9.52s/it]                                                     {'loss': 2.0723, 'learning_rate': 5.0796702783340035e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2084/2180 [5:31:02<15:13,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2085/2180 [5:31:11<15:04,  9.52s/it]                                                     {'loss': 2.0118, 'learning_rate': 4.97456991216233e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2085/2180 [5:31:11<15:04,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2086/2180 [5:31:21<14:54,  9.51s/it]                                                     {'loss': 2.0513, 'learning_rate': 4.870562790259325e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2086/2180 [5:31:21<14:54,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2087/2180 [5:31:30<14:45,  9.52s/it]                                                     {'loss': 1.9319, 'learning_rate': 4.7676491423208625e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2087/2180 [5:31:30<14:45,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2088/2180 [5:31:40<14:34,  9.51s/it]                                                     {'loss': 2.0299, 'learning_rate': 4.66582919562758e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2088/2180 [5:31:40<14:34,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2089/2180 [5:31:49<14:25,  9.51s/it]                                                     {'loss': 1.931, 'learning_rate': 4.5651031750448825e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2089/2180 [5:31:49<14:25,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2090/2180 [5:31:59<14:15,  9.51s/it]                                                     {'loss': 2.0375, 'learning_rate': 4.465471303022217e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2090/2180 [5:31:59<14:15,  9.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2091/2180 [5:32:09<14:08,  9.53s/it]                                                     {'loss': 2.1097, 'learning_rate': 4.366933799592743e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2091/2180 [5:32:09<14:08,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2092/2180 [5:32:18<13:59,  9.54s/it]                                                     {'loss': 2.0577, 'learning_rate': 4.269490882372551e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2092/2180 [5:32:18<13:59,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2093/2180 [5:32:28<13:52,  9.56s/it]                                                     {'loss': 1.987, 'learning_rate': 4.1731427665606115e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2093/2180 [5:32:28<13:52,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2094/2180 [5:32:37<13:40,  9.54s/it]                                                     {'loss': 2.0495, 'learning_rate': 4.077889664937884e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2094/2180 [5:32:37<13:40,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2095/2180 [5:32:47<13:33,  9.57s/it]                                                     {'loss': 2.0581, 'learning_rate': 3.983731787867207e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2095/2180 [5:32:47<13:33,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2096/2180 [5:32:56<13:22,  9.56s/it]                                                     {'loss': 2.1051, 'learning_rate': 3.890669343292464e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2096/2180 [5:32:56<13:22,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2097/2180 [5:33:06<13:11,  9.54s/it]                                                     {'loss': 1.9834, 'learning_rate': 3.7987025367384743e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2097/2180 [5:33:06<13:11,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2098/2180 [5:33:15<13:00,  9.52s/it]                                                     {'loss': 2.0998, 'learning_rate': 3.707831571310327e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2098/2180 [5:33:15<13:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2099/2180 [5:33:25<12:52,  9.54s/it]                                                     {'loss': 1.936, 'learning_rate': 3.6180566476929912e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2099/2180 [5:33:25<12:52,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2100/2180 [5:33:35<12:43,  9.54s/it]                                                     {'loss': 1.9863, 'learning_rate': 3.529377964150815e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2100/2180 [5:33:35<12:43,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2101/2180 [5:33:44<12:33,  9.54s/it]                                                     {'loss': 2.1218, 'learning_rate': 3.441795716527307e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2101/2180 [5:33:44<12:33,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2102/2180 [5:33:54<12:23,  9.53s/it]                                                     {'loss': 2.0034, 'learning_rate': 3.355310098244302e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2102/2180 [5:33:54<12:23,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2103/2180 [5:34:03<12:15,  9.55s/it]                                                     {'loss': 2.0797, 'learning_rate': 3.269921300301959e-06, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2103/2180 [5:34:03<12:15,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2104/2180 [5:34:13<12:05,  9.55s/it]                                                     {'loss': 1.9963, 'learning_rate': 3.1856295112780988e-06, 'epoch': 0.96}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2104/2180 [5:34:13<12:05,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2105/2180 [5:34:22<11:56,  9.56s/it]                                                     {'loss': 2.0011, 'learning_rate': 3.102434917327812e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2105/2180 [5:34:22<11:56,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2106/2180 [5:34:32<11:46,  9.55s/it]                                                     {'loss': 2.0013, 'learning_rate': 3.0203377021831292e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2106/2180 [5:34:32<11:46,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2107/2180 [5:34:41<11:36,  9.54s/it]                                                     {'loss': 1.9882, 'learning_rate': 2.939338047152573e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2107/2180 [5:34:41<11:36,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2108/2180 [5:34:51<11:28,  9.56s/it]                                                     {'loss': 2.0423, 'learning_rate': 2.8594361311206073e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2108/2180 [5:34:51<11:28,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2109/2180 [5:35:00<11:17,  9.55s/it]                                                     {'loss': 1.9827, 'learning_rate': 2.7806321305475225e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2109/2180 [5:35:00<11:17,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2110/2180 [5:35:10<11:07,  9.54s/it]                                                     {'loss': 2.0228, 'learning_rate': 2.7029262194688818e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2110/2180 [5:35:10<11:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2111/2180 [5:35:19<10:57,  9.53s/it]                                                     {'loss': 1.9793, 'learning_rate': 2.626318569495134e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2111/2180 [5:35:19<10:57,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2112/2180 [5:35:29<10:48,  9.53s/it]                                                     {'loss': 1.9958, 'learning_rate': 2.550809349811334e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2112/2180 [5:35:29<10:48,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2113/2180 [5:35:39<10:38,  9.53s/it]                                                     {'loss': 1.9549, 'learning_rate': 2.476398727176532e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2113/2180 [5:35:39<10:38,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2114/2180 [5:35:48<10:28,  9.53s/it]                                                     {'loss': 1.901, 'learning_rate': 2.4030868659237204e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2114/2180 [5:35:48<10:28,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2115/2180 [5:35:58<10:21,  9.57s/it]                                                     {'loss': 2.0169, 'learning_rate': 2.3308739279593317e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2115/2180 [5:35:58<10:21,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2116/2180 [5:36:07<10:12,  9.56s/it]                                                     {'loss': 1.9862, 'learning_rate': 2.2597600727626845e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2116/2180 [5:36:07<10:12,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2117/2180 [5:36:17<10:01,  9.55s/it]                                                     {'loss': 1.9597, 'learning_rate': 2.1897454573860387e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2117/2180 [5:36:17<10:01,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2118/2180 [5:36:26<09:52,  9.55s/it]                                                     {'loss': 1.9909, 'learning_rate': 2.1208302364538746e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2118/2180 [5:36:26<09:52,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2119/2180 [5:36:36<09:42,  9.54s/it]                                                     {'loss': 1.9758, 'learning_rate': 2.0530145621627804e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2119/2180 [5:36:36<09:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2120/2180 [5:36:45<09:32,  9.54s/it]                                                     {'loss': 2.0665, 'learning_rate': 1.9862985842810653e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2120/2180 [5:36:45<09:32,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2121/2180 [5:36:55<09:23,  9.56s/it]                                                     {'loss': 1.995, 'learning_rate': 1.920682450148259e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2121/2180 [5:36:55<09:23,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2122/2180 [5:37:04<09:13,  9.54s/it]                                                     {'loss': 1.9947, 'learning_rate': 1.856166304675111e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2122/2180 [5:37:04<09:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2123/2180 [5:37:14<09:02,  9.53s/it]                                                     {'loss': 2.0558, 'learning_rate': 1.792750290342926e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2123/2180 [5:37:14<09:02,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2124/2180 [5:37:24<08:54,  9.54s/it]                                                     {'loss': 2.0159, 'learning_rate': 1.7304345472035632e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2124/2180 [5:37:24<08:54,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2125/2180 [5:37:33<08:44,  9.54s/it]                                                     {'loss': 2.0084, 'learning_rate': 1.6692192128788253e-06, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2125/2180 [5:37:33<08:44,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2126/2180 [5:37:43<08:35,  9.56s/it]                                                     {'loss': 2.0325, 'learning_rate': 1.6091044225604035e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2126/2180 [5:37:43<08:35,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2127/2180 [5:37:52<08:26,  9.55s/it]                                                     {'loss': 2.003, 'learning_rate': 1.5500903090094888e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2127/2180 [5:37:52<08:26,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2128/2180 [5:38:02<08:16,  9.55s/it]                                                     {'loss': 2.0356, 'learning_rate': 1.492177002556383e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2128/2180 [5:38:02<08:16,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2129/2180 [5:38:11<08:06,  9.55s/it]                                                     {'loss': 2.0316, 'learning_rate': 1.4353646311004443e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2129/2180 [5:38:11<08:06,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2130/2180 [5:38:21<07:56,  9.54s/it]                                                     {'loss': 1.9645, 'learning_rate': 1.3796533201094752e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2130/2180 [5:38:21<07:56,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2131/2180 [5:38:30<07:48,  9.55s/it]                                                     {'loss': 2.0161, 'learning_rate': 1.3250431926197793e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2131/2180 [5:38:30<07:48,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2132/2180 [5:38:40<07:37,  9.54s/it]                                                     {'loss': 2.0991, 'learning_rate': 1.2715343692356607e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2132/2180 [5:38:40<07:37,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2133/2180 [5:38:49<07:27,  9.53s/it]                                                     {'loss': 2.027, 'learning_rate': 1.2191269681292582e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2133/2180 [5:38:49<07:27,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2134/2180 [5:38:59<07:18,  9.52s/it]                                                     {'loss': 2.0779, 'learning_rate': 1.1678211050402676e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2134/2180 [5:38:59<07:18,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2135/2180 [5:39:08<07:08,  9.52s/it]                                                     {'loss': 2.0568, 'learning_rate': 1.117616893275719e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2135/2180 [5:39:08<07:08,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2136/2180 [5:39:18<06:58,  9.52s/it]                                                     {'loss': 2.0693, 'learning_rate': 1.068514443709534e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2136/2180 [5:39:18<06:58,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2137/2180 [5:39:28<06:49,  9.53s/it]                                                     {'loss': 2.0837, 'learning_rate': 1.0205138647826905e-06, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2137/2180 [5:39:28<06:49,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2138/2180 [5:39:37<06:40,  9.53s/it]                                                     {'loss': 1.9847, 'learning_rate': 9.73615262502503e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2138/2180 [5:39:37<06:40,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2139/2180 [5:39:47<06:30,  9.53s/it]                                                     {'loss': 2.0177, 'learning_rate': 9.278187404426763e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2139/2180 [5:39:47<06:30,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2140/2180 [5:39:56<06:21,  9.55s/it]                                                     {'loss': 1.98, 'learning_rate': 8.831243997431404e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2140/2180 [5:39:56<06:21,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2141/2180 [5:40:06<06:12,  9.55s/it]                                                     {'loss': 2.0757, 'learning_rate': 8.395323391094944e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2141/2180 [5:40:06<06:12,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2142/2180 [5:40:15<06:03,  9.56s/it]                                                     {'loss': 2.0113, 'learning_rate': 7.970426548131183e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2142/2180 [5:40:15<06:03,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2143/2180 [5:40:25<05:53,  9.54s/it]                                                     {'loss': 1.9993, 'learning_rate': 7.556554406908389e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2143/2180 [5:40:25<05:53,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2144/2180 [5:40:34<05:43,  9.55s/it]                                                     {'loss': 2.0855, 'learning_rate': 7.153707881446536e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2144/2180 [5:40:34<05:43,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2145/2180 [5:40:44<05:34,  9.55s/it]                                                     {'loss': 2.006, 'learning_rate': 6.761887861417293e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2145/2180 [5:40:44<05:34,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2146/2180 [5:40:53<05:24,  9.54s/it]                                                     {'loss': 2.0703, 'learning_rate': 6.381095212139032e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2146/2180 [5:40:53<05:24,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2147/2180 [5:41:03<05:15,  9.56s/it]                                                     {'loss': 2.0524, 'learning_rate': 6.011330774577384e-07, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2147/2180 [5:41:03<05:15,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2148/2180 [5:41:13<05:05,  9.55s/it]                                                     {'loss': 2.1508, 'learning_rate': 5.652595365343016e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2148/2180 [5:41:13<05:05,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2149/2180 [5:41:22<04:55,  9.54s/it]                                                     {'loss': 2.0199, 'learning_rate': 5.304889776688859e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2149/2180 [5:41:22<04:55,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2150/2180 [5:41:32<04:45,  9.53s/it]                                                     {'loss': 1.9843, 'learning_rate': 4.968214776508994e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2150/2180 [5:41:32<04:45,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2151/2180 [5:41:41<04:36,  9.53s/it]                                                     {'loss': 1.9686, 'learning_rate': 4.6425711083375454e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2151/2180 [5:41:41<04:36,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2152/2180 [5:41:51<04:26,  9.54s/it]                                                     {'loss': 2.0164, 'learning_rate': 4.3279594913447906e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2152/2180 [5:41:51<04:26,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2153/2180 [5:42:00<04:17,  9.54s/it]                                                     {'loss': 2.0053, 'learning_rate': 4.02438062033883e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2153/2180 [5:42:00<04:17,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2154/2180 [5:42:10<04:07,  9.54s/it]                                                     {'loss': 1.9781, 'learning_rate': 3.7318351657616987e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2154/2180 [5:42:10<04:07,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2155/2180 [5:42:19<03:58,  9.55s/it]                                                     {'loss': 2.0073, 'learning_rate': 3.4503237736882573e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2155/2180 [5:42:19<03:58,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2156/2180 [5:42:29<03:49,  9.54s/it]                                                     {'loss': 2.0768, 'learning_rate': 3.179847065825081e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2156/2180 [5:42:29<03:49,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2157/2180 [5:42:38<03:39,  9.54s/it]                                                     {'loss': 1.885, 'learning_rate': 2.9204056395104594e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2157/2180 [5:42:38<03:39,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2158/2180 [5:42:48<03:29,  9.54s/it]                                                     {'loss': 1.9762, 'learning_rate': 2.672000067709956e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2158/2180 [5:42:48<03:29,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2159/2180 [5:42:57<03:20,  9.53s/it]                                                     {'loss': 2.0541, 'learning_rate': 2.4346308990175204e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2159/2180 [5:42:57<03:20,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2160/2180 [5:43:07<03:10,  9.53s/it]                                                     {'loss': 2.0355, 'learning_rate': 2.208298657653818e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2160/2180 [5:43:07<03:10,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2161/2180 [5:43:16<03:00,  9.52s/it]                                                     {'loss': 2.1314, 'learning_rate': 1.9930038434645692e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2161/2180 [5:43:16<03:00,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2162/2180 [5:43:26<02:51,  9.53s/it]                                                     {'loss': 2.03, 'learning_rate': 1.7887469319205484e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2162/2180 [5:43:26<02:51,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2163/2180 [5:43:36<02:42,  9.54s/it]                                                     {'loss': 1.9943, 'learning_rate': 1.5955283741142523e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2163/2180 [5:43:36<02:42,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2164/2180 [5:43:45<02:32,  9.53s/it]                                                     {'loss': 2.0752, 'learning_rate': 1.4133485967615655e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2164/2180 [5:43:45<02:32,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2165/2180 [5:43:55<02:22,  9.52s/it]                                                     {'loss': 2.1041, 'learning_rate': 1.2422080021995407e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2165/2180 [5:43:55<02:22,  9.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2166/2180 [5:44:04<02:13,  9.54s/it]                                                     {'loss': 1.999, 'learning_rate': 1.082106968385288e-07, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2166/2180 [5:44:04<02:13,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2167/2180 [5:44:14<02:04,  9.55s/it]                                                     {'loss': 2.0898, 'learning_rate': 9.330458488959748e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2167/2180 [5:44:14<02:04,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2168/2180 [5:44:23<01:54,  9.54s/it]                                                     {'loss': 2.0505, 'learning_rate': 7.950249729271608e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2168/2180 [5:44:23<01:54,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2169/2180 [5:44:33<01:44,  9.54s/it]                                                     {'loss': 1.9733, 'learning_rate': 6.680446452922429e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2169/2180 [5:44:33<01:44,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2170/2180 [5:44:42<01:35,  9.54s/it]                                                     {'loss': 1.929, 'learning_rate': 5.521051464230098e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2170/2180 [5:44:42<01:35,  9.54s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2171/2180 [5:44:52<01:25,  9.53s/it]                                                     {'loss': 2.0522, 'learning_rate': 4.4720673236631206e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2171/2180 [5:44:52<01:25,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2172/2180 [5:45:01<01:16,  9.55s/it]                                                     {'loss': 1.9542, 'learning_rate': 3.53349634786837e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2172/2180 [5:45:01<01:16,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2173/2180 [5:45:11<01:06,  9.57s/it]                                                     {'loss': 1.9551, 'learning_rate': 2.7053406096433365e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2173/2180 [5:45:11<01:06,  9.57s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2174/2180 [5:45:21<00:57,  9.55s/it]                                                     {'loss': 1.9962, 'learning_rate': 1.987601937930572e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2174/2180 [5:45:21<00:57,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2175/2180 [5:45:30<00:47,  9.53s/it]                                                     {'loss': 2.1569, 'learning_rate': 1.3802819178398984e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2175/2180 [5:45:30<00:47,  9.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2176/2180 [5:45:40<00:38,  9.55s/it]                                                     {'loss': 2.0056, 'learning_rate': 8.833818906039959e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2176/2180 [5:45:40<00:38,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2177/2180 [5:45:49<00:28,  9.58s/it]                                                     {'loss': 2.1202, 'learning_rate': 4.969029536061598e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2177/2180 [5:45:49<00:28,  9.58s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2178/2180 [5:45:59<00:19,  9.56s/it]                                                     {'loss': 1.9438, 'learning_rate': 2.2084596038030037e-09, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2178/2180 [5:45:59<00:19,  9.56s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2179/2180 [5:46:08<00:09,  9.55s/it]                                                     {'loss': 1.9799, 'learning_rate': 5.521152057763601e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2179/2180 [5:46:08<00:09,  9.55s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2180/2180 [5:46:18<00:00,  9.53s/it]                                                     {'loss': 2.1286, 'learning_rate': 0.0, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2180/2180 [5:46:18<00:00,  9.53s/it]Saving mm adapterSaving mm adapterSaving mm adapter


                                                     {'train_runtime': 20781.9497, 'train_samples_per_second': 26.856, 'train_steps_per_second': 0.105, 'train_loss': 2.183590207023358, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2180/2180 [5:46:18<00:00,  9.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2180/2180 [5:46:18<00:00,  9.53s/it]
Saving mm adapter
Saving resampler
Saving resampler
Saving resampler
Saving resampler
wandb: - 0.047 MB of 0.047 MB uploaded[2024-04-11 08:41:31,516] [INFO] [launch.py:347:main] Process 3108238 exits successfully.
wandb: \ 0.047 MB of 0.047 MB uploaded[2024-04-11 08:41:32,519] [INFO] [launch.py:347:main] Process 3108237 exits successfully.
[2024-04-11 08:41:32,522] [INFO] [launch.py:347:main] Process 3108236 exits successfully.
wandb: | 0.047 MB of 0.047 MB uploadedwandb: / 0.047 MB of 0.047 MB uploadedwandb: - 0.091 MB of 0.512 MB uploaded (0.002 MB deduped)wandb: \ 0.512 MB of 0.512 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                     train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 2180
wandb:            train/learning_rate 0.0
wandb:                     train/loss 2.1286
wandb:               train/total_flos 7.735153662225285e+17
wandb:               train/train_loss 2.18359
wandb:            train/train_runtime 20781.9497
wandb: train/train_samples_per_second 26.856
wandb:   train/train_steps_per_second 0.105
wandb: 
wandb: ðŸš€ View run mve-clip-dino-pretrain-7b at: https://wandb.ai/compyle/multi-ve-llava/runs/7y7bgv1j
wandb: ï¸âš¡ View job at https://wandb.ai/compyle/multi-ve-llava/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NDcyNDExNg==/version_details/v16
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240411_025506-7y7bgv1j/logs
[2024-04-11 08:41:40,531] [INFO] [launch.py:347:main] Process 3108235 exits successfully.
\n================================================= Thu Apr 11 05:09:00 PM UTC 2024 =========================================================\n
[2024-04-11 13:09:02,676] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:05,419] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 13:09:05,419] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 13:09:07,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:09,291] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 13:09:09,291] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 13:09:09,291] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 13:09:09,291] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 13:09:09,291] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 13:09:13,037] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:13,164] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:13,220] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:13,252] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:09:14,470] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:09:14,479] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:09:14,503] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:09:14,503] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 13:09:14,611] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.90s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.46s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.14s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.11s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.12s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.21s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Formatting inputs...Skip in lazy mode
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240411_131033-fdz86fpo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mve-clip-dino-pretrain-7b
wandb: â­ï¸ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: ðŸš€ View run at https://wandb.ai/compyle/multi-ve-llava/runs/fdz86fpo
  0%|          | 0/2180 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2180 [00:21<13:09:03, 21.73s/it]                                                   {'loss': 8.9275, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:21<13:09:03, 21.73s/it]  0%|          | 2/2180 [00:31<8:47:13, 14.52s/it]                                                   {'loss': 8.8822, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:31<8:47:13, 14.52s/it]  0%|          | 3/2180 [00:40<7:24:44, 12.26s/it]                                                  {'loss': 8.078, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [00:40<7:24:44, 12.26s/it]  0%|          | 4/2180 [00:50<6:46:22, 11.21s/it]                                                  {'loss': 6.4173, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [00:50<6:46:22, 11.21s/it]  0%|          | 5/2180 [00:59<6:24:14, 10.60s/it]                                                  {'loss': 5.7278, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [00:59<6:24:14, 10.60s/it]  0%|          | 6/2180 [01:09<6:10:56, 10.24s/it]                                                  {'loss': 5.5634, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [01:09<6:10:56, 10.24s/it]  0%|          | 7/2180 [01:18<6:02:48, 10.02s/it]                                                  {'loss': 5.3408, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2180 [01:18<6:02:48, 10.02s/it]  0%|          | 8/2180 [01:28<5:57:24,  9.87s/it]                                                  {'loss': 5.0063, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2180 [01:28<5:57:24,  9.87s/it]  0%|          | 9/2180 [01:38<5:53:34,  9.77s/it]                                                  {'loss': 4.9254, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2180 [01:38<5:53:34,  9.77s/it]  0%|          | 10/2180 [01:47<5:50:55,  9.70s/it]                                                   {'loss': 4.7708, 'learning_rate': 0.00015151515151515152, 'epoch': 0.0}
  0%|          | 10/2180 [01:47<5:50:55,  9.70s/it]  1%|          | 11/2180 [01:57<5:49:17,  9.66s/it]                                                   {'loss': 4.5568, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 11/2180 [01:57<5:49:17,  9.66s/it]  1%|          | 12/2180 [02:06<5:47:46,  9.62s/it]                                                   {'loss': 4.4421, 'learning_rate': 0.00018181818181818183, 'epoch': 0.01}
  1%|          | 12/2180 [02:06<5:47:46,  9.62s/it]  1%|          | 13/2180 [02:16<5:46:40,  9.60s/it]                                                   {'loss': 4.5041, 'learning_rate': 0.00019696969696969695, 'epoch': 0.01}
  1%|          | 13/2180 [02:16<5:46:40,  9.60s/it]  1%|          | 14/2180 [02:25<5:46:05,  9.59s/it]                                                   {'loss': 4.3565, 'learning_rate': 0.00021212121212121213, 'epoch': 0.01}
  1%|          | 14/2180 [02:25<5:46:05,  9.59s/it]  1%|          | 15/2180 [02:35<5:45:26,  9.57s/it]                                                   {'loss': 4.3149, 'learning_rate': 0.00022727272727272727, 'epoch': 0.01}
  1%|          | 15/2180 [02:35<5:45:26,  9.57s/it]  1%|          | 16/2180 [02:45<5:45:47,  9.59s/it]                                                   {'loss': 4.1934, 'learning_rate': 0.00024242424242424245, 'epoch': 0.01}
  1%|          | 16/2180 [02:45<5:45:47,  9.59s/it]  1%|          | 17/2180 [02:54<5:45:14,  9.58s/it]                                                   {'loss': 4.2785, 'learning_rate': 0.00025757575757575756, 'epoch': 0.01}
  1%|          | 17/2180 [02:54<5:45:14,  9.58s/it]  1%|          | 18/2180 [03:04<5:44:55,  9.57s/it]                                                   {'loss': 4.1637, 'learning_rate': 0.00027272727272727274, 'epoch': 0.01}
  1%|          | 18/2180 [03:04<5:44:55,  9.57s/it]  1%|          | 19/2180 [03:13<5:44:30,  9.57s/it]                                                   {'loss': 4.1601, 'learning_rate': 0.0002878787878787879, 'epoch': 0.01}
  1%|          | 19/2180 [03:13<5:44:30,  9.57s/it]  1%|          | 20/2180 [03:23<5:48:43,  9.69s/it]                                                   {'loss': 3.9722, 'learning_rate': 0.00030303030303030303, 'epoch': 0.01}
  1%|          | 20/2180 [03:23<5:48:43,  9.69s/it]  1%|          | 21/2180 [03:33<5:47:29,  9.66s/it]                                                   {'loss': 4.0697, 'learning_rate': 0.0003181818181818182, 'epoch': 0.01}
  1%|          | 21/2180 [03:33<5:47:29,  9.66s/it]  1%|          | 22/2180 [03:42<5:47:22,  9.66s/it]                                                   {'loss': 3.8787, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 22/2180 [03:42<5:47:22,  9.66s/it]  1%|          | 23/2180 [03:52<5:46:17,  9.63s/it]                                                   {'loss': 3.8043, 'learning_rate': 0.0003484848484848485, 'epoch': 0.01}
  1%|          | 23/2180 [03:52<5:46:17,  9.63s/it]  1%|          | 24/2180 [04:02<5:45:19,  9.61s/it]                                                   {'loss': 3.9836, 'learning_rate': 0.00036363636363636367, 'epoch': 0.01}
  1%|          | 24/2180 [04:02<5:45:19,  9.61s/it]  1%|          | 25/2180 [04:11<5:44:18,  9.59s/it]                                                   {'loss': 3.8619, 'learning_rate': 0.0003787878787878788, 'epoch': 0.01}
  1%|          | 25/2180 [04:11<5:44:18,  9.59s/it]  1%|          | 26/2180 [04:21<5:43:15,  9.56s/it]                                                   {'loss': 3.7466, 'learning_rate': 0.0003939393939393939, 'epoch': 0.01}
  1%|          | 26/2180 [04:21<5:43:15,  9.56s/it]  1%|          | 27/2180 [04:30<5:42:49,  9.55s/it]                                                   {'loss': 3.8625, 'learning_rate': 0.00040909090909090913, 'epoch': 0.01}
  1%|          | 27/2180 [04:30<5:42:49,  9.55s/it]  1%|â–         | 28/2180 [04:40<5:42:35,  9.55s/it]                                                   {'loss': 3.8315, 'learning_rate': 0.00042424242424242425, 'epoch': 0.01}
  1%|â–         | 28/2180 [04:40<5:42:35,  9.55s/it]  1%|â–         | 29/2180 [04:49<5:43:43,  9.59s/it]                                                   {'loss': 3.7254, 'learning_rate': 0.0004393939393939394, 'epoch': 0.01}
  1%|â–         | 29/2180 [04:49<5:43:43,  9.59s/it]  1%|â–         | 30/2180 [04:59<5:42:53,  9.57s/it]                                                   {'loss': 3.7993, 'learning_rate': 0.00045454545454545455, 'epoch': 0.01}
  1%|â–         | 30/2180 [04:59<5:42:53,  9.57s/it]  1%|â–         | 31/2180 [05:08<5:43:08,  9.58s/it]                                                   {'loss': 3.6923, 'learning_rate': 0.0004696969696969697, 'epoch': 0.01}
  1%|â–         | 31/2180 [05:08<5:43:08,  9.58s/it]  1%|â–         | 32/2180 [05:18<5:42:47,  9.58s/it]                                                   {'loss': 3.6358, 'learning_rate': 0.0004848484848484849, 'epoch': 0.01}
  1%|â–         | 32/2180 [05:18<5:42:47,  9.58s/it]  2%|â–         | 33/2180 [05:28<5:42:31,  9.57s/it]                                                   {'loss': 3.5959, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|â–         | 33/2180 [05:28<5:42:31,  9.57s/it]  2%|â–         | 34/2180 [05:37<5:42:13,  9.57s/it]                                                   {'loss': 3.6158, 'learning_rate': 0.0005151515151515151, 'epoch': 0.02}
  2%|â–         | 34/2180 [05:37<5:42:13,  9.57s/it]  2%|â–         | 35/2180 [05:47<5:41:23,  9.55s/it]                                                   {'loss': 3.5414, 'learning_rate': 0.0005303030303030302, 'epoch': 0.02}
  2%|â–         | 35/2180 [05:47<5:41:23,  9.55s/it]  2%|â–         | 36/2180 [05:56<5:41:05,  9.55s/it]                                                   {'loss': 3.5029, 'learning_rate': 0.0005454545454545455, 'epoch': 0.02}
  2%|â–         | 36/2180 [05:56<5:41:05,  9.55s/it]  2%|â–         | 37/2180 [06:06<5:41:10,  9.55s/it]                                                   {'loss': 3.5526, 'learning_rate': 0.0005606060606060606, 'epoch': 0.02}
  2%|â–         | 37/2180 [06:06<5:41:10,  9.55s/it]  2%|â–         | 38/2180 [06:15<5:41:08,  9.56s/it]                                                   {'loss': 3.5993, 'learning_rate': 0.0005757575757575758, 'epoch': 0.02}
  2%|â–         | 38/2180 [06:15<5:41:08,  9.56s/it]  2%|â–         | 39/2180 [06:25<5:40:56,  9.55s/it]                                                   {'loss': 3.5979, 'learning_rate': 0.0005909090909090909, 'epoch': 0.02}
  2%|â–         | 39/2180 [06:25<5:40:56,  9.55s/it]  2%|â–         | 40/2180 [06:34<5:41:13,  9.57s/it]                                                   {'loss': 3.51, 'learning_rate': 0.0006060606060606061, 'epoch': 0.02}
  2%|â–         | 40/2180 [06:34<5:41:13,  9.57s/it]  2%|â–         | 41/2180 [06:44<5:41:31,  9.58s/it]                                                   {'loss': 3.3653, 'learning_rate': 0.0006212121212121212, 'epoch': 0.02}
  2%|â–         | 41/2180 [06:44<5:41:31,  9.58s/it]  2%|â–         | 42/2180 [06:54<5:41:09,  9.57s/it]                                                   {'loss': 3.4799, 'learning_rate': 0.0006363636363636364, 'epoch': 0.02}
  2%|â–         | 42/2180 [06:54<5:41:09,  9.57s/it]  2%|â–         | 43/2180 [07:03<5:40:37,  9.56s/it]                                                   {'loss': 3.4409, 'learning_rate': 0.0006515151515151515, 'epoch': 0.02}
  2%|â–         | 43/2180 [07:03<5:40:37,  9.56s/it]  2%|â–         | 44/2180 [07:13<5:40:31,  9.57s/it]                                                   {'loss': 3.3174, 'learning_rate': 0.0006666666666666666, 'epoch': 0.02}
  2%|â–         | 44/2180 [07:13<5:40:31,  9.57s/it]  2%|â–         | 45/2180 [07:22<5:40:14,  9.56s/it]                                                   {'loss': 3.2167, 'learning_rate': 0.0006818181818181818, 'epoch': 0.02}
  2%|â–         | 45/2180 [07:22<5:40:14,  9.56s/it]  2%|â–         | 46/2180 [07:32<5:39:43,  9.55s/it]                                                   {'loss': 3.3601, 'learning_rate': 0.000696969696969697, 'epoch': 0.02}
  2%|â–         | 46/2180 [07:32<5:39:43,  9.55s/it]  2%|â–         | 47/2180 [07:41<5:40:14,  9.57s/it]                                                   {'loss': 3.3597, 'learning_rate': 0.0007121212121212122, 'epoch': 0.02}
  2%|â–         | 47/2180 [07:41<5:40:14,  9.57s/it]  2%|â–         | 48/2180 [07:51<5:40:14,  9.58s/it]                                                   {'loss': 3.3031, 'learning_rate': 0.0007272727272727273, 'epoch': 0.02}
  2%|â–         | 48/2180 [07:51<5:40:14,  9.58s/it]  2%|â–         | 49/2180 [08:01<5:40:45,  9.59s/it]                                                   {'loss': 3.2935, 'learning_rate': 0.0007424242424242425, 'epoch': 0.02}
  2%|â–         | 49/2180 [08:01<5:40:45,  9.59s/it]  2%|â–         | 50/2180 [08:10<5:39:54,  9.57s/it]                                                   {'loss': 3.3405, 'learning_rate': 0.0007575757575757576, 'epoch': 0.02}
  2%|â–         | 50/2180 [08:10<5:39:54,  9.57s/it]  2%|â–         | 51/2180 [08:20<5:39:28,  9.57s/it]                                                   {'loss': 3.2399, 'learning_rate': 0.0007727272727272727, 'epoch': 0.02}
  2%|â–         | 51/2180 [08:20<5:39:28,  9.57s/it]  2%|â–         | 52/2180 [08:29<5:38:56,  9.56s/it]                                                   {'loss': 3.2519, 'learning_rate': 0.0007878787878787878, 'epoch': 0.02}
  2%|â–         | 52/2180 [08:29<5:38:56,  9.56s/it]  2%|â–         | 53/2180 [08:39<5:38:44,  9.56s/it]                                                   {'loss': 3.2065, 'learning_rate': 0.000803030303030303, 'epoch': 0.02}
  2%|â–         | 53/2180 [08:39<5:38:44,  9.56s/it]  2%|â–         | 54/2180 [08:48<5:38:15,  9.55s/it]                                                   {'loss': 3.2085, 'learning_rate': 0.0008181818181818183, 'epoch': 0.02}
  2%|â–         | 54/2180 [08:48<5:38:15,  9.55s/it]  3%|â–Ž         | 55/2180 [08:58<5:38:15,  9.55s/it]                                                   {'loss': 3.239, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}
  3%|â–Ž         | 55/2180 [08:58<5:38:15,  9.55s/it]  3%|â–Ž         | 56/2180 [09:08<5:39:02,  9.58s/it]                                                   {'loss': 3.2732, 'learning_rate': 0.0008484848484848485, 'epoch': 0.03}
  3%|â–Ž         | 56/2180 [09:08<5:39:02,  9.58s/it]  3%|â–Ž         | 57/2180 [09:17<5:39:03,  9.58s/it]                                                   {'loss': 3.1255, 'learning_rate': 0.0008636363636363636, 'epoch': 0.03}
  3%|â–Ž         | 57/2180 [09:17<5:39:03,  9.58s/it]  3%|â–Ž         | 58/2180 [09:27<5:38:54,  9.58s/it]                                                   {'loss': 3.1817, 'learning_rate': 0.0008787878787878789, 'epoch': 0.03}
  3%|â–Ž         | 58/2180 [09:27<5:38:54,  9.58s/it]  3%|â–Ž         | 59/2180 [09:36<5:38:48,  9.58s/it]                                                   {'loss': 3.1183, 'learning_rate': 0.000893939393939394, 'epoch': 0.03}
  3%|â–Ž         | 59/2180 [09:36<5:38:48,  9.58s/it]  3%|â–Ž         | 60/2180 [09:46<5:38:42,  9.59s/it]                                                   {'loss': 3.144, 'learning_rate': 0.0009090909090909091, 'epoch': 0.03}
  3%|â–Ž         | 60/2180 [09:46<5:38:42,  9.59s/it]  3%|â–Ž         | 61/2180 [09:55<5:38:19,  9.58s/it]                                                   {'loss': 3.0819, 'learning_rate': 0.0009242424242424242, 'epoch': 0.03}
  3%|â–Ž         | 61/2180 [09:55<5:38:19,  9.58s/it]  3%|â–Ž         | 62/2180 [10:05<5:38:15,  9.58s/it]                                                   {'loss': 3.0573, 'learning_rate': 0.0009393939393939394, 'epoch': 0.03}
  3%|â–Ž         | 62/2180 [10:05<5:38:15,  9.58s/it]  3%|â–Ž         | 63/2180 [10:15<5:38:19,  9.59s/it]                                                   {'loss': 3.1055, 'learning_rate': 0.0009545454545454546, 'epoch': 0.03}
  3%|â–Ž         | 63/2180 [10:15<5:38:19,  9.59s/it]  3%|â–Ž         | 64/2180 [10:24<5:38:03,  9.59s/it]                                                   {'loss': 2.9894, 'learning_rate': 0.0009696969696969698, 'epoch': 0.03}
  3%|â–Ž         | 64/2180 [10:24<5:38:03,  9.59s/it]  3%|â–Ž         | 65/2180 [10:34<5:38:01,  9.59s/it]                                                   {'loss': 3.0693, 'learning_rate': 0.000984848484848485, 'epoch': 0.03}
  3%|â–Ž         | 65/2180 [10:34<5:38:01,  9.59s/it]  3%|â–Ž         | 66/2180 [10:43<5:37:33,  9.58s/it]                                                   {'loss': 2.9353, 'learning_rate': 0.001, 'epoch': 0.03}
  3%|â–Ž         | 66/2180 [10:43<5:37:33,  9.58s/it]  3%|â–Ž         | 67/2180 [10:53<5:37:19,  9.58s/it]                                                   {'loss': 2.9686, 'learning_rate': 0.0009999994478847943, 'epoch': 0.03}
  3%|â–Ž         | 67/2180 [10:53<5:37:19,  9.58s/it]  3%|â–Ž         | 68/2180 [11:03<5:37:06,  9.58s/it]                                                   {'loss': 2.9481, 'learning_rate': 0.0009999977915403962, 'epoch': 0.03}
  3%|â–Ž         | 68/2180 [11:03<5:37:06,  9.58s/it]  3%|â–Ž         | 69/2180 [11:12<5:37:17,  9.59s/it]                                                   {'loss': 2.8999, 'learning_rate': 0.0009999950309704639, 'epoch': 0.03}
  3%|â–Ž         | 69/2180 [11:12<5:37:17,  9.59s/it]  3%|â–Ž         | 70/2180 [11:22<5:36:45,  9.58s/it]                                                   {'loss': 2.9548, 'learning_rate': 0.000999991166181094, 'epoch': 0.03}
  3%|â–Ž         | 70/2180 [11:22<5:36:45,  9.58s/it]  3%|â–Ž         | 71/2180 [11:31<5:36:18,  9.57s/it]                                                   {'loss': 2.8591, 'learning_rate': 0.0009999861971808216, 'epoch': 0.03}
  3%|â–Ž         | 71/2180 [11:31<5:36:18,  9.57s/it]  3%|â–Ž         | 72/2180 [11:41<5:36:16,  9.57s/it]                                                   {'loss': 2.9189, 'learning_rate': 0.0009999801239806208, 'epoch': 0.03}
  3%|â–Ž         | 72/2180 [11:41<5:36:16,  9.57s/it]  3%|â–Ž         | 73/2180 [11:50<5:36:09,  9.57s/it]                                                   {'loss': 2.8016, 'learning_rate': 0.0009999729465939035, 'epoch': 0.03}
  3%|â–Ž         | 73/2180 [11:50<5:36:09,  9.57s/it]  3%|â–Ž         | 74/2180 [12:00<5:36:09,  9.58s/it]                                                   {'loss': 2.9367, 'learning_rate': 0.0009999646650365212, 'epoch': 0.03}
  3%|â–Ž         | 74/2180 [12:00<5:36:09,  9.58s/it]  3%|â–Ž         | 75/2180 [12:10<5:36:13,  9.58s/it]                                                   {'loss': 2.8172, 'learning_rate': 0.0009999552793267634, 'epoch': 0.03}
  3%|â–Ž         | 75/2180 [12:10<5:36:13,  9.58s/it]  3%|â–Ž         | 76/2180 [12:19<5:36:22,  9.59s/it]                                                   {'loss': 2.8453, 'learning_rate': 0.0009999447894853577, 'epoch': 0.03}
  3%|â–Ž         | 76/2180 [12:19<5:36:22,  9.59s/it]  4%|â–Ž         | 77/2180 [12:29<5:35:57,  9.58s/it]                                                   {'loss': 2.92, 'learning_rate': 0.0009999331955354708, 'epoch': 0.04}
  4%|â–Ž         | 77/2180 [12:29<5:35:57,  9.58s/it]  4%|â–Ž         | 78/2180 [12:38<5:35:32,  9.58s/it]                                                   {'loss': 2.7173, 'learning_rate': 0.0009999204975027073, 'epoch': 0.04}
  4%|â–Ž         | 78/2180 [12:38<5:35:32,  9.58s/it]  4%|â–Ž         | 79/2180 [12:48<5:36:13,  9.60s/it]                                                   {'loss': 2.7432, 'learning_rate': 0.0009999066954151103, 'epoch': 0.04}
  4%|â–Ž         | 79/2180 [12:48<5:36:13,  9.60s/it]  4%|â–Ž         | 80/2180 [12:58<5:35:42,  9.59s/it]                                                   {'loss': 2.8592, 'learning_rate': 0.0009998917893031614, 'epoch': 0.04}
  4%|â–Ž         | 80/2180 [12:58<5:35:42,  9.59s/it]  4%|â–Ž         | 81/2180 [13:07<5:35:30,  9.59s/it]                                                   {'loss': 2.8644, 'learning_rate': 0.0009998757791997801, 'epoch': 0.04}
  4%|â–Ž         | 81/2180 [13:07<5:35:30,  9.59s/it]  4%|â–         | 82/2180 [13:17<5:35:47,  9.60s/it]                                                   {'loss': 2.8339, 'learning_rate': 0.0009998586651403238, 'epoch': 0.04}
  4%|â–         | 82/2180 [13:17<5:35:47,  9.60s/it]  4%|â–         | 83/2180 [13:26<5:35:19,  9.59s/it]                                                   {'loss': 2.7263, 'learning_rate': 0.0009998404471625885, 'epoch': 0.04}
  4%|â–         | 83/2180 [13:26<5:35:19,  9.59s/it]  4%|â–         | 84/2180 [13:36<5:35:12,  9.60s/it]                                                   {'loss': 2.6395, 'learning_rate': 0.0009998211253068078, 'epoch': 0.04}
  4%|â–         | 84/2180 [13:36<5:35:12,  9.60s/it]  4%|â–         | 85/2180 [13:46<5:35:13,  9.60s/it]                                                   {'loss': 2.7981, 'learning_rate': 0.0009998006996156535, 'epoch': 0.04}
  4%|â–         | 85/2180 [13:46<5:35:13,  9.60s/it]  4%|â–         | 86/2180 [13:55<5:35:06,  9.60s/it]                                                   {'loss': 2.6193, 'learning_rate': 0.0009997791701342347, 'epoch': 0.04}
  4%|â–         | 86/2180 [13:55<5:35:06,  9.60s/it]  4%|â–         | 87/2180 [14:05<5:34:18,  9.58s/it]                                                   {'loss': 2.7108, 'learning_rate': 0.0009997565369100983, 'epoch': 0.04}
  4%|â–         | 87/2180 [14:05<5:34:18,  9.58s/it]  4%|â–         | 88/2180 [14:14<5:34:38,  9.60s/it]                                                   {'loss': 2.6467, 'learning_rate': 0.0009997327999932291, 'epoch': 0.04}
  4%|â–         | 88/2180 [14:14<5:34:38,  9.60s/it]  4%|â–         | 89/2180 [14:24<5:34:34,  9.60s/it]                                                   {'loss': 2.6775, 'learning_rate': 0.000999707959436049, 'epoch': 0.04}
  4%|â–         | 89/2180 [14:24<5:34:34,  9.60s/it]  4%|â–         | 90/2180 [14:34<5:34:10,  9.59s/it]                                                   {'loss': 2.6913, 'learning_rate': 0.0009996820152934176, 'epoch': 0.04}
  4%|â–         | 90/2180 [14:34<5:34:10,  9.59s/it]  4%|â–         | 91/2180 [14:43<5:34:00,  9.59s/it]                                                   {'loss': 2.625, 'learning_rate': 0.000999654967622631, 'epoch': 0.04}
  4%|â–         | 91/2180 [14:43<5:34:00,  9.59s/it]  4%|â–         | 92/2180 [14:53<5:34:27,  9.61s/it]                                                   {'loss': 2.6348, 'learning_rate': 0.0009996268164834238, 'epoch': 0.04}
  4%|â–         | 92/2180 [14:53<5:34:27,  9.61s/it]  4%|â–         | 93/2180 [15:02<5:33:33,  9.59s/it]                                                   {'loss': 2.5988, 'learning_rate': 0.000999597561937966, 'epoch': 0.04}
  4%|â–         | 93/2180 [15:02<5:33:33,  9.59s/it]  4%|â–         | 94/2180 [15:12<5:33:41,  9.60s/it]                                                   {'loss': 2.5833, 'learning_rate': 0.0009995672040508656, 'epoch': 0.04}
  4%|â–         | 94/2180 [15:12<5:33:41,  9.60s/it]  4%|â–         | 95/2180 [15:22<5:33:15,  9.59s/it]                                                   {'loss': 2.6006, 'learning_rate': 0.0009995357428891662, 'epoch': 0.04}
  4%|â–         | 95/2180 [15:22<5:33:15,  9.59s/it]  4%|â–         | 96/2180 [15:31<5:33:12,  9.59s/it]                                                   {'loss': 2.6156, 'learning_rate': 0.0009995031785223491, 'epoch': 0.04}
  4%|â–         | 96/2180 [15:31<5:33:12,  9.59s/it]  4%|â–         | 97/2180 [15:41<5:33:05,  9.59s/it]                                                   {'loss': 2.7321, 'learning_rate': 0.000999469511022331, 'epoch': 0.04}
  4%|â–         | 97/2180 [15:41<5:33:05,  9.59s/it]  4%|â–         | 98/2180 [15:50<5:32:48,  9.59s/it]                                                   {'loss': 2.5546, 'learning_rate': 0.0009994347404634657, 'epoch': 0.04}
  4%|â–         | 98/2180 [15:50<5:32:48,  9.59s/it]  5%|â–         | 99/2180 [16:00<5:32:27,  9.59s/it]                                                   {'loss': 2.6771, 'learning_rate': 0.0009993988669225423, 'epoch': 0.05}
  5%|â–         | 99/2180 [16:00<5:32:27,  9.59s/it]  5%|â–         | 100/2180 [16:09<5:32:12,  9.58s/it]                                                    {'loss': 2.5959, 'learning_rate': 0.000999361890478786, 'epoch': 0.05}
  5%|â–         | 100/2180 [16:09<5:32:12,  9.58s/it]  5%|â–         | 101/2180 [16:19<5:31:51,  9.58s/it]                                                    {'loss': 2.676, 'learning_rate': 0.0009993238112138583, 'epoch': 0.05}
  5%|â–         | 101/2180 [16:19<5:31:51,  9.58s/it]  5%|â–         | 102/2180 [16:29<5:31:34,  9.57s/it]                                                    {'loss': 2.5459, 'learning_rate': 0.0009992846292118554, 'epoch': 0.05}
  5%|â–         | 102/2180 [16:29<5:31:34,  9.57s/it]  5%|â–         | 103/2180 [16:38<5:31:31,  9.58s/it]                                                    {'loss': 2.5736, 'learning_rate': 0.000999244344559309, 'epoch': 0.05}
  5%|â–         | 103/2180 [16:38<5:31:31,  9.58s/it]  5%|â–         | 104/2180 [16:48<5:32:28,  9.61s/it]                                                    {'loss': 2.493, 'learning_rate': 0.0009992029573451869, 'epoch': 0.05}
  5%|â–         | 104/2180 [16:48<5:32:28,  9.61s/it]  5%|â–         | 105/2180 [16:57<5:31:55,  9.60s/it]                                                    {'loss': 2.5194, 'learning_rate': 0.0009991604676608905, 'epoch': 0.05}
  5%|â–         | 105/2180 [16:57<5:31:55,  9.60s/it]  5%|â–         | 106/2180 [17:07<5:31:26,  9.59s/it]                                                    {'loss': 2.5294, 'learning_rate': 0.0009991168756002568, 'epoch': 0.05}
  5%|â–         | 106/2180 [17:07<5:31:26,  9.59s/it]  5%|â–         | 107/2180 [17:17<5:30:43,  9.57s/it]                                                    {'loss': 2.4972, 'learning_rate': 0.0009990721812595574, 'epoch': 0.05}
  5%|â–         | 107/2180 [17:17<5:30:43,  9.57s/it]  5%|â–         | 108/2180 [17:26<5:30:44,  9.58s/it]                                                    {'loss': 2.5042, 'learning_rate': 0.0009990263847374976, 'epoch': 0.05}
  5%|â–         | 108/2180 [17:26<5:30:44,  9.58s/it]  5%|â–Œ         | 109/2180 [17:36<5:30:31,  9.58s/it]                                                    {'loss': 2.5279, 'learning_rate': 0.0009989794861352173, 'epoch': 0.05}
  5%|â–Œ         | 109/2180 [17:36<5:30:31,  9.58s/it]  5%|â–Œ         | 110/2180 [17:45<5:29:57,  9.56s/it]                                                    {'loss': 2.4964, 'learning_rate': 0.0009989314855562905, 'epoch': 0.05}
  5%|â–Œ         | 110/2180 [17:45<5:29:57,  9.56s/it]  5%|â–Œ         | 111/2180 [17:55<5:29:48,  9.56s/it]                                                    {'loss': 2.423, 'learning_rate': 0.0009988823831067245, 'epoch': 0.05}
  5%|â–Œ         | 111/2180 [17:55<5:29:48,  9.56s/it]  5%|â–Œ         | 112/2180 [18:04<5:29:19,  9.55s/it]                                                    {'loss': 2.561, 'learning_rate': 0.0009988321788949597, 'epoch': 0.05}
  5%|â–Œ         | 112/2180 [18:04<5:29:19,  9.55s/it]  5%|â–Œ         | 113/2180 [18:14<5:31:05,  9.61s/it]                                                    {'loss': 2.4707, 'learning_rate': 0.0009987808730318709, 'epoch': 0.05}
  5%|â–Œ         | 113/2180 [18:14<5:31:05,  9.61s/it]  5%|â–Œ         | 114/2180 [18:24<5:31:04,  9.61s/it]                                                    {'loss': 2.6021, 'learning_rate': 0.0009987284656307644, 'epoch': 0.05}
  5%|â–Œ         | 114/2180 [18:24<5:31:04,  9.61s/it]  5%|â–Œ         | 115/2180 [18:33<5:31:11,  9.62s/it]                                                    {'loss': 2.4189, 'learning_rate': 0.0009986749568073802, 'epoch': 0.05}
  5%|â–Œ         | 115/2180 [18:33<5:31:11,  9.62s/it]  5%|â–Œ         | 116/2180 [18:43<5:30:47,  9.62s/it]                                                    {'loss': 2.4076, 'learning_rate': 0.0009986203466798905, 'epoch': 0.05}
  5%|â–Œ         | 116/2180 [18:43<5:30:47,  9.62s/it]  5%|â–Œ         | 117/2180 [18:52<5:29:56,  9.60s/it]                                                    {'loss': 2.4559, 'learning_rate': 0.0009985646353688996, 'epoch': 0.05}
  5%|â–Œ         | 117/2180 [18:52<5:29:56,  9.60s/it]  5%|â–Œ         | 118/2180 [19:02<5:29:35,  9.59s/it]                                                    {'loss': 2.5009, 'learning_rate': 0.0009985078229974437, 'epoch': 0.05}
  5%|â–Œ         | 118/2180 [19:02<5:29:35,  9.59s/it]  5%|â–Œ         | 119/2180 [19:12<5:29:14,  9.59s/it]                                                    {'loss': 2.4857, 'learning_rate': 0.0009984499096909905, 'epoch': 0.05}
  5%|â–Œ         | 119/2180 [19:12<5:29:14,  9.59s/it]  6%|â–Œ         | 120/2180 [19:21<5:29:10,  9.59s/it]                                                    {'loss': 2.496, 'learning_rate': 0.0009983908955774397, 'epoch': 0.06}
  6%|â–Œ         | 120/2180 [19:21<5:29:10,  9.59s/it]  6%|â–Œ         | 121/2180 [19:31<5:28:29,  9.57s/it]                                                    {'loss': 2.4444, 'learning_rate': 0.0009983307807871211, 'epoch': 0.06}
  6%|â–Œ         | 121/2180 [19:31<5:28:29,  9.57s/it]  6%|â–Œ         | 122/2180 [19:40<5:28:21,  9.57s/it]                                                    {'loss': 2.4948, 'learning_rate': 0.0009982695654527965, 'epoch': 0.06}
  6%|â–Œ         | 122/2180 [19:40<5:28:21,  9.57s/it]  6%|â–Œ         | 123/2180 [19:50<5:28:31,  9.58s/it]                                                    {'loss': 2.5346, 'learning_rate': 0.0009982072497096571, 'epoch': 0.06}
  6%|â–Œ         | 123/2180 [19:50<5:28:31,  9.58s/it]  6%|â–Œ         | 124/2180 [20:00<5:28:58,  9.60s/it]                                                    {'loss': 2.4594, 'learning_rate': 0.000998143833695325, 'epoch': 0.06}
  6%|â–Œ         | 124/2180 [20:00<5:28:58,  9.60s/it]  6%|â–Œ         | 125/2180 [20:09<5:28:33,  9.59s/it]                                                    {'loss': 2.3754, 'learning_rate': 0.0009980793175498517, 'epoch': 0.06}
  6%|â–Œ         | 125/2180 [20:09<5:28:33,  9.59s/it]  6%|â–Œ         | 126/2180 [20:19<5:28:25,  9.59s/it]                                                    {'loss': 2.4151, 'learning_rate': 0.000998013701415719, 'epoch': 0.06}
  6%|â–Œ         | 126/2180 [20:19<5:28:25,  9.59s/it]  6%|â–Œ         | 127/2180 [20:28<5:28:39,  9.61s/it]                                                    {'loss': 2.3948, 'learning_rate': 0.0009979469854378372, 'epoch': 0.06}
  6%|â–Œ         | 127/2180 [20:28<5:28:39,  9.61s/it]  6%|â–Œ         | 128/2180 [20:38<5:28:24,  9.60s/it]                                                    {'loss': 2.462, 'learning_rate': 0.000997879169763546, 'epoch': 0.06}
  6%|â–Œ         | 128/2180 [20:38<5:28:24,  9.60s/it]  6%|â–Œ         | 129/2180 [20:48<5:28:08,  9.60s/it]                                                    {'loss': 2.4128, 'learning_rate': 0.000997810254542614, 'epoch': 0.06}
  6%|â–Œ         | 129/2180 [20:48<5:28:08,  9.60s/it]  6%|â–Œ         | 130/2180 [20:57<5:27:46,  9.59s/it]                                                    {'loss': 2.3947, 'learning_rate': 0.0009977402399272374, 'epoch': 0.06}
  6%|â–Œ         | 130/2180 [20:57<5:27:46,  9.59s/it]  6%|â–Œ         | 131/2180 [21:07<5:27:05,  9.58s/it]                                                    {'loss': 2.3978, 'learning_rate': 0.0009976691260720407, 'epoch': 0.06}
  6%|â–Œ         | 131/2180 [21:07<5:27:05,  9.58s/it]  6%|â–Œ         | 132/2180 [21:16<5:26:41,  9.57s/it]                                                    {'loss': 2.4018, 'learning_rate': 0.0009975969131340763, 'epoch': 0.06}
  6%|â–Œ         | 132/2180 [21:16<5:26:41,  9.57s/it]  6%|â–Œ         | 133/2180 [21:26<5:27:11,  9.59s/it]                                                    {'loss': 2.4148, 'learning_rate': 0.0009975236012728236, 'epoch': 0.06}
  6%|â–Œ         | 133/2180 [21:26<5:27:11,  9.59s/it]  6%|â–Œ         | 134/2180 [21:36<5:27:34,  9.61s/it]                                                    {'loss': 2.394, 'learning_rate': 0.0009974491906501886, 'epoch': 0.06}
  6%|â–Œ         | 134/2180 [21:36<5:27:34,  9.61s/it]  6%|â–Œ         | 135/2180 [21:45<5:27:21,  9.60s/it]                                                    {'loss': 2.3918, 'learning_rate': 0.0009973736814305049, 'epoch': 0.06}
  6%|â–Œ         | 135/2180 [21:45<5:27:21,  9.60s/it]  6%|â–Œ         | 136/2180 [21:55<5:27:01,  9.60s/it]                                                    {'loss': 2.4025, 'learning_rate': 0.0009972970737805312, 'epoch': 0.06}
  6%|â–Œ         | 136/2180 [21:55<5:27:01,  9.60s/it]  6%|â–‹         | 137/2180 [22:04<5:26:33,  9.59s/it]                                                    {'loss': 2.3856, 'learning_rate': 0.0009972193678694525, 'epoch': 0.06}
  6%|â–‹         | 137/2180 [22:04<5:26:33,  9.59s/it]  6%|â–‹         | 138/2180 [22:14<5:26:12,  9.59s/it]                                                    {'loss': 2.3555, 'learning_rate': 0.0009971405638688794, 'epoch': 0.06}
  6%|â–‹         | 138/2180 [22:14<5:26:12,  9.59s/it]  6%|â–‹         | 139/2180 [22:24<5:27:03,  9.61s/it]                                                    {'loss': 2.3448, 'learning_rate': 0.0009970606619528475, 'epoch': 0.06}
  6%|â–‹         | 139/2180 [22:24<5:27:03,  9.61s/it]  6%|â–‹         | 140/2180 [22:33<5:26:08,  9.59s/it]                                                    {'loss': 2.3665, 'learning_rate': 0.000996979662297817, 'epoch': 0.06}
  6%|â–‹         | 140/2180 [22:33<5:26:08,  9.59s/it]  6%|â–‹         | 141/2180 [22:43<5:26:12,  9.60s/it]                                                    {'loss': 2.3974, 'learning_rate': 0.0009968975650826721, 'epoch': 0.06}
  6%|â–‹         | 141/2180 [22:43<5:26:12,  9.60s/it]  7%|â–‹         | 142/2180 [22:52<5:25:38,  9.59s/it]                                                    {'loss': 2.3959, 'learning_rate': 0.000996814370488722, 'epoch': 0.07}
  7%|â–‹         | 142/2180 [22:52<5:25:38,  9.59s/it]  7%|â–‹         | 143/2180 [23:02<5:25:36,  9.59s/it]                                                    {'loss': 2.405, 'learning_rate': 0.000996730078699698, 'epoch': 0.07}
  7%|â–‹         | 143/2180 [23:02<5:25:36,  9.59s/it]  7%|â–‹         | 144/2180 [23:11<5:25:13,  9.58s/it]                                                    {'loss': 2.45, 'learning_rate': 0.0009966446899017558, 'epoch': 0.07}
  7%|â–‹         | 144/2180 [23:11<5:25:13,  9.58s/it]  7%|â–‹         | 145/2180 [23:21<5:25:00,  9.58s/it]                                                    {'loss': 2.3878, 'learning_rate': 0.0009965582042834728, 'epoch': 0.07}
  7%|â–‹         | 145/2180 [23:21<5:25:00,  9.58s/it]  7%|â–‹         | 146/2180 [23:31<5:24:40,  9.58s/it]                                                    {'loss': 2.3452, 'learning_rate': 0.0009964706220358492, 'epoch': 0.07}
  7%|â–‹         | 146/2180 [23:31<5:24:40,  9.58s/it]  7%|â–‹         | 147/2180 [23:40<5:26:12,  9.63s/it]                                                    {'loss': 2.3129, 'learning_rate': 0.000996381943352307, 'epoch': 0.07}
  7%|â–‹         | 147/2180 [23:40<5:26:12,  9.63s/it]  7%|â–‹         | 148/2180 [23:50<5:25:28,  9.61s/it]                                                    {'loss': 2.4353, 'learning_rate': 0.0009962921684286896, 'epoch': 0.07}
  7%|â–‹         | 148/2180 [23:50<5:25:28,  9.61s/it]  7%|â–‹         | 149/2180 [23:59<5:25:04,  9.60s/it]                                                    {'loss': 2.3578, 'learning_rate': 0.0009962012974632614, 'epoch': 0.07}
  7%|â–‹         | 149/2180 [23:59<5:25:04,  9.60s/it]  7%|â–‹         | 150/2180 [24:09<5:24:42,  9.60s/it]                                                    {'loss': 2.3484, 'learning_rate': 0.0009961093306567075, 'epoch': 0.07}
  7%|â–‹         | 150/2180 [24:09<5:24:42,  9.60s/it]  7%|â–‹         | 151/2180 [24:19<5:25:37,  9.63s/it]                                                    {'loss': 2.3074, 'learning_rate': 0.0009960162682121328, 'epoch': 0.07}
  7%|â–‹         | 151/2180 [24:19<5:25:37,  9.63s/it]  7%|â–‹         | 152/2180 [24:28<5:24:59,  9.62s/it]                                                    {'loss': 2.3361, 'learning_rate': 0.0009959221103350623, 'epoch': 0.07}
  7%|â–‹         | 152/2180 [24:28<5:24:59,  9.62s/it]  7%|â–‹         | 153/2180 [24:38<5:24:11,  9.60s/it]                                                    {'loss': 2.407, 'learning_rate': 0.0009958268572334394, 'epoch': 0.07}
  7%|â–‹         | 153/2180 [24:38<5:24:11,  9.60s/it]  7%|â–‹         | 154/2180 [24:47<5:23:51,  9.59s/it]                                                    {'loss': 2.3449, 'learning_rate': 0.0009957305091176274, 'epoch': 0.07}
  7%|â–‹         | 154/2180 [24:47<5:23:51,  9.59s/it]  7%|â–‹         | 155/2180 [24:57<5:23:44,  9.59s/it]                                                    {'loss': 2.3564, 'learning_rate': 0.0009956330662004075, 'epoch': 0.07}
  7%|â–‹         | 155/2180 [24:57<5:23:44,  9.59s/it]  7%|â–‹         | 156/2180 [25:07<5:23:40,  9.60s/it]                                                    {'loss': 2.3998, 'learning_rate': 0.0009955345286969779, 'epoch': 0.07}
  7%|â–‹         | 156/2180 [25:07<5:23:40,  9.60s/it]  7%|â–‹         | 157/2180 [25:16<5:23:53,  9.61s/it]                                                    {'loss': 2.3382, 'learning_rate': 0.0009954348968249551, 'epoch': 0.07}
  7%|â–‹         | 157/2180 [25:16<5:23:53,  9.61s/it]  7%|â–‹         | 158/2180 [25:26<5:23:22,  9.60s/it]                                                    {'loss': 2.3102, 'learning_rate': 0.0009953341708043724, 'epoch': 0.07}
  7%|â–‹         | 158/2180 [25:26<5:23:22,  9.60s/it]  7%|â–‹         | 159/2180 [25:35<5:23:09,  9.59s/it]                                                    {'loss': 2.295, 'learning_rate': 0.0009952323508576793, 'epoch': 0.07}
  7%|â–‹         | 159/2180 [25:35<5:23:09,  9.59s/it]  7%|â–‹         | 160/2180 [25:45<5:24:40,  9.64s/it]                                                    {'loss': 2.3207, 'learning_rate': 0.0009951294372097406, 'epoch': 0.07}
  7%|â–‹         | 160/2180 [25:45<5:24:40,  9.64s/it]  7%|â–‹         | 161/2180 [25:55<5:23:41,  9.62s/it]                                                    {'loss': 2.2334, 'learning_rate': 0.0009950254300878378, 'epoch': 0.07}
  7%|â–‹         | 161/2180 [25:55<5:23:41,  9.62s/it]  7%|â–‹         | 162/2180 [26:04<5:23:29,  9.62s/it]                                                    {'loss': 2.2652, 'learning_rate': 0.000994920329721666, 'epoch': 0.07}
  7%|â–‹         | 162/2180 [26:04<5:23:29,  9.62s/it]  7%|â–‹         | 163/2180 [26:14<5:23:10,  9.61s/it]                                                    {'loss': 2.2312, 'learning_rate': 0.0009948141363433356, 'epoch': 0.07}
  7%|â–‹         | 163/2180 [26:14<5:23:10,  9.61s/it]  8%|â–Š         | 164/2180 [26:24<5:23:56,  9.64s/it]                                                    {'loss': 2.2467, 'learning_rate': 0.00099470685018737, 'epoch': 0.08}
  8%|â–Š         | 164/2180 [26:24<5:23:56,  9.64s/it]  8%|â–Š         | 165/2180 [26:33<5:23:36,  9.64s/it]                                                    {'loss': 2.2791, 'learning_rate': 0.0009945984714907073, 'epoch': 0.08}
  8%|â–Š         | 165/2180 [26:33<5:23:36,  9.64s/it]  8%|â–Š         | 166/2180 [26:43<5:23:22,  9.63s/it]                                                    {'loss': 2.3416, 'learning_rate': 0.000994489000492697, 'epoch': 0.08}
  8%|â–Š         | 166/2180 [26:43<5:23:22,  9.63s/it]  8%|â–Š         | 167/2180 [26:53<5:22:51,  9.62s/it]                                                    {'loss': 2.3363, 'learning_rate': 0.0009943784374351016, 'epoch': 0.08}
  8%|â–Š         | 167/2180 [26:53<5:22:51,  9.62s/it]  8%|â–Š         | 168/2180 [27:02<5:21:59,  9.60s/it]                                                    {'loss': 2.3079, 'learning_rate': 0.0009942667825620951, 'epoch': 0.08}
  8%|â–Š         | 168/2180 [27:02<5:21:59,  9.60s/it]  8%|â–Š         | 169/2180 [27:12<5:21:51,  9.60s/it]                                                    {'loss': 2.3594, 'learning_rate': 0.0009941540361202634, 'epoch': 0.08}
  8%|â–Š         | 169/2180 [27:12<5:21:51,  9.60s/it]  8%|â–Š         | 170/2180 [27:21<5:22:28,  9.63s/it]                                                    {'loss': 2.2614, 'learning_rate': 0.0009940401983586022, 'epoch': 0.08}
  8%|â–Š         | 170/2180 [27:21<5:22:28,  9.63s/it]  8%|â–Š         | 171/2180 [27:31<5:21:34,  9.60s/it]                                                    {'loss': 2.2626, 'learning_rate': 0.000993925269528518, 'epoch': 0.08}
  8%|â–Š         | 171/2180 [27:31<5:21:34,  9.60s/it]  8%|â–Š         | 172/2180 [27:41<5:21:12,  9.60s/it]                                                    {'loss': 2.3021, 'learning_rate': 0.0009938092498838265, 'epoch': 0.08}
  8%|â–Š         | 172/2180 [27:41<5:21:12,  9.60s/it]  8%|â–Š         | 173/2180 [27:50<5:21:14,  9.60s/it]                                                    {'loss': 2.3143, 'learning_rate': 0.0009936921396807524, 'epoch': 0.08}
  8%|â–Š         | 173/2180 [27:50<5:21:14,  9.60s/it]  8%|â–Š         | 174/2180 [28:00<5:20:50,  9.60s/it]                                                    {'loss': 2.2177, 'learning_rate': 0.0009935739391779292, 'epoch': 0.08}
  8%|â–Š         | 174/2180 [28:00<5:20:50,  9.60s/it]  8%|â–Š         | 175/2180 [28:09<5:20:19,  9.59s/it]                                                    {'loss': 2.3909, 'learning_rate': 0.000993454648636398, 'epoch': 0.08}
  8%|â–Š         | 175/2180 [28:09<5:20:19,  9.59s/it]  8%|â–Š         | 176/2180 [28:19<5:20:11,  9.59s/it]                                                    {'loss': 2.3626, 'learning_rate': 0.0009933342683196074, 'epoch': 0.08}
  8%|â–Š         | 176/2180 [28:19<5:20:11,  9.59s/it]  8%|â–Š         | 177/2180 [28:28<5:20:05,  9.59s/it]                                                    {'loss': 2.2392, 'learning_rate': 0.0009932127984934125, 'epoch': 0.08}
  8%|â–Š         | 177/2180 [28:28<5:20:05,  9.59s/it]  8%|â–Š         | 178/2180 [28:38<5:19:55,  9.59s/it]                                                    {'loss': 2.3526, 'learning_rate': 0.0009930902394260745, 'epoch': 0.08}
  8%|â–Š         | 178/2180 [28:38<5:19:55,  9.59s/it]  8%|â–Š         | 179/2180 [28:48<5:19:50,  9.59s/it]                                                    {'loss': 2.2949, 'learning_rate': 0.0009929665913882607, 'epoch': 0.08}
  8%|â–Š         | 179/2180 [28:48<5:19:50,  9.59s/it]  8%|â–Š         | 180/2180 [28:57<5:19:34,  9.59s/it]                                                    {'loss': 2.2845, 'learning_rate': 0.0009928418546530425, 'epoch': 0.08}
  8%|â–Š         | 180/2180 [28:57<5:19:34,  9.59s/it]  8%|â–Š         | 181/2180 [29:07<5:19:14,  9.58s/it]                                                    {'loss': 2.2831, 'learning_rate': 0.0009927160294958964, 'epoch': 0.08}
  8%|â–Š         | 181/2180 [29:07<5:19:14,  9.58s/it]  8%|â–Š         | 182/2180 [29:16<5:19:37,  9.60s/it]                                                    {'loss': 2.304, 'learning_rate': 0.000992589116194702, 'epoch': 0.08}
  8%|â–Š         | 182/2180 [29:16<5:19:37,  9.60s/it]  8%|â–Š         | 183/2180 [29:26<5:19:06,  9.59s/it]                                                    {'loss': 2.3588, 'learning_rate': 0.000992461115029743, 'epoch': 0.08}
  8%|â–Š         | 183/2180 [29:26<5:19:06,  9.59s/it]  8%|â–Š         | 184/2180 [29:36<5:18:47,  9.58s/it]                                                    {'loss': 2.2729, 'learning_rate': 0.000992332026283704, 'epoch': 0.08}
  8%|â–Š         | 184/2180 [29:36<5:18:47,  9.58s/it]  8%|â–Š         | 185/2180 [29:45<5:18:29,  9.58s/it]                                                    {'loss': 2.2853, 'learning_rate': 0.0009922018502416736, 'epoch': 0.08}
  8%|â–Š         | 185/2180 [29:45<5:18:29,  9.58s/it]  9%|â–Š         | 186/2180 [29:55<5:18:06,  9.57s/it]                                                    {'loss': 2.2176, 'learning_rate': 0.0009920705871911395, 'epoch': 0.09}
  9%|â–Š         | 186/2180 [29:55<5:18:06,  9.57s/it]  9%|â–Š         | 187/2180 [30:04<5:17:54,  9.57s/it]                                                    {'loss': 2.3043, 'learning_rate': 0.0009919382374219915, 'epoch': 0.09}
  9%|â–Š         | 187/2180 [30:04<5:17:54,  9.57s/it]  9%|â–Š         | 188/2180 [30:14<5:17:31,  9.56s/it]                                                    {'loss': 2.2754, 'learning_rate': 0.0009918048012265187, 'epoch': 0.09}
  9%|â–Š         | 188/2180 [30:14<5:17:31,  9.56s/it]  9%|â–Š         | 189/2180 [30:23<5:17:56,  9.58s/it]                                                    {'loss': 2.4062, 'learning_rate': 0.0009916702788994097, 'epoch': 0.09}
  9%|â–Š         | 189/2180 [30:23<5:17:56,  9.58s/it]  9%|â–Š         | 190/2180 [30:33<5:17:35,  9.58s/it]                                                    {'loss': 2.2274, 'learning_rate': 0.0009915346707377519, 'epoch': 0.09}
  9%|â–Š         | 190/2180 [30:33<5:17:35,  9.58s/it]  9%|â–‰         | 191/2180 [30:43<5:17:44,  9.59s/it]                                                    {'loss': 2.1718, 'learning_rate': 0.0009913979770410305, 'epoch': 0.09}
  9%|â–‰         | 191/2180 [30:43<5:17:44,  9.59s/it]  9%|â–‰         | 192/2180 [30:52<5:17:37,  9.59s/it]                                                    {'loss': 2.2275, 'learning_rate': 0.0009912601981111285, 'epoch': 0.09}
  9%|â–‰         | 192/2180 [30:52<5:17:37,  9.59s/it]  9%|â–‰         | 193/2180 [31:02<5:17:27,  9.59s/it]                                                    {'loss': 2.2351, 'learning_rate': 0.0009911213342523248, 'epoch': 0.09}
  9%|â–‰         | 193/2180 [31:02<5:17:27,  9.59s/it]  9%|â–‰         | 194/2180 [31:11<5:16:55,  9.57s/it]                                                    {'loss': 2.3357, 'learning_rate': 0.000990981385771295, 'epoch': 0.09}
  9%|â–‰         | 194/2180 [31:11<5:16:55,  9.57s/it]  9%|â–‰         | 195/2180 [31:21<5:16:26,  9.56s/it]                                                    {'loss': 2.25, 'learning_rate': 0.00099084035297711, 'epoch': 0.09}
  9%|â–‰         | 195/2180 [31:21<5:16:26,  9.56s/it]  9%|â–‰         | 196/2180 [31:30<5:16:23,  9.57s/it]                                                    {'loss': 2.2626, 'learning_rate': 0.000990698236181235, 'epoch': 0.09}
  9%|â–‰         | 196/2180 [31:30<5:16:23,  9.57s/it]  9%|â–‰         | 197/2180 [31:40<5:17:26,  9.60s/it]                                                    {'loss': 2.2358, 'learning_rate': 0.0009905550356975293, 'epoch': 0.09}
  9%|â–‰         | 197/2180 [31:40<5:17:26,  9.60s/it]  9%|â–‰         | 198/2180 [31:50<5:16:33,  9.58s/it]                                                    {'loss': 2.2489, 'learning_rate': 0.0009904107518422457, 'epoch': 0.09}
  9%|â–‰         | 198/2180 [31:50<5:16:33,  9.58s/it]  9%|â–‰         | 199/2180 [31:59<5:16:01,  9.57s/it]                                                    {'loss': 2.1858, 'learning_rate': 0.0009902653849340295, 'epoch': 0.09}
  9%|â–‰         | 199/2180 [31:59<5:16:01,  9.57s/it]  9%|â–‰         | 200/2180 [32:09<5:16:02,  9.58s/it]                                                    {'loss': 2.2738, 'learning_rate': 0.0009901189352939177, 'epoch': 0.09}
  9%|â–‰         | 200/2180 [32:09<5:16:02,  9.58s/it]  9%|â–‰         | 201/2180 [32:18<5:15:50,  9.58s/it]                                                    {'loss': 2.2985, 'learning_rate': 0.0009899714032453387, 'epoch': 0.09}
  9%|â–‰         | 201/2180 [32:18<5:15:50,  9.58s/it]  9%|â–‰         | 202/2180 [32:28<5:15:55,  9.58s/it]                                                    {'loss': 2.1952, 'learning_rate': 0.000989822789114111, 'epoch': 0.09}
  9%|â–‰         | 202/2180 [32:28<5:15:55,  9.58s/it]  9%|â–‰         | 203/2180 [32:38<5:15:29,  9.57s/it]                                                    {'loss': 2.174, 'learning_rate': 0.0009896730932284434, 'epoch': 0.09}
  9%|â–‰         | 203/2180 [32:38<5:15:29,  9.57s/it]  9%|â–‰         | 204/2180 [32:47<5:15:01,  9.57s/it]                                                    {'loss': 2.1714, 'learning_rate': 0.0009895223159189332, 'epoch': 0.09}
  9%|â–‰         | 204/2180 [32:47<5:15:01,  9.57s/it]  9%|â–‰         | 205/2180 [32:57<5:15:12,  9.58s/it]                                                    {'loss': 2.2784, 'learning_rate': 0.0009893704575185663, 'epoch': 0.09}
  9%|â–‰         | 205/2180 [32:57<5:15:12,  9.58s/it]  9%|â–‰         | 206/2180 [33:06<5:15:54,  9.60s/it]                                                    {'loss': 2.3115, 'learning_rate': 0.000989217518362716, 'epoch': 0.09}
  9%|â–‰         | 206/2180 [33:06<5:15:54,  9.60s/it]  9%|â–‰         | 207/2180 [33:16<5:15:33,  9.60s/it]                                                    {'loss': 2.2185, 'learning_rate': 0.0009890634987891425, 'epoch': 0.09}
  9%|â–‰         | 207/2180 [33:16<5:15:33,  9.60s/it] 10%|â–‰         | 208/2180 [33:26<5:15:34,  9.60s/it]                                                    {'loss': 2.2715, 'learning_rate': 0.0009889083991379917, 'epoch': 0.1}
 10%|â–‰         | 208/2180 [33:26<5:15:34,  9.60s/it] 10%|â–‰         | 209/2180 [33:35<5:15:01,  9.59s/it]                                                    {'loss': 2.2163, 'learning_rate': 0.0009887522197517954, 'epoch': 0.1}
 10%|â–‰         | 209/2180 [33:35<5:15:01,  9.59s/it] 10%|â–‰         | 210/2180 [33:45<5:14:46,  9.59s/it]                                                    {'loss': 2.2771, 'learning_rate': 0.0009885949609754693, 'epoch': 0.1}
 10%|â–‰         | 210/2180 [33:45<5:14:46,  9.59s/it] 10%|â–‰         | 211/2180 [33:54<5:14:28,  9.58s/it]                                                    {'loss': 2.2957, 'learning_rate': 0.000988436623156314, 'epoch': 0.1}
 10%|â–‰         | 211/2180 [33:54<5:14:28,  9.58s/it] 10%|â–‰         | 212/2180 [34:04<5:14:37,  9.59s/it]                                                    {'loss': 2.2518, 'learning_rate': 0.0009882772066440114, 'epoch': 0.1}
 10%|â–‰         | 212/2180 [34:04<5:14:37,  9.59s/it] 10%|â–‰         | 213/2180 [34:13<5:14:29,  9.59s/it]                                                    {'loss': 2.2614, 'learning_rate': 0.0009881167117906276, 'epoch': 0.1}
 10%|â–‰         | 213/2180 [34:13<5:14:29,  9.59s/it] 10%|â–‰         | 214/2180 [34:23<5:14:03,  9.58s/it]                                                    {'loss': 2.2783, 'learning_rate': 0.0009879551389506084, 'epoch': 0.1}
 10%|â–‰         | 214/2180 [34:23<5:14:03,  9.58s/it] 10%|â–‰         | 215/2180 [34:33<5:14:08,  9.59s/it]                                                    {'loss': 2.1865, 'learning_rate': 0.0009877924884807814, 'epoch': 0.1}
 10%|â–‰         | 215/2180 [34:33<5:14:08,  9.59s/it] 10%|â–‰         | 216/2180 [34:42<5:13:43,  9.58s/it]                                                    {'loss': 2.25, 'learning_rate': 0.000987628760740354, 'epoch': 0.1}
 10%|â–‰         | 216/2180 [34:42<5:13:43,  9.58s/it] 10%|â–‰         | 217/2180 [34:52<5:14:24,  9.61s/it]                                                    {'loss': 2.2371, 'learning_rate': 0.0009874639560909118, 'epoch': 0.1}
 10%|â–‰         | 217/2180 [34:52<5:14:24,  9.61s/it] 10%|â–ˆ         | 218/2180 [35:01<5:13:59,  9.60s/it]                                                    {'loss': 2.3328, 'learning_rate': 0.0009872980748964202, 'epoch': 0.1}
 10%|â–ˆ         | 218/2180 [35:01<5:13:59,  9.60s/it] 10%|â–ˆ         | 219/2180 [35:11<5:13:32,  9.59s/it]                                                    {'loss': 2.2851, 'learning_rate': 0.000987131117523221, 'epoch': 0.1}
 10%|â–ˆ         | 219/2180 [35:11<5:13:32,  9.59s/it] 10%|â–ˆ         | 220/2180 [35:21<5:13:37,  9.60s/it]                                                    {'loss': 2.2829, 'learning_rate': 0.000986963084340033, 'epoch': 0.1}
 10%|â–ˆ         | 220/2180 [35:21<5:13:37,  9.60s/it] 10%|â–ˆ         | 221/2180 [35:30<5:13:02,  9.59s/it]                                                    {'loss': 2.1925, 'learning_rate': 0.0009867939757179508, 'epoch': 0.1}
 10%|â–ˆ         | 221/2180 [35:30<5:13:02,  9.59s/it] 10%|â–ˆ         | 222/2180 [35:40<5:12:27,  9.57s/it]                                                    {'loss': 2.2407, 'learning_rate': 0.0009866237920304443, 'epoch': 0.1}
 10%|â–ˆ         | 222/2180 [35:40<5:12:27,  9.57s/it] 10%|â–ˆ         | 223/2180 [35:49<5:12:18,  9.58s/it]                                                    {'loss': 2.2927, 'learning_rate': 0.0009864525336533577, 'epoch': 0.1}
 10%|â–ˆ         | 223/2180 [35:49<5:12:18,  9.58s/it] 10%|â–ˆ         | 224/2180 [35:59<5:12:36,  9.59s/it]                                                    {'loss': 2.2907, 'learning_rate': 0.000986280200964908, 'epoch': 0.1}
 10%|â–ˆ         | 224/2180 [35:59<5:12:36,  9.59s/it] 10%|â–ˆ         | 225/2180 [36:08<5:12:04,  9.58s/it]                                                    {'loss': 2.2381, 'learning_rate': 0.0009861067943456856, 'epoch': 0.1}
 10%|â–ˆ         | 225/2180 [36:08<5:12:04,  9.58s/it] 10%|â–ˆ         | 226/2180 [36:18<5:11:53,  9.58s/it]                                                    {'loss': 2.188, 'learning_rate': 0.000985932314178652, 'epoch': 0.1}
 10%|â–ˆ         | 226/2180 [36:18<5:11:53,  9.58s/it] 10%|â–ˆ         | 227/2180 [36:28<5:11:27,  9.57s/it]                                                    {'loss': 2.2979, 'learning_rate': 0.00098575676084914, 'epoch': 0.1}
 10%|â–ˆ         | 227/2180 [36:28<5:11:27,  9.57s/it] 10%|â–ˆ         | 228/2180 [36:37<5:11:36,  9.58s/it]                                                    {'loss': 2.2501, 'learning_rate': 0.0009855801347448518, 'epoch': 0.1}
 10%|â–ˆ         | 228/2180 [36:37<5:11:36,  9.58s/it] 11%|â–ˆ         | 229/2180 [36:47<5:11:02,  9.57s/it]                                                    {'loss': 2.3001, 'learning_rate': 0.0009854024362558596, 'epoch': 0.11}
 11%|â–ˆ         | 229/2180 [36:47<5:11:02,  9.57s/it] 11%|â–ˆ         | 230/2180 [36:56<5:11:21,  9.58s/it]                                                    {'loss': 2.1924, 'learning_rate': 0.0009852236657746035, 'epoch': 0.11}
 11%|â–ˆ         | 230/2180 [36:56<5:11:21,  9.58s/it] 11%|â–ˆ         | 231/2180 [37:06<5:11:10,  9.58s/it]                                                    {'loss': 2.3455, 'learning_rate': 0.0009850438236958911, 'epoch': 0.11}
 11%|â–ˆ         | 231/2180 [37:06<5:11:10,  9.58s/it] 11%|â–ˆ         | 232/2180 [37:16<5:11:20,  9.59s/it]                                                    {'loss': 2.2995, 'learning_rate': 0.0009848629104168966, 'epoch': 0.11}
 11%|â–ˆ         | 232/2180 [37:16<5:11:20,  9.59s/it] 11%|â–ˆ         | 233/2180 [37:25<5:11:11,  9.59s/it]                                                    {'loss': 2.203, 'learning_rate': 0.00098468092633716, 'epoch': 0.11}
 11%|â–ˆ         | 233/2180 [37:25<5:11:11,  9.59s/it] 11%|â–ˆ         | 234/2180 [37:35<5:11:00,  9.59s/it]                                                    {'loss': 2.233, 'learning_rate': 0.0009844978718585855, 'epoch': 0.11}
 11%|â–ˆ         | 234/2180 [37:35<5:11:00,  9.59s/it] 11%|â–ˆ         | 235/2180 [37:44<5:11:05,  9.60s/it]                                                    {'loss': 2.2138, 'learning_rate': 0.0009843137473854423, 'epoch': 0.11}
 11%|â–ˆ         | 235/2180 [37:44<5:11:05,  9.60s/it] 11%|â–ˆ         | 236/2180 [37:54<5:10:56,  9.60s/it]                                                    {'loss': 2.2544, 'learning_rate': 0.000984128553324362, 'epoch': 0.11}
 11%|â–ˆ         | 236/2180 [37:54<5:10:56,  9.60s/it] 11%|â–ˆ         | 237/2180 [38:04<5:10:42,  9.59s/it]                                                    {'loss': 2.2918, 'learning_rate': 0.0009839422900843383, 'epoch': 0.11}
 11%|â–ˆ         | 237/2180 [38:04<5:10:42,  9.59s/it] 11%|â–ˆ         | 238/2180 [38:13<5:10:42,  9.60s/it]                                                    {'loss': 2.2551, 'learning_rate': 0.0009837549580767261, 'epoch': 0.11}
 11%|â–ˆ         | 238/2180 [38:13<5:10:42,  9.60s/it] 11%|â–ˆ         | 239/2180 [38:23<5:10:25,  9.60s/it]                                                    {'loss': 2.2453, 'learning_rate': 0.0009835665577152411, 'epoch': 0.11}
 11%|â–ˆ         | 239/2180 [38:23<5:10:25,  9.60s/it] 11%|â–ˆ         | 240/2180 [38:32<5:10:15,  9.60s/it]                                                    {'loss': 2.2376, 'learning_rate': 0.000983377089415958, 'epoch': 0.11}
 11%|â–ˆ         | 240/2180 [38:32<5:10:15,  9.60s/it] 11%|â–ˆ         | 241/2180 [38:42<5:10:03,  9.59s/it]                                                    {'loss': 2.1472, 'learning_rate': 0.0009831865535973102, 'epoch': 0.11}
 11%|â–ˆ         | 241/2180 [38:42<5:10:03,  9.59s/it] 11%|â–ˆ         | 242/2180 [38:51<5:09:43,  9.59s/it]                                                    {'loss': 2.2599, 'learning_rate': 0.0009829949506800885, 'epoch': 0.11}
 11%|â–ˆ         | 242/2180 [38:51<5:09:43,  9.59s/it] 11%|â–ˆ         | 243/2180 [39:01<5:09:50,  9.60s/it]                                                    {'loss': 2.27, 'learning_rate': 0.0009828022810874405, 'epoch': 0.11}
 11%|â–ˆ         | 243/2180 [39:01<5:09:50,  9.60s/it] 11%|â–ˆ         | 244/2180 [39:11<5:09:23,  9.59s/it]                                                    {'loss': 2.2656, 'learning_rate': 0.0009826085452448693, 'epoch': 0.11}
 11%|â–ˆ         | 244/2180 [39:11<5:09:23,  9.59s/it] 11%|â–ˆ         | 245/2180 [39:20<5:09:02,  9.58s/it]                                                    {'loss': 2.3265, 'learning_rate': 0.000982413743580233, 'epoch': 0.11}
 11%|â–ˆ         | 245/2180 [39:20<5:09:02,  9.58s/it] 11%|â–ˆâ–        | 246/2180 [39:30<5:08:52,  9.58s/it]                                                    {'loss': 2.2219, 'learning_rate': 0.0009822178765237436, 'epoch': 0.11}
 11%|â–ˆâ–        | 246/2180 [39:30<5:08:52,  9.58s/it] 11%|â–ˆâ–        | 247/2180 [39:39<5:08:41,  9.58s/it]                                                    {'loss': 2.3072, 'learning_rate': 0.0009820209445079654, 'epoch': 0.11}
 11%|â–ˆâ–        | 247/2180 [39:39<5:08:41,  9.58s/it] 11%|â–ˆâ–        | 248/2180 [39:49<5:08:18,  9.57s/it]                                                    {'loss': 2.2575, 'learning_rate': 0.0009818229479678158, 'epoch': 0.11}
 11%|â–ˆâ–        | 248/2180 [39:49<5:08:18,  9.57s/it] 11%|â–ˆâ–        | 249/2180 [39:59<5:08:13,  9.58s/it]                                                    {'loss': 2.2415, 'learning_rate': 0.0009816238873405615, 'epoch': 0.11}
 11%|â–ˆâ–        | 249/2180 [39:59<5:08:13,  9.58s/it] 11%|â–ˆâ–        | 250/2180 [40:08<5:08:30,  9.59s/it]                                                    {'loss': 2.1936, 'learning_rate': 0.0009814237630658207, 'epoch': 0.11}
 11%|â–ˆâ–        | 250/2180 [40:08<5:08:30,  9.59s/it] 12%|â–ˆâ–        | 251/2180 [40:18<5:08:46,  9.60s/it]                                                    {'loss': 2.2312, 'learning_rate': 0.00098122257558556, 'epoch': 0.12}
 12%|â–ˆâ–        | 251/2180 [40:18<5:08:46,  9.60s/it] 12%|â–ˆâ–        | 252/2180 [40:27<5:08:31,  9.60s/it]                                                    {'loss': 2.1844, 'learning_rate': 0.0009810203253440937, 'epoch': 0.12}
 12%|â–ˆâ–        | 252/2180 [40:27<5:08:31,  9.60s/it] 12%|â–ˆâ–        | 253/2180 [40:37<5:08:09,  9.60s/it]                                                    {'loss': 2.1751, 'learning_rate': 0.0009808170127880837, 'epoch': 0.12}
 12%|â–ˆâ–        | 253/2180 [40:37<5:08:09,  9.60s/it] 12%|â–ˆâ–        | 254/2180 [40:47<5:08:20,  9.61s/it]                                                    {'loss': 2.1818, 'learning_rate': 0.000980612638366538, 'epoch': 0.12}
 12%|â–ˆâ–        | 254/2180 [40:47<5:08:20,  9.61s/it] 12%|â–ˆâ–        | 255/2180 [40:56<5:07:58,  9.60s/it]                                                    {'loss': 2.2502, 'learning_rate': 0.0009804072025308096, 'epoch': 0.12}
 12%|â–ˆâ–        | 255/2180 [40:56<5:07:58,  9.60s/it] 12%|â–ˆâ–        | 256/2180 [41:06<5:07:34,  9.59s/it]                                                    {'loss': 2.2223, 'learning_rate': 0.000980200705734595, 'epoch': 0.12}
 12%|â–ˆâ–        | 256/2180 [41:06<5:07:34,  9.59s/it] 12%|â–ˆâ–        | 257/2180 [41:15<5:06:56,  9.58s/it]                                                    {'loss': 2.2542, 'learning_rate': 0.0009799931484339344, 'epoch': 0.12}
 12%|â–ˆâ–        | 257/2180 [41:15<5:06:56,  9.58s/it] 12%|â–ˆâ–        | 258/2180 [41:25<5:06:38,  9.57s/it]                                                    {'loss': 2.1681, 'learning_rate': 0.0009797845310872103, 'epoch': 0.12}
 12%|â–ˆâ–        | 258/2180 [41:25<5:06:38,  9.57s/it] 12%|â–ˆâ–        | 259/2180 [41:34<5:06:36,  9.58s/it]                                                    {'loss': 2.2437, 'learning_rate': 0.0009795748541551457, 'epoch': 0.12}
 12%|â–ˆâ–        | 259/2180 [41:34<5:06:36,  9.58s/it] 12%|â–ˆâ–        | 260/2180 [41:44<5:06:47,  9.59s/it]                                                    {'loss': 2.1696, 'learning_rate': 0.000979364118100804, 'epoch': 0.12}
 12%|â–ˆâ–        | 260/2180 [41:44<5:06:47,  9.59s/it] 12%|â–ˆâ–        | 261/2180 [41:54<5:06:22,  9.58s/it]                                                    {'loss': 2.3099, 'learning_rate': 0.0009791523233895875, 'epoch': 0.12}
 12%|â–ˆâ–        | 261/2180 [41:54<5:06:22,  9.58s/it] 12%|â–ˆâ–        | 262/2180 [42:03<5:05:57,  9.57s/it]                                                    {'loss': 2.1901, 'learning_rate': 0.0009789394704892364, 'epoch': 0.12}
 12%|â–ˆâ–        | 262/2180 [42:03<5:05:57,  9.57s/it] 12%|â–ˆâ–        | 263/2180 [42:13<5:06:16,  9.59s/it]                                                    {'loss': 2.2302, 'learning_rate': 0.0009787255598698282, 'epoch': 0.12}
 12%|â–ˆâ–        | 263/2180 [42:13<5:06:16,  9.59s/it] 12%|â–ˆâ–        | 264/2180 [42:22<5:06:19,  9.59s/it]                                                    {'loss': 2.3455, 'learning_rate': 0.0009785105920037758, 'epoch': 0.12}
 12%|â–ˆâ–        | 264/2180 [42:22<5:06:19,  9.59s/it] 12%|â–ˆâ–        | 265/2180 [42:32<5:05:50,  9.58s/it]                                                    {'loss': 2.2543, 'learning_rate': 0.0009782945673658275, 'epoch': 0.12}
 12%|â–ˆâ–        | 265/2180 [42:32<5:05:50,  9.58s/it] 12%|â–ˆâ–        | 266/2180 [42:42<5:05:31,  9.58s/it]                                                    {'loss': 2.2372, 'learning_rate': 0.0009780774864330654, 'epoch': 0.12}
 12%|â–ˆâ–        | 266/2180 [42:42<5:05:31,  9.58s/it] 12%|â–ˆâ–        | 267/2180 [42:51<5:05:35,  9.58s/it]                                                    {'loss': 2.189, 'learning_rate': 0.000977859349684904, 'epoch': 0.12}
 12%|â–ˆâ–        | 267/2180 [42:51<5:05:35,  9.58s/it] 12%|â–ˆâ–        | 268/2180 [43:01<5:05:03,  9.57s/it]                                                    {'loss': 2.2349, 'learning_rate': 0.00097764015760309, 'epoch': 0.12}
 12%|â–ˆâ–        | 268/2180 [43:01<5:05:03,  9.57s/it] 12%|â–ˆâ–        | 269/2180 [43:10<5:04:42,  9.57s/it]                                                    {'loss': 2.2106, 'learning_rate': 0.0009774199106717004, 'epoch': 0.12}
 12%|â–ˆâ–        | 269/2180 [43:10<5:04:42,  9.57s/it] 12%|â–ˆâ–        | 270/2180 [43:20<5:04:44,  9.57s/it]                                                    {'loss': 2.1859, 'learning_rate': 0.0009771986093771417, 'epoch': 0.12}
 12%|â–ˆâ–        | 270/2180 [43:20<5:04:44,  9.57s/it] 12%|â–ˆâ–        | 271/2180 [43:29<5:04:23,  9.57s/it]                                                    {'loss': 2.1988, 'learning_rate': 0.0009769762542081496, 'epoch': 0.12}
 12%|â–ˆâ–        | 271/2180 [43:29<5:04:23,  9.57s/it] 12%|â–ˆâ–        | 272/2180 [43:39<5:04:19,  9.57s/it]                                                    {'loss': 2.2321, 'learning_rate': 0.000976752845655786, 'epoch': 0.12}
 12%|â–ˆâ–        | 272/2180 [43:39<5:04:19,  9.57s/it] 13%|â–ˆâ–Ž        | 273/2180 [43:49<5:04:37,  9.58s/it]                                                    {'loss': 2.1594, 'learning_rate': 0.0009765283842134411, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 273/2180 [43:49<5:04:37,  9.58s/it] 13%|â–ˆâ–Ž        | 274/2180 [43:58<5:04:09,  9.57s/it]                                                    {'loss': 2.2671, 'learning_rate': 0.0009763028703768282, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 274/2180 [43:58<5:04:09,  9.57s/it] 13%|â–ˆâ–Ž        | 275/2180 [44:08<5:04:59,  9.61s/it]                                                    {'loss': 2.1894, 'learning_rate': 0.0009760763046439862, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 275/2180 [44:08<5:04:59,  9.61s/it] 13%|â–ˆâ–Ž        | 276/2180 [44:17<5:04:29,  9.60s/it]                                                    {'loss': 2.2185, 'learning_rate': 0.0009758486875152766, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 276/2180 [44:17<5:04:29,  9.60s/it] 13%|â–ˆâ–Ž        | 277/2180 [44:27<5:04:43,  9.61s/it]                                                    {'loss': 2.24, 'learning_rate': 0.0009756200194933829, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 277/2180 [44:27<5:04:43,  9.61s/it] 13%|â–ˆâ–Ž        | 278/2180 [44:37<5:04:10,  9.60s/it]                                                    {'loss': 2.2807, 'learning_rate': 0.0009753903010833094, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 278/2180 [44:37<5:04:10,  9.60s/it] 13%|â–ˆâ–Ž        | 279/2180 [44:46<5:03:31,  9.58s/it]                                                    {'loss': 2.2455, 'learning_rate': 0.0009751595327923803, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 279/2180 [44:46<5:03:31,  9.58s/it] 13%|â–ˆâ–Ž        | 280/2180 [44:56<5:03:03,  9.57s/it]                                                    {'loss': 2.2376, 'learning_rate': 0.0009749277151302382, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 280/2180 [44:56<5:03:03,  9.57s/it] 13%|â–ˆâ–Ž        | 281/2180 [45:05<5:04:01,  9.61s/it]                                                    {'loss': 2.2048, 'learning_rate': 0.0009746948486088435, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 281/2180 [45:05<5:04:01,  9.61s/it] 13%|â–ˆâ–Ž        | 282/2180 [45:15<5:03:43,  9.60s/it]                                                    {'loss': 2.1654, 'learning_rate': 0.0009744609337424727, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 282/2180 [45:15<5:03:43,  9.60s/it] 13%|â–ˆâ–Ž        | 283/2180 [45:25<5:03:25,  9.60s/it]                                                    {'loss': 2.169, 'learning_rate': 0.0009742259710477177, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 283/2180 [45:25<5:03:25,  9.60s/it] 13%|â–ˆâ–Ž        | 284/2180 [45:34<5:03:23,  9.60s/it]                                                    {'loss': 2.1998, 'learning_rate': 0.0009739899610434841, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 284/2180 [45:34<5:03:23,  9.60s/it] 13%|â–ˆâ–Ž        | 285/2180 [45:44<5:03:03,  9.60s/it]                                                    {'loss': 2.194, 'learning_rate': 0.0009737529042509913, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 285/2180 [45:44<5:03:03,  9.60s/it] 13%|â–ˆâ–Ž        | 286/2180 [45:53<5:02:58,  9.60s/it]                                                    {'loss': 2.2583, 'learning_rate': 0.0009735148011937693, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 286/2180 [45:53<5:02:58,  9.60s/it] 13%|â–ˆâ–Ž        | 287/2180 [46:03<5:02:32,  9.59s/it]                                                    {'loss': 2.2592, 'learning_rate': 0.00097327565239766, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 287/2180 [46:03<5:02:32,  9.59s/it] 13%|â–ˆâ–Ž        | 288/2180 [46:12<5:02:28,  9.59s/it]                                                    {'loss': 2.2582, 'learning_rate': 0.0009730354583908136, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 288/2180 [46:13<5:02:28,  9.59s/it] 13%|â–ˆâ–Ž        | 289/2180 [46:22<5:02:23,  9.59s/it]                                                    {'loss': 2.1412, 'learning_rate': 0.0009727942197036895, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 289/2180 [46:22<5:02:23,  9.59s/it] 13%|â–ˆâ–Ž        | 290/2180 [46:32<5:02:11,  9.59s/it]                                                    {'loss': 2.2347, 'learning_rate': 0.0009725519368690539, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 290/2180 [46:32<5:02:11,  9.59s/it] 13%|â–ˆâ–Ž        | 291/2180 [46:41<5:02:16,  9.60s/it]                                                    {'loss': 2.1976, 'learning_rate': 0.0009723086104219787, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 291/2180 [46:41<5:02:16,  9.60s/it] 13%|â–ˆâ–Ž        | 292/2180 [46:51<5:01:36,  9.58s/it]                                                    {'loss': 2.1851, 'learning_rate': 0.0009720642408998409, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 292/2180 [46:51<5:01:36,  9.58s/it] 13%|â–ˆâ–Ž        | 293/2180 [47:00<5:01:37,  9.59s/it]                                                    {'loss': 2.1344, 'learning_rate': 0.0009718188288423211, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 293/2180 [47:00<5:01:37,  9.59s/it] 13%|â–ˆâ–Ž        | 294/2180 [47:10<5:01:37,  9.60s/it]                                                    {'loss': 2.2176, 'learning_rate': 0.0009715723747914022, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 294/2180 [47:10<5:01:37,  9.60s/it] 14%|â–ˆâ–Ž        | 295/2180 [47:20<5:01:30,  9.60s/it]                                                    {'loss': 2.1975, 'learning_rate': 0.0009713248792913685, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 295/2180 [47:20<5:01:30,  9.60s/it] 14%|â–ˆâ–Ž        | 296/2180 [47:29<5:01:10,  9.59s/it]                                                    {'loss': 2.1928, 'learning_rate': 0.0009710763428888037, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 296/2180 [47:29<5:01:10,  9.59s/it] 14%|â–ˆâ–Ž        | 297/2180 [47:39<5:00:46,  9.58s/it]                                                    {'loss': 2.2444, 'learning_rate': 0.0009708267661325909, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 297/2180 [47:39<5:00:46,  9.58s/it] 14%|â–ˆâ–Ž        | 298/2180 [47:48<5:00:29,  9.58s/it]                                                    {'loss': 2.2387, 'learning_rate': 0.0009705761495739107, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 298/2180 [47:48<5:00:29,  9.58s/it] 14%|â–ˆâ–Ž        | 299/2180 [47:58<5:00:37,  9.59s/it]                                                    {'loss': 2.2087, 'learning_rate': 0.0009703244937662399, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 299/2180 [47:58<5:00:37,  9.59s/it] 14%|â–ˆâ–        | 300/2180 [48:08<5:00:10,  9.58s/it]                                                    {'loss': 2.1557, 'learning_rate': 0.0009700717992653505, 'epoch': 0.14}
 14%|â–ˆâ–        | 300/2180 [48:08<5:00:10,  9.58s/it] 14%|â–ˆâ–        | 301/2180 [48:17<5:00:10,  9.59s/it]                                                    {'loss': 2.1623, 'learning_rate': 0.0009698180666293083, 'epoch': 0.14}
 14%|â–ˆâ–        | 301/2180 [48:17<5:00:10,  9.59s/it] 14%|â–ˆâ–        | 302/2180 [48:27<5:00:03,  9.59s/it]                                                    {'loss': 2.2093, 'learning_rate': 0.000969563296418472, 'epoch': 0.14}
 14%|â–ˆâ–        | 302/2180 [48:27<5:00:03,  9.59s/it] 14%|â–ˆâ–        | 303/2180 [48:36<5:00:15,  9.60s/it]                                                    {'loss': 2.1494, 'learning_rate': 0.0009693074891954914, 'epoch': 0.14}
 14%|â–ˆâ–        | 303/2180 [48:36<5:00:15,  9.60s/it] 14%|â–ˆâ–        | 304/2180 [48:46<4:59:40,  9.58s/it]                                                    {'loss': 2.1858, 'learning_rate': 0.0009690506455253072, 'epoch': 0.14}
 14%|â–ˆâ–        | 304/2180 [48:46<4:59:40,  9.58s/it] 14%|â–ˆâ–        | 305/2180 [48:56<4:59:32,  9.59s/it]                                                    {'loss': 2.2384, 'learning_rate': 0.0009687927659751481, 'epoch': 0.14}
 14%|â–ˆâ–        | 305/2180 [48:56<4:59:32,  9.59s/it] 14%|â–ˆâ–        | 306/2180 [49:05<4:59:28,  9.59s/it]                                                    {'loss': 2.1507, 'learning_rate': 0.0009685338511145312, 'epoch': 0.14}
 14%|â–ˆâ–        | 306/2180 [49:05<4:59:28,  9.59s/it] 14%|â–ˆâ–        | 307/2180 [49:15<4:59:31,  9.60s/it]                                                    {'loss': 2.2103, 'learning_rate': 0.0009682739015152598, 'epoch': 0.14}
 14%|â–ˆâ–        | 307/2180 [49:15<4:59:31,  9.60s/it] 14%|â–ˆâ–        | 308/2180 [49:24<4:59:25,  9.60s/it]                                                    {'loss': 2.2374, 'learning_rate': 0.0009680129177514226, 'epoch': 0.14}
 14%|â–ˆâ–        | 308/2180 [49:24<4:59:25,  9.60s/it] 14%|â–ˆâ–        | 309/2180 [49:34<4:59:13,  9.60s/it]                                                    {'loss': 2.2035, 'learning_rate': 0.0009677509003993915, 'epoch': 0.14}
 14%|â–ˆâ–        | 309/2180 [49:34<4:59:13,  9.60s/it] 14%|â–ˆâ–        | 310/2180 [49:44<4:59:14,  9.60s/it]                                                    {'loss': 2.2157, 'learning_rate': 0.0009674878500378221, 'epoch': 0.14}
 14%|â–ˆâ–        | 310/2180 [49:44<4:59:14,  9.60s/it] 14%|â–ˆâ–        | 311/2180 [49:53<4:59:05,  9.60s/it]                                                    {'loss': 2.1441, 'learning_rate': 0.0009672237672476505, 'epoch': 0.14}
 14%|â–ˆâ–        | 311/2180 [49:53<4:59:05,  9.60s/it] 14%|â–ˆâ–        | 312/2180 [50:03<4:59:45,  9.63s/it]                                                    {'loss': 2.1653, 'learning_rate': 0.0009669586526120935, 'epoch': 0.14}
 14%|â–ˆâ–        | 312/2180 [50:03<4:59:45,  9.63s/it] 14%|â–ˆâ–        | 313/2180 [50:12<5:00:08,  9.65s/it]                                                    {'loss': 2.1922, 'learning_rate': 0.0009666925067166459, 'epoch': 0.14}
 14%|â–ˆâ–        | 313/2180 [50:13<5:00:08,  9.65s/it] 14%|â–ˆâ–        | 314/2180 [50:22<4:59:31,  9.63s/it]                                                    {'loss': 2.1939, 'learning_rate': 0.000966425330149081, 'epoch': 0.14}
 14%|â–ˆâ–        | 314/2180 [50:22<4:59:31,  9.63s/it] 14%|â–ˆâ–        | 315/2180 [50:32<4:58:48,  9.61s/it]                                                    {'loss': 2.1677, 'learning_rate': 0.0009661571234994475, 'epoch': 0.14}
 14%|â–ˆâ–        | 315/2180 [50:32<4:58:48,  9.61s/it] 14%|â–ˆâ–        | 316/2180 [50:41<4:58:46,  9.62s/it]                                                    {'loss': 2.1424, 'learning_rate': 0.0009658878873600691, 'epoch': 0.14}
 14%|â–ˆâ–        | 316/2180 [50:41<4:58:46,  9.62s/it] 15%|â–ˆâ–        | 317/2180 [50:51<4:58:26,  9.61s/it]                                                    {'loss': 2.2017, 'learning_rate': 0.0009656176223255438, 'epoch': 0.15}
 15%|â–ˆâ–        | 317/2180 [50:51<4:58:26,  9.61s/it] 15%|â–ˆâ–        | 318/2180 [51:00<4:57:58,  9.60s/it]                                                    {'loss': 2.2406, 'learning_rate': 0.000965346328992741, 'epoch': 0.15}
 15%|â–ˆâ–        | 318/2180 [51:00<4:57:58,  9.60s/it] 15%|â–ˆâ–        | 319/2180 [51:10<4:57:44,  9.60s/it]                                                    {'loss': 2.2605, 'learning_rate': 0.0009650740079608014, 'epoch': 0.15}
 15%|â–ˆâ–        | 319/2180 [51:10<4:57:44,  9.60s/it] 15%|â–ˆâ–        | 320/2180 [51:20<4:57:18,  9.59s/it]                                                    {'loss': 2.1882, 'learning_rate': 0.0009648006598311353, 'epoch': 0.15}
 15%|â–ˆâ–        | 320/2180 [51:20<4:57:18,  9.59s/it] 15%|â–ˆâ–        | 321/2180 [51:29<4:56:41,  9.58s/it]                                                    {'loss': 2.1037, 'learning_rate': 0.0009645262852074214, 'epoch': 0.15}
 15%|â–ˆâ–        | 321/2180 [51:29<4:56:41,  9.58s/it] 15%|â–ˆâ–        | 322/2180 [51:39<4:56:35,  9.58s/it]                                                    {'loss': 2.143, 'learning_rate': 0.0009642508846956053, 'epoch': 0.15}
 15%|â–ˆâ–        | 322/2180 [51:39<4:56:35,  9.58s/it] 15%|â–ˆâ–        | 323/2180 [51:48<4:56:43,  9.59s/it]                                                    {'loss': 2.1924, 'learning_rate': 0.0009639744589038983, 'epoch': 0.15}
 15%|â–ˆâ–        | 323/2180 [51:48<4:56:43,  9.59s/it] 15%|â–ˆâ–        | 324/2180 [51:58<4:56:54,  9.60s/it]                                                    {'loss': 2.1639, 'learning_rate': 0.0009636970084427759, 'epoch': 0.15}
 15%|â–ˆâ–        | 324/2180 [51:58<4:56:54,  9.60s/it] 15%|â–ˆâ–        | 325/2180 [52:08<4:56:52,  9.60s/it]                                                    {'loss': 2.2704, 'learning_rate': 0.0009634185339249766, 'epoch': 0.15}
 15%|â–ˆâ–        | 325/2180 [52:08<4:56:52,  9.60s/it] 15%|â–ˆâ–        | 326/2180 [52:17<4:56:31,  9.60s/it]                                                    {'loss': 2.2198, 'learning_rate': 0.0009631390359655003, 'epoch': 0.15}
 15%|â–ˆâ–        | 326/2180 [52:17<4:56:31,  9.60s/it] 15%|â–ˆâ–Œ        | 327/2180 [52:27<4:56:04,  9.59s/it]                                                    {'loss': 2.2037, 'learning_rate': 0.0009628585151816074, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 327/2180 [52:27<4:56:04,  9.59s/it] 15%|â–ˆâ–Œ        | 328/2180 [52:36<4:56:01,  9.59s/it]                                                    {'loss': 2.2587, 'learning_rate': 0.0009625769721928172, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 328/2180 [52:36<4:56:01,  9.59s/it] 15%|â–ˆâ–Œ        | 329/2180 [52:46<4:57:14,  9.64s/it]                                                    {'loss': 2.1763, 'learning_rate': 0.0009622944076209061, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 329/2180 [52:46<4:57:14,  9.64s/it] 15%|â–ˆâ–Œ        | 330/2180 [52:56<4:56:12,  9.61s/it]                                                    {'loss': 2.2542, 'learning_rate': 0.0009620108220899071, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 330/2180 [52:56<4:56:12,  9.61s/it] 15%|â–ˆâ–Œ        | 331/2180 [53:05<4:55:42,  9.60s/it]                                                    {'loss': 2.2698, 'learning_rate': 0.0009617262162261075, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 331/2180 [53:05<4:55:42,  9.60s/it] 15%|â–ˆâ–Œ        | 332/2180 [53:15<4:55:36,  9.60s/it]                                                    {'loss': 2.2504, 'learning_rate': 0.0009614405906580486, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 332/2180 [53:15<4:55:36,  9.60s/it] 15%|â–ˆâ–Œ        | 333/2180 [53:24<4:55:23,  9.60s/it]                                                    {'loss': 2.2034, 'learning_rate': 0.000961153946016523, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 333/2180 [53:24<4:55:23,  9.60s/it] 15%|â–ˆâ–Œ        | 334/2180 [53:34<4:54:49,  9.58s/it]                                                    {'loss': 2.1907, 'learning_rate': 0.000960866282934574, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 334/2180 [53:34<4:54:49,  9.58s/it] 15%|â–ˆâ–Œ        | 335/2180 [53:44<4:55:35,  9.61s/it]                                                    {'loss': 2.2542, 'learning_rate': 0.0009605776020474945, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 335/2180 [53:44<4:55:35,  9.61s/it] 15%|â–ˆâ–Œ        | 336/2180 [53:53<4:54:50,  9.59s/it]                                                    {'loss': 2.1444, 'learning_rate': 0.0009602879039928249, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 336/2180 [53:53<4:54:50,  9.59s/it] 15%|â–ˆâ–Œ        | 337/2180 [54:03<4:54:30,  9.59s/it]                                                    {'loss': 2.2023, 'learning_rate': 0.0009599971894103521, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 337/2180 [54:03<4:54:30,  9.59s/it] 16%|â–ˆâ–Œ        | 338/2180 [54:12<4:55:11,  9.62s/it]                                                    {'loss': 2.1788, 'learning_rate': 0.0009597054589421077, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 338/2180 [54:12<4:55:11,  9.62s/it] 16%|â–ˆâ–Œ        | 339/2180 [54:22<4:54:54,  9.61s/it]                                                    {'loss': 2.1526, 'learning_rate': 0.0009594127132323669, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 339/2180 [54:22<4:54:54,  9.61s/it] 16%|â–ˆâ–Œ        | 340/2180 [54:32<4:54:20,  9.60s/it]                                                    {'loss': 2.2539, 'learning_rate': 0.0009591189529276474, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 340/2180 [54:32<4:54:20,  9.60s/it] 16%|â–ˆâ–Œ        | 341/2180 [54:41<4:53:27,  9.57s/it]                                                    {'loss': 2.1704, 'learning_rate': 0.0009588241786767072, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 341/2180 [54:41<4:53:27,  9.57s/it] 16%|â–ˆâ–Œ        | 342/2180 [54:51<4:53:29,  9.58s/it]                                                    {'loss': 2.1697, 'learning_rate': 0.0009585283911305436, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 342/2180 [54:51<4:53:29,  9.58s/it] 16%|â–ˆâ–Œ        | 343/2180 [55:00<4:53:44,  9.59s/it]                                                    {'loss': 2.2198, 'learning_rate': 0.000958231590942392, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 343/2180 [55:00<4:53:44,  9.59s/it] 16%|â–ˆâ–Œ        | 344/2180 [55:10<4:53:52,  9.60s/it]                                                    {'loss': 2.1102, 'learning_rate': 0.0009579337787677238, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 344/2180 [55:10<4:53:52,  9.60s/it] 16%|â–ˆâ–Œ        | 345/2180 [55:20<4:54:13,  9.62s/it]                                                    {'loss': 2.2774, 'learning_rate': 0.0009576349552642456, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 345/2180 [55:20<4:54:13,  9.62s/it] 16%|â–ˆâ–Œ        | 346/2180 [55:29<4:53:59,  9.62s/it]                                                    {'loss': 2.2096, 'learning_rate': 0.0009573351210918975, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 346/2180 [55:29<4:53:59,  9.62s/it] 16%|â–ˆâ–Œ        | 347/2180 [55:39<4:54:51,  9.65s/it]                                                    {'loss': 2.2709, 'learning_rate': 0.0009570342769128514, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 347/2180 [55:39<4:54:51,  9.65s/it] 16%|â–ˆâ–Œ        | 348/2180 [55:49<4:53:55,  9.63s/it]                                                    {'loss': 2.1925, 'learning_rate': 0.0009567324233915099, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 348/2180 [55:49<4:53:55,  9.63s/it] 16%|â–ˆâ–Œ        | 349/2180 [55:58<4:53:36,  9.62s/it]                                                    {'loss': 2.2347, 'learning_rate': 0.0009564295611945047, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 349/2180 [55:58<4:53:36,  9.62s/it] 16%|â–ˆâ–Œ        | 350/2180 [56:08<4:53:06,  9.61s/it]                                                    {'loss': 2.1653, 'learning_rate': 0.000956125690990695, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 350/2180 [56:08<4:53:06,  9.61s/it] 16%|â–ˆâ–Œ        | 351/2180 [56:17<4:53:16,  9.62s/it]                                                    {'loss': 2.174, 'learning_rate': 0.0009558208134511665, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 351/2180 [56:17<4:53:16,  9.62s/it] 16%|â–ˆâ–Œ        | 352/2180 [56:27<4:52:43,  9.61s/it]                                                    {'loss': 2.2169, 'learning_rate': 0.0009555149292492289, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 352/2180 [56:27<4:52:43,  9.61s/it] 16%|â–ˆâ–Œ        | 353/2180 [56:36<4:51:57,  9.59s/it]                                                    {'loss': 2.199, 'learning_rate': 0.0009552080390604159, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 353/2180 [56:36<4:51:57,  9.59s/it] 16%|â–ˆâ–Œ        | 354/2180 [56:46<4:52:07,  9.60s/it]                                                    {'loss': 2.2534, 'learning_rate': 0.0009549001435624823, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 354/2180 [56:46<4:52:07,  9.60s/it] 16%|â–ˆâ–‹        | 355/2180 [56:56<4:52:14,  9.61s/it]                                                    {'loss': 2.1941, 'learning_rate': 0.0009545912434354029, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 355/2180 [56:56<4:52:14,  9.61s/it] 16%|â–ˆâ–‹        | 356/2180 [57:05<4:52:47,  9.63s/it]                                                    {'loss': 2.2097, 'learning_rate': 0.0009542813393613721, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 356/2180 [57:05<4:52:47,  9.63s/it] 16%|â–ˆâ–‹        | 357/2180 [57:15<4:52:15,  9.62s/it]                                                    {'loss': 2.0773, 'learning_rate': 0.0009539704320248006, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 357/2180 [57:15<4:52:15,  9.62s/it] 16%|â–ˆâ–‹        | 358/2180 [57:25<4:51:45,  9.61s/it]                                                    {'loss': 2.2165, 'learning_rate': 0.0009536585221123151, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 358/2180 [57:25<4:51:45,  9.61s/it] 16%|â–ˆâ–‹        | 359/2180 [57:34<4:51:37,  9.61s/it]                                                    {'loss': 2.188, 'learning_rate': 0.0009533456103127565, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 359/2180 [57:34<4:51:37,  9.61s/it] 17%|â–ˆâ–‹        | 360/2180 [57:44<4:51:11,  9.60s/it]                                                    {'loss': 2.2344, 'learning_rate': 0.000953031697317178, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 360/2180 [57:44<4:51:11,  9.60s/it] 17%|â–ˆâ–‹        | 361/2180 [57:53<4:50:56,  9.60s/it]                                                    {'loss': 2.1565, 'learning_rate': 0.0009527167838188445, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 361/2180 [57:53<4:50:56,  9.60s/it] 17%|â–ˆâ–‹        | 362/2180 [58:03<4:50:54,  9.60s/it]                                                    {'loss': 2.1666, 'learning_rate': 0.0009524008705132299, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 362/2180 [58:03<4:50:54,  9.60s/it] 17%|â–ˆâ–‹        | 363/2180 [58:13<4:51:01,  9.61s/it]                                                    {'loss': 2.1762, 'learning_rate': 0.0009520839580980166, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 363/2180 [58:13<4:51:01,  9.61s/it] 17%|â–ˆâ–‹        | 364/2180 [58:22<4:50:31,  9.60s/it]                                                    {'loss': 2.1529, 'learning_rate': 0.0009517660472730929, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 364/2180 [58:22<4:50:31,  9.60s/it] 17%|â–ˆâ–‹        | 365/2180 [58:32<4:50:08,  9.59s/it]                                                    {'loss': 2.1967, 'learning_rate': 0.0009514471387405526, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 365/2180 [58:32<4:50:08,  9.59s/it] 17%|â–ˆâ–‹        | 366/2180 [58:41<4:49:57,  9.59s/it]                                                    {'loss': 2.2119, 'learning_rate': 0.0009511272332046926, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 366/2180 [58:41<4:49:57,  9.59s/it] 17%|â–ˆâ–‹        | 367/2180 [58:51<4:49:35,  9.58s/it]                                                    {'loss': 2.2309, 'learning_rate': 0.0009508063313720119, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 367/2180 [58:51<4:49:35,  9.58s/it] 17%|â–ˆâ–‹        | 368/2180 [59:01<4:49:37,  9.59s/it]                                                    {'loss': 2.1343, 'learning_rate': 0.0009504844339512095, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 368/2180 [59:01<4:49:37,  9.59s/it] 17%|â–ˆâ–‹        | 369/2180 [59:10<4:49:18,  9.58s/it]                                                    {'loss': 2.2299, 'learning_rate': 0.0009501615416531835, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 369/2180 [59:10<4:49:18,  9.58s/it] 17%|â–ˆâ–‹        | 370/2180 [59:20<4:49:12,  9.59s/it]                                                    {'loss': 2.1565, 'learning_rate': 0.0009498376551910285, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 370/2180 [59:20<4:49:12,  9.59s/it] 17%|â–ˆâ–‹        | 371/2180 [59:29<4:49:19,  9.60s/it]                                                    {'loss': 2.162, 'learning_rate': 0.0009495127752800352, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 371/2180 [59:29<4:49:19,  9.60s/it] 17%|â–ˆâ–‹        | 372/2180 [59:39<4:49:03,  9.59s/it]                                                    {'loss': 2.1459, 'learning_rate': 0.0009491869026376882, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 372/2180 [59:39<4:49:03,  9.59s/it] 17%|â–ˆâ–‹        | 373/2180 [59:48<4:48:40,  9.59s/it]                                                    {'loss': 2.1725, 'learning_rate': 0.0009488600379836648, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 373/2180 [59:48<4:48:40,  9.59s/it] 17%|â–ˆâ–‹        | 374/2180 [59:58<4:48:05,  9.57s/it]                                                    {'loss': 2.2087, 'learning_rate': 0.0009485321820398321, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 374/2180 [59:58<4:48:05,  9.57s/it] 17%|â–ˆâ–‹        | 375/2180 [1:00:08<4:48:02,  9.57s/it]                                                      {'loss': 2.319, 'learning_rate': 0.0009482033355302475, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 375/2180 [1:00:08<4:48:02,  9.57s/it] 17%|â–ˆâ–‹        | 376/2180 [1:00:17<4:47:37,  9.57s/it]                                                      {'loss': 2.1521, 'learning_rate': 0.0009478734991811556, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 376/2180 [1:00:17<4:47:37,  9.57s/it] 17%|â–ˆâ–‹        | 377/2180 [1:00:27<4:47:56,  9.58s/it]                                                      {'loss': 2.1598, 'learning_rate': 0.0009475426737209871, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 377/2180 [1:00:27<4:47:56,  9.58s/it] 17%|â–ˆâ–‹        | 378/2180 [1:00:36<4:47:52,  9.58s/it]                                                      {'loss': 2.2001, 'learning_rate': 0.000947210859880357, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 378/2180 [1:00:36<4:47:52,  9.58s/it] 17%|â–ˆâ–‹        | 379/2180 [1:00:46<4:48:04,  9.60s/it]                                                      {'loss': 2.2374, 'learning_rate': 0.0009468780583920631, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 379/2180 [1:00:46<4:48:04,  9.60s/it] 17%|â–ˆâ–‹        | 380/2180 [1:00:56<4:48:09,  9.61s/it]                                                      {'loss': 2.1442, 'learning_rate': 0.0009465442699910846, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 380/2180 [1:00:56<4:48:09,  9.61s/it] 17%|â–ˆâ–‹        | 381/2180 [1:01:05<4:48:03,  9.61s/it]                                                      {'loss': 2.1223, 'learning_rate': 0.0009462094954145801, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 381/2180 [1:01:05<4:48:03,  9.61s/it] 18%|â–ˆâ–Š        | 382/2180 [1:01:15<4:47:45,  9.60s/it]                                                      {'loss': 2.139, 'learning_rate': 0.0009458737354018859, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 382/2180 [1:01:15<4:47:45,  9.60s/it] 18%|â–ˆâ–Š        | 383/2180 [1:01:24<4:47:35,  9.60s/it]                                                      {'loss': 2.2755, 'learning_rate': 0.000945536990694515, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 383/2180 [1:01:24<4:47:35,  9.60s/it] 18%|â–ˆâ–Š        | 384/2180 [1:01:34<4:47:25,  9.60s/it]                                                      {'loss': 2.2623, 'learning_rate': 0.0009451992620361551, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 384/2180 [1:01:34<4:47:25,  9.60s/it] 18%|â–ˆâ–Š        | 385/2180 [1:01:44<4:47:05,  9.60s/it]                                                      {'loss': 2.19, 'learning_rate': 0.0009448605501726664, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 385/2180 [1:01:44<4:47:05,  9.60s/it] 18%|â–ˆâ–Š        | 386/2180 [1:01:53<4:46:48,  9.59s/it]                                                      {'loss': 2.1509, 'learning_rate': 0.000944520855852081, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 386/2180 [1:01:53<4:46:48,  9.59s/it] 18%|â–ˆâ–Š        | 387/2180 [1:02:03<4:46:22,  9.58s/it]                                                      {'loss': 2.0943, 'learning_rate': 0.0009441801798246002, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 387/2180 [1:02:03<4:46:22,  9.58s/it] 18%|â–ˆâ–Š        | 388/2180 [1:02:12<4:46:39,  9.60s/it]                                                      {'loss': 2.1844, 'learning_rate': 0.0009438385228425939, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 388/2180 [1:02:12<4:46:39,  9.60s/it] 18%|â–ˆâ–Š        | 389/2180 [1:02:22<4:46:39,  9.60s/it]                                                      {'loss': 2.2178, 'learning_rate': 0.0009434958856605982, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 389/2180 [1:02:22<4:46:39,  9.60s/it] 18%|â–ˆâ–Š        | 390/2180 [1:02:32<4:46:00,  9.59s/it]                                                      {'loss': 2.1614, 'learning_rate': 0.0009431522690353137, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 390/2180 [1:02:32<4:46:00,  9.59s/it] 18%|â–ˆâ–Š        | 391/2180 [1:02:41<4:45:54,  9.59s/it]                                                      {'loss': 2.1206, 'learning_rate': 0.0009428076737256044, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 391/2180 [1:02:41<4:45:54,  9.59s/it] 18%|â–ˆâ–Š        | 392/2180 [1:02:51<4:45:38,  9.59s/it]                                                      {'loss': 2.2314, 'learning_rate': 0.0009424621004924954, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 392/2180 [1:02:51<4:45:38,  9.59s/it] 18%|â–ˆâ–Š        | 393/2180 [1:03:00<4:45:15,  9.58s/it]                                                      {'loss': 2.2742, 'learning_rate': 0.0009421155500991719, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 393/2180 [1:03:00<4:45:15,  9.58s/it] 18%|â–ˆâ–Š        | 394/2180 [1:03:10<4:45:19,  9.59s/it]                                                      {'loss': 2.0907, 'learning_rate': 0.0009417680233109767, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 394/2180 [1:03:10<4:45:19,  9.59s/it] 18%|â–ˆâ–Š        | 395/2180 [1:03:19<4:45:15,  9.59s/it]                                                      {'loss': 2.2175, 'learning_rate': 0.000941419520895409, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 395/2180 [1:03:19<4:45:15,  9.59s/it] 18%|â–ˆâ–Š        | 396/2180 [1:03:29<4:45:00,  9.59s/it]                                                      {'loss': 2.1696, 'learning_rate': 0.0009410700436221229, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 396/2180 [1:03:29<4:45:00,  9.59s/it] 18%|â–ˆâ–Š        | 397/2180 [1:03:39<4:45:24,  9.60s/it]                                                      {'loss': 2.1217, 'learning_rate': 0.0009407195922629252, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 397/2180 [1:03:39<4:45:24,  9.60s/it] 18%|â–ˆâ–Š        | 398/2180 [1:03:48<4:44:55,  9.59s/it]                                                      {'loss': 2.2344, 'learning_rate': 0.000940368167591774, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 398/2180 [1:03:48<4:44:55,  9.59s/it] 18%|â–ˆâ–Š        | 399/2180 [1:03:58<4:44:29,  9.58s/it]                                                      {'loss': 2.1545, 'learning_rate': 0.0009400157703847769, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 399/2180 [1:03:58<4:44:29,  9.58s/it] 18%|â–ˆâ–Š        | 400/2180 [1:04:08<4:45:13,  9.61s/it]                                                      {'loss': 2.1721, 'learning_rate': 0.0009396624014201895, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 400/2180 [1:04:08<4:45:13,  9.61s/it] 18%|â–ˆâ–Š        | 401/2180 [1:04:17<4:44:51,  9.61s/it]                                                      {'loss': 2.2915, 'learning_rate': 0.000939308061478413, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 401/2180 [1:04:17<4:44:51,  9.61s/it] 18%|â–ˆâ–Š        | 402/2180 [1:04:27<4:45:04,  9.62s/it]                                                      {'loss': 2.1734, 'learning_rate': 0.0009389527513419935, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 402/2180 [1:04:27<4:45:04,  9.62s/it] 18%|â–ˆâ–Š        | 403/2180 [1:04:36<4:44:53,  9.62s/it]                                                      {'loss': 2.1435, 'learning_rate': 0.0009385964717956195, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 403/2180 [1:04:36<4:44:53,  9.62s/it] 19%|â–ˆâ–Š        | 404/2180 [1:04:46<4:44:15,  9.60s/it]                                                      {'loss': 2.1691, 'learning_rate': 0.0009382392236261201, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 404/2180 [1:04:46<4:44:15,  9.60s/it] 19%|â–ˆâ–Š        | 405/2180 [1:04:56<4:44:00,  9.60s/it]                                                      {'loss': 2.2243, 'learning_rate': 0.0009378810076224644, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 405/2180 [1:04:56<4:44:00,  9.60s/it] 19%|â–ˆâ–Š        | 406/2180 [1:05:05<4:43:43,  9.60s/it]                                                      {'loss': 2.1515, 'learning_rate': 0.0009375218245757582, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 406/2180 [1:05:05<4:43:43,  9.60s/it] 19%|â–ˆâ–Š        | 407/2180 [1:05:15<4:43:30,  9.59s/it]                                                      {'loss': 2.1315, 'learning_rate': 0.0009371616752792432, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 407/2180 [1:05:15<4:43:30,  9.59s/it] 19%|â–ˆâ–Š        | 408/2180 [1:05:24<4:43:12,  9.59s/it]                                                      {'loss': 2.2596, 'learning_rate': 0.0009368005605282949, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 408/2180 [1:05:24<4:43:12,  9.59s/it] 19%|â–ˆâ–‰        | 409/2180 [1:05:34<4:43:00,  9.59s/it]                                                      {'loss': 2.222, 'learning_rate': 0.0009364384811204212, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 409/2180 [1:05:34<4:43:00,  9.59s/it] 19%|â–ˆâ–‰        | 410/2180 [1:05:43<4:42:50,  9.59s/it]                                                      {'loss': 2.1443, 'learning_rate': 0.00093607543785526, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 410/2180 [1:05:43<4:42:50,  9.59s/it] 19%|â–ˆâ–‰        | 411/2180 [1:05:53<4:42:52,  9.59s/it]                                                      {'loss': 2.1678, 'learning_rate': 0.0009357114315345787, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 411/2180 [1:05:53<4:42:52,  9.59s/it] 19%|â–ˆâ–‰        | 412/2180 [1:06:03<4:43:05,  9.61s/it]                                                      {'loss': 2.1594, 'learning_rate': 0.0009353464629622705, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 412/2180 [1:06:03<4:43:05,  9.61s/it] 19%|â–ˆâ–‰        | 413/2180 [1:06:12<4:42:34,  9.60s/it]                                                      {'loss': 2.2005, 'learning_rate': 0.0009349805329443544, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 413/2180 [1:06:12<4:42:34,  9.60s/it] 19%|â–ˆâ–‰        | 414/2180 [1:06:22<4:42:24,  9.59s/it]                                                      {'loss': 2.1808, 'learning_rate': 0.0009346136422889724, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 414/2180 [1:06:22<4:42:24,  9.59s/it] 19%|â–ˆâ–‰        | 415/2180 [1:06:32<4:42:52,  9.62s/it]                                                      {'loss': 2.1689, 'learning_rate': 0.0009342457918063882, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 415/2180 [1:06:32<4:42:52,  9.62s/it] 19%|â–ˆâ–‰        | 416/2180 [1:06:41<4:42:21,  9.60s/it]                                                      {'loss': 2.1905, 'learning_rate': 0.0009338769823089853, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 416/2180 [1:06:41<4:42:21,  9.60s/it] 19%|â–ˆâ–‰        | 417/2180 [1:06:51<4:42:03,  9.60s/it]                                                      {'loss': 2.2859, 'learning_rate': 0.0009335072146112648, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 417/2180 [1:06:51<4:42:03,  9.60s/it] 19%|â–ˆâ–‰        | 418/2180 [1:07:00<4:41:22,  9.58s/it]                                                      {'loss': 2.1001, 'learning_rate': 0.0009331364895298444, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 418/2180 [1:07:00<4:41:22,  9.58s/it] 19%|â–ˆâ–‰        | 419/2180 [1:07:10<4:41:09,  9.58s/it]                                                      {'loss': 2.2265, 'learning_rate': 0.0009327648078834559, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 419/2180 [1:07:10<4:41:09,  9.58s/it] 19%|â–ˆâ–‰        | 420/2180 [1:07:19<4:41:05,  9.58s/it]                                                      {'loss': 2.182, 'learning_rate': 0.0009323921704929434, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 420/2180 [1:07:19<4:41:05,  9.58s/it] 19%|â–ˆâ–‰        | 421/2180 [1:07:29<4:40:41,  9.57s/it]                                                      {'loss': 2.0791, 'learning_rate': 0.0009320185781812623, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 421/2180 [1:07:29<4:40:41,  9.57s/it] 19%|â–ˆâ–‰        | 422/2180 [1:07:39<4:40:38,  9.58s/it]                                                      {'loss': 2.1286, 'learning_rate': 0.0009316440317734762, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 422/2180 [1:07:39<4:40:38,  9.58s/it] 19%|â–ˆâ–‰        | 423/2180 [1:07:48<4:40:48,  9.59s/it]                                                      {'loss': 2.2289, 'learning_rate': 0.0009312685320967565, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 423/2180 [1:07:48<4:40:48,  9.59s/it] 19%|â–ˆâ–‰        | 424/2180 [1:07:58<4:41:05,  9.60s/it]                                                      {'loss': 2.1284, 'learning_rate': 0.0009308920799803793, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 424/2180 [1:07:58<4:41:05,  9.60s/it] 19%|â–ˆâ–‰        | 425/2180 [1:08:07<4:40:39,  9.60s/it]                                                      {'loss': 2.1908, 'learning_rate': 0.0009305146762557246, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 425/2180 [1:08:07<4:40:39,  9.60s/it] 20%|â–ˆâ–‰        | 426/2180 [1:08:17<4:40:27,  9.59s/it]                                                      {'loss': 2.1684, 'learning_rate': 0.0009301363217562736, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 426/2180 [1:08:17<4:40:27,  9.59s/it] 20%|â–ˆâ–‰        | 427/2180 [1:08:27<4:40:46,  9.61s/it]                                                      {'loss': 2.1679, 'learning_rate': 0.0009297570173176074, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 427/2180 [1:08:27<4:40:46,  9.61s/it] 20%|â–ˆâ–‰        | 428/2180 [1:08:36<4:40:23,  9.60s/it]                                                      {'loss': 2.1613, 'learning_rate': 0.000929376763777405, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 428/2180 [1:08:36<4:40:23,  9.60s/it] 20%|â–ˆâ–‰        | 429/2180 [1:08:46<4:39:52,  9.59s/it]                                                      {'loss': 2.1717, 'learning_rate': 0.0009289955619754413, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 429/2180 [1:08:46<4:39:52,  9.59s/it] 20%|â–ˆâ–‰        | 430/2180 [1:08:55<4:39:57,  9.60s/it]                                                      {'loss': 2.2136, 'learning_rate': 0.0009286134127535859, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 430/2180 [1:08:55<4:39:57,  9.60s/it] 20%|â–ˆâ–‰        | 431/2180 [1:09:05<4:39:57,  9.60s/it]                                                      {'loss': 2.2186, 'learning_rate': 0.0009282303169558, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 431/2180 [1:09:05<4:39:57,  9.60s/it] 20%|â–ˆâ–‰        | 432/2180 [1:09:15<4:40:06,  9.61s/it]                                                      {'loss': 2.1622, 'learning_rate': 0.0009278462754281359, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 432/2180 [1:09:15<4:40:06,  9.61s/it] 20%|â–ˆâ–‰        | 433/2180 [1:09:24<4:39:46,  9.61s/it]                                                      {'loss': 2.1739, 'learning_rate': 0.0009274612890187342, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 433/2180 [1:09:24<4:39:46,  9.61s/it] 20%|â–ˆâ–‰        | 434/2180 [1:09:34<4:39:25,  9.60s/it]                                                      {'loss': 2.1418, 'learning_rate': 0.0009270753585778222, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 434/2180 [1:09:34<4:39:25,  9.60s/it] 20%|â–ˆâ–‰        | 435/2180 [1:09:43<4:39:02,  9.59s/it]                                                      {'loss': 2.1621, 'learning_rate': 0.0009266884849577124, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 435/2180 [1:09:43<4:39:02,  9.59s/it] 20%|â–ˆâ–ˆ        | 436/2180 [1:09:53<4:38:48,  9.59s/it]                                                      {'loss': 2.1426, 'learning_rate': 0.0009263006690127998, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 436/2180 [1:09:53<4:38:48,  9.59s/it] 20%|â–ˆâ–ˆ        | 437/2180 [1:10:03<4:38:13,  9.58s/it]                                                      {'loss': 2.1602, 'learning_rate': 0.0009259119115995609, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 437/2180 [1:10:03<4:38:13,  9.58s/it] 20%|â–ˆâ–ˆ        | 438/2180 [1:10:12<4:38:08,  9.58s/it]                                                      {'loss': 2.1429, 'learning_rate': 0.0009255222135765511, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 438/2180 [1:10:12<4:38:08,  9.58s/it] 20%|â–ˆâ–ˆ        | 439/2180 [1:10:22<4:37:57,  9.58s/it]                                                      {'loss': 2.2314, 'learning_rate': 0.0009251315758044032, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 439/2180 [1:10:22<4:37:57,  9.58s/it] 20%|â–ˆâ–ˆ        | 440/2180 [1:10:31<4:37:38,  9.57s/it]                                                      {'loss': 2.1167, 'learning_rate': 0.0009247399991458255, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 440/2180 [1:10:31<4:37:38,  9.57s/it] 20%|â–ˆâ–ˆ        | 441/2180 [1:10:41<4:37:39,  9.58s/it]                                                      {'loss': 2.1926, 'learning_rate': 0.0009243474844655996, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 441/2180 [1:10:41<4:37:39,  9.58s/it] 20%|â–ˆâ–ˆ        | 442/2180 [1:10:50<4:37:20,  9.57s/it]                                                      {'loss': 2.1511, 'learning_rate': 0.0009239540326305791, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 442/2180 [1:10:50<4:37:20,  9.57s/it] 20%|â–ˆâ–ˆ        | 443/2180 [1:11:00<4:37:28,  9.58s/it]                                                      {'loss': 2.1998, 'learning_rate': 0.0009235596445096864, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 443/2180 [1:11:00<4:37:28,  9.58s/it] 20%|â–ˆâ–ˆ        | 444/2180 [1:11:10<4:37:22,  9.59s/it]                                                      {'loss': 2.1712, 'learning_rate': 0.0009231643209739127, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 444/2180 [1:11:10<4:37:22,  9.59s/it] 20%|â–ˆâ–ˆ        | 445/2180 [1:11:19<4:37:05,  9.58s/it]                                                      {'loss': 2.1957, 'learning_rate': 0.0009227680628963145, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 445/2180 [1:11:19<4:37:05,  9.58s/it] 20%|â–ˆâ–ˆ        | 446/2180 [1:11:29<4:37:06,  9.59s/it]                                                      {'loss': 2.1967, 'learning_rate': 0.000922370871152012, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 446/2180 [1:11:29<4:37:06,  9.59s/it] 21%|â–ˆâ–ˆ        | 447/2180 [1:11:38<4:36:32,  9.57s/it]                                                      {'loss': 2.2101, 'learning_rate': 0.0009219727466181877, 'epoch': 0.2}
 21%|â–ˆâ–ˆ        | 447/2180 [1:11:38<4:36:32,  9.57s/it] 21%|â–ˆâ–ˆ        | 448/2180 [1:11:48<4:36:39,  9.58s/it]                                                      {'loss': 2.1716, 'learning_rate': 0.0009215736901740841, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 448/2180 [1:11:48<4:36:39,  9.58s/it] 21%|â–ˆâ–ˆ        | 449/2180 [1:11:58<4:37:02,  9.60s/it]                                                      {'loss': 2.0724, 'learning_rate': 0.0009211737027010016, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 449/2180 [1:11:58<4:37:02,  9.60s/it] 21%|â–ˆâ–ˆ        | 450/2180 [1:12:07<4:36:46,  9.60s/it]                                                      {'loss': 2.0534, 'learning_rate': 0.0009207727850822971, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 450/2180 [1:12:07<4:36:46,  9.60s/it] 21%|â–ˆâ–ˆ        | 451/2180 [1:12:17<4:37:35,  9.63s/it]                                                      {'loss': 2.2091, 'learning_rate': 0.0009203709382033814, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 451/2180 [1:12:17<4:37:35,  9.63s/it] 21%|â–ˆâ–ˆ        | 452/2180 [1:12:26<4:37:05,  9.62s/it]                                                      {'loss': 2.1936, 'learning_rate': 0.0009199681629517173, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 452/2180 [1:12:26<4:37:05,  9.62s/it] 21%|â–ˆâ–ˆ        | 453/2180 [1:12:36<4:37:07,  9.63s/it]                                                      {'loss': 2.1429, 'learning_rate': 0.0009195644602168184, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 453/2180 [1:12:36<4:37:07,  9.63s/it] 21%|â–ˆâ–ˆ        | 454/2180 [1:12:46<4:36:45,  9.62s/it]                                                      {'loss': 2.1807, 'learning_rate': 0.0009191598308902464, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 454/2180 [1:12:46<4:36:45,  9.62s/it] 21%|â–ˆâ–ˆ        | 455/2180 [1:12:55<4:36:02,  9.60s/it]                                                      {'loss': 2.0937, 'learning_rate': 0.0009187542758656091, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 455/2180 [1:12:55<4:36:02,  9.60s/it] 21%|â–ˆâ–ˆ        | 456/2180 [1:13:05<4:35:42,  9.60s/it]                                                      {'loss': 2.0802, 'learning_rate': 0.0009183477960385591, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 456/2180 [1:13:05<4:35:42,  9.60s/it] 21%|â–ˆâ–ˆ        | 457/2180 [1:13:15<4:37:10,  9.65s/it]                                                      {'loss': 2.1864, 'learning_rate': 0.0009179403923067912, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 457/2180 [1:13:15<4:37:10,  9.65s/it] 21%|â–ˆâ–ˆ        | 458/2180 [1:13:24<4:36:13,  9.62s/it]                                                      {'loss': 2.162, 'learning_rate': 0.0009175320655700406, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 458/2180 [1:13:24<4:36:13,  9.62s/it] 21%|â–ˆâ–ˆ        | 459/2180 [1:13:34<4:36:36,  9.64s/it]                                                      {'loss': 2.189, 'learning_rate': 0.0009171228167300805, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 459/2180 [1:13:34<4:36:36,  9.64s/it] 21%|â–ˆâ–ˆ        | 460/2180 [1:13:43<4:36:04,  9.63s/it]                                                      {'loss': 2.1233, 'learning_rate': 0.0009167126466907215, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 460/2180 [1:13:43<4:36:04,  9.63s/it] 21%|â–ˆâ–ˆ        | 461/2180 [1:13:53<4:35:37,  9.62s/it]                                                      {'loss': 2.2583, 'learning_rate': 0.0009163015563578074, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 461/2180 [1:13:53<4:35:37,  9.62s/it] 21%|â–ˆâ–ˆ        | 462/2180 [1:14:03<4:34:56,  9.60s/it]                                                      {'loss': 2.0897, 'learning_rate': 0.0009158895466392158, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 462/2180 [1:14:03<4:34:56,  9.60s/it] 21%|â–ˆâ–ˆ        | 463/2180 [1:14:12<4:34:49,  9.60s/it]                                                      {'loss': 2.1149, 'learning_rate': 0.0009154766184448535, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 463/2180 [1:14:12<4:34:49,  9.60s/it] 21%|â–ˆâ–ˆâ–       | 464/2180 [1:14:22<4:34:07,  9.58s/it]                                                      {'loss': 2.1678, 'learning_rate': 0.0009150627726866568, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 464/2180 [1:14:22<4:34:07,  9.58s/it] 21%|â–ˆâ–ˆâ–       | 465/2180 [1:14:31<4:33:47,  9.58s/it]                                                      {'loss': 2.1643, 'learning_rate': 0.000914648010278587, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 465/2180 [1:14:31<4:33:47,  9.58s/it] 21%|â–ˆâ–ˆâ–       | 466/2180 [1:14:41<4:33:35,  9.58s/it]                                                      {'loss': 2.1395, 'learning_rate': 0.0009142323321366315, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 466/2180 [1:14:41<4:33:35,  9.58s/it] 21%|â–ˆâ–ˆâ–       | 467/2180 [1:14:50<4:33:15,  9.57s/it]                                                      {'loss': 2.1646, 'learning_rate': 0.0009138157391787986, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 467/2180 [1:14:50<4:33:15,  9.57s/it] 21%|â–ˆâ–ˆâ–       | 468/2180 [1:15:00<4:33:13,  9.58s/it]                                                      {'loss': 2.058, 'learning_rate': 0.0009133982323251177, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 468/2180 [1:15:00<4:33:13,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 469/2180 [1:15:10<4:32:48,  9.57s/it]                                                      {'loss': 2.2305, 'learning_rate': 0.0009129798124976365, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 469/2180 [1:15:10<4:32:48,  9.57s/it] 22%|â–ˆâ–ˆâ–       | 470/2180 [1:15:19<4:32:52,  9.57s/it]                                                      {'loss': 2.2004, 'learning_rate': 0.0009125604806204187, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 470/2180 [1:15:19<4:32:52,  9.57s/it] 22%|â–ˆâ–ˆâ–       | 471/2180 [1:15:29<4:33:24,  9.60s/it]                                                      {'loss': 2.1044, 'learning_rate': 0.0009121402376195421, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 471/2180 [1:15:29<4:33:24,  9.60s/it] 22%|â–ˆâ–ˆâ–       | 472/2180 [1:15:39<4:34:01,  9.63s/it]                                                      {'loss': 2.0822, 'learning_rate': 0.0009117190844230972, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 472/2180 [1:15:39<4:34:01,  9.63s/it] 22%|â–ˆâ–ˆâ–       | 473/2180 [1:15:48<4:34:08,  9.64s/it]                                                      {'loss': 2.1498, 'learning_rate': 0.0009112970219611841, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 473/2180 [1:15:48<4:34:08,  9.64s/it] 22%|â–ˆâ–ˆâ–       | 474/2180 [1:15:58<4:33:21,  9.61s/it]                                                      {'loss': 2.2315, 'learning_rate': 0.0009108740511659115, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 474/2180 [1:15:58<4:33:21,  9.61s/it] 22%|â–ˆâ–ˆâ–       | 475/2180 [1:16:07<4:32:35,  9.59s/it]                                                      {'loss': 2.1075, 'learning_rate': 0.0009104501729713935, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 475/2180 [1:16:07<4:32:35,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 476/2180 [1:16:17<4:31:58,  9.58s/it]                                                      {'loss': 2.2251, 'learning_rate': 0.0009100253883137488, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 476/2180 [1:16:17<4:31:58,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 477/2180 [1:16:26<4:31:52,  9.58s/it]                                                      {'loss': 2.1594, 'learning_rate': 0.0009095996981310974, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 477/2180 [1:16:26<4:31:52,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 478/2180 [1:16:36<4:32:07,  9.59s/it]                                                      {'loss': 2.212, 'learning_rate': 0.0009091731033635596, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 478/2180 [1:16:36<4:32:07,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 479/2180 [1:16:46<4:31:38,  9.58s/it]                                                      {'loss': 2.1176, 'learning_rate': 0.0009087456049532529, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 479/2180 [1:16:46<4:31:38,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 480/2180 [1:16:55<4:31:46,  9.59s/it]                                                      {'loss': 2.1511, 'learning_rate': 0.0009083172038442914, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 480/2180 [1:16:55<4:31:46,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 481/2180 [1:17:05<4:31:36,  9.59s/it]                                                      {'loss': 2.081, 'learning_rate': 0.0009078879009827817, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 481/2180 [1:17:05<4:31:36,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 482/2180 [1:17:14<4:31:25,  9.59s/it]                                                      {'loss': 2.1169, 'learning_rate': 0.0009074576973168223, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 482/2180 [1:17:14<4:31:25,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 483/2180 [1:17:24<4:30:52,  9.58s/it]                                                      {'loss': 2.2289, 'learning_rate': 0.0009070265937965015, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 483/2180 [1:17:24<4:30:52,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 484/2180 [1:17:34<4:30:40,  9.58s/it]                                                      {'loss': 2.0849, 'learning_rate': 0.0009065945913738942, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 484/2180 [1:17:34<4:30:40,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 485/2180 [1:17:43<4:30:07,  9.56s/it]                                                      {'loss': 2.1394, 'learning_rate': 0.0009061616910030609, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 485/2180 [1:17:43<4:30:07,  9.56s/it] 22%|â–ˆâ–ˆâ–       | 486/2180 [1:17:53<4:30:36,  9.58s/it]                                                      {'loss': 2.0915, 'learning_rate': 0.0009057278936400453, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 486/2180 [1:17:53<4:30:36,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 487/2180 [1:18:02<4:30:28,  9.59s/it]                                                      {'loss': 2.2667, 'learning_rate': 0.0009052932002428715, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 487/2180 [1:18:02<4:30:28,  9.59s/it] 22%|â–ˆâ–ˆâ–       | 488/2180 [1:18:12<4:30:11,  9.58s/it]                                                      {'loss': 2.1821, 'learning_rate': 0.0009048576117715435, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 488/2180 [1:18:12<4:30:11,  9.58s/it] 22%|â–ˆâ–ˆâ–       | 489/2180 [1:18:21<4:29:42,  9.57s/it]                                                      {'loss': 2.0951, 'learning_rate': 0.0009044211291880407, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 489/2180 [1:18:21<4:29:42,  9.57s/it] 22%|â–ˆâ–ˆâ–       | 490/2180 [1:18:31<4:29:30,  9.57s/it]                                                      {'loss': 2.1098, 'learning_rate': 0.000903983753456318, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 490/2180 [1:18:31<4:29:30,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 491/2180 [1:18:41<4:29:24,  9.57s/it]                                                      {'loss': 2.1472, 'learning_rate': 0.0009035454855423026, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 491/2180 [1:18:41<4:29:24,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 492/2180 [1:18:50<4:29:17,  9.57s/it]                                                      {'loss': 2.2212, 'learning_rate': 0.0009031063264138922, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 492/2180 [1:18:50<4:29:17,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 493/2180 [1:19:00<4:29:20,  9.58s/it]                                                      {'loss': 2.1489, 'learning_rate': 0.0009026662770409522, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 493/2180 [1:19:00<4:29:20,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 494/2180 [1:19:09<4:28:50,  9.57s/it]                                                      {'loss': 2.2778, 'learning_rate': 0.0009022253383953147, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 494/2180 [1:19:09<4:28:50,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 495/2180 [1:19:19<4:28:39,  9.57s/it]                                                      {'loss': 2.1685, 'learning_rate': 0.0009017835114507753, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 495/2180 [1:19:19<4:28:39,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 496/2180 [1:19:28<4:28:39,  9.57s/it]                                                      {'loss': 2.0935, 'learning_rate': 0.0009013407971830914, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 496/2180 [1:19:28<4:28:39,  9.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 497/2180 [1:19:38<4:28:50,  9.58s/it]                                                      {'loss': 2.1362, 'learning_rate': 0.0009008971965699801, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 497/2180 [1:19:38<4:28:50,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 498/2180 [1:19:48<4:28:32,  9.58s/it]                                                      {'loss': 2.1281, 'learning_rate': 0.0009004527105911163, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 498/2180 [1:19:48<4:28:32,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 499/2180 [1:19:57<4:28:24,  9.58s/it]                                                      {'loss': 2.0592, 'learning_rate': 0.0009000073402281295, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 499/2180 [1:19:57<4:28:24,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 500/2180 [1:20:07<4:28:08,  9.58s/it]                                                      {'loss': 2.1746, 'learning_rate': 0.0008995610864646028, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 500/2180 [1:20:07<4:28:08,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 501/2180 [1:20:16<4:28:18,  9.59s/it]                                                      {'loss': 2.0853, 'learning_rate': 0.0008991139502860703, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 501/2180 [1:20:16<4:28:18,  9.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 502/2180 [1:20:26<4:28:00,  9.58s/it]                                                      {'loss': 2.1555, 'learning_rate': 0.0008986659326800146, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 502/2180 [1:20:26<4:28:00,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 503/2180 [1:20:35<4:27:42,  9.58s/it]                                                      {'loss': 2.1612, 'learning_rate': 0.0008982170346358651, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 503/2180 [1:20:35<4:27:42,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 504/2180 [1:20:45<4:27:08,  9.56s/it]                                                      {'loss': 2.0984, 'learning_rate': 0.0008977672571449956, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 504/2180 [1:20:45<4:27:08,  9.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 505/2180 [1:20:55<4:27:52,  9.60s/it]                                                      {'loss': 2.1337, 'learning_rate': 0.0008973166012007217, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 505/2180 [1:20:55<4:27:52,  9.60s/it] 23%|â–ˆâ–ˆâ–Ž       | 506/2180 [1:21:04<4:27:53,  9.60s/it]                                                      {'loss': 2.149, 'learning_rate': 0.0008968650677982998, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 506/2180 [1:21:04<4:27:53,  9.60s/it] 23%|â–ˆâ–ˆâ–Ž       | 507/2180 [1:21:14<4:27:08,  9.58s/it]                                                      {'loss': 2.0766, 'learning_rate': 0.0008964126579349236, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 507/2180 [1:21:14<4:27:08,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 508/2180 [1:21:23<4:27:16,  9.59s/it]                                                      {'loss': 2.1903, 'learning_rate': 0.0008959593726097226, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 508/2180 [1:21:23<4:27:16,  9.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 509/2180 [1:21:33<4:26:46,  9.58s/it]                                                      {'loss': 2.1531, 'learning_rate': 0.0008955052128237596, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 509/2180 [1:21:33<4:26:46,  9.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 510/2180 [1:21:43<4:26:53,  9.59s/it]                                                      {'loss': 2.1584, 'learning_rate': 0.0008950501795800288, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 510/2180 [1:21:43<4:26:53,  9.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 511/2180 [1:21:52<4:27:09,  9.60s/it]                                                      {'loss': 2.0993, 'learning_rate': 0.0008945942738834532, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 511/2180 [1:21:52<4:27:09,  9.60s/it] 23%|â–ˆâ–ˆâ–Ž       | 512/2180 [1:22:02<4:27:09,  9.61s/it]                                                      {'loss': 2.1477, 'learning_rate': 0.0008941374967408826, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 512/2180 [1:22:02<4:27:09,  9.61s/it] 24%|â–ˆâ–ˆâ–Ž       | 513/2180 [1:22:12<4:27:19,  9.62s/it]                                                      {'loss': 2.098, 'learning_rate': 0.0008936798491610916, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 513/2180 [1:22:12<4:27:19,  9.62s/it] 24%|â–ˆâ–ˆâ–Ž       | 514/2180 [1:22:21<4:26:48,  9.61s/it]                                                      {'loss': 2.2242, 'learning_rate': 0.0008932213321547768, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 514/2180 [1:22:21<4:26:48,  9.61s/it] 24%|â–ˆâ–ˆâ–Ž       | 515/2180 [1:22:31<4:26:28,  9.60s/it]                                                      {'loss': 2.1877, 'learning_rate': 0.0008927619467345554, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 515/2180 [1:22:31<4:26:28,  9.60s/it] 24%|â–ˆâ–ˆâ–Ž       | 516/2180 [1:22:40<4:26:00,  9.59s/it]                                                      {'loss': 2.0622, 'learning_rate': 0.0008923016939149615, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 516/2180 [1:22:40<4:26:00,  9.59s/it] 24%|â–ˆâ–ˆâ–Ž       | 517/2180 [1:22:50<4:25:29,  9.58s/it]                                                      {'loss': 2.1688, 'learning_rate': 0.0008918405747124458, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 517/2180 [1:22:50<4:25:29,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 518/2180 [1:22:59<4:25:27,  9.58s/it]                                                      {'loss': 2.1461, 'learning_rate': 0.0008913785901453721, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 518/2180 [1:22:59<4:25:27,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 519/2180 [1:23:09<4:25:15,  9.58s/it]                                                      {'loss': 2.2357, 'learning_rate': 0.000890915741234015, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 519/2180 [1:23:09<4:25:15,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 520/2180 [1:23:19<4:25:13,  9.59s/it]                                                      {'loss': 2.2246, 'learning_rate': 0.0008904520290005582, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 520/2180 [1:23:19<4:25:13,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 521/2180 [1:23:28<4:24:47,  9.58s/it]                                                      {'loss': 2.1231, 'learning_rate': 0.000889987454469092, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 521/2180 [1:23:28<4:24:47,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 522/2180 [1:23:38<4:24:53,  9.59s/it]                                                      {'loss': 2.2051, 'learning_rate': 0.0008895220186656111, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 522/2180 [1:23:38<4:24:53,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 523/2180 [1:23:47<4:24:56,  9.59s/it]                                                      {'loss': 2.2127, 'learning_rate': 0.0008890557226180122, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 523/2180 [1:23:47<4:24:56,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 524/2180 [1:23:57<4:24:38,  9.59s/it]                                                      {'loss': 2.035, 'learning_rate': 0.0008885885673560921, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 524/2180 [1:23:57<4:24:38,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 525/2180 [1:24:07<4:24:24,  9.59s/it]                                                      {'loss': 2.1713, 'learning_rate': 0.0008881205539115444, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 525/2180 [1:24:07<4:24:24,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 526/2180 [1:24:16<4:24:26,  9.59s/it]                                                      {'loss': 2.1572, 'learning_rate': 0.0008876516833179589, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 526/2180 [1:24:16<4:24:26,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 527/2180 [1:24:26<4:24:11,  9.59s/it]                                                      {'loss': 2.1015, 'learning_rate': 0.0008871819566108177, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 527/2180 [1:24:26<4:24:11,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 528/2180 [1:24:35<4:24:14,  9.60s/it]                                                      {'loss': 2.1733, 'learning_rate': 0.000886711374827494, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 528/2180 [1:24:35<4:24:14,  9.60s/it] 24%|â–ˆâ–ˆâ–       | 529/2180 [1:24:45<4:23:38,  9.58s/it]                                                      {'loss': 2.2041, 'learning_rate': 0.0008862399390072491, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 529/2180 [1:24:45<4:23:38,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 530/2180 [1:24:54<4:23:43,  9.59s/it]                                                      {'loss': 2.1181, 'learning_rate': 0.0008857676501912305, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 530/2180 [1:24:54<4:23:43,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 531/2180 [1:25:04<4:23:15,  9.58s/it]                                                      {'loss': 2.1088, 'learning_rate': 0.0008852945094224697, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 531/2180 [1:25:04<4:23:15,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 532/2180 [1:25:14<4:23:20,  9.59s/it]                                                      {'loss': 2.1586, 'learning_rate': 0.0008848205177458795, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 532/2180 [1:25:14<4:23:20,  9.59s/it] 24%|â–ˆâ–ˆâ–       | 533/2180 [1:25:23<4:23:06,  9.58s/it]                                                      {'loss': 2.0842, 'learning_rate': 0.0008843456762082518, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 533/2180 [1:25:23<4:23:06,  9.58s/it] 24%|â–ˆâ–ˆâ–       | 534/2180 [1:25:33<4:22:56,  9.59s/it]                                                      {'loss': 2.1042, 'learning_rate': 0.0008838699858582557, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 534/2180 [1:25:33<4:22:56,  9.59s/it] 25%|â–ˆâ–ˆâ–       | 535/2180 [1:25:42<4:22:50,  9.59s/it]                                                      {'loss': 2.2013, 'learning_rate': 0.0008833934477464347, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 535/2180 [1:25:42<4:22:50,  9.59s/it] 25%|â–ˆâ–ˆâ–       | 536/2180 [1:25:52<4:22:30,  9.58s/it]                                                      {'loss': 2.126, 'learning_rate': 0.0008829160629252045, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 536/2180 [1:25:52<4:22:30,  9.58s/it] 25%|â–ˆâ–ˆâ–       | 537/2180 [1:26:02<4:22:22,  9.58s/it]                                                      {'loss': 2.1549, 'learning_rate': 0.0008824378324488509, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 537/2180 [1:26:02<4:22:22,  9.58s/it] 25%|â–ˆâ–ˆâ–       | 538/2180 [1:26:11<4:22:01,  9.57s/it]                                                      {'loss': 2.1842, 'learning_rate': 0.0008819587573735268, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 538/2180 [1:26:11<4:22:01,  9.57s/it] 25%|â–ˆâ–ˆâ–       | 539/2180 [1:26:21<4:21:49,  9.57s/it]                                                      {'loss': 2.1982, 'learning_rate': 0.0008814788387572513, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 539/2180 [1:26:21<4:21:49,  9.57s/it] 25%|â–ˆâ–ˆâ–       | 540/2180 [1:26:30<4:21:51,  9.58s/it]                                                      {'loss': 2.164, 'learning_rate': 0.0008809980776599053, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 540/2180 [1:26:30<4:21:51,  9.58s/it] 25%|â–ˆâ–ˆâ–       | 541/2180 [1:26:40<4:21:39,  9.58s/it]                                                      {'loss': 2.1387, 'learning_rate': 0.0008805164751432312, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 541/2180 [1:26:40<4:21:39,  9.58s/it] 25%|â–ˆâ–ˆâ–       | 542/2180 [1:26:49<4:21:29,  9.58s/it]                                                      {'loss': 2.1423, 'learning_rate': 0.0008800340322708292, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 542/2180 [1:26:49<4:21:29,  9.58s/it] 25%|â–ˆâ–ˆâ–       | 543/2180 [1:26:59<4:21:13,  9.57s/it]                                                      {'loss': 2.0937, 'learning_rate': 0.0008795507501081555, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 543/2180 [1:26:59<4:21:13,  9.57s/it] 25%|â–ˆâ–ˆâ–       | 544/2180 [1:27:09<4:20:46,  9.56s/it]                                                      {'loss': 2.1114, 'learning_rate': 0.0008790666297225196, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 544/2180 [1:27:09<4:20:46,  9.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 545/2180 [1:27:18<4:20:32,  9.56s/it]                                                      {'loss': 2.0486, 'learning_rate': 0.0008785816721830829, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 545/2180 [1:27:18<4:20:32,  9.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 546/2180 [1:27:28<4:20:25,  9.56s/it]                                                      {'loss': 2.1033, 'learning_rate': 0.0008780958785608546, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 546/2180 [1:27:28<4:20:25,  9.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 547/2180 [1:27:37<4:20:21,  9.57s/it]                                                      {'loss': 2.1414, 'learning_rate': 0.0008776092499286912, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 547/2180 [1:27:37<4:20:21,  9.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 548/2180 [1:27:47<4:20:10,  9.56s/it]                                                      {'loss': 2.0265, 'learning_rate': 0.0008771217873612929, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 548/2180 [1:27:47<4:20:10,  9.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 549/2180 [1:27:56<4:19:58,  9.56s/it]                                                      {'loss': 2.1441, 'learning_rate': 0.0008766334919352017, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 549/2180 [1:27:56<4:19:58,  9.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 550/2180 [1:28:06<4:20:39,  9.59s/it]                                                      {'loss': 2.1767, 'learning_rate': 0.0008761443647287987, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 550/2180 [1:28:06<4:20:39,  9.59s/it] 25%|â–ˆâ–ˆâ–Œ       | 551/2180 [1:28:16<4:20:31,  9.60s/it]                                                      {'loss': 2.1964, 'learning_rate': 0.0008756544068223026, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 551/2180 [1:28:16<4:20:31,  9.60s/it] 25%|â–ˆâ–ˆâ–Œ       | 552/2180 [1:28:25<4:20:06,  9.59s/it]                                                      {'loss': 2.2192, 'learning_rate': 0.0008751636192977659, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 552/2180 [1:28:25<4:20:06,  9.59s/it] 25%|â–ˆâ–ˆâ–Œ       | 553/2180 [1:28:35<4:19:37,  9.57s/it]                                                      {'loss': 2.0672, 'learning_rate': 0.0008746720032390737, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 553/2180 [1:28:35<4:19:37,  9.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 554/2180 [1:28:44<4:19:12,  9.57s/it]                                                      {'loss': 2.1009, 'learning_rate': 0.0008741795597319408, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 554/2180 [1:28:44<4:19:12,  9.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 555/2180 [1:28:54<4:18:44,  9.55s/it]                                                      {'loss': 2.0896, 'learning_rate': 0.0008736862898639095, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 555/2180 [1:28:54<4:18:44,  9.55s/it] 26%|â–ˆâ–ˆâ–Œ       | 556/2180 [1:29:03<4:19:03,  9.57s/it]                                                      {'loss': 2.1911, 'learning_rate': 0.0008731921947243468, 'epoch': 0.25}
 26%|â–ˆâ–ˆâ–Œ       | 556/2180 [1:29:03<4:19:03,  9.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 557/2180 [1:29:13<4:19:00,  9.58s/it]                                                      {'loss': 2.163, 'learning_rate': 0.0008726972754044427, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 557/2180 [1:29:13<4:19:00,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 558/2180 [1:29:23<4:19:10,  9.59s/it]                                                      {'loss': 2.2335, 'learning_rate': 0.0008722015329972069, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 558/2180 [1:29:23<4:19:10,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 559/2180 [1:29:32<4:19:03,  9.59s/it]                                                      {'loss': 2.1828, 'learning_rate': 0.0008717049685974672, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 559/2180 [1:29:32<4:19:03,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 560/2180 [1:29:42<4:18:35,  9.58s/it]                                                      {'loss': 2.2458, 'learning_rate': 0.0008712075833018665, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 560/2180 [1:29:42<4:18:35,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 561/2180 [1:29:51<4:18:03,  9.56s/it]                                                      {'loss': 2.0773, 'learning_rate': 0.0008707093782088608, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 561/2180 [1:29:51<4:18:03,  9.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 562/2180 [1:30:01<4:18:14,  9.58s/it]                                                      {'loss': 2.154, 'learning_rate': 0.0008702103544187167, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 562/2180 [1:30:01<4:18:14,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 563/2180 [1:30:10<4:18:16,  9.58s/it]                                                      {'loss': 2.1475, 'learning_rate': 0.0008697105130335085, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 563/2180 [1:30:10<4:18:16,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 564/2180 [1:30:20<4:18:22,  9.59s/it]                                                      {'loss': 2.1792, 'learning_rate': 0.0008692098551571164, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 564/2180 [1:30:20<4:18:22,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 565/2180 [1:30:30<4:18:01,  9.59s/it]                                                      {'loss': 2.1724, 'learning_rate': 0.0008687083818952235, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 565/2180 [1:30:30<4:18:01,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 566/2180 [1:30:39<4:18:02,  9.59s/it]                                                      {'loss': 2.1247, 'learning_rate': 0.0008682060943553143, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 566/2180 [1:30:39<4:18:02,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 567/2180 [1:30:49<4:17:51,  9.59s/it]                                                      {'loss': 2.1799, 'learning_rate': 0.0008677029936466707, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 567/2180 [1:30:49<4:17:51,  9.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 568/2180 [1:30:58<4:17:30,  9.58s/it]                                                      {'loss': 2.3029, 'learning_rate': 0.0008671990808803711, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 568/2180 [1:30:58<4:17:30,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 569/2180 [1:31:08<4:17:11,  9.58s/it]                                                      {'loss': 2.1419, 'learning_rate': 0.0008666943571692871, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 569/2180 [1:31:08<4:17:11,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 570/2180 [1:31:18<4:16:42,  9.57s/it]                                                      {'loss': 2.1449, 'learning_rate': 0.0008661888236280813, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 570/2180 [1:31:18<4:16:42,  9.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 571/2180 [1:31:27<4:16:47,  9.58s/it]                                                      {'loss': 2.1096, 'learning_rate': 0.0008656824813732045, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 571/2180 [1:31:27<4:16:47,  9.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 572/2180 [1:31:37<4:16:23,  9.57s/it]                                                      {'loss': 2.1505, 'learning_rate': 0.000865175331522894, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 572/2180 [1:31:37<4:16:23,  9.57s/it] 26%|â–ˆâ–ˆâ–‹       | 573/2180 [1:31:46<4:16:29,  9.58s/it]                                                      {'loss': 2.1678, 'learning_rate': 0.0008646673751971703, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 573/2180 [1:31:46<4:16:29,  9.58s/it] 26%|â–ˆâ–ˆâ–‹       | 574/2180 [1:31:56<4:16:17,  9.57s/it]                                                      {'loss': 2.0821, 'learning_rate': 0.000864158613517835, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 574/2180 [1:31:56<4:16:17,  9.57s/it] 26%|â–ˆâ–ˆâ–‹       | 575/2180 [1:32:05<4:16:09,  9.58s/it]                                                      {'loss': 2.1769, 'learning_rate': 0.0008636490476084681, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 575/2180 [1:32:05<4:16:09,  9.58s/it] 26%|â–ˆâ–ˆâ–‹       | 576/2180 [1:32:15<4:16:12,  9.58s/it]                                                      {'loss': 2.1348, 'learning_rate': 0.0008631386785944264, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 576/2180 [1:32:15<4:16:12,  9.58s/it] 26%|â–ˆâ–ˆâ–‹       | 577/2180 [1:32:25<4:16:16,  9.59s/it]                                                      {'loss': 2.1676, 'learning_rate': 0.0008626275076028397, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–‹       | 577/2180 [1:32:25<4:16:16,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 578/2180 [1:32:34<4:16:02,  9.59s/it]                                                      {'loss': 2.1279, 'learning_rate': 0.0008621155357626091, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 578/2180 [1:32:34<4:16:02,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 579/2180 [1:32:44<4:16:13,  9.60s/it]                                                      {'loss': 2.1633, 'learning_rate': 0.0008616027642044042, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 579/2180 [1:32:44<4:16:13,  9.60s/it] 27%|â–ˆâ–ˆâ–‹       | 580/2180 [1:32:53<4:15:50,  9.59s/it]                                                      {'loss': 2.1101, 'learning_rate': 0.000861089194060661, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 580/2180 [1:32:53<4:15:50,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 581/2180 [1:33:03<4:15:32,  9.59s/it]                                                      {'loss': 2.1323, 'learning_rate': 0.000860574826465579, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 581/2180 [1:33:03<4:15:32,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 582/2180 [1:33:13<4:15:22,  9.59s/it]                                                      {'loss': 2.2367, 'learning_rate': 0.0008600596625551191, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 582/2180 [1:33:13<4:15:22,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 583/2180 [1:33:22<4:15:24,  9.60s/it]                                                      {'loss': 2.0974, 'learning_rate': 0.0008595437034670006, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 583/2180 [1:33:22<4:15:24,  9.60s/it] 27%|â–ˆâ–ˆâ–‹       | 584/2180 [1:33:32<4:15:41,  9.61s/it]                                                      {'loss': 2.1294, 'learning_rate': 0.0008590269503406985, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 584/2180 [1:33:32<4:15:41,  9.61s/it] 27%|â–ˆâ–ˆâ–‹       | 585/2180 [1:33:41<4:15:25,  9.61s/it]                                                      {'loss': 2.0806, 'learning_rate': 0.0008585094043174423, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 585/2180 [1:33:41<4:15:25,  9.61s/it] 27%|â–ˆâ–ˆâ–‹       | 586/2180 [1:33:51<4:14:40,  9.59s/it]                                                      {'loss': 2.1387, 'learning_rate': 0.0008579910665402118, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 586/2180 [1:33:51<4:14:40,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 587/2180 [1:34:01<4:14:17,  9.58s/it]                                                      {'loss': 2.1306, 'learning_rate': 0.000857471938153736, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 587/2180 [1:34:01<4:14:17,  9.58s/it] 27%|â–ˆâ–ˆâ–‹       | 588/2180 [1:34:10<4:13:56,  9.57s/it]                                                      {'loss': 2.0473, 'learning_rate': 0.0008569520203044892, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 588/2180 [1:34:10<4:13:56,  9.57s/it] 27%|â–ˆâ–ˆâ–‹       | 589/2180 [1:34:20<4:14:26,  9.60s/it]                                                      {'loss': 2.0629, 'learning_rate': 0.0008564313141406901, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 589/2180 [1:34:20<4:14:26,  9.60s/it] 27%|â–ˆâ–ˆâ–‹       | 590/2180 [1:34:29<4:14:24,  9.60s/it]                                                      {'loss': 2.0776, 'learning_rate': 0.0008559098208122973, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 590/2180 [1:34:29<4:14:24,  9.60s/it] 27%|â–ˆâ–ˆâ–‹       | 591/2180 [1:34:39<4:13:44,  9.58s/it]                                                      {'loss': 2.1496, 'learning_rate': 0.0008553875414710089, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 591/2180 [1:34:39<4:13:44,  9.58s/it] 27%|â–ˆâ–ˆâ–‹       | 592/2180 [1:34:48<4:13:30,  9.58s/it]                                                      {'loss': 2.1056, 'learning_rate': 0.0008548644772702579, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 592/2180 [1:34:48<4:13:30,  9.58s/it] 27%|â–ˆâ–ˆâ–‹       | 593/2180 [1:34:58<4:13:18,  9.58s/it]                                                      {'loss': 2.1595, 'learning_rate': 0.0008543406293652116, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 593/2180 [1:34:58<4:13:18,  9.58s/it] 27%|â–ˆâ–ˆâ–‹       | 594/2180 [1:35:08<4:13:02,  9.57s/it]                                                      {'loss': 2.1224, 'learning_rate': 0.0008538159989127671, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 594/2180 [1:35:08<4:13:02,  9.57s/it] 27%|â–ˆâ–ˆâ–‹       | 595/2180 [1:35:17<4:13:10,  9.58s/it]                                                      {'loss': 2.1448, 'learning_rate': 0.0008532905870715505, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 595/2180 [1:35:17<4:13:10,  9.58s/it] 27%|â–ˆâ–ˆâ–‹       | 596/2180 [1:35:27<4:13:03,  9.59s/it]                                                      {'loss': 2.121, 'learning_rate': 0.0008527643950019131, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 596/2180 [1:35:27<4:13:03,  9.59s/it] 27%|â–ˆâ–ˆâ–‹       | 597/2180 [1:35:36<4:13:44,  9.62s/it]                                                      {'loss': 2.1133, 'learning_rate': 0.0008522374238659296, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 597/2180 [1:35:37<4:13:44,  9.62s/it] 27%|â–ˆâ–ˆâ–‹       | 598/2180 [1:35:46<4:13:53,  9.63s/it]                                                      {'loss': 2.2324, 'learning_rate': 0.0008517096748273951, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 598/2180 [1:35:46<4:13:53,  9.63s/it] 27%|â–ˆâ–ˆâ–‹       | 599/2180 [1:35:56<4:13:28,  9.62s/it]                                                      {'loss': 2.2059, 'learning_rate': 0.0008511811490518227, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 599/2180 [1:35:56<4:13:28,  9.62s/it] 28%|â–ˆâ–ˆâ–Š       | 600/2180 [1:36:05<4:12:57,  9.61s/it]                                                      {'loss': 2.1908, 'learning_rate': 0.0008506518477064405, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 600/2180 [1:36:05<4:12:57,  9.61s/it] 28%|â–ˆâ–ˆâ–Š       | 601/2180 [1:36:15<4:12:21,  9.59s/it]                                                      {'loss': 2.1793, 'learning_rate': 0.0008501217719601903, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 601/2180 [1:36:15<4:12:21,  9.59s/it] 28%|â–ˆâ–ˆâ–Š       | 602/2180 [1:36:24<4:11:57,  9.58s/it]                                                      {'loss': 2.1745, 'learning_rate': 0.0008495909229837233, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 602/2180 [1:36:24<4:11:57,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 603/2180 [1:36:34<4:11:26,  9.57s/it]                                                      {'loss': 2.0697, 'learning_rate': 0.000849059301949399, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 603/2180 [1:36:34<4:11:26,  9.57s/it] 28%|â–ˆâ–ˆâ–Š       | 604/2180 [1:36:44<4:12:04,  9.60s/it]                                                      {'loss': 2.1057, 'learning_rate': 0.0008485269100312812, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 604/2180 [1:36:44<4:12:04,  9.60s/it] 28%|â–ˆâ–ˆâ–Š       | 605/2180 [1:36:53<4:11:29,  9.58s/it]                                                      {'loss': 2.171, 'learning_rate': 0.0008479937484051368, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 605/2180 [1:36:53<4:11:29,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 606/2180 [1:37:03<4:11:25,  9.58s/it]                                                      {'loss': 2.1447, 'learning_rate': 0.0008474598182484323, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 606/2180 [1:37:03<4:11:25,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 607/2180 [1:37:12<4:10:49,  9.57s/it]                                                      {'loss': 2.1403, 'learning_rate': 0.0008469251207403317, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 607/2180 [1:37:12<4:10:49,  9.57s/it] 28%|â–ˆâ–ˆâ–Š       | 608/2180 [1:37:22<4:11:01,  9.58s/it]                                                      {'loss': 2.1631, 'learning_rate': 0.0008463896570616934, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 608/2180 [1:37:22<4:11:01,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 609/2180 [1:37:31<4:10:40,  9.57s/it]                                                      {'loss': 2.1128, 'learning_rate': 0.0008458534283950678, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 609/2180 [1:37:31<4:10:40,  9.57s/it] 28%|â–ˆâ–ˆâ–Š       | 610/2180 [1:37:41<4:10:37,  9.58s/it]                                                      {'loss': 2.2167, 'learning_rate': 0.0008453164359246952, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 610/2180 [1:37:41<4:10:37,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 611/2180 [1:37:51<4:11:40,  9.62s/it]                                                      {'loss': 2.1284, 'learning_rate': 0.0008447786808365022, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 611/2180 [1:37:51<4:11:40,  9.62s/it] 28%|â–ˆâ–ˆâ–Š       | 612/2180 [1:38:00<4:11:09,  9.61s/it]                                                      {'loss': 2.1527, 'learning_rate': 0.0008442401643181, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 612/2180 [1:38:00<4:11:09,  9.61s/it] 28%|â–ˆâ–ˆâ–Š       | 613/2180 [1:38:10<4:11:05,  9.61s/it]                                                      {'loss': 2.144, 'learning_rate': 0.0008437008875587811, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 613/2180 [1:38:10<4:11:05,  9.61s/it] 28%|â–ˆâ–ˆâ–Š       | 614/2180 [1:38:20<4:10:25,  9.59s/it]                                                      {'loss': 2.1596, 'learning_rate': 0.0008431608517495171, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 614/2180 [1:38:20<4:10:25,  9.59s/it] 28%|â–ˆâ–ˆâ–Š       | 615/2180 [1:38:29<4:10:04,  9.59s/it]                                                      {'loss': 2.0663, 'learning_rate': 0.0008426200580829561, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 615/2180 [1:38:29<4:10:04,  9.59s/it] 28%|â–ˆâ–ˆâ–Š       | 616/2180 [1:38:39<4:09:45,  9.58s/it]                                                      {'loss': 2.1808, 'learning_rate': 0.0008420785077534195, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 616/2180 [1:38:39<4:09:45,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 617/2180 [1:38:48<4:09:16,  9.57s/it]                                                      {'loss': 2.0502, 'learning_rate': 0.0008415362019569001, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 617/2180 [1:38:48<4:09:16,  9.57s/it] 28%|â–ˆâ–ˆâ–Š       | 618/2180 [1:38:58<4:08:58,  9.56s/it]                                                      {'loss': 2.175, 'learning_rate': 0.0008409931418910591, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 618/2180 [1:38:58<4:08:58,  9.56s/it] 28%|â–ˆâ–ˆâ–Š       | 619/2180 [1:39:07<4:08:52,  9.57s/it]                                                      {'loss': 2.1338, 'learning_rate': 0.0008404493287552232, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 619/2180 [1:39:07<4:08:52,  9.57s/it] 28%|â–ˆâ–ˆâ–Š       | 620/2180 [1:39:17<4:08:59,  9.58s/it]                                                      {'loss': 2.1195, 'learning_rate': 0.0008399047637503825, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 620/2180 [1:39:17<4:08:59,  9.58s/it] 28%|â–ˆâ–ˆâ–Š       | 621/2180 [1:39:27<4:09:09,  9.59s/it]                                                      {'loss': 2.1704, 'learning_rate': 0.0008393594480791875, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 621/2180 [1:39:27<4:09:09,  9.59s/it] 29%|â–ˆâ–ˆâ–Š       | 622/2180 [1:39:36<4:08:54,  9.59s/it]                                                      {'loss': 2.1376, 'learning_rate': 0.0008388133829459463, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 622/2180 [1:39:36<4:08:54,  9.59s/it] 29%|â–ˆâ–ˆâ–Š       | 623/2180 [1:39:46<4:08:30,  9.58s/it]                                                      {'loss': 2.1091, 'learning_rate': 0.0008382665695566227, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 623/2180 [1:39:46<4:08:30,  9.58s/it] 29%|â–ˆâ–ˆâ–Š       | 624/2180 [1:39:55<4:08:18,  9.57s/it]                                                      {'loss': 2.1147, 'learning_rate': 0.0008377190091188324, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 624/2180 [1:39:55<4:08:18,  9.57s/it] 29%|â–ˆâ–ˆâ–Š       | 625/2180 [1:40:05<4:08:41,  9.60s/it]                                                      {'loss': 2.1922, 'learning_rate': 0.0008371707028418413, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 625/2180 [1:40:05<4:08:41,  9.60s/it] 29%|â–ˆâ–ˆâ–Š       | 626/2180 [1:40:15<4:09:28,  9.63s/it]                                                      {'loss': 2.1743, 'learning_rate': 0.0008366216519365621, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–Š       | 626/2180 [1:40:15<4:09:28,  9.63s/it] 29%|â–ˆâ–ˆâ–‰       | 627/2180 [1:40:24<4:08:49,  9.61s/it]                                                      {'loss': 2.1452, 'learning_rate': 0.0008360718576155525, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 627/2180 [1:40:24<4:08:49,  9.61s/it] 29%|â–ˆâ–ˆâ–‰       | 628/2180 [1:40:34<4:08:42,  9.61s/it]                                                      {'loss': 2.1521, 'learning_rate': 0.0008355213210930118, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 628/2180 [1:40:34<4:08:42,  9.61s/it] 29%|â–ˆâ–ˆâ–‰       | 629/2180 [1:40:43<4:08:58,  9.63s/it]                                                      {'loss': 2.1571, 'learning_rate': 0.0008349700435847778, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 629/2180 [1:40:43<4:08:58,  9.63s/it] 29%|â–ˆâ–ˆâ–‰       | 630/2180 [1:40:53<4:08:05,  9.60s/it]                                                      {'loss': 2.1522, 'learning_rate': 0.0008344180263083256, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 630/2180 [1:40:53<4:08:05,  9.60s/it] 29%|â–ˆâ–ˆâ–‰       | 631/2180 [1:41:03<4:07:36,  9.59s/it]                                                      {'loss': 2.1772, 'learning_rate': 0.000833865270482764, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 631/2180 [1:41:03<4:07:36,  9.59s/it] 29%|â–ˆâ–ˆâ–‰       | 632/2180 [1:41:12<4:07:30,  9.59s/it]                                                      {'loss': 2.085, 'learning_rate': 0.0008333117773288324, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 632/2180 [1:41:12<4:07:30,  9.59s/it] 29%|â–ˆâ–ˆâ–‰       | 633/2180 [1:41:22<4:07:13,  9.59s/it]                                                      {'loss': 2.0602, 'learning_rate': 0.0008327575480688985, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 633/2180 [1:41:22<4:07:13,  9.59s/it] 29%|â–ˆâ–ˆâ–‰       | 634/2180 [1:41:31<4:07:13,  9.59s/it]                                                      {'loss': 2.1983, 'learning_rate': 0.000832202583926956, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 634/2180 [1:41:31<4:07:13,  9.59s/it] 29%|â–ˆâ–ˆâ–‰       | 635/2180 [1:41:41<4:07:08,  9.60s/it]                                                      {'loss': 2.1635, 'learning_rate': 0.0008316468861286217, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 635/2180 [1:41:41<4:07:08,  9.60s/it] 29%|â–ˆâ–ˆâ–‰       | 636/2180 [1:41:51<4:07:05,  9.60s/it]                                                      {'loss': 2.1488, 'learning_rate': 0.0008310904559011323, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 636/2180 [1:41:51<4:07:05,  9.60s/it] 29%|â–ˆâ–ˆâ–‰       | 637/2180 [1:42:00<4:07:18,  9.62s/it]                                                      {'loss': 2.1954, 'learning_rate': 0.0008305332944733419, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 637/2180 [1:42:00<4:07:18,  9.62s/it] 29%|â–ˆâ–ˆâ–‰       | 638/2180 [1:42:10<4:06:46,  9.60s/it]                                                      {'loss': 2.1111, 'learning_rate': 0.0008299754030757202, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 638/2180 [1:42:10<4:06:46,  9.60s/it] 29%|â–ˆâ–ˆâ–‰       | 639/2180 [1:42:19<4:06:20,  9.59s/it]                                                      {'loss': 2.0899, 'learning_rate': 0.0008294167829403481, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 639/2180 [1:42:19<4:06:20,  9.59s/it] 29%|â–ˆâ–ˆâ–‰       | 640/2180 [1:42:29<4:05:58,  9.58s/it]                                                      {'loss': 2.1853, 'learning_rate': 0.0008288574353009164, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 640/2180 [1:42:29<4:05:58,  9.58s/it] 29%|â–ˆâ–ˆâ–‰       | 641/2180 [1:42:39<4:05:49,  9.58s/it]                                                      {'loss': 2.1083, 'learning_rate': 0.0008282973613927225, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 641/2180 [1:42:39<4:05:49,  9.58s/it] 29%|â–ˆâ–ˆâ–‰       | 642/2180 [1:42:48<4:05:20,  9.57s/it]                                                      {'loss': 2.0772, 'learning_rate': 0.0008277365624526675, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 642/2180 [1:42:48<4:05:20,  9.57s/it] 29%|â–ˆâ–ˆâ–‰       | 643/2180 [1:42:58<4:04:58,  9.56s/it]                                                      {'loss': 2.1834, 'learning_rate': 0.0008271750397192541, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 643/2180 [1:42:58<4:04:58,  9.56s/it] 30%|â–ˆâ–ˆâ–‰       | 644/2180 [1:43:07<4:05:43,  9.60s/it]                                                      {'loss': 1.9835, 'learning_rate': 0.0008266127944325832, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 644/2180 [1:43:07<4:05:43,  9.60s/it] 30%|â–ˆâ–ˆâ–‰       | 645/2180 [1:43:17<4:05:43,  9.61s/it]                                                      {'loss': 2.1584, 'learning_rate': 0.0008260498278343513, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 645/2180 [1:43:17<4:05:43,  9.61s/it] 30%|â–ˆâ–ˆâ–‰       | 646/2180 [1:43:26<4:05:22,  9.60s/it]                                                      {'loss': 2.1468, 'learning_rate': 0.0008254861411678485, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 646/2180 [1:43:26<4:05:22,  9.60s/it] 30%|â–ˆâ–ˆâ–‰       | 647/2180 [1:43:36<4:04:57,  9.59s/it]                                                      {'loss': 2.1692, 'learning_rate': 0.0008249217356779544, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 647/2180 [1:43:36<4:04:57,  9.59s/it] 30%|â–ˆâ–ˆâ–‰       | 648/2180 [1:43:46<4:04:34,  9.58s/it]                                                      {'loss': 2.1255, 'learning_rate': 0.0008243566126111363, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 648/2180 [1:43:46<4:04:34,  9.58s/it] 30%|â–ˆâ–ˆâ–‰       | 649/2180 [1:43:55<4:04:31,  9.58s/it]                                                      {'loss': 2.1613, 'learning_rate': 0.0008237907732154466, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 649/2180 [1:43:55<4:04:31,  9.58s/it] 30%|â–ˆâ–ˆâ–‰       | 650/2180 [1:44:05<4:03:57,  9.57s/it]                                                      {'loss': 2.0939, 'learning_rate': 0.0008232242187405194, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 650/2180 [1:44:05<4:03:57,  9.57s/it] 30%|â–ˆâ–ˆâ–‰       | 651/2180 [1:44:14<4:04:01,  9.58s/it]                                                      {'loss': 2.1578, 'learning_rate': 0.000822656950437568, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 651/2180 [1:44:14<4:04:01,  9.58s/it] 30%|â–ˆâ–ˆâ–‰       | 652/2180 [1:44:24<4:03:30,  9.56s/it]                                                      {'loss': 2.0457, 'learning_rate': 0.0008220889695593823, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 652/2180 [1:44:24<4:03:30,  9.56s/it] 30%|â–ˆâ–ˆâ–‰       | 653/2180 [1:44:33<4:03:41,  9.58s/it]                                                      {'loss': 2.102, 'learning_rate': 0.0008215202773603259, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 653/2180 [1:44:33<4:03:41,  9.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 654/2180 [1:44:43<4:03:25,  9.57s/it]                                                      {'loss': 2.0891, 'learning_rate': 0.0008209508750963328, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 654/2180 [1:44:43<4:03:25,  9.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 655/2180 [1:44:53<4:03:27,  9.58s/it]                                                      {'loss': 2.1507, 'learning_rate': 0.0008203807640249062, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 655/2180 [1:44:53<4:03:27,  9.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 656/2180 [1:45:02<4:02:53,  9.56s/it]                                                      {'loss': 2.1642, 'learning_rate': 0.0008198099454051136, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 656/2180 [1:45:02<4:02:53,  9.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 657/2180 [1:45:12<4:02:47,  9.56s/it]                                                      {'loss': 2.134, 'learning_rate': 0.0008192384204975857, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 657/2180 [1:45:12<4:02:47,  9.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 658/2180 [1:45:21<4:02:38,  9.57s/it]                                                      {'loss': 2.1457, 'learning_rate': 0.000818666190564513, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 658/2180 [1:45:21<4:02:38,  9.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 659/2180 [1:45:31<4:02:20,  9.56s/it]                                                      {'loss': 2.0724, 'learning_rate': 0.0008180932568696426, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 659/2180 [1:45:31<4:02:20,  9.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 660/2180 [1:45:40<4:02:02,  9.55s/it]                                                      {'loss': 2.074, 'learning_rate': 0.0008175196206782764, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 660/2180 [1:45:40<4:02:02,  9.55s/it] 30%|â–ˆâ–ˆâ–ˆ       | 661/2180 [1:45:50<4:01:44,  9.55s/it]                                                      {'loss': 2.1967, 'learning_rate': 0.0008169452832572675, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 661/2180 [1:45:50<4:01:44,  9.55s/it] 30%|â–ˆâ–ˆâ–ˆ       | 662/2180 [1:46:00<4:01:57,  9.56s/it]                                                      {'loss': 2.1689, 'learning_rate': 0.0008163702458750173, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 662/2180 [1:46:00<4:01:57,  9.56s/it] 30%|â–ˆâ–ˆâ–ˆ       | 663/2180 [1:46:09<4:02:46,  9.60s/it]                                                      {'loss': 2.0635, 'learning_rate': 0.0008157945098014734, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 663/2180 [1:46:09<4:02:46,  9.60s/it] 30%|â–ˆâ–ˆâ–ˆ       | 664/2180 [1:46:19<4:02:48,  9.61s/it]                                                      {'loss': 2.0717, 'learning_rate': 0.0008152180763081267, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 664/2180 [1:46:19<4:02:48,  9.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 665/2180 [1:46:28<4:02:05,  9.59s/it]                                                      {'loss': 2.0621, 'learning_rate': 0.0008146409466680076, 'epoch': 0.3}
 31%|â–ˆâ–ˆâ–ˆ       | 665/2180 [1:46:28<4:02:05,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 666/2180 [1:46:38<4:02:30,  9.61s/it]                                                      {'loss': 2.1186, 'learning_rate': 0.0008140631221556845, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 666/2180 [1:46:38<4:02:30,  9.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 667/2180 [1:46:48<4:02:31,  9.62s/it]                                                      {'loss': 2.1722, 'learning_rate': 0.0008134846040472599, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 667/2180 [1:46:48<4:02:31,  9.62s/it] 31%|â–ˆâ–ˆâ–ˆ       | 668/2180 [1:46:57<4:02:02,  9.60s/it]                                                      {'loss': 2.1469, 'learning_rate': 0.0008129053936203688, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 668/2180 [1:46:57<4:02:02,  9.60s/it] 31%|â–ˆâ–ˆâ–ˆ       | 669/2180 [1:47:07<4:01:54,  9.61s/it]                                                      {'loss': 2.1226, 'learning_rate': 0.0008123254921541745, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 669/2180 [1:47:07<4:01:54,  9.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 670/2180 [1:47:16<4:01:20,  9.59s/it]                                                      {'loss': 2.0174, 'learning_rate': 0.0008117449009293668, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 670/2180 [1:47:16<4:01:20,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 671/2180 [1:47:26<4:01:13,  9.59s/it]                                                      {'loss': 2.1122, 'learning_rate': 0.0008111636212281586, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 671/2180 [1:47:26<4:01:13,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 672/2180 [1:47:36<4:00:42,  9.58s/it]                                                      {'loss': 2.2156, 'learning_rate': 0.0008105816543342833, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 672/2180 [1:47:36<4:00:42,  9.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 673/2180 [1:47:45<4:00:53,  9.59s/it]                                                      {'loss': 2.1374, 'learning_rate': 0.0008099990015329919, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 673/2180 [1:47:45<4:00:53,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 674/2180 [1:47:55<4:01:02,  9.60s/it]                                                      {'loss': 2.1188, 'learning_rate': 0.0008094156641110504, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 674/2180 [1:47:55<4:01:02,  9.60s/it] 31%|â–ˆâ–ˆâ–ˆ       | 675/2180 [1:48:04<4:00:32,  9.59s/it]                                                      {'loss': 2.2025, 'learning_rate': 0.0008088316433567369, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 675/2180 [1:48:04<4:00:32,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 676/2180 [1:48:14<4:00:20,  9.59s/it]                                                      {'loss': 2.1658, 'learning_rate': 0.0008082469405598378, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 676/2180 [1:48:14<4:00:20,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 677/2180 [1:48:24<4:00:04,  9.58s/it]                                                      {'loss': 2.169, 'learning_rate': 0.0008076615570116468, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 677/2180 [1:48:24<4:00:04,  9.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 678/2180 [1:48:33<4:00:03,  9.59s/it]                                                      {'loss': 2.0627, 'learning_rate': 0.0008070754940049603, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 678/2180 [1:48:33<4:00:03,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 679/2180 [1:48:43<3:59:45,  9.58s/it]                                                      {'loss': 2.1939, 'learning_rate': 0.0008064887528340756, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 679/2180 [1:48:43<3:59:45,  9.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 680/2180 [1:48:52<3:59:39,  9.59s/it]                                                      {'loss': 2.1394, 'learning_rate': 0.0008059013347947874, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 680/2180 [1:48:52<3:59:39,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 681/2180 [1:49:02<4:00:19,  9.62s/it]                                                      {'loss': 2.1928, 'learning_rate': 0.0008053132411843857, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 681/2180 [1:49:02<4:00:19,  9.62s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 682/2180 [1:49:12<3:59:36,  9.60s/it]                                                      {'loss': 2.1068, 'learning_rate': 0.0008047244733016521, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 682/2180 [1:49:12<3:59:36,  9.60s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 683/2180 [1:49:21<3:59:27,  9.60s/it]                                                      {'loss': 2.086, 'learning_rate': 0.0008041350324468573, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 683/2180 [1:49:21<3:59:27,  9.60s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 684/2180 [1:49:31<3:59:10,  9.59s/it]                                                      {'loss': 2.1612, 'learning_rate': 0.0008035449199217583, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 684/2180 [1:49:31<3:59:10,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 685/2180 [1:49:40<3:58:59,  9.59s/it]                                                      {'loss': 2.0515, 'learning_rate': 0.0008029541370295957, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 685/2180 [1:49:40<3:58:59,  9.59s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 686/2180 [1:49:50<3:59:00,  9.60s/it]                                                      {'loss': 2.0615, 'learning_rate': 0.0008023626850750903, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 686/2180 [1:49:50<3:59:00,  9.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 687/2180 [1:49:59<3:58:40,  9.59s/it]                                                      {'loss': 2.2018, 'learning_rate': 0.0008017705653644406, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 687/2180 [1:49:59<3:58:40,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 688/2180 [1:50:09<3:59:08,  9.62s/it]                                                      {'loss': 2.1375, 'learning_rate': 0.0008011777792053195, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 688/2180 [1:50:09<3:59:08,  9.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 689/2180 [1:50:19<3:58:38,  9.60s/it]                                                      {'loss': 2.0581, 'learning_rate': 0.0008005843279068725, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 689/2180 [1:50:19<3:58:38,  9.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 690/2180 [1:50:28<3:58:33,  9.61s/it]                                                      {'loss': 2.2311, 'learning_rate': 0.000799990212779713, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 690/2180 [1:50:28<3:58:33,  9.61s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 691/2180 [1:50:38<3:58:17,  9.60s/it]                                                      {'loss': 2.1581, 'learning_rate': 0.0007993954351359214, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 691/2180 [1:50:38<3:58:17,  9.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 692/2180 [1:50:48<3:58:04,  9.60s/it]                                                      {'loss': 2.1125, 'learning_rate': 0.0007987999962890406, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 692/2180 [1:50:48<3:58:04,  9.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 693/2180 [1:50:57<3:57:39,  9.59s/it]                                                      {'loss': 2.0858, 'learning_rate': 0.0007982038975540742, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 693/2180 [1:50:57<3:57:39,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 694/2180 [1:51:07<3:57:38,  9.60s/it]                                                      {'loss': 2.2149, 'learning_rate': 0.0007976071402474826, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 694/2180 [1:51:07<3:57:38,  9.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 695/2180 [1:51:16<3:57:16,  9.59s/it]                                                      {'loss': 2.179, 'learning_rate': 0.0007970097256871811, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 695/2180 [1:51:16<3:57:16,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 696/2180 [1:51:26<3:56:55,  9.58s/it]                                                      {'loss': 2.144, 'learning_rate': 0.0007964116551925364, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 696/2180 [1:51:26<3:56:55,  9.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 697/2180 [1:51:35<3:56:43,  9.58s/it]                                                      {'loss': 2.0746, 'learning_rate': 0.0007958129300843637, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 697/2180 [1:51:35<3:56:43,  9.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 698/2180 [1:51:45<3:56:30,  9.57s/it]                                                      {'loss': 2.1163, 'learning_rate': 0.0007952135516849239, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 698/2180 [1:51:45<3:56:30,  9.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 699/2180 [1:51:55<3:56:10,  9.57s/it]                                                      {'loss': 2.1797, 'learning_rate': 0.0007946135213179207, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 699/2180 [1:51:55<3:56:10,  9.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 700/2180 [1:52:04<3:56:21,  9.58s/it]                                                      {'loss': 2.2031, 'learning_rate': 0.0007940128403084977, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 700/2180 [1:52:04<3:56:21,  9.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 701/2180 [1:52:14<3:56:25,  9.59s/it]                                                      {'loss': 2.1467, 'learning_rate': 0.0007934115099832355, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 701/2180 [1:52:14<3:56:25,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 702/2180 [1:52:23<3:56:19,  9.59s/it]                                                      {'loss': 2.148, 'learning_rate': 0.0007928095316701483, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 702/2180 [1:52:23<3:56:19,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 703/2180 [1:52:33<3:56:11,  9.59s/it]                                                      {'loss': 2.0239, 'learning_rate': 0.0007922069066986819, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 703/2180 [1:52:33<3:56:11,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 704/2180 [1:52:43<3:56:00,  9.59s/it]                                                      {'loss': 2.1743, 'learning_rate': 0.0007916036363997097, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 704/2180 [1:52:43<3:56:00,  9.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 705/2180 [1:52:52<3:55:30,  9.58s/it]                                                      {'loss': 2.0334, 'learning_rate': 0.0007909997221055308, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 705/2180 [1:52:52<3:55:30,  9.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 706/2180 [1:53:02<3:56:02,  9.61s/it]                                                      {'loss': 2.0441, 'learning_rate': 0.0007903951651498658, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 706/2180 [1:53:02<3:56:02,  9.61s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 707/2180 [1:53:11<3:55:49,  9.61s/it]                                                      {'loss': 2.1472, 'learning_rate': 0.0007897899668678557, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 707/2180 [1:53:11<3:55:49,  9.61s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 708/2180 [1:53:21<3:55:33,  9.60s/it]                                                      {'loss': 2.1339, 'learning_rate': 0.0007891841285960566, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 708/2180 [1:53:21<3:55:33,  9.60s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 709/2180 [1:53:30<3:55:01,  9.59s/it]                                                      {'loss': 2.1454, 'learning_rate': 0.0007885776516724388, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 709/2180 [1:53:31<3:55:01,  9.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 710/2180 [1:53:40<3:54:42,  9.58s/it]                                                      {'loss': 2.1916, 'learning_rate': 0.0007879705374363831, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 710/2180 [1:53:40<3:54:42,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 711/2180 [1:53:50<3:54:33,  9.58s/it]                                                      {'loss': 2.1622, 'learning_rate': 0.000787362787228677, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 711/2180 [1:53:50<3:54:33,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 712/2180 [1:53:59<3:54:33,  9.59s/it]                                                      {'loss': 2.1196, 'learning_rate': 0.0007867544023915134, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 712/2180 [1:53:59<3:54:33,  9.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 713/2180 [1:54:09<3:54:17,  9.58s/it]                                                      {'loss': 2.134, 'learning_rate': 0.0007861453842684861, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 713/2180 [1:54:09<3:54:17,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 714/2180 [1:54:18<3:54:17,  9.59s/it]                                                      {'loss': 2.0516, 'learning_rate': 0.0007855357342045882, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 714/2180 [1:54:18<3:54:17,  9.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 715/2180 [1:54:28<3:53:59,  9.58s/it]                                                      {'loss': 2.2295, 'learning_rate': 0.0007849254535462074, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 715/2180 [1:54:28<3:53:59,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 716/2180 [1:54:38<3:53:46,  9.58s/it]                                                      {'loss': 2.0952, 'learning_rate': 0.0007843145436411252, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 716/2180 [1:54:38<3:53:46,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 717/2180 [1:54:47<3:53:46,  9.59s/it]                                                      {'loss': 2.0744, 'learning_rate': 0.0007837030058385117, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 717/2180 [1:54:47<3:53:46,  9.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 718/2180 [1:54:57<3:53:31,  9.58s/it]                                                      {'loss': 2.1772, 'learning_rate': 0.0007830908414889246, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 718/2180 [1:54:57<3:53:31,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 719/2180 [1:55:06<3:53:19,  9.58s/it]                                                      {'loss': 2.1347, 'learning_rate': 0.0007824780519443046, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 719/2180 [1:55:06<3:53:19,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 720/2180 [1:55:16<3:53:40,  9.60s/it]                                                      {'loss': 2.0828, 'learning_rate': 0.0007818646385579735, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 720/2180 [1:55:16<3:53:40,  9.60s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 721/2180 [1:55:26<3:53:26,  9.60s/it]                                                      {'loss': 2.0497, 'learning_rate': 0.0007812506026846307, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 721/2180 [1:55:26<3:53:26,  9.60s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 722/2180 [1:55:35<3:52:49,  9.58s/it]                                                      {'loss': 2.1805, 'learning_rate': 0.0007806359456803504, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 722/2180 [1:55:35<3:52:49,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2180 [1:55:45<3:52:21,  9.57s/it]                                                      {'loss': 2.1276, 'learning_rate': 0.0007800206689025785, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2180 [1:55:45<3:52:21,  9.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2180 [1:55:54<3:52:16,  9.57s/it]                                                      {'loss': 2.0899, 'learning_rate': 0.0007794047737101297, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2180 [1:55:54<3:52:16,  9.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2180 [1:56:04<3:52:31,  9.59s/it]                                                      {'loss': 2.0523, 'learning_rate': 0.0007787882614631843, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2180 [1:56:04<3:52:31,  9.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2180 [1:56:13<3:52:12,  9.58s/it]                                                      {'loss': 2.1453, 'learning_rate': 0.0007781711335232856, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2180 [1:56:13<3:52:12,  9.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2180 [1:56:23<3:51:45,  9.57s/it]                                                      {'loss': 2.1175, 'learning_rate': 0.0007775533912533363, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2180 [1:56:23<3:51:45,  9.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2180 [1:56:33<3:51:34,  9.57s/it]                                                      {'loss': 2.1833, 'learning_rate': 0.0007769350360175962, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2180 [1:56:33<3:51:34,  9.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2180 [1:56:42<3:51:18,  9.56s/it]                                                      {'loss': 2.1417, 'learning_rate': 0.0007763160691816784, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2180 [1:56:42<3:51:18,  9.56s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2180 [1:56:52<3:51:12,  9.57s/it]                                                      {'loss': 2.1046, 'learning_rate': 0.000775696492112547, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2180 [1:56:52<3:51:12,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2180 [1:57:01<3:51:10,  9.57s/it]                                                      {'loss': 2.1781, 'learning_rate': 0.0007750763061785137, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2180 [1:57:01<3:51:10,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2180 [1:57:11<3:50:41,  9.56s/it]                                                      {'loss': 2.0462, 'learning_rate': 0.000774455512749235, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2180 [1:57:11<3:50:41,  9.56s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2180 [1:57:20<3:51:09,  9.58s/it]                                                      {'loss': 2.1243, 'learning_rate': 0.0007738341131957085, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2180 [1:57:20<3:51:09,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2180 [1:57:30<3:50:47,  9.58s/it]                                                      {'loss': 2.0933, 'learning_rate': 0.000773212108890271, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2180 [1:57:30<3:50:47,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2180 [1:57:40<3:50:28,  9.57s/it]                                                      {'loss': 2.1465, 'learning_rate': 0.0007725895012065947, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2180 [1:57:40<3:50:28,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 736/2180 [1:57:49<3:50:34,  9.58s/it]                                                      {'loss': 2.0551, 'learning_rate': 0.0007719662915196844, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 736/2180 [1:57:49<3:50:34,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 737/2180 [1:57:59<3:50:39,  9.59s/it]                                                      {'loss': 2.0709, 'learning_rate': 0.0007713424812058736, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 737/2180 [1:57:59<3:50:39,  9.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 738/2180 [1:58:08<3:50:20,  9.58s/it]                                                      {'loss': 2.1212, 'learning_rate': 0.0007707180716428237, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 738/2180 [1:58:08<3:50:20,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 739/2180 [1:58:18<3:50:45,  9.61s/it]                                                      {'loss': 2.105, 'learning_rate': 0.0007700930642095184, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 739/2180 [1:58:18<3:50:45,  9.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 740/2180 [1:58:28<3:50:23,  9.60s/it]                                                      {'loss': 2.1636, 'learning_rate': 0.0007694674602862621, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 740/2180 [1:58:28<3:50:23,  9.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 741/2180 [1:58:37<3:50:08,  9.60s/it]                                                      {'loss': 2.2216, 'learning_rate': 0.0007688412612546769, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 741/2180 [1:58:37<3:50:08,  9.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 742/2180 [1:58:47<3:49:31,  9.58s/it]                                                      {'loss': 2.1335, 'learning_rate': 0.0007682144684976983, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 742/2180 [1:58:47<3:49:31,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 743/2180 [1:58:56<3:49:22,  9.58s/it]                                                      {'loss': 2.1484, 'learning_rate': 0.0007675870833995739, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 743/2180 [1:58:56<3:49:22,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 744/2180 [1:59:06<3:49:08,  9.57s/it]                                                      {'loss': 2.1419, 'learning_rate': 0.0007669591073458592, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 744/2180 [1:59:06<3:49:08,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 745/2180 [1:59:15<3:48:49,  9.57s/it]                                                      {'loss': 2.1521, 'learning_rate': 0.0007663305417234146, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 745/2180 [1:59:15<3:48:49,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 746/2180 [1:59:25<3:48:40,  9.57s/it]                                                      {'loss': 2.1322, 'learning_rate': 0.0007657013879204022, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 746/2180 [1:59:25<3:48:40,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 747/2180 [1:59:35<3:48:36,  9.57s/it]                                                      {'loss': 2.0379, 'learning_rate': 0.0007650716473262842, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 747/2180 [1:59:35<3:48:36,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 748/2180 [1:59:44<3:48:17,  9.57s/it]                                                      {'loss': 2.1311, 'learning_rate': 0.0007644413213318177, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 748/2180 [1:59:44<3:48:17,  9.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 749/2180 [1:59:54<3:48:31,  9.58s/it]                                                      {'loss': 2.1315, 'learning_rate': 0.0007638104113290531, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 749/2180 [1:59:54<3:48:31,  9.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 750/2180 [2:00:03<3:48:37,  9.59s/it]                                                      {'loss': 2.0978, 'learning_rate': 0.0007631789187113303, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 750/2180 [2:00:03<3:48:37,  9.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 751/2180 [2:00:13<3:48:31,  9.60s/it]                                                      {'loss': 2.0875, 'learning_rate': 0.000762546844873276, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 751/2180 [2:00:13<3:48:31,  9.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 752/2180 [2:00:22<3:48:01,  9.58s/it]                                                      {'loss': 2.1559, 'learning_rate': 0.0007619141912108007, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 752/2180 [2:00:22<3:48:01,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 753/2180 [2:00:32<3:47:43,  9.58s/it]                                                      {'loss': 2.0608, 'learning_rate': 0.000761280959121095, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 753/2180 [2:00:32<3:47:43,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 754/2180 [2:00:42<3:47:30,  9.57s/it]                                                      {'loss': 2.1625, 'learning_rate': 0.0007606471500026273, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 754/2180 [2:00:42<3:47:30,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 755/2180 [2:00:51<3:47:16,  9.57s/it]                                                      {'loss': 2.128, 'learning_rate': 0.0007600127652551401, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 755/2180 [2:00:51<3:47:16,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 756/2180 [2:01:01<3:46:53,  9.56s/it]                                                      {'loss': 2.1059, 'learning_rate': 0.0007593778062796472, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 756/2180 [2:01:01<3:46:53,  9.56s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 757/2180 [2:01:10<3:47:06,  9.58s/it]                                                      {'loss': 2.1018, 'learning_rate': 0.000758742274478431, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 757/2180 [2:01:10<3:47:06,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 758/2180 [2:01:20<3:46:56,  9.58s/it]                                                      {'loss': 2.1738, 'learning_rate': 0.0007581061712550381, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 758/2180 [2:01:20<3:46:56,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 759/2180 [2:01:30<3:47:39,  9.61s/it]                                                      {'loss': 2.1414, 'learning_rate': 0.0007574694980142779, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 759/2180 [2:01:30<3:47:39,  9.61s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 760/2180 [2:01:39<3:47:12,  9.60s/it]                                                      {'loss': 2.1482, 'learning_rate': 0.0007568322561622183, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 760/2180 [2:01:39<3:47:12,  9.60s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 761/2180 [2:01:49<3:46:42,  9.59s/it]                                                      {'loss': 2.133, 'learning_rate': 0.0007561944471061826, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 761/2180 [2:01:49<3:46:42,  9.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 762/2180 [2:01:58<3:46:17,  9.58s/it]                                                      {'loss': 2.1879, 'learning_rate': 0.0007555560722547475, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 762/2180 [2:01:58<3:46:17,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 763/2180 [2:02:08<3:46:08,  9.58s/it]                                                      {'loss': 2.0346, 'learning_rate': 0.0007549171330177387, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 763/2180 [2:02:08<3:46:08,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 764/2180 [2:02:17<3:46:00,  9.58s/it]                                                      {'loss': 2.1983, 'learning_rate': 0.0007542776308062285, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 764/2180 [2:02:17<3:46:00,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 765/2180 [2:02:27<3:45:26,  9.56s/it]                                                      {'loss': 2.1444, 'learning_rate': 0.0007536375670325325, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 765/2180 [2:02:27<3:45:26,  9.56s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 766/2180 [2:02:37<3:45:26,  9.57s/it]                                                      {'loss': 2.0859, 'learning_rate': 0.0007529969431102063, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 766/2180 [2:02:37<3:45:26,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 767/2180 [2:02:46<3:45:29,  9.58s/it]                                                      {'loss': 2.1269, 'learning_rate': 0.000752355760454043, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 767/2180 [2:02:46<3:45:29,  9.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 768/2180 [2:02:56<3:45:16,  9.57s/it]                                                      {'loss': 2.1166, 'learning_rate': 0.0007517140204800693, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 768/2180 [2:02:56<3:45:16,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 769/2180 [2:03:05<3:45:02,  9.57s/it]                                                      {'loss': 2.0789, 'learning_rate': 0.0007510717246055425, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 769/2180 [2:03:05<3:45:02,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 770/2180 [2:03:15<3:44:48,  9.57s/it]                                                      {'loss': 2.1254, 'learning_rate': 0.0007504288742489482, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 770/2180 [2:03:15<3:44:48,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 771/2180 [2:03:24<3:44:39,  9.57s/it]                                                      {'loss': 2.1438, 'learning_rate': 0.0007497854708299963, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 771/2180 [2:03:24<3:44:39,  9.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 772/2180 [2:03:34<3:44:23,  9.56s/it]                                                      {'loss': 2.1099, 'learning_rate': 0.0007491415157696178, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 772/2180 [2:03:34<3:44:23,  9.56s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 773/2180 [2:03:43<3:44:14,  9.56s/it]                                                      {'loss': 2.1331, 'learning_rate': 0.0007484970104899623, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 773/2180 [2:03:43<3:44:14,  9.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 774/2180 [2:03:53<3:44:27,  9.58s/it]                                                      {'loss': 2.1811, 'learning_rate': 0.0007478519564143945, 'epoch': 0.35}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 774/2180 [2:03:53<3:44:27,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 775/2180 [2:04:03<3:44:09,  9.57s/it]                                                      {'loss': 2.1563, 'learning_rate': 0.000747206354967491, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 775/2180 [2:04:03<3:44:09,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 776/2180 [2:04:12<3:44:02,  9.57s/it]                                                      {'loss': 2.2181, 'learning_rate': 0.0007465602075750373, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 776/2180 [2:04:12<3:44:02,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 777/2180 [2:04:22<3:44:04,  9.58s/it]                                                      {'loss': 2.098, 'learning_rate': 0.0007459135156640247, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 777/2180 [2:04:22<3:44:04,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 778/2180 [2:04:31<3:43:47,  9.58s/it]                                                      {'loss': 2.1158, 'learning_rate': 0.0007452662806626468, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 778/2180 [2:04:31<3:43:47,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2180 [2:04:41<3:43:33,  9.57s/it]                                                      {'loss': 2.0947, 'learning_rate': 0.0007446185040002967, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2180 [2:04:41<3:43:33,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2180 [2:04:51<3:43:33,  9.58s/it]                                                      {'loss': 2.1836, 'learning_rate': 0.0007439701871075642, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2180 [2:04:51<3:43:33,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2180 [2:05:00<3:43:17,  9.58s/it]                                                      {'loss': 2.1302, 'learning_rate': 0.0007433213314162313, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2180 [2:05:00<3:43:17,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2180 [2:05:10<3:43:16,  9.58s/it]                                                      {'loss': 2.1174, 'learning_rate': 0.0007426719383592705, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2180 [2:05:10<3:43:16,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2180 [2:05:19<3:43:05,  9.58s/it]                                                      {'loss': 2.1571, 'learning_rate': 0.000742022009370841, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2180 [2:05:19<3:43:05,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2180 [2:05:29<3:42:43,  9.57s/it]                                                      {'loss': 2.1146, 'learning_rate': 0.0007413715458862855, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2180 [2:05:29<3:42:43,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2180 [2:05:38<3:42:32,  9.57s/it]                                                      {'loss': 2.1032, 'learning_rate': 0.0007407205493421272, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2180 [2:05:38<3:42:32,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2180 [2:05:48<3:42:19,  9.57s/it]                                                      {'loss': 2.1057, 'learning_rate': 0.0007400690211760661, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2180 [2:05:48<3:42:19,  9.57s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2180 [2:05:58<3:42:28,  9.58s/it]                                                      {'loss': 2.0628, 'learning_rate': 0.0007394169628269771, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2180 [2:05:58<3:42:28,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2180 [2:06:07<3:42:21,  9.58s/it]                                                      {'loss': 2.1064, 'learning_rate': 0.0007387643757349051, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2180 [2:06:07<3:42:21,  9.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2180 [2:06:17<3:42:24,  9.59s/it]                                                      {'loss': 2.1562, 'learning_rate': 0.0007381112613410635, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2180 [2:06:17<3:42:24,  9.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2180 [2:06:27<3:45:00,  9.71s/it]                                                      {'loss': 2.042, 'learning_rate': 0.0007374576210878298, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2180 [2:06:27<3:45:00,  9.71s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 791/2180 [2:06:36<3:44:01,  9.68s/it]                                                      {'loss': 2.14, 'learning_rate': 0.0007368034564187425, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 791/2180 [2:06:36<3:44:01,  9.68s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 792/2180 [2:06:46<3:43:17,  9.65s/it]                                                      {'loss': 2.0948, 'learning_rate': 0.0007361487687784989, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 792/2180 [2:06:46<3:43:17,  9.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 793/2180 [2:06:56<3:42:18,  9.62s/it]                                                      {'loss': 2.05, 'learning_rate': 0.0007354935596129513, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 793/2180 [2:06:56<3:42:18,  9.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 794/2180 [2:07:05<3:41:36,  9.59s/it]                                                      {'loss': 2.1469, 'learning_rate': 0.000734837830369103, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 794/2180 [2:07:05<3:41:36,  9.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 795/2180 [2:07:15<3:41:15,  9.59s/it]                                                      {'loss': 2.0981, 'learning_rate': 0.0007341815824951066, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 795/2180 [2:07:15<3:41:15,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 796/2180 [2:07:24<3:41:19,  9.59s/it]                                                      {'loss': 2.1645, 'learning_rate': 0.0007335248174402597, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 796/2180 [2:07:24<3:41:19,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 797/2180 [2:07:34<3:41:18,  9.60s/it]                                                      {'loss': 2.1255, 'learning_rate': 0.0007328675366550023, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 797/2180 [2:07:34<3:41:18,  9.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 798/2180 [2:07:44<3:41:34,  9.62s/it]                                                      {'loss': 2.0436, 'learning_rate': 0.0007322097415909134, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 798/2180 [2:07:44<3:41:34,  9.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 799/2180 [2:07:53<3:41:55,  9.64s/it]                                                      {'loss': 2.1411, 'learning_rate': 0.0007315514337007071, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 799/2180 [2:07:53<3:41:55,  9.64s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 800/2180 [2:08:03<3:41:24,  9.63s/it]                                                      {'loss': 2.0599, 'learning_rate': 0.0007308926144382312, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 800/2180 [2:08:03<3:41:24,  9.63s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 801/2180 [2:08:12<3:41:01,  9.62s/it]                                                      {'loss': 2.101, 'learning_rate': 0.0007302332852584619, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 801/2180 [2:08:12<3:41:01,  9.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 802/2180 [2:08:22<3:40:21,  9.59s/it]                                                      {'loss': 2.2092, 'learning_rate': 0.0007295734476175018, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 802/2180 [2:08:22<3:40:21,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 803/2180 [2:08:32<3:40:09,  9.59s/it]                                                      {'loss': 2.113, 'learning_rate': 0.0007289131029725768, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 803/2180 [2:08:32<3:40:09,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 804/2180 [2:08:41<3:40:15,  9.60s/it]                                                      {'loss': 2.1539, 'learning_rate': 0.0007282522527820319, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 804/2180 [2:08:41<3:40:15,  9.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 805/2180 [2:08:51<3:39:53,  9.60s/it]                                                      {'loss': 2.1106, 'learning_rate': 0.000727590898505329, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 805/2180 [2:08:51<3:39:53,  9.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 806/2180 [2:09:00<3:39:57,  9.60s/it]                                                      {'loss': 1.9762, 'learning_rate': 0.0007269290416030429, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 806/2180 [2:09:00<3:39:57,  9.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2180 [2:09:10<3:39:24,  9.59s/it]                                                      {'loss': 2.0543, 'learning_rate': 0.000726266683536859, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2180 [2:09:10<3:39:24,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2180 [2:09:19<3:39:12,  9.59s/it]                                                      {'loss': 2.1183, 'learning_rate': 0.0007256038257695687, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2180 [2:09:19<3:39:12,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2180 [2:09:29<3:38:56,  9.58s/it]                                                      {'loss': 2.1348, 'learning_rate': 0.0007249404697650678, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2180 [2:09:29<3:38:56,  9.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2180 [2:09:39<3:38:46,  9.58s/it]                                                      {'loss': 2.1273, 'learning_rate': 0.0007242766169883518, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2180 [2:09:39<3:38:46,  9.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2180 [2:09:48<3:38:51,  9.59s/it]                                                      {'loss': 2.177, 'learning_rate': 0.0007236122689055138, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2180 [2:09:48<3:38:51,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2180 [2:09:58<3:38:36,  9.59s/it]                                                      {'loss': 2.1243, 'learning_rate': 0.0007229474269837401, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2180 [2:09:58<3:38:36,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2180 [2:10:07<3:38:18,  9.58s/it]                                                      {'loss': 2.0568, 'learning_rate': 0.0007222820926913085, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2180 [2:10:07<3:38:18,  9.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2180 [2:10:17<3:38:18,  9.59s/it]                                                      {'loss': 2.1132, 'learning_rate': 0.0007216162674975833, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2180 [2:10:17<3:38:18,  9.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2180 [2:10:27<3:37:52,  9.58s/it]                                                      {'loss': 2.1354, 'learning_rate': 0.0007209499528730138, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2180 [2:10:27<3:37:52,  9.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2180 [2:10:36<3:37:43,  9.58s/it]                                                      {'loss': 2.0262, 'learning_rate': 0.0007202831502891294, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2180 [2:10:36<3:37:43,  9.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2180 [2:10:46<3:37:40,  9.58s/it]                                                      {'loss': 2.1357, 'learning_rate': 0.0007196158612185375, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2180 [2:10:46<3:37:40,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 818/2180 [2:10:55<3:37:22,  9.58s/it]                                                      {'loss': 2.0341, 'learning_rate': 0.0007189480871349201, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 818/2180 [2:10:55<3:37:22,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 819/2180 [2:11:05<3:37:00,  9.57s/it]                                                      {'loss': 2.012, 'learning_rate': 0.0007182798295130299, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 819/2180 [2:11:05<3:37:00,  9.57s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 820/2180 [2:11:14<3:36:52,  9.57s/it]                                                      {'loss': 2.0007, 'learning_rate': 0.0007176110898286878, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 820/2180 [2:11:14<3:36:52,  9.57s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 821/2180 [2:11:24<3:36:39,  9.57s/it]                                                      {'loss': 2.0419, 'learning_rate': 0.0007169418695587791, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 821/2180 [2:11:24<3:36:39,  9.57s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 822/2180 [2:11:34<3:36:22,  9.56s/it]                                                      {'loss': 2.1212, 'learning_rate': 0.0007162721701812506, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 822/2180 [2:11:34<3:36:22,  9.56s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 823/2180 [2:11:43<3:36:14,  9.56s/it]                                                      {'loss': 2.0164, 'learning_rate': 0.0007156019931751072, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 823/2180 [2:11:43<3:36:14,  9.56s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 824/2180 [2:11:53<3:36:07,  9.56s/it]                                                      {'loss': 2.094, 'learning_rate': 0.0007149313400204082, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 824/2180 [2:11:53<3:36:07,  9.56s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 825/2180 [2:12:02<3:36:10,  9.57s/it]                                                      {'loss': 2.0681, 'learning_rate': 0.0007142602121982653, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 825/2180 [2:12:02<3:36:10,  9.57s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 826/2180 [2:12:12<3:36:11,  9.58s/it]                                                      {'loss': 2.1523, 'learning_rate': 0.0007135886111908379, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 826/2180 [2:12:12<3:36:11,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 827/2180 [2:12:21<3:35:57,  9.58s/it]                                                      {'loss': 2.1108, 'learning_rate': 0.0007129165384813303, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 827/2180 [2:12:21<3:35:57,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 828/2180 [2:12:31<3:36:21,  9.60s/it]                                                      {'loss': 2.1513, 'learning_rate': 0.0007122439955539888, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 828/2180 [2:12:31<3:36:21,  9.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 829/2180 [2:12:41<3:35:53,  9.59s/it]                                                      {'loss': 2.1372, 'learning_rate': 0.0007115709838940983, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 829/2180 [2:12:41<3:35:53,  9.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 830/2180 [2:12:50<3:36:12,  9.61s/it]                                                      {'loss': 2.1295, 'learning_rate': 0.0007108975049879785, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 830/2180 [2:12:50<3:36:12,  9.61s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 831/2180 [2:13:00<3:35:34,  9.59s/it]                                                      {'loss': 2.0481, 'learning_rate': 0.0007102235603229814, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 831/2180 [2:13:00<3:35:34,  9.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 832/2180 [2:13:09<3:35:11,  9.58s/it]                                                      {'loss': 2.066, 'learning_rate': 0.000709549151387487, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 832/2180 [2:13:09<3:35:11,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 833/2180 [2:13:19<3:34:59,  9.58s/it]                                                      {'loss': 2.1124, 'learning_rate': 0.0007088742796709013, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 833/2180 [2:13:19<3:34:59,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2180 [2:13:29<3:34:59,  9.58s/it]                                                      {'loss': 2.112, 'learning_rate': 0.000708198946663652, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2180 [2:13:29<3:34:59,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2180 [2:13:38<3:34:43,  9.58s/it]                                                      {'loss': 2.1264, 'learning_rate': 0.0007075231538571856, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2180 [2:13:38<3:34:43,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2180 [2:13:48<3:34:36,  9.58s/it]                                                      {'loss': 2.1912, 'learning_rate': 0.0007068469027439641, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2180 [2:13:48<3:34:36,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2180 [2:13:57<3:34:29,  9.58s/it]                                                      {'loss': 2.1589, 'learning_rate': 0.0007061701948174613, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2180 [2:13:57<3:34:29,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2180 [2:14:07<3:34:14,  9.58s/it]                                                      {'loss': 2.0675, 'learning_rate': 0.0007054930315721606, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2180 [2:14:07<3:34:14,  9.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2180 [2:14:16<3:34:06,  9.58s/it]                                                      {'loss': 2.0919, 'learning_rate': 0.0007048154145035501, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2180 [2:14:16<3:34:06,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2180 [2:14:26<3:33:50,  9.57s/it]                                                      {'loss': 2.1271, 'learning_rate': 0.0007041373451081207, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2180 [2:14:26<3:33:50,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2180 [2:14:36<3:33:34,  9.57s/it]                                                      {'loss': 2.184, 'learning_rate': 0.0007034588248833621, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2180 [2:14:36<3:33:34,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2180 [2:14:45<3:33:21,  9.57s/it]                                                      {'loss': 2.0809, 'learning_rate': 0.0007027798553277595, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2180 [2:14:45<3:33:21,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2180 [2:14:55<3:33:00,  9.56s/it]                                                      {'loss': 2.0867, 'learning_rate': 0.0007021004379407909, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2180 [2:14:55<3:33:00,  9.56s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2180 [2:15:04<3:33:22,  9.58s/it]                                                      {'loss': 2.177, 'learning_rate': 0.0007014205742229227, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2180 [2:15:04<3:33:22,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 845/2180 [2:15:14<3:33:03,  9.58s/it]                                                      {'loss': 2.1393, 'learning_rate': 0.0007007402656756072, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 845/2180 [2:15:14<3:33:03,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 846/2180 [2:15:23<3:32:39,  9.56s/it]                                                      {'loss': 2.1112, 'learning_rate': 0.0007000595138012797, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 846/2180 [2:15:23<3:32:39,  9.56s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 847/2180 [2:15:33<3:32:43,  9.57s/it]                                                      {'loss': 2.0462, 'learning_rate': 0.0006993783201033535, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 847/2180 [2:15:33<3:32:43,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 848/2180 [2:15:43<3:32:56,  9.59s/it]                                                      {'loss': 2.1393, 'learning_rate': 0.0006986966860862182, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 848/2180 [2:15:43<3:32:56,  9.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 849/2180 [2:15:52<3:32:25,  9.58s/it]                                                      {'loss': 2.0204, 'learning_rate': 0.000698014613255236, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 849/2180 [2:15:52<3:32:25,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 850/2180 [2:16:02<3:32:10,  9.57s/it]                                                      {'loss': 2.1464, 'learning_rate': 0.0006973321031167382, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 850/2180 [2:16:02<3:32:10,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 851/2180 [2:16:11<3:32:10,  9.58s/it]                                                      {'loss': 2.1406, 'learning_rate': 0.0006966491571780216, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 851/2180 [2:16:11<3:32:10,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 852/2180 [2:16:21<3:31:50,  9.57s/it]                                                      {'loss': 2.042, 'learning_rate': 0.0006959657769473453, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 852/2180 [2:16:21<3:31:50,  9.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 853/2180 [2:16:31<3:32:25,  9.60s/it]                                                      {'loss': 2.1246, 'learning_rate': 0.000695281963933928, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 853/2180 [2:16:31<3:32:25,  9.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 854/2180 [2:16:40<3:31:55,  9.59s/it]                                                      {'loss': 2.0421, 'learning_rate': 0.0006945977196479438, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 854/2180 [2:16:40<3:31:55,  9.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 855/2180 [2:16:50<3:31:38,  9.58s/it]                                                      {'loss': 2.1246, 'learning_rate': 0.0006939130456005196, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 855/2180 [2:16:50<3:31:38,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 856/2180 [2:16:59<3:31:42,  9.59s/it]                                                      {'loss': 2.112, 'learning_rate': 0.0006932279433037311, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 856/2180 [2:16:59<3:31:42,  9.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 857/2180 [2:17:09<3:31:12,  9.58s/it]                                                      {'loss': 2.1115, 'learning_rate': 0.0006925424142705997, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 857/2180 [2:17:09<3:31:12,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 858/2180 [2:17:18<3:31:06,  9.58s/it]                                                      {'loss': 2.0852, 'learning_rate': 0.0006918564600150896, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 858/2180 [2:17:18<3:31:06,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 859/2180 [2:17:28<3:30:58,  9.58s/it]                                                      {'loss': 2.0756, 'learning_rate': 0.0006911700820521042, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 859/2180 [2:17:28<3:30:58,  9.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 860/2180 [2:17:38<3:31:01,  9.59s/it]                                                      {'loss': 1.9935, 'learning_rate': 0.0006904832818974818, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 860/2180 [2:17:38<3:31:01,  9.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 861/2180 [2:17:47<3:30:37,  9.58s/it]                                                      {'loss': 2.1104, 'learning_rate': 0.0006897960610679939, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 861/2180 [2:17:47<3:30:37,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2180 [2:17:57<3:30:17,  9.57s/it]                                                      {'loss': 1.9781, 'learning_rate': 0.0006891084210813407, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2180 [2:17:57<3:30:17,  9.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2180 [2:18:06<3:30:11,  9.58s/it]                                                      {'loss': 1.9935, 'learning_rate': 0.0006884203634561483, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2180 [2:18:06<3:30:11,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2180 [2:18:16<3:30:15,  9.59s/it]                                                      {'loss': 2.0872, 'learning_rate': 0.0006877318897119651, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2180 [2:18:16<3:30:15,  9.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2180 [2:18:26<3:30:20,  9.60s/it]                                                      {'loss': 2.1765, 'learning_rate': 0.0006870430013692579, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2180 [2:18:26<3:30:20,  9.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2180 [2:18:35<3:30:03,  9.59s/it]                                                      {'loss': 2.0478, 'learning_rate': 0.0006863536999494101, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2180 [2:18:35<3:30:03,  9.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2180 [2:18:45<3:29:59,  9.60s/it]                                                      {'loss': 2.1362, 'learning_rate': 0.0006856639869747167, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2180 [2:18:45<3:29:59,  9.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2180 [2:18:54<3:29:39,  9.59s/it]                                                      {'loss': 2.1027, 'learning_rate': 0.0006849738639683818, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2180 [2:18:54<3:29:39,  9.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2180 [2:19:04<3:29:23,  9.58s/it]                                                      {'loss': 2.083, 'learning_rate': 0.000684283332454515, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2180 [2:19:04<3:29:23,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2180 [2:19:13<3:29:06,  9.58s/it]                                                      {'loss': 2.1058, 'learning_rate': 0.0006835923939581281, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2180 [2:19:13<3:29:06,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2180 [2:19:23<3:28:46,  9.57s/it]                                                      {'loss': 2.0514, 'learning_rate': 0.0006829010500051318, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2180 [2:19:23<3:28:46,  9.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 872/2180 [2:19:33<3:28:33,  9.57s/it]                                                      {'loss': 2.0394, 'learning_rate': 0.0006822093021223321, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 872/2180 [2:19:33<3:28:33,  9.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 873/2180 [2:19:42<3:29:19,  9.61s/it]                                                      {'loss': 2.0692, 'learning_rate': 0.0006815171518374268, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 873/2180 [2:19:42<3:29:19,  9.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 874/2180 [2:19:52<3:28:54,  9.60s/it]                                                      {'loss': 2.1526, 'learning_rate': 0.0006808246006790031, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 874/2180 [2:19:52<3:28:54,  9.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 875/2180 [2:20:01<3:28:33,  9.59s/it]                                                      {'loss': 2.1398, 'learning_rate': 0.0006801316501765329, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 875/2180 [2:20:01<3:28:33,  9.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 876/2180 [2:20:11<3:28:03,  9.57s/it]                                                      {'loss': 2.0803, 'learning_rate': 0.0006794383018603704, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 876/2180 [2:20:11<3:28:03,  9.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 877/2180 [2:20:21<3:27:57,  9.58s/it]                                                      {'loss': 2.149, 'learning_rate': 0.0006787445572617481, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 877/2180 [2:20:21<3:27:57,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 878/2180 [2:20:30<3:27:50,  9.58s/it]                                                      {'loss': 2.0918, 'learning_rate': 0.0006780504179127734, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 878/2180 [2:20:30<3:27:50,  9.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 879/2180 [2:20:40<3:28:31,  9.62s/it]                                                      {'loss': 2.1018, 'learning_rate': 0.0006773558853464265, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 879/2180 [2:20:40<3:28:31,  9.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 880/2180 [2:20:49<3:28:40,  9.63s/it]                                                      {'loss': 2.0979, 'learning_rate': 0.000676660961096555, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 880/2180 [2:20:49<3:28:40,  9.63s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 881/2180 [2:20:59<3:27:59,  9.61s/it]                                                      {'loss': 2.0741, 'learning_rate': 0.000675965646697872, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 881/2180 [2:20:59<3:27:59,  9.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 882/2180 [2:21:09<3:27:38,  9.60s/it]                                                      {'loss': 2.0548, 'learning_rate': 0.0006752699436859519, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 882/2180 [2:21:09<3:27:38,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 883/2180 [2:21:18<3:27:25,  9.60s/it]                                                      {'loss': 2.0273, 'learning_rate': 0.0006745738535972279, 'epoch': 0.4}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 883/2180 [2:21:18<3:27:25,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 884/2180 [2:21:28<3:27:27,  9.60s/it]                                                      {'loss': 2.1426, 'learning_rate': 0.0006738773779689874, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 884/2180 [2:21:28<3:27:27,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 885/2180 [2:21:37<3:27:02,  9.59s/it]                                                      {'loss': 2.0965, 'learning_rate': 0.0006731805183393696, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 885/2180 [2:21:37<3:27:02,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 886/2180 [2:21:47<3:26:45,  9.59s/it]                                                      {'loss': 2.1619, 'learning_rate': 0.0006724832762473618, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 886/2180 [2:21:47<3:26:45,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 887/2180 [2:21:57<3:26:35,  9.59s/it]                                                      {'loss': 2.0984, 'learning_rate': 0.0006717856532327956, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 887/2180 [2:21:57<3:26:35,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 888/2180 [2:22:06<3:26:35,  9.59s/it]                                                      {'loss': 2.0949, 'learning_rate': 0.0006710876508363444, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 888/2180 [2:22:06<3:26:35,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 889/2180 [2:22:16<3:26:13,  9.58s/it]                                                      {'loss': 2.0663, 'learning_rate': 0.0006703892705995189, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 889/2180 [2:22:16<3:26:13,  9.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2180 [2:22:25<3:26:24,  9.60s/it]                                                      {'loss': 2.1513, 'learning_rate': 0.0006696905140646647, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2180 [2:22:25<3:26:24,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2180 [2:22:35<3:26:12,  9.60s/it]                                                      {'loss': 2.1185, 'learning_rate': 0.0006689913827749581, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2180 [2:22:35<3:26:12,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2180 [2:22:45<3:25:58,  9.59s/it]                                                      {'loss': 2.0902, 'learning_rate': 0.0006682918782744032, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2180 [2:22:45<3:25:58,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2180 [2:22:54<3:25:33,  9.58s/it]                                                      {'loss': 2.1369, 'learning_rate': 0.0006675920021078282, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2180 [2:22:54<3:25:33,  9.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2180 [2:23:04<3:25:24,  9.58s/it]                                                      {'loss': 2.0814, 'learning_rate': 0.0006668917558208823, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2180 [2:23:04<3:25:24,  9.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2180 [2:23:13<3:25:07,  9.58s/it]                                                      {'loss': 2.1396, 'learning_rate': 0.0006661911409600321, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2180 [2:23:13<3:25:07,  9.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2180 [2:23:23<3:25:15,  9.59s/it]                                                      {'loss': 2.1436, 'learning_rate': 0.0006654901590725577, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2180 [2:23:23<3:25:15,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2180 [2:23:32<3:25:09,  9.59s/it]                                                      {'loss': 2.067, 'learning_rate': 0.0006647888117065507, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2180 [2:23:32<3:25:09,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2180 [2:23:42<3:24:49,  9.59s/it]                                                      {'loss': 2.1218, 'learning_rate': 0.0006640871004109086, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2180 [2:23:42<3:24:49,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2180 [2:23:52<3:24:54,  9.60s/it]                                                      {'loss': 2.1629, 'learning_rate': 0.000663385026735334, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2180 [2:23:52<3:24:54,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 900/2180 [2:24:01<3:25:27,  9.63s/it]                                                      {'loss': 2.1889, 'learning_rate': 0.0006626825922303285, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 900/2180 [2:24:01<3:25:27,  9.63s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 901/2180 [2:24:11<3:24:40,  9.60s/it]                                                      {'loss': 2.0281, 'learning_rate': 0.0006619797984471915, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 901/2180 [2:24:11<3:24:40,  9.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 902/2180 [2:24:20<3:24:21,  9.59s/it]                                                      {'loss': 2.0475, 'learning_rate': 0.0006612766469380158, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 902/2180 [2:24:20<3:24:21,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 903/2180 [2:24:30<3:24:11,  9.59s/it]                                                      {'loss': 2.0847, 'learning_rate': 0.0006605731392556833, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 903/2180 [2:24:30<3:24:11,  9.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 904/2180 [2:24:40<3:23:57,  9.59s/it]                                                      {'loss': 2.1277, 'learning_rate': 0.0006598692769538637, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 904/2180 [2:24:40<3:23:57,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 905/2180 [2:24:49<3:24:36,  9.63s/it]                                                      {'loss': 2.0163, 'learning_rate': 0.0006591650615870091, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 905/2180 [2:24:49<3:24:36,  9.63s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 906/2180 [2:24:59<3:24:40,  9.64s/it]                                                      {'loss': 2.0041, 'learning_rate': 0.0006584604947103514, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 906/2180 [2:24:59<3:24:40,  9.64s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 907/2180 [2:25:09<3:24:04,  9.62s/it]                                                      {'loss': 2.1106, 'learning_rate': 0.0006577555778798993, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 907/2180 [2:25:09<3:24:04,  9.62s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 908/2180 [2:25:18<3:23:25,  9.60s/it]                                                      {'loss': 2.1199, 'learning_rate': 0.0006570503126524336, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 908/2180 [2:25:18<3:23:25,  9.60s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 909/2180 [2:25:28<3:23:08,  9.59s/it]                                                      {'loss': 2.1091, 'learning_rate': 0.0006563447005855054, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 909/2180 [2:25:28<3:23:08,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 910/2180 [2:25:37<3:22:42,  9.58s/it]                                                      {'loss': 2.0993, 'learning_rate': 0.000655638743237431, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 910/2180 [2:25:37<3:22:42,  9.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 911/2180 [2:25:47<3:22:28,  9.57s/it]                                                      {'loss': 2.1533, 'learning_rate': 0.0006549324421672894, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 911/2180 [2:25:47<3:22:28,  9.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 912/2180 [2:25:56<3:22:40,  9.59s/it]                                                      {'loss': 1.9912, 'learning_rate': 0.0006542257989349194, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 912/2180 [2:25:56<3:22:40,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 913/2180 [2:26:06<3:22:18,  9.58s/it]                                                      {'loss': 2.0233, 'learning_rate': 0.0006535188151009142, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 913/2180 [2:26:06<3:22:18,  9.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 914/2180 [2:26:16<3:22:05,  9.58s/it]                                                      {'loss': 2.1079, 'learning_rate': 0.0006528114922266204, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 914/2180 [2:26:16<3:22:05,  9.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 915/2180 [2:26:25<3:21:45,  9.57s/it]                                                      {'loss': 2.1323, 'learning_rate': 0.0006521038318741327, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 915/2180 [2:26:25<3:21:45,  9.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 916/2180 [2:26:35<3:21:41,  9.57s/it]                                                      {'loss': 2.1006, 'learning_rate': 0.0006513958356062912, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 916/2180 [2:26:35<3:21:41,  9.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 917/2180 [2:26:44<3:21:24,  9.57s/it]                                                      {'loss': 2.1722, 'learning_rate': 0.0006506875049866781, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 917/2180 [2:26:44<3:21:24,  9.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2180 [2:26:54<3:21:30,  9.58s/it]                                                      {'loss': 2.2256, 'learning_rate': 0.0006499788415796137, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2180 [2:26:54<3:21:30,  9.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2180 [2:27:04<3:21:34,  9.59s/it]                                                      {'loss': 2.0429, 'learning_rate': 0.0006492698469501532, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2180 [2:27:04<3:21:34,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2180 [2:27:13<3:21:19,  9.59s/it]                                                      {'loss': 2.1343, 'learning_rate': 0.0006485605226640837, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2180 [2:27:13<3:21:19,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2180 [2:27:23<3:21:19,  9.59s/it]                                                      {'loss': 2.1421, 'learning_rate': 0.00064785087028792, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2180 [2:27:23<3:21:19,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2180 [2:27:32<3:20:58,  9.59s/it]                                                      {'loss': 2.0745, 'learning_rate': 0.0006471408913889019, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2180 [2:27:32<3:20:58,  9.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2180 [2:27:42<3:21:41,  9.63s/it]                                                      {'loss': 2.1221, 'learning_rate': 0.0006464305875349892, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2180 [2:27:42<3:21:41,  9.63s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2180 [2:27:52<3:21:10,  9.61s/it]                                                      {'loss': 2.1249, 'learning_rate': 0.000645719960294861, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2180 [2:27:52<3:21:10,  9.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2180 [2:28:01<3:20:49,  9.60s/it]                                                      {'loss': 2.1032, 'learning_rate': 0.0006450090112379092, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2180 [2:28:01<3:20:49,  9.60s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2180 [2:28:11<3:20:27,  9.59s/it]                                                      {'loss': 2.1776, 'learning_rate': 0.0006442977419342371, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2180 [2:28:11<3:20:27,  9.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 927/2180 [2:28:20<3:20:07,  9.58s/it]                                                      {'loss': 2.0909, 'learning_rate': 0.000643586153954655, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 927/2180 [2:28:20<3:20:07,  9.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 928/2180 [2:28:30<3:19:50,  9.58s/it]                                                      {'loss': 2.0885, 'learning_rate': 0.0006428742488706772, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 928/2180 [2:28:30<3:19:50,  9.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 929/2180 [2:28:39<3:19:35,  9.57s/it]                                                      {'loss': 2.1312, 'learning_rate': 0.0006421620282545182, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 929/2180 [2:28:39<3:19:35,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 930/2180 [2:28:49<3:19:20,  9.57s/it]                                                      {'loss': 2.0462, 'learning_rate': 0.0006414494936790892, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 930/2180 [2:28:49<3:19:20,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 931/2180 [2:28:59<3:19:07,  9.57s/it]                                                      {'loss': 1.9829, 'learning_rate': 0.0006407366467179951, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 931/2180 [2:28:59<3:19:07,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 932/2180 [2:29:08<3:19:09,  9.57s/it]                                                      {'loss': 2.1273, 'learning_rate': 0.0006400234889455301, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 932/2180 [2:29:08<3:19:09,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 933/2180 [2:29:18<3:18:55,  9.57s/it]                                                      {'loss': 2.0727, 'learning_rate': 0.0006393100219366755, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 933/2180 [2:29:18<3:18:55,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 934/2180 [2:29:27<3:18:53,  9.58s/it]                                                      {'loss': 2.0818, 'learning_rate': 0.0006385962472670953, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 934/2180 [2:29:27<3:18:53,  9.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 935/2180 [2:29:37<3:19:11,  9.60s/it]                                                      {'loss': 2.206, 'learning_rate': 0.0006378821665131328, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 935/2180 [2:29:37<3:19:11,  9.60s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 936/2180 [2:29:46<3:18:51,  9.59s/it]                                                      {'loss': 2.1273, 'learning_rate': 0.0006371677812518072, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 936/2180 [2:29:46<3:18:51,  9.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 937/2180 [2:29:56<3:18:17,  9.57s/it]                                                      {'loss': 2.0735, 'learning_rate': 0.0006364530930608107, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 937/2180 [2:29:56<3:18:17,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 938/2180 [2:30:06<3:18:26,  9.59s/it]                                                      {'loss': 2.0184, 'learning_rate': 0.0006357381035185038, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 938/2180 [2:30:06<3:18:26,  9.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 939/2180 [2:30:15<3:18:19,  9.59s/it]                                                      {'loss': 2.0521, 'learning_rate': 0.0006350228142039131, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 939/2180 [2:30:15<3:18:19,  9.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 940/2180 [2:30:25<3:17:57,  9.58s/it]                                                      {'loss': 2.1017, 'learning_rate': 0.000634307226696727, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 940/2180 [2:30:25<3:17:57,  9.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 941/2180 [2:30:34<3:18:04,  9.59s/it]                                                      {'loss': 2.0709, 'learning_rate': 0.0006335913425772926, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 941/2180 [2:30:34<3:18:04,  9.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 942/2180 [2:30:44<3:17:38,  9.58s/it]                                                      {'loss': 2.1278, 'learning_rate': 0.0006328751634266117, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 942/2180 [2:30:44<3:17:38,  9.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 943/2180 [2:30:54<3:17:13,  9.57s/it]                                                      {'loss': 2.0114, 'learning_rate': 0.0006321586908263382, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 943/2180 [2:30:54<3:17:13,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 944/2180 [2:31:03<3:17:03,  9.57s/it]                                                      {'loss': 2.1703, 'learning_rate': 0.0006314419263587732, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 944/2180 [2:31:03<3:17:03,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 945/2180 [2:31:13<3:16:38,  9.55s/it]                                                      {'loss': 2.0435, 'learning_rate': 0.0006307248716068637, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 945/2180 [2:31:13<3:16:38,  9.55s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2180 [2:31:22<3:16:35,  9.56s/it]                                                      {'loss': 2.0795, 'learning_rate': 0.0006300075281541964, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2180 [2:31:22<3:16:35,  9.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2180 [2:31:32<3:16:41,  9.57s/it]                                                      {'loss': 2.0798, 'learning_rate': 0.0006292898975849966, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2180 [2:31:32<3:16:41,  9.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2180 [2:31:41<3:16:23,  9.56s/it]                                                      {'loss': 2.0257, 'learning_rate': 0.000628571981484123, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2180 [2:31:41<3:16:23,  9.56s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2180 [2:31:51<3:16:27,  9.58s/it]                                                      {'loss': 2.0727, 'learning_rate': 0.0006278537814370654, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2180 [2:31:51<3:16:27,  9.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2180 [2:32:01<3:16:36,  9.59s/it]                                                      {'loss': 2.1169, 'learning_rate': 0.0006271352990299406, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2180 [2:32:01<3:16:36,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2180 [2:32:10<3:16:39,  9.60s/it]                                                      {'loss': 2.0933, 'learning_rate': 0.0006264165358494885, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2180 [2:32:10<3:16:39,  9.60s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2180 [2:32:20<3:16:11,  9.59s/it]                                                      {'loss': 2.1673, 'learning_rate': 0.0006256974934830694, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2180 [2:32:20<3:16:11,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2180 [2:32:29<3:16:07,  9.59s/it]                                                      {'loss': 2.0792, 'learning_rate': 0.0006249781735186606, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2180 [2:32:29<3:16:07,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 954/2180 [2:32:39<3:16:01,  9.59s/it]                                                      {'loss': 2.067, 'learning_rate': 0.0006242585775448518, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 954/2180 [2:32:39<3:16:01,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 955/2180 [2:32:48<3:15:43,  9.59s/it]                                                      {'loss': 2.0269, 'learning_rate': 0.0006235387071508427, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 955/2180 [2:32:48<3:15:43,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 956/2180 [2:32:58<3:15:41,  9.59s/it]                                                      {'loss': 2.1514, 'learning_rate': 0.0006228185639264384, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 956/2180 [2:32:58<3:15:41,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 957/2180 [2:33:08<3:15:56,  9.61s/it]                                                      {'loss': 2.1116, 'learning_rate': 0.0006220981494620475, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 957/2180 [2:33:08<3:15:56,  9.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 958/2180 [2:33:17<3:15:24,  9.59s/it]                                                      {'loss': 2.1607, 'learning_rate': 0.000621377465348677, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 958/2180 [2:33:17<3:15:24,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 959/2180 [2:33:27<3:14:52,  9.58s/it]                                                      {'loss': 2.0977, 'learning_rate': 0.0006206565131779293, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 959/2180 [2:33:27<3:14:52,  9.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 960/2180 [2:33:36<3:14:37,  9.57s/it]                                                      {'loss': 2.1441, 'learning_rate': 0.0006199352945419994, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 960/2180 [2:33:36<3:14:37,  9.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 961/2180 [2:33:46<3:14:17,  9.56s/it]                                                      {'loss': 2.0761, 'learning_rate': 0.00061921381103367, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 961/2180 [2:33:46<3:14:17,  9.56s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 962/2180 [2:33:56<3:14:15,  9.57s/it]                                                      {'loss': 2.1551, 'learning_rate': 0.0006184920642463094, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 962/2180 [2:33:56<3:14:15,  9.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 963/2180 [2:34:05<3:14:01,  9.57s/it]                                                      {'loss': 2.1827, 'learning_rate': 0.0006177700557738672, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 963/2180 [2:34:05<3:14:01,  9.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 964/2180 [2:34:15<3:14:16,  9.59s/it]                                                      {'loss': 1.9489, 'learning_rate': 0.0006170477872108706, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 964/2180 [2:34:15<3:14:16,  9.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 965/2180 [2:34:24<3:13:58,  9.58s/it]                                                      {'loss': 2.1668, 'learning_rate': 0.0006163252601524216, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 965/2180 [2:34:24<3:13:58,  9.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 966/2180 [2:34:34<3:13:42,  9.57s/it]                                                      {'loss': 2.0995, 'learning_rate': 0.0006156024761941925, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 966/2180 [2:34:34<3:13:42,  9.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 967/2180 [2:34:43<3:13:32,  9.57s/it]                                                      {'loss': 2.1102, 'learning_rate': 0.000614879436932424, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 967/2180 [2:34:43<3:13:32,  9.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 968/2180 [2:34:53<3:13:29,  9.58s/it]                                                      {'loss': 2.1834, 'learning_rate': 0.0006141561439639196, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 968/2180 [2:34:53<3:13:29,  9.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 969/2180 [2:35:03<3:14:01,  9.61s/it]                                                      {'loss': 2.1104, 'learning_rate': 0.0006134325988860433, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 969/2180 [2:35:03<3:14:01,  9.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 970/2180 [2:35:12<3:13:25,  9.59s/it]                                                      {'loss': 2.1734, 'learning_rate': 0.0006127088032967165, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 970/2180 [2:35:12<3:13:25,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 971/2180 [2:35:22<3:12:58,  9.58s/it]                                                      {'loss': 2.0915, 'learning_rate': 0.0006119847587944131, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 971/2180 [2:35:22<3:12:58,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 972/2180 [2:35:31<3:12:50,  9.58s/it]                                                      {'loss': 2.0538, 'learning_rate': 0.0006112604669781572, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 972/2180 [2:35:31<3:12:50,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2180 [2:35:41<3:12:58,  9.59s/it]                                                      {'loss': 2.0869, 'learning_rate': 0.0006105359294475188, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2180 [2:35:41<3:12:58,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2180 [2:35:51<3:12:47,  9.59s/it]                                                      {'loss': 2.1205, 'learning_rate': 0.0006098111478026107, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2180 [2:35:51<3:12:47,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2180 [2:36:00<3:12:48,  9.60s/it]                                                      {'loss': 2.0867, 'learning_rate': 0.0006090861236440848, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2180 [2:36:00<3:12:48,  9.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2180 [2:36:10<3:12:20,  9.59s/it]                                                      {'loss': 2.0589, 'learning_rate': 0.0006083608585731282, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2180 [2:36:10<3:12:20,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2180 [2:36:19<3:12:13,  9.59s/it]                                                      {'loss': 2.0681, 'learning_rate': 0.0006076353541914609, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2180 [2:36:19<3:12:13,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2180 [2:36:29<3:12:00,  9.58s/it]                                                      {'loss': 2.0679, 'learning_rate': 0.0006069096121013307, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2180 [2:36:29<3:12:00,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2180 [2:36:38<3:11:49,  9.58s/it]                                                      {'loss': 2.0137, 'learning_rate': 0.0006061836339055105, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2180 [2:36:39<3:11:49,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2180 [2:36:48<3:11:43,  9.59s/it]                                                      {'loss': 2.0466, 'learning_rate': 0.0006054574212072948, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2180 [2:36:48<3:11:43,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 981/2180 [2:36:58<3:11:42,  9.59s/it]                                                      {'loss': 2.1157, 'learning_rate': 0.0006047309756104958, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 981/2180 [2:36:58<3:11:42,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 982/2180 [2:37:07<3:11:43,  9.60s/it]                                                      {'loss': 2.04, 'learning_rate': 0.00060400429871944, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 982/2180 [2:37:07<3:11:43,  9.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 983/2180 [2:37:17<3:11:12,  9.58s/it]                                                      {'loss': 2.1594, 'learning_rate': 0.0006032773921389654, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 983/2180 [2:37:17<3:11:12,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 984/2180 [2:37:26<3:11:07,  9.59s/it]                                                      {'loss': 2.1497, 'learning_rate': 0.0006025502574744162, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 984/2180 [2:37:26<3:11:07,  9.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 985/2180 [2:37:36<3:10:48,  9.58s/it]                                                      {'loss': 2.0344, 'learning_rate': 0.000601822896331641, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 985/2180 [2:37:36<3:10:48,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 986/2180 [2:37:46<3:10:24,  9.57s/it]                                                      {'loss': 2.0215, 'learning_rate': 0.0006010953103169883, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 986/2180 [2:37:46<3:10:24,  9.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 987/2180 [2:37:55<3:10:30,  9.58s/it]                                                      {'loss': 2.1142, 'learning_rate': 0.0006003675010373034, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 987/2180 [2:37:55<3:10:30,  9.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 988/2180 [2:38:05<3:10:12,  9.57s/it]                                                      {'loss': 2.0729, 'learning_rate': 0.0005996394700999246, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 988/2180 [2:38:05<3:10:12,  9.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 989/2180 [2:38:14<3:09:54,  9.57s/it]                                                      {'loss': 2.1855, 'learning_rate': 0.0005989112191126794, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 989/2180 [2:38:14<3:09:54,  9.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 990/2180 [2:38:24<3:09:53,  9.57s/it]                                                      {'loss': 2.1741, 'learning_rate': 0.0005981827496838822, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 990/2180 [2:38:24<3:09:53,  9.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 991/2180 [2:38:33<3:09:50,  9.58s/it]                                                      {'loss': 2.0909, 'learning_rate': 0.0005974540634223286, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 991/2180 [2:38:33<3:09:50,  9.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 992/2180 [2:38:43<3:09:26,  9.57s/it]                                                      {'loss': 2.1095, 'learning_rate': 0.0005967251619372939, 'epoch': 0.45}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 992/2180 [2:38:43<3:09:26,  9.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 993/2180 [2:38:53<3:09:12,  9.56s/it]                                                      {'loss': 2.0691, 'learning_rate': 0.0005959960468385284, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 993/2180 [2:38:53<3:09:12,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 994/2180 [2:39:02<3:08:58,  9.56s/it]                                                      {'loss': 2.0558, 'learning_rate': 0.0005952667197362542, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 994/2180 [2:39:02<3:08:58,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 995/2180 [2:39:12<3:08:44,  9.56s/it]                                                      {'loss': 2.1018, 'learning_rate': 0.0005945371822411621, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 995/2180 [2:39:12<3:08:44,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 996/2180 [2:39:21<3:09:01,  9.58s/it]                                                      {'loss': 2.0728, 'learning_rate': 0.0005938074359644063, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 996/2180 [2:39:21<3:09:01,  9.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 997/2180 [2:39:31<3:08:58,  9.58s/it]                                                      {'loss': 2.1059, 'learning_rate': 0.0005930774825176034, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 997/2180 [2:39:31<3:08:58,  9.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 998/2180 [2:39:40<3:08:36,  9.57s/it]                                                      {'loss': 1.971, 'learning_rate': 0.0005923473235128268, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 998/2180 [2:39:40<3:08:36,  9.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 999/2180 [2:39:50<3:08:25,  9.57s/it]                                                      {'loss': 2.0819, 'learning_rate': 0.0005916169605626042, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 999/2180 [2:39:50<3:08:25,  9.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1000/2180 [2:40:00<3:07:58,  9.56s/it]                                                       {'loss': 2.0766, 'learning_rate': 0.0005908863952799134, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1000/2180 [2:40:00<3:07:58,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2180 [2:40:09<3:07:37,  9.55s/it]                                                       {'loss': 2.1388, 'learning_rate': 0.0005901556292781793, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2180 [2:40:09<3:07:37,  9.55s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2180 [2:40:19<3:07:39,  9.56s/it]                                                       {'loss': 2.0899, 'learning_rate': 0.0005894246641712698, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2180 [2:40:19<3:07:39,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2180 [2:40:28<3:07:20,  9.55s/it]                                                       {'loss': 2.1541, 'learning_rate': 0.0005886935015734931, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2180 [2:40:28<3:07:20,  9.55s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2180 [2:40:38<3:07:16,  9.55s/it]                                                       {'loss': 2.0621, 'learning_rate': 0.0005879621430995928, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2180 [2:40:38<3:07:16,  9.55s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2180 [2:40:47<3:07:17,  9.56s/it]                                                       {'loss': 2.0828, 'learning_rate': 0.0005872305903647455, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2180 [2:40:47<3:07:17,  9.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2180 [2:40:57<3:07:25,  9.58s/it]                                                       {'loss': 2.1353, 'learning_rate': 0.0005864988449845569, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2180 [2:40:57<3:07:25,  9.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2180 [2:41:06<3:07:01,  9.57s/it]                                                       {'loss': 2.0426, 'learning_rate': 0.0005857669085750578, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2180 [2:41:06<3:07:01,  9.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2180 [2:41:16<3:06:54,  9.57s/it]                                                       {'loss': 2.0482, 'learning_rate': 0.0005850347827527013, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2180 [2:41:16<3:06:54,  9.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1009/2180 [2:41:26<3:07:07,  9.59s/it]                                                       {'loss': 2.0997, 'learning_rate': 0.0005843024691343584, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1009/2180 [2:41:26<3:07:07,  9.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1010/2180 [2:41:35<3:06:50,  9.58s/it]                                                       {'loss': 2.1835, 'learning_rate': 0.000583569969337315, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1010/2180 [2:41:35<3:06:50,  9.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1011/2180 [2:41:45<3:06:55,  9.59s/it]                                                       {'loss': 2.0932, 'learning_rate': 0.0005828372849792686, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1011/2180 [2:41:45<3:06:55,  9.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1012/2180 [2:41:54<3:06:52,  9.60s/it]                                                       {'loss': 2.1616, 'learning_rate': 0.0005821044176783234, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1012/2180 [2:41:54<3:06:52,  9.60s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1013/2180 [2:42:04<3:06:32,  9.59s/it]                                                       {'loss': 2.1272, 'learning_rate': 0.0005813713690529886, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1013/2180 [2:42:04<3:06:32,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1014/2180 [2:42:14<3:06:23,  9.59s/it]                                                       {'loss': 2.0925, 'learning_rate': 0.0005806381407221729, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1014/2180 [2:42:14<3:06:23,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1015/2180 [2:42:23<3:06:00,  9.58s/it]                                                       {'loss': 2.0731, 'learning_rate': 0.0005799047343051826, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1015/2180 [2:42:23<3:06:00,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1016/2180 [2:42:33<3:05:37,  9.57s/it]                                                       {'loss': 2.0863, 'learning_rate': 0.0005791711514217171, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1016/2180 [2:42:33<3:05:37,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1017/2180 [2:42:42<3:05:33,  9.57s/it]                                                       {'loss': 2.06, 'learning_rate': 0.0005784373936918654, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1017/2180 [2:42:42<3:05:33,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1018/2180 [2:42:52<3:05:38,  9.59s/it]                                                       {'loss': 2.0877, 'learning_rate': 0.0005777034627361025, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1018/2180 [2:42:52<3:05:38,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1019/2180 [2:43:02<3:05:21,  9.58s/it]                                                       {'loss': 2.1192, 'learning_rate': 0.0005769693601752864, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1019/2180 [2:43:02<3:05:21,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1020/2180 [2:43:11<3:05:00,  9.57s/it]                                                       {'loss': 2.0704, 'learning_rate': 0.0005762350876306537, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1020/2180 [2:43:11<3:05:00,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1021/2180 [2:43:21<3:04:53,  9.57s/it]                                                       {'loss': 2.134, 'learning_rate': 0.0005755006467238168, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1021/2180 [2:43:21<3:04:53,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1022/2180 [2:43:30<3:05:08,  9.59s/it]                                                       {'loss': 2.101, 'learning_rate': 0.0005747660390767593, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1022/2180 [2:43:30<3:05:08,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1023/2180 [2:43:40<3:04:54,  9.59s/it]                                                       {'loss': 2.1167, 'learning_rate': 0.0005740312663118338, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1023/2180 [2:43:40<3:04:54,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1024/2180 [2:43:49<3:04:32,  9.58s/it]                                                       {'loss': 2.0348, 'learning_rate': 0.0005732963300517568, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1024/2180 [2:43:49<3:04:32,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1025/2180 [2:43:59<3:04:09,  9.57s/it]                                                       {'loss': 2.1, 'learning_rate': 0.0005725612319196064, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1025/2180 [2:43:59<3:04:09,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1026/2180 [2:44:09<3:04:18,  9.58s/it]                                                       {'loss': 2.1583, 'learning_rate': 0.000571825973538818, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1026/2180 [2:44:09<3:04:18,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1027/2180 [2:44:18<3:04:01,  9.58s/it]                                                       {'loss': 2.09, 'learning_rate': 0.0005710905565331811, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1027/2180 [2:44:18<3:04:01,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1028/2180 [2:44:28<3:03:54,  9.58s/it]                                                       {'loss': 2.1137, 'learning_rate': 0.0005703549825268353, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1028/2180 [2:44:28<3:03:54,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2180 [2:44:37<3:03:49,  9.58s/it]                                                       {'loss': 2.0259, 'learning_rate': 0.0005696192531442667, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2180 [2:44:37<3:03:49,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2180 [2:44:47<3:03:31,  9.58s/it]                                                       {'loss': 2.1688, 'learning_rate': 0.000568883370010305, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2180 [2:44:47<3:03:31,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2180 [2:44:56<3:03:14,  9.57s/it]                                                       {'loss': 2.0397, 'learning_rate': 0.0005681473347501192, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2180 [2:44:56<3:03:14,  9.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2180 [2:45:06<3:03:17,  9.58s/it]                                                       {'loss': 2.0177, 'learning_rate': 0.0005674111489892144, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2180 [2:45:06<3:03:17,  9.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2180 [2:45:16<3:03:24,  9.59s/it]                                                       {'loss': 2.026, 'learning_rate': 0.0005666748143534282, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2180 [2:45:16<3:03:24,  9.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2180 [2:45:25<3:03:45,  9.62s/it]                                                       {'loss': 2.1237, 'learning_rate': 0.0005659383324689266, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2180 [2:45:25<3:03:45,  9.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2180 [2:45:35<3:03:23,  9.61s/it]                                                       {'loss': 2.1054, 'learning_rate': 0.0005652017049622007, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2180 [2:45:35<3:03:23,  9.61s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1036/2180 [2:45:45<3:03:07,  9.60s/it]                                                       {'loss': 2.0674, 'learning_rate': 0.0005644649334600641, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1036/2180 [2:45:45<3:03:07,  9.60s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1037/2180 [2:45:54<3:02:36,  9.59s/it]                                                       {'loss': 2.1094, 'learning_rate': 0.0005637280195896474, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1037/2180 [2:45:54<3:02:36,  9.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1038/2180 [2:46:04<3:02:25,  9.58s/it]                                                       {'loss': 2.0718, 'learning_rate': 0.0005629909649783961, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1038/2180 [2:46:04<3:02:25,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1039/2180 [2:46:13<3:02:15,  9.58s/it]                                                       {'loss': 2.1194, 'learning_rate': 0.0005622537712540664, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1039/2180 [2:46:13<3:02:15,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1040/2180 [2:46:23<3:02:03,  9.58s/it]                                                       {'loss': 2.0194, 'learning_rate': 0.0005615164400447218, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1040/2180 [2:46:23<3:02:03,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1041/2180 [2:46:32<3:01:54,  9.58s/it]                                                       {'loss': 2.1249, 'learning_rate': 0.0005607789729787294, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1041/2180 [2:46:32<3:01:54,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1042/2180 [2:46:42<3:01:36,  9.58s/it]                                                       {'loss': 2.1075, 'learning_rate': 0.0005600413716847564, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1042/2180 [2:46:42<3:01:36,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1043/2180 [2:46:51<3:01:16,  9.57s/it]                                                       {'loss': 2.1573, 'learning_rate': 0.000559303637791766, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1043/2180 [2:46:51<3:01:16,  9.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1044/2180 [2:47:01<3:01:01,  9.56s/it]                                                       {'loss': 2.0502, 'learning_rate': 0.0005585657729290151, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1044/2180 [2:47:01<3:01:01,  9.56s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1045/2180 [2:47:11<3:00:58,  9.57s/it]                                                       {'loss': 2.103, 'learning_rate': 0.000557827778726049, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1045/2180 [2:47:11<3:00:58,  9.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1046/2180 [2:47:20<3:01:25,  9.60s/it]                                                       {'loss': 2.0703, 'learning_rate': 0.0005570896568126993, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1046/2180 [2:47:20<3:01:25,  9.60s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1047/2180 [2:47:30<3:00:59,  9.59s/it]                                                       {'loss': 2.0511, 'learning_rate': 0.0005563514088190788, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1047/2180 [2:47:30<3:00:59,  9.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1048/2180 [2:47:39<3:00:53,  9.59s/it]                                                       {'loss': 2.1074, 'learning_rate': 0.0005556130363755798, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1048/2180 [2:47:39<3:00:53,  9.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1049/2180 [2:47:49<3:00:37,  9.58s/it]                                                       {'loss': 2.0225, 'learning_rate': 0.0005548745411128688, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1049/2180 [2:47:49<3:00:37,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1050/2180 [2:47:59<3:00:36,  9.59s/it]                                                       {'loss': 2.1373, 'learning_rate': 0.0005541359246618835, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1050/2180 [2:47:59<3:00:36,  9.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1051/2180 [2:48:08<3:00:20,  9.58s/it]                                                       {'loss': 2.1196, 'learning_rate': 0.0005533971886538293, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1051/2180 [2:48:08<3:00:20,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1052/2180 [2:48:18<3:00:11,  9.58s/it]                                                       {'loss': 2.0942, 'learning_rate': 0.000552658334720176, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1052/2180 [2:48:18<3:00:11,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1053/2180 [2:48:27<2:59:53,  9.58s/it]                                                       {'loss': 2.0817, 'learning_rate': 0.0005519193644926535, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1053/2180 [2:48:27<2:59:53,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1054/2180 [2:48:37<2:59:29,  9.56s/it]                                                       {'loss': 2.1124, 'learning_rate': 0.0005511802796032485, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1054/2180 [2:48:37<2:59:29,  9.56s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1055/2180 [2:48:46<2:59:22,  9.57s/it]                                                       {'loss': 2.095, 'learning_rate': 0.0005504410816842009, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1055/2180 [2:48:46<2:59:22,  9.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1056/2180 [2:48:56<2:59:26,  9.58s/it]                                                       {'loss': 2.079, 'learning_rate': 0.0005497017723680009, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1056/2180 [2:48:56<2:59:26,  9.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2180 [2:49:06<2:59:23,  9.58s/it]                                                       {'loss': 2.0853, 'learning_rate': 0.0005489623532873836, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2180 [2:49:06<2:59:23,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2180 [2:49:15<2:59:23,  9.59s/it]                                                       {'loss': 2.0949, 'learning_rate': 0.0005482228260753273, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2180 [2:49:15<2:59:23,  9.59s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2180 [2:49:25<2:59:53,  9.63s/it]                                                       {'loss': 2.1364, 'learning_rate': 0.0005474831923650488, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2180 [2:49:25<2:59:53,  9.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2180 [2:49:35<2:59:27,  9.61s/it]                                                       {'loss': 2.0562, 'learning_rate': 0.00054674345379, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2180 [2:49:35<2:59:27,  9.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2180 [2:49:44<2:58:59,  9.60s/it]                                                       {'loss': 2.0981, 'learning_rate': 0.000546003611983865, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2180 [2:49:44<2:58:59,  9.60s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2180 [2:49:54<2:58:36,  9.59s/it]                                                       {'loss': 2.12, 'learning_rate': 0.0005452636685805552, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2180 [2:49:54<2:58:36,  9.59s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1063/2180 [2:50:03<2:58:17,  9.58s/it]                                                       {'loss': 2.0012, 'learning_rate': 0.0005445236252142066, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1063/2180 [2:50:03<2:58:17,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1064/2180 [2:50:13<2:58:15,  9.58s/it]                                                       {'loss': 2.0649, 'learning_rate': 0.000543783483519176, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1064/2180 [2:50:13<2:58:15,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1065/2180 [2:50:22<2:57:58,  9.58s/it]                                                       {'loss': 2.1365, 'learning_rate': 0.0005430432451300374, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1065/2180 [2:50:22<2:57:58,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1066/2180 [2:50:32<2:57:41,  9.57s/it]                                                       {'loss': 2.0682, 'learning_rate': 0.0005423029116815781, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1066/2180 [2:50:32<2:57:41,  9.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1067/2180 [2:50:41<2:57:19,  9.56s/it]                                                       {'loss': 2.0551, 'learning_rate': 0.0005415624848087959, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1067/2180 [2:50:41<2:57:19,  9.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1068/2180 [2:50:51<2:57:16,  9.57s/it]                                                       {'loss': 2.1433, 'learning_rate': 0.000540821966146894, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1068/2180 [2:50:51<2:57:16,  9.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1069/2180 [2:51:01<2:57:00,  9.56s/it]                                                       {'loss': 2.0835, 'learning_rate': 0.0005400813573312793, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1069/2180 [2:51:01<2:57:00,  9.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1070/2180 [2:51:10<2:56:53,  9.56s/it]                                                       {'loss': 2.0453, 'learning_rate': 0.0005393406599975572, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1070/2180 [2:51:10<2:56:53,  9.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1071/2180 [2:51:20<2:57:09,  9.59s/it]                                                       {'loss': 2.1174, 'learning_rate': 0.0005385998757815287, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1071/2180 [2:51:20<2:57:09,  9.59s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1072/2180 [2:51:29<2:56:54,  9.58s/it]                                                       {'loss': 2.054, 'learning_rate': 0.0005378590063191867, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1072/2180 [2:51:29<2:56:54,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1073/2180 [2:51:39<2:56:43,  9.58s/it]                                                       {'loss': 2.0322, 'learning_rate': 0.0005371180532467124, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1073/2180 [2:51:39<2:56:43,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1074/2180 [2:51:49<2:56:40,  9.58s/it]                                                       {'loss': 2.0645, 'learning_rate': 0.000536377018200472, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1074/2180 [2:51:49<2:56:40,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1075/2180 [2:51:58<2:56:20,  9.58s/it]                                                       {'loss': 2.0946, 'learning_rate': 0.0005356359028170118, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1075/2180 [2:51:58<2:56:20,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1076/2180 [2:52:08<2:56:03,  9.57s/it]                                                       {'loss': 2.1136, 'learning_rate': 0.0005348947087330564, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1076/2180 [2:52:08<2:56:03,  9.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1077/2180 [2:52:17<2:56:06,  9.58s/it]                                                       {'loss': 2.15, 'learning_rate': 0.0005341534375855037, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1077/2180 [2:52:17<2:56:06,  9.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1078/2180 [2:52:27<2:55:41,  9.57s/it]                                                       {'loss': 2.0111, 'learning_rate': 0.0005334120910114222, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1078/2180 [2:52:27<2:55:41,  9.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1079/2180 [2:52:36<2:55:39,  9.57s/it]                                                       {'loss': 2.0939, 'learning_rate': 0.0005326706706480467, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1079/2180 [2:52:36<2:55:39,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1080/2180 [2:52:46<2:55:29,  9.57s/it]                                                       {'loss': 2.0745, 'learning_rate': 0.0005319291781327749, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1080/2180 [2:52:46<2:55:29,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1081/2180 [2:52:56<2:55:17,  9.57s/it]                                                       {'loss': 2.0886, 'learning_rate': 0.0005311876151031642, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1081/2180 [2:52:56<2:55:17,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1082/2180 [2:53:05<2:54:58,  9.56s/it]                                                       {'loss': 2.0078, 'learning_rate': 0.0005304459831969274, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1082/2180 [2:53:05<2:54:58,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1083/2180 [2:53:15<2:55:00,  9.57s/it]                                                       {'loss': 2.0905, 'learning_rate': 0.0005297042840519294, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1083/2180 [2:53:15<2:55:00,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1084/2180 [2:53:24<2:54:48,  9.57s/it]                                                       {'loss': 2.0962, 'learning_rate': 0.0005289625193061838, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1084/2180 [2:53:24<2:54:48,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2180 [2:53:34<2:54:24,  9.56s/it]                                                       {'loss': 2.0689, 'learning_rate': 0.0005282206905978489, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2180 [2:53:34<2:54:24,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2180 [2:53:43<2:54:06,  9.55s/it]                                                       {'loss': 2.1344, 'learning_rate': 0.0005274787995652246, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2180 [2:53:43<2:54:06,  9.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2180 [2:53:53<2:54:01,  9.55s/it]                                                       {'loss': 2.0559, 'learning_rate': 0.000526736847846748, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2180 [2:53:53<2:54:01,  9.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2180 [2:54:02<2:53:46,  9.55s/it]                                                       {'loss': 2.044, 'learning_rate': 0.0005259948370809901, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2180 [2:54:02<2:53:46,  9.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2180 [2:54:12<2:53:35,  9.55s/it]                                                       {'loss': 2.0586, 'learning_rate': 0.0005252527689066533, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2180 [2:54:12<2:53:35,  9.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1090/2180 [2:54:21<2:53:36,  9.56s/it]                                                       {'loss': 2.1208, 'learning_rate': 0.0005245106449625654, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1090/2180 [2:54:21<2:53:36,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1091/2180 [2:54:31<2:53:32,  9.56s/it]                                                       {'loss': 2.0763, 'learning_rate': 0.0005237684668876785, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1091/2180 [2:54:31<2:53:32,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1092/2180 [2:54:41<2:53:26,  9.56s/it]                                                       {'loss': 2.0095, 'learning_rate': 0.0005230262363210637, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1092/2180 [2:54:41<2:53:26,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1093/2180 [2:54:50<2:53:26,  9.57s/it]                                                       {'loss': 2.0237, 'learning_rate': 0.0005222839549019079, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1093/2180 [2:54:50<2:53:26,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1094/2180 [2:55:00<2:53:43,  9.60s/it]                                                       {'loss': 2.0388, 'learning_rate': 0.0005215416242695108, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1094/2180 [2:55:00<2:53:43,  9.60s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1095/2180 [2:55:09<2:53:20,  9.59s/it]                                                       {'loss': 2.0265, 'learning_rate': 0.0005207992460632804, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1095/2180 [2:55:09<2:53:20,  9.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1096/2180 [2:55:19<2:52:58,  9.57s/it]                                                       {'loss': 2.1119, 'learning_rate': 0.0005200568219227299, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1096/2180 [2:55:19<2:52:58,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1097/2180 [2:55:29<2:52:46,  9.57s/it]                                                       {'loss': 2.177, 'learning_rate': 0.000519314353487474, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1097/2180 [2:55:29<2:52:46,  9.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1098/2180 [2:55:38<2:52:24,  9.56s/it]                                                       {'loss': 2.0723, 'learning_rate': 0.0005185718423972251, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1098/2180 [2:55:38<2:52:24,  9.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1099/2180 [2:55:48<2:52:45,  9.59s/it]                                                       {'loss': 2.0671, 'learning_rate': 0.0005178292902917898, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1099/2180 [2:55:48<2:52:45,  9.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1100/2180 [2:55:57<2:52:44,  9.60s/it]                                                       {'loss': 2.0154, 'learning_rate': 0.0005170866988110656, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1100/2180 [2:55:57<2:52:44,  9.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1101/2180 [2:56:07<2:53:05,  9.62s/it]                                                       {'loss': 2.096, 'learning_rate': 0.0005163440695950362, 'epoch': 0.5}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1101/2180 [2:56:07<2:53:05,  9.62s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1102/2180 [2:56:17<2:52:30,  9.60s/it]                                                       {'loss': 2.0376, 'learning_rate': 0.0005156014042837695, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1102/2180 [2:56:17<2:52:30,  9.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1103/2180 [2:56:26<2:51:57,  9.58s/it]                                                       {'loss': 2.0722, 'learning_rate': 0.0005148587045174128, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1103/2180 [2:56:26<2:51:57,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1104/2180 [2:56:36<2:51:30,  9.56s/it]                                                       {'loss': 2.0874, 'learning_rate': 0.0005141159719361891, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1104/2180 [2:56:36<2:51:30,  9.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1105/2180 [2:56:45<2:51:19,  9.56s/it]                                                       {'loss': 2.0107, 'learning_rate': 0.0005133732081803945, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1105/2180 [2:56:45<2:51:19,  9.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1106/2180 [2:56:55<2:51:14,  9.57s/it]                                                       {'loss': 2.0637, 'learning_rate': 0.0005126304148903936, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1106/2180 [2:56:55<2:51:14,  9.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1107/2180 [2:57:04<2:51:09,  9.57s/it]                                                       {'loss': 2.0834, 'learning_rate': 0.0005118875937066161, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1107/2180 [2:57:04<2:51:09,  9.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1108/2180 [2:57:14<2:51:06,  9.58s/it]                                                       {'loss': 2.0345, 'learning_rate': 0.0005111447462695537, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1108/2180 [2:57:14<2:51:06,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1109/2180 [2:57:24<2:50:56,  9.58s/it]                                                       {'loss': 2.0492, 'learning_rate': 0.0005104018742197557, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1109/2180 [2:57:24<2:50:56,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1110/2180 [2:57:33<2:50:54,  9.58s/it]                                                       {'loss': 2.0126, 'learning_rate': 0.0005096589791978261, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1110/2180 [2:57:33<2:50:54,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1111/2180 [2:57:43<2:50:30,  9.57s/it]                                                       {'loss': 2.0744, 'learning_rate': 0.0005089160628444192, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1111/2180 [2:57:43<2:50:30,  9.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2180 [2:57:52<2:50:36,  9.58s/it]                                                       {'loss': 2.0607, 'learning_rate': 0.0005081731268002371, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2180 [2:57:52<2:50:36,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2180 [2:58:02<2:50:44,  9.60s/it]                                                       {'loss': 2.1094, 'learning_rate': 0.0005074301727060243, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2180 [2:58:02<2:50:44,  9.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2180 [2:58:11<2:50:20,  9.59s/it]                                                       {'loss': 2.1121, 'learning_rate': 0.0005066872022025663, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2180 [2:58:11<2:50:20,  9.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2180 [2:58:21<2:50:03,  9.58s/it]                                                       {'loss': 2.0591, 'learning_rate': 0.0005059442169306844, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2180 [2:58:21<2:50:03,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2180 [2:58:31<2:49:55,  9.58s/it]                                                       {'loss': 2.0025, 'learning_rate': 0.0005052012185312321, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2180 [2:58:31<2:49:55,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2180 [2:58:40<2:49:59,  9.59s/it]                                                       {'loss': 2.0795, 'learning_rate': 0.0005044582086450925, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2180 [2:58:40<2:49:59,  9.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1118/2180 [2:58:50<2:49:44,  9.59s/it]                                                       {'loss': 2.0336, 'learning_rate': 0.0005037151889131737, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1118/2180 [2:58:50<2:49:44,  9.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1119/2180 [2:58:59<2:49:38,  9.59s/it]                                                       {'loss': 2.0816, 'learning_rate': 0.0005029721609764059, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1119/2180 [2:58:59<2:49:38,  9.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1120/2180 [2:59:09<2:49:33,  9.60s/it]                                                       {'loss': 2.1205, 'learning_rate': 0.000502229126475737, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1120/2180 [2:59:09<2:49:33,  9.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1121/2180 [2:59:19<2:49:00,  9.58s/it]                                                       {'loss': 2.0653, 'learning_rate': 0.0005014860870521293, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1121/2180 [2:59:19<2:49:00,  9.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1122/2180 [2:59:28<2:48:46,  9.57s/it]                                                       {'loss': 2.0444, 'learning_rate': 0.0005007430443465569, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1122/2180 [2:59:28<2:48:46,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1123/2180 [2:59:38<2:48:33,  9.57s/it]                                                       {'loss': 2.0818, 'learning_rate': 0.0005, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1123/2180 [2:59:38<2:48:33,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1124/2180 [2:59:47<2:48:22,  9.57s/it]                                                       {'loss': 2.0843, 'learning_rate': 0.0004992569556534432, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1124/2180 [2:59:47<2:48:22,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1125/2180 [2:59:57<2:48:26,  9.58s/it]                                                       {'loss': 2.0543, 'learning_rate': 0.0004985139129478707, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1125/2180 [2:59:57<2:48:26,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1126/2180 [3:00:06<2:48:18,  9.58s/it]                                                       {'loss': 2.1152, 'learning_rate': 0.0004977708735242633, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1126/2180 [3:00:06<2:48:18,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1127/2180 [3:00:16<2:48:08,  9.58s/it]                                                       {'loss': 2.0604, 'learning_rate': 0.0004970278390235942, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1127/2180 [3:00:16<2:48:08,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1128/2180 [3:00:26<2:47:54,  9.58s/it]                                                       {'loss': 2.0095, 'learning_rate': 0.0004962848110868262, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1128/2180 [3:00:26<2:47:54,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1129/2180 [3:00:35<2:47:46,  9.58s/it]                                                       {'loss': 2.0881, 'learning_rate': 0.0004955417913549074, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1129/2180 [3:00:35<2:47:46,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1130/2180 [3:00:45<2:47:32,  9.57s/it]                                                       {'loss': 2.0696, 'learning_rate': 0.0004947987814687679, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1130/2180 [3:00:45<2:47:32,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1131/2180 [3:00:54<2:47:24,  9.58s/it]                                                       {'loss': 2.0953, 'learning_rate': 0.0004940557830693157, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1131/2180 [3:00:54<2:47:24,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1132/2180 [3:01:04<2:47:29,  9.59s/it]                                                       {'loss': 2.1285, 'learning_rate': 0.0004933127977974338, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1132/2180 [3:01:04<2:47:29,  9.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1133/2180 [3:01:14<2:47:35,  9.60s/it]                                                       {'loss': 2.0457, 'learning_rate': 0.0004925698272939757, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1133/2180 [3:01:14<2:47:35,  9.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1134/2180 [3:01:23<2:47:16,  9.60s/it]                                                       {'loss': 2.0354, 'learning_rate': 0.0004918268731997632, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1134/2180 [3:01:23<2:47:16,  9.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1135/2180 [3:01:33<2:47:02,  9.59s/it]                                                       {'loss': 2.1233, 'learning_rate': 0.0004910839371555809, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1135/2180 [3:01:33<2:47:02,  9.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1136/2180 [3:01:42<2:46:47,  9.59s/it]                                                       {'loss': 2.1092, 'learning_rate': 0.0004903410208021739, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1136/2180 [3:01:42<2:46:47,  9.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1137/2180 [3:01:52<2:46:23,  9.57s/it]                                                       {'loss': 2.0842, 'learning_rate': 0.0004895981257802443, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1137/2180 [3:01:52<2:46:23,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1138/2180 [3:02:01<2:46:10,  9.57s/it]                                                       {'loss': 2.1172, 'learning_rate': 0.0004888552537304463, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1138/2180 [3:02:01<2:46:10,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1139/2180 [3:02:11<2:45:53,  9.56s/it]                                                       {'loss': 2.0715, 'learning_rate': 0.00048811240629338394, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1139/2180 [3:02:11<2:45:53,  9.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2180 [3:02:21<2:45:57,  9.57s/it]                                                       {'loss': 2.1155, 'learning_rate': 0.00048736958510960663, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2180 [3:02:21<2:45:57,  9.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2180 [3:02:30<2:45:54,  9.58s/it]                                                       {'loss': 1.9951, 'learning_rate': 0.00048662679181960564, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2180 [3:02:30<2:45:54,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2180 [3:02:40<2:45:42,  9.58s/it]                                                       {'loss': 2.0752, 'learning_rate': 0.00048588402806381094, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2180 [3:02:40<2:45:42,  9.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2180 [3:02:49<2:45:41,  9.59s/it]                                                       {'loss': 2.0883, 'learning_rate': 0.0004851412954825874, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2180 [3:02:49<2:45:41,  9.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2180 [3:02:59<2:45:26,  9.58s/it]                                                       {'loss': 2.0565, 'learning_rate': 0.00048439859571623034, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2180 [3:02:59<2:45:26,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1145/2180 [3:03:09<2:45:22,  9.59s/it]                                                       {'loss': 2.0673, 'learning_rate': 0.00048365593040496373, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1145/2180 [3:03:09<2:45:22,  9.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1146/2180 [3:03:18<2:44:56,  9.57s/it]                                                       {'loss': 2.0961, 'learning_rate': 0.00048291330118893443, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1146/2180 [3:03:18<2:44:56,  9.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1147/2180 [3:03:28<2:44:46,  9.57s/it]                                                       {'loss': 2.0724, 'learning_rate': 0.0004821707097082102, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1147/2180 [3:03:28<2:44:46,  9.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1148/2180 [3:03:37<2:44:29,  9.56s/it]                                                       {'loss': 2.1103, 'learning_rate': 0.0004814281576027749, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1148/2180 [3:03:37<2:44:29,  9.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1149/2180 [3:03:47<2:44:37,  9.58s/it]                                                       {'loss': 2.0564, 'learning_rate': 0.000480685646512526, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1149/2180 [3:03:47<2:44:37,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1150/2180 [3:03:56<2:44:50,  9.60s/it]                                                       {'loss': 2.0507, 'learning_rate': 0.00047994317807727025, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1150/2180 [3:03:56<2:44:50,  9.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1151/2180 [3:04:06<2:44:32,  9.59s/it]                                                       {'loss': 2.069, 'learning_rate': 0.00047920075393671974, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1151/2180 [3:04:06<2:44:32,  9.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1152/2180 [3:04:16<2:44:16,  9.59s/it]                                                       {'loss': 2.1358, 'learning_rate': 0.0004784583757304893, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1152/2180 [3:04:16<2:44:16,  9.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1153/2180 [3:04:25<2:44:05,  9.59s/it]                                                       {'loss': 2.1409, 'learning_rate': 0.00047771604509809214, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1153/2180 [3:04:25<2:44:05,  9.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1154/2180 [3:04:35<2:44:07,  9.60s/it]                                                       {'loss': 2.1131, 'learning_rate': 0.0004769737636789364, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1154/2180 [3:04:35<2:44:07,  9.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1155/2180 [3:04:44<2:43:42,  9.58s/it]                                                       {'loss': 2.1204, 'learning_rate': 0.00047623153311232157, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1155/2180 [3:04:44<2:43:42,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1156/2180 [3:04:54<2:43:46,  9.60s/it]                                                       {'loss': 2.068, 'learning_rate': 0.0004754893550374346, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1156/2180 [3:04:54<2:43:46,  9.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1157/2180 [3:05:04<2:43:29,  9.59s/it]                                                       {'loss': 2.0278, 'learning_rate': 0.00047474723109334685, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1157/2180 [3:05:04<2:43:29,  9.59s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1158/2180 [3:05:13<2:43:12,  9.58s/it]                                                       {'loss': 2.0262, 'learning_rate': 0.00047400516291900993, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1158/2180 [3:05:13<2:43:12,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1159/2180 [3:05:23<2:43:00,  9.58s/it]                                                       {'loss': 2.0619, 'learning_rate': 0.0004732631521532522, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1159/2180 [3:05:23<2:43:00,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1160/2180 [3:05:32<2:42:40,  9.57s/it]                                                       {'loss': 2.039, 'learning_rate': 0.0004725212004347755, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1160/2180 [3:05:32<2:42:40,  9.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1161/2180 [3:05:42<2:42:44,  9.58s/it]                                                       {'loss': 2.0254, 'learning_rate': 0.00047177930940215095, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1161/2180 [3:05:42<2:42:44,  9.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1162/2180 [3:05:52<2:43:24,  9.63s/it]                                                       {'loss': 1.9801, 'learning_rate': 0.00047103748069381624, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1162/2180 [3:05:52<2:43:24,  9.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1163/2180 [3:06:01<2:43:06,  9.62s/it]                                                       {'loss': 2.0441, 'learning_rate': 0.0004702957159480707, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1163/2180 [3:06:01<2:43:06,  9.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1164/2180 [3:06:11<2:42:37,  9.60s/it]                                                       {'loss': 2.0694, 'learning_rate': 0.00046955401680307267, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1164/2180 [3:06:11<2:42:37,  9.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1165/2180 [3:06:20<2:42:23,  9.60s/it]                                                       {'loss': 2.0627, 'learning_rate': 0.0004688123848968359, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1165/2180 [3:06:20<2:42:23,  9.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1166/2180 [3:06:30<2:42:13,  9.60s/it]                                                       {'loss': 2.0507, 'learning_rate': 0.00046807082186722516, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1166/2180 [3:06:30<2:42:13,  9.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1167/2180 [3:06:39<2:41:49,  9.58s/it]                                                       {'loss': 2.0112, 'learning_rate': 0.0004673293293519535, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1167/2180 [3:06:39<2:41:49,  9.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2180 [3:06:49<2:41:41,  9.59s/it]                                                       {'loss': 2.0999, 'learning_rate': 0.00046658790898857806, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2180 [3:06:49<2:41:41,  9.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2180 [3:06:59<2:41:24,  9.58s/it]                                                       {'loss': 2.0353, 'learning_rate': 0.0004658465624144963, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2180 [3:06:59<2:41:24,  9.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2180 [3:07:08<2:41:30,  9.59s/it]                                                       {'loss': 2.0935, 'learning_rate': 0.0004651052912669438, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2180 [3:07:08<2:41:30,  9.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2180 [3:07:18<2:41:33,  9.61s/it]                                                       {'loss': 2.1841, 'learning_rate': 0.0004643640971829883, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2180 [3:07:18<2:41:33,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2180 [3:07:28<2:41:36,  9.62s/it]                                                       {'loss': 1.9738, 'learning_rate': 0.0004636229817995281, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2180 [3:07:28<2:41:36,  9.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2180 [3:07:37<2:41:16,  9.61s/it]                                                       {'loss': 1.9799, 'learning_rate': 0.0004628819467532876, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2180 [3:07:37<2:41:16,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1174/2180 [3:07:47<2:41:13,  9.62s/it]                                                       {'loss': 2.0758, 'learning_rate': 0.00046214099368081335, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1174/2180 [3:07:47<2:41:13,  9.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1175/2180 [3:07:56<2:40:52,  9.60s/it]                                                       {'loss': 2.0482, 'learning_rate': 0.0004614001242184714, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1175/2180 [3:07:56<2:40:52,  9.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1176/2180 [3:08:06<2:40:32,  9.59s/it]                                                       {'loss': 2.046, 'learning_rate': 0.000460659340002443, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1176/2180 [3:08:06<2:40:32,  9.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1177/2180 [3:08:16<2:40:23,  9.60s/it]                                                       {'loss': 2.053, 'learning_rate': 0.00045991864266872073, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1177/2180 [3:08:16<2:40:23,  9.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1178/2180 [3:08:25<2:40:20,  9.60s/it]                                                       {'loss': 2.096, 'learning_rate': 0.00045917803385310595, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1178/2180 [3:08:25<2:40:20,  9.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1179/2180 [3:08:35<2:40:47,  9.64s/it]                                                       {'loss': 2.0606, 'learning_rate': 0.00045843751519120417, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1179/2180 [3:08:35<2:40:47,  9.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2180 [3:08:44<2:40:10,  9.61s/it]                                                       {'loss': 2.1143, 'learning_rate': 0.00045769708831842193, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2180 [3:08:44<2:40:10,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1181/2180 [3:08:54<2:40:22,  9.63s/it]                                                       {'loss': 2.0465, 'learning_rate': 0.00045695675486996266, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1181/2180 [3:08:54<2:40:22,  9.63s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1182/2180 [3:09:04<2:40:03,  9.62s/it]                                                       {'loss': 2.0659, 'learning_rate': 0.00045621651648082405, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1182/2180 [3:09:04<2:40:03,  9.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1183/2180 [3:09:13<2:39:39,  9.61s/it]                                                       {'loss': 2.0244, 'learning_rate': 0.00045547637478579356, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1183/2180 [3:09:13<2:39:39,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1184/2180 [3:09:23<2:39:25,  9.60s/it]                                                       {'loss': 2.0372, 'learning_rate': 0.0004547363314194449, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1184/2180 [3:09:23<2:39:25,  9.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1185/2180 [3:09:32<2:39:06,  9.59s/it]                                                       {'loss': 2.1281, 'learning_rate': 0.000453996388016135, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1185/2180 [3:09:32<2:39:06,  9.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1186/2180 [3:09:42<2:39:08,  9.61s/it]                                                       {'loss': 2.0598, 'learning_rate': 0.0004532565462099999, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1186/2180 [3:09:42<2:39:08,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1187/2180 [3:09:52<2:39:01,  9.61s/it]                                                       {'loss': 2.0545, 'learning_rate': 0.0004525168076349513, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1187/2180 [3:09:52<2:39:01,  9.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1188/2180 [3:10:01<2:38:41,  9.60s/it]                                                       {'loss': 2.1095, 'learning_rate': 0.0004517771739246729, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1188/2180 [3:10:01<2:38:41,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1189/2180 [3:10:11<2:38:28,  9.60s/it]                                                       {'loss': 2.0948, 'learning_rate': 0.0004510376467126165, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1189/2180 [3:10:11<2:38:28,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1190/2180 [3:10:20<2:38:29,  9.61s/it]                                                       {'loss': 2.0943, 'learning_rate': 0.0004502982276319992, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1190/2180 [3:10:20<2:38:29,  9.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1191/2180 [3:10:30<2:38:01,  9.59s/it]                                                       {'loss': 2.0956, 'learning_rate': 0.0004495589183157991, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1191/2180 [3:10:30<2:38:01,  9.59s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1192/2180 [3:10:40<2:37:46,  9.58s/it]                                                       {'loss': 2.019, 'learning_rate': 0.0004488197203967517, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1192/2180 [3:10:40<2:37:46,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1193/2180 [3:10:49<2:38:01,  9.61s/it]                                                       {'loss': 2.1055, 'learning_rate': 0.0004480806355073467, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1193/2180 [3:10:49<2:38:01,  9.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1194/2180 [3:10:59<2:37:41,  9.60s/it]                                                       {'loss': 2.0446, 'learning_rate': 0.000447341665279824, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1194/2180 [3:10:59<2:37:41,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1195/2180 [3:11:08<2:37:31,  9.60s/it]                                                       {'loss': 2.0859, 'learning_rate': 0.0004466028113461708, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1195/2180 [3:11:08<2:37:31,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2180 [3:11:18<2:37:24,  9.60s/it]                                                       {'loss': 2.1825, 'learning_rate': 0.0004458640753381167, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2180 [3:11:18<2:37:24,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2180 [3:11:28<2:37:10,  9.59s/it]                                                       {'loss': 2.0553, 'learning_rate': 0.0004451254588871313, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2180 [3:11:28<2:37:10,  9.59s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2180 [3:11:37<2:37:08,  9.60s/it]                                                       {'loss': 2.0801, 'learning_rate': 0.0004443869636244203, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2180 [3:11:37<2:37:08,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1199/2180 [3:11:47<2:37:01,  9.60s/it]                                                       {'loss': 2.0561, 'learning_rate': 0.0004436485911809212, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1199/2180 [3:11:47<2:37:01,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1200/2180 [3:11:56<2:36:51,  9.60s/it]                                                       {'loss': 2.0895, 'learning_rate': 0.00044291034318730087, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1200/2180 [3:11:56<2:36:51,  9.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1201/2180 [3:12:06<2:36:22,  9.58s/it]                                                       {'loss': 2.1143, 'learning_rate': 0.0004421722212739511, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1201/2180 [3:12:06<2:36:22,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1202/2180 [3:12:16<2:36:08,  9.58s/it]                                                       {'loss': 2.0465, 'learning_rate': 0.0004414342270709848, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1202/2180 [3:12:16<2:36:08,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1203/2180 [3:12:25<2:35:54,  9.57s/it]                                                       {'loss': 2.0534, 'learning_rate': 0.00044069636220823397, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1203/2180 [3:12:25<2:35:54,  9.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1204/2180 [3:12:35<2:35:45,  9.58s/it]                                                       {'loss': 2.0653, 'learning_rate': 0.0004399586283152437, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1204/2180 [3:12:35<2:35:45,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1205/2180 [3:12:44<2:35:38,  9.58s/it]                                                       {'loss': 2.123, 'learning_rate': 0.0004392210270212706, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1205/2180 [3:12:44<2:35:38,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1206/2180 [3:12:54<2:35:29,  9.58s/it]                                                       {'loss': 2.036, 'learning_rate': 0.00043848355995527825, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1206/2180 [3:12:54<2:35:29,  9.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1207/2180 [3:13:04<2:35:47,  9.61s/it]                                                       {'loss': 2.0328, 'learning_rate': 0.00043774622874593374, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1207/2180 [3:13:04<2:35:47,  9.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1208/2180 [3:13:13<2:35:22,  9.59s/it]                                                       {'loss': 2.1481, 'learning_rate': 0.000437009035021604, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1208/2180 [3:13:13<2:35:22,  9.59s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1209/2180 [3:13:23<2:35:38,  9.62s/it]                                                       {'loss': 2.0809, 'learning_rate': 0.00043627198041035274, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1209/2180 [3:13:23<2:35:38,  9.62s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1210/2180 [3:13:32<2:35:10,  9.60s/it]                                                       {'loss': 2.0374, 'learning_rate': 0.00043553506653993597, 'epoch': 0.55}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1210/2180 [3:13:32<2:35:10,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1211/2180 [3:13:42<2:34:52,  9.59s/it]                                                       {'loss': 2.0786, 'learning_rate': 0.0004347982950377992, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1211/2180 [3:13:42<2:34:52,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1212/2180 [3:13:51<2:34:43,  9.59s/it]                                                       {'loss': 2.0961, 'learning_rate': 0.0004340616675310735, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1212/2180 [3:13:51<2:34:43,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1213/2180 [3:14:01<2:34:24,  9.58s/it]                                                       {'loss': 2.027, 'learning_rate': 0.00043332518564657193, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1213/2180 [3:14:01<2:34:24,  9.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1214/2180 [3:14:11<2:34:13,  9.58s/it]                                                       {'loss': 2.0352, 'learning_rate': 0.0004325888510107856, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1214/2180 [3:14:11<2:34:13,  9.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1215/2180 [3:14:20<2:34:31,  9.61s/it]                                                       {'loss': 2.0977, 'learning_rate': 0.0004318526652498809, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1215/2180 [3:14:20<2:34:31,  9.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1216/2180 [3:14:30<2:34:11,  9.60s/it]                                                       {'loss': 2.1037, 'learning_rate': 0.00043111662998969523, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1216/2180 [3:14:30<2:34:11,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1217/2180 [3:14:39<2:33:46,  9.58s/it]                                                       {'loss': 2.1271, 'learning_rate': 0.0004303807468557335, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1217/2180 [3:14:39<2:33:46,  9.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1218/2180 [3:14:49<2:33:39,  9.58s/it]                                                       {'loss': 2.0468, 'learning_rate': 0.0004296450174731648, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1218/2180 [3:14:49<2:33:39,  9.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1219/2180 [3:14:59<2:33:30,  9.58s/it]                                                       {'loss': 2.0675, 'learning_rate': 0.0004289094434668188, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1219/2180 [3:14:59<2:33:30,  9.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1220/2180 [3:15:08<2:33:24,  9.59s/it]                                                       {'loss': 2.051, 'learning_rate': 0.00042817402646118185, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1220/2180 [3:15:08<2:33:24,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1221/2180 [3:15:18<2:33:19,  9.59s/it]                                                       {'loss': 2.0195, 'learning_rate': 0.0004274387680803936, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1221/2180 [3:15:18<2:33:19,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1222/2180 [3:15:27<2:33:23,  9.61s/it]                                                       {'loss': 1.9692, 'learning_rate': 0.00042670366994824327, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1222/2180 [3:15:27<2:33:23,  9.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1223/2180 [3:15:37<2:32:56,  9.59s/it]                                                       {'loss': 2.1334, 'learning_rate': 0.0004259687336881663, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1223/2180 [3:15:37<2:32:56,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2180 [3:15:47<2:33:00,  9.60s/it]                                                       {'loss': 2.0425, 'learning_rate': 0.0004252339609232408, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2180 [3:15:47<2:33:00,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2180 [3:15:56<2:32:59,  9.61s/it]                                                       {'loss': 2.0255, 'learning_rate': 0.0004244993532761834, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2180 [3:15:56<2:32:59,  9.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2180 [3:16:06<2:32:38,  9.60s/it]                                                       {'loss': 1.9929, 'learning_rate': 0.00042376491236934634, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2180 [3:16:06<2:32:38,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1227/2180 [3:16:15<2:32:39,  9.61s/it]                                                       {'loss': 2.1066, 'learning_rate': 0.0004230306398247136, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1227/2180 [3:16:15<2:32:39,  9.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1228/2180 [3:16:25<2:32:23,  9.60s/it]                                                       {'loss': 2.0802, 'learning_rate': 0.0004222965372638976, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1228/2180 [3:16:25<2:32:23,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1229/2180 [3:16:35<2:32:11,  9.60s/it]                                                       {'loss': 2.1065, 'learning_rate': 0.0004215626063081348, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1229/2180 [3:16:35<2:32:11,  9.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1230/2180 [3:16:44<2:31:53,  9.59s/it]                                                       {'loss': 2.0423, 'learning_rate': 0.000420828848578283, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1230/2180 [3:16:44<2:31:53,  9.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1231/2180 [3:16:54<2:31:44,  9.59s/it]                                                       {'loss': 2.0331, 'learning_rate': 0.0004200952656948175, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1231/2180 [3:16:54<2:31:44,  9.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1232/2180 [3:17:03<2:31:36,  9.60s/it]                                                       {'loss': 2.1245, 'learning_rate': 0.0004193618592778272, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1232/2180 [3:17:03<2:31:36,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1233/2180 [3:17:13<2:31:29,  9.60s/it]                                                       {'loss': 2.1101, 'learning_rate': 0.0004186286309470116, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1233/2180 [3:17:13<2:31:29,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1234/2180 [3:17:23<2:31:23,  9.60s/it]                                                       {'loss': 2.1291, 'learning_rate': 0.0004178955823216767, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1234/2180 [3:17:23<2:31:23,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1235/2180 [3:17:32<2:31:15,  9.60s/it]                                                       {'loss': 2.0651, 'learning_rate': 0.00041716271502073137, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1235/2180 [3:17:32<2:31:15,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1236/2180 [3:17:42<2:31:08,  9.61s/it]                                                       {'loss': 2.0912, 'learning_rate': 0.000416430030662685, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1236/2180 [3:17:42<2:31:08,  9.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1237/2180 [3:17:51<2:30:51,  9.60s/it]                                                       {'loss': 2.0728, 'learning_rate': 0.00041569753086564173, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1237/2180 [3:17:51<2:30:51,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1238/2180 [3:18:01<2:30:32,  9.59s/it]                                                       {'loss': 2.0895, 'learning_rate': 0.0004149652172472988, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1238/2180 [3:18:01<2:30:32,  9.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1239/2180 [3:18:11<2:30:19,  9.58s/it]                                                       {'loss': 2.0036, 'learning_rate': 0.00041423309142494234, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1239/2180 [3:18:11<2:30:19,  9.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1240/2180 [3:18:20<2:30:08,  9.58s/it]                                                       {'loss': 2.1771, 'learning_rate': 0.0004135011550154433, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1240/2180 [3:18:20<2:30:08,  9.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1241/2180 [3:18:30<2:30:12,  9.60s/it]                                                       {'loss': 2.0721, 'learning_rate': 0.0004127694096352546, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1241/2180 [3:18:30<2:30:12,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1242/2180 [3:18:39<2:30:04,  9.60s/it]                                                       {'loss': 2.0349, 'learning_rate': 0.00041203785690040743, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1242/2180 [3:18:39<2:30:04,  9.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1243/2180 [3:18:49<2:29:50,  9.59s/it]                                                       {'loss': 2.1018, 'learning_rate': 0.00041130649842650694, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1243/2180 [3:18:49<2:29:50,  9.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1244/2180 [3:18:59<2:29:59,  9.61s/it]                                                       {'loss': 2.1303, 'learning_rate': 0.00041057533582873016, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1244/2180 [3:18:59<2:29:59,  9.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1245/2180 [3:19:08<2:30:08,  9.64s/it]                                                       {'loss': 2.0592, 'learning_rate': 0.0004098443707218208, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1245/2180 [3:19:08<2:30:08,  9.64s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1246/2180 [3:19:18<2:29:48,  9.62s/it]                                                       {'loss': 1.9814, 'learning_rate': 0.00040911360472008673, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1246/2180 [3:19:18<2:29:48,  9.62s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1247/2180 [3:19:27<2:29:35,  9.62s/it]                                                       {'loss': 2.1165, 'learning_rate': 0.0004083830394373959, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1247/2180 [3:19:27<2:29:35,  9.62s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1248/2180 [3:19:37<2:29:17,  9.61s/it]                                                       {'loss': 2.098, 'learning_rate': 0.00040765267648717324, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1248/2180 [3:19:37<2:29:17,  9.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1249/2180 [3:19:47<2:29:03,  9.61s/it]                                                       {'loss': 2.059, 'learning_rate': 0.00040692251748239677, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1249/2180 [3:19:47<2:29:03,  9.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1250/2180 [3:19:56<2:28:38,  9.59s/it]                                                       {'loss': 2.1041, 'learning_rate': 0.00040619256403559383, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1250/2180 [3:19:56<2:28:38,  9.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2180 [3:20:06<2:28:21,  9.58s/it]                                                       {'loss': 2.1364, 'learning_rate': 0.000405462817758838, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2180 [3:20:06<2:28:21,  9.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2180 [3:20:15<2:28:15,  9.59s/it]                                                       {'loss': 1.9962, 'learning_rate': 0.0004047332802637457, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2180 [3:20:15<2:28:15,  9.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2180 [3:20:25<2:27:51,  9.57s/it]                                                       {'loss': 1.9978, 'learning_rate': 0.00040400395316147157, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2180 [3:20:25<2:27:51,  9.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1254/2180 [3:20:35<2:27:50,  9.58s/it]                                                       {'loss': 1.99, 'learning_rate': 0.00040327483806270627, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1254/2180 [3:20:35<2:27:50,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1255/2180 [3:20:44<2:27:40,  9.58s/it]                                                       {'loss': 2.1194, 'learning_rate': 0.0004025459365776715, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1255/2180 [3:20:44<2:27:40,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1256/2180 [3:20:54<2:27:27,  9.58s/it]                                                       {'loss': 2.0942, 'learning_rate': 0.00040181725031611794, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1256/2180 [3:20:54<2:27:27,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1257/2180 [3:21:03<2:27:11,  9.57s/it]                                                       {'loss': 2.0946, 'learning_rate': 0.0004010887808873206, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1257/2180 [3:21:03<2:27:11,  9.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1258/2180 [3:21:13<2:27:41,  9.61s/it]                                                       {'loss': 2.0029, 'learning_rate': 0.00040036052990007553, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1258/2180 [3:21:13<2:27:41,  9.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1259/2180 [3:21:22<2:27:20,  9.60s/it]                                                       {'loss': 2.1129, 'learning_rate': 0.0003996324989626967, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1259/2180 [3:21:22<2:27:20,  9.60s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1260/2180 [3:21:32<2:27:06,  9.59s/it]                                                       {'loss': 2.0123, 'learning_rate': 0.00039890468968301166, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1260/2180 [3:21:32<2:27:06,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1261/2180 [3:21:42<2:26:55,  9.59s/it]                                                       {'loss': 2.1069, 'learning_rate': 0.0003981771036683591, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1261/2180 [3:21:42<2:26:55,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1262/2180 [3:21:51<2:27:05,  9.61s/it]                                                       {'loss': 1.9847, 'learning_rate': 0.00039744974252558385, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1262/2180 [3:21:51<2:27:05,  9.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1263/2180 [3:22:01<2:26:45,  9.60s/it]                                                       {'loss': 2.0373, 'learning_rate': 0.00039672260786103463, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1263/2180 [3:22:01<2:26:45,  9.60s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1264/2180 [3:22:10<2:26:18,  9.58s/it]                                                       {'loss': 2.0378, 'learning_rate': 0.00039599570128055994, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1264/2180 [3:22:10<2:26:18,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1265/2180 [3:22:20<2:26:15,  9.59s/it]                                                       {'loss': 2.1274, 'learning_rate': 0.0003952690243895044, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1265/2180 [3:22:20<2:26:15,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1266/2180 [3:22:30<2:26:09,  9.59s/it]                                                       {'loss': 2.0481, 'learning_rate': 0.0003945425787927054, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1266/2180 [3:22:30<2:26:09,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1267/2180 [3:22:39<2:25:56,  9.59s/it]                                                       {'loss': 2.0964, 'learning_rate': 0.00039381636609448975, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1267/2180 [3:22:39<2:25:56,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1268/2180 [3:22:49<2:25:34,  9.58s/it]                                                       {'loss': 2.0918, 'learning_rate': 0.0003930903878986693, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1268/2180 [3:22:49<2:25:34,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1269/2180 [3:22:58<2:25:28,  9.58s/it]                                                       {'loss': 2.0467, 'learning_rate': 0.00039236464580853916, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1269/2180 [3:22:58<2:25:28,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1270/2180 [3:23:08<2:25:16,  9.58s/it]                                                       {'loss': 2.1233, 'learning_rate': 0.0003916391414268718, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1270/2180 [3:23:08<2:25:16,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1271/2180 [3:23:18<2:25:07,  9.58s/it]                                                       {'loss': 2.1168, 'learning_rate': 0.00039091387635591536, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1271/2180 [3:23:18<2:25:07,  9.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1272/2180 [3:23:27<2:25:05,  9.59s/it]                                                       {'loss': 2.0802, 'learning_rate': 0.0003901888521973894, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1272/2180 [3:23:27<2:25:05,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1273/2180 [3:23:37<2:24:59,  9.59s/it]                                                       {'loss': 2.0734, 'learning_rate': 0.0003894640705524813, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1273/2180 [3:23:37<2:24:59,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1274/2180 [3:23:46<2:24:47,  9.59s/it]                                                       {'loss': 2.0528, 'learning_rate': 0.00038873953302184284, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1274/2180 [3:23:46<2:24:47,  9.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1275/2180 [3:23:56<2:24:33,  9.58s/it]                                                       {'loss': 2.0646, 'learning_rate': 0.000388015241205587, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1275/2180 [3:23:56<2:24:33,  9.58s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1276/2180 [3:24:05<2:24:30,  9.59s/it]                                                       {'loss': 2.1057, 'learning_rate': 0.00038729119670328355, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1276/2180 [3:24:05<2:24:30,  9.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1277/2180 [3:24:15<2:24:25,  9.60s/it]                                                       {'loss': 1.9986, 'learning_rate': 0.00038656740111395665, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1277/2180 [3:24:15<2:24:25,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1278/2180 [3:24:25<2:24:19,  9.60s/it]                                                       {'loss': 2.177, 'learning_rate': 0.00038584385603608053, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1278/2180 [3:24:25<2:24:19,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2180 [3:24:34<2:24:13,  9.60s/it]                                                       {'loss': 2.124, 'learning_rate': 0.00038512056306757615, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2180 [3:24:34<2:24:13,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2180 [3:24:44<2:23:55,  9.60s/it]                                                       {'loss': 2.1707, 'learning_rate': 0.0003843975238058075, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2180 [3:24:44<2:23:55,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1281/2180 [3:24:53<2:23:42,  9.59s/it]                                                       {'loss': 2.0338, 'learning_rate': 0.00038367473984757863, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1281/2180 [3:24:53<2:23:42,  9.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1282/2180 [3:25:03<2:23:41,  9.60s/it]                                                       {'loss': 1.9965, 'learning_rate': 0.0003829522127891296, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1282/2180 [3:25:03<2:23:41,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1283/2180 [3:25:13<2:23:36,  9.61s/it]                                                       {'loss': 2.1158, 'learning_rate': 0.0003822299442261329, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1283/2180 [3:25:13<2:23:36,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1284/2180 [3:25:22<2:23:25,  9.60s/it]                                                       {'loss': 2.0333, 'learning_rate': 0.00038150793575369063, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1284/2180 [3:25:22<2:23:25,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1285/2180 [3:25:32<2:23:25,  9.61s/it]                                                       {'loss': 2.0329, 'learning_rate': 0.0003807861889663299, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1285/2180 [3:25:32<2:23:25,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1286/2180 [3:25:42<2:23:09,  9.61s/it]                                                       {'loss': 2.1099, 'learning_rate': 0.0003800647054580006, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1286/2180 [3:25:42<2:23:09,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1287/2180 [3:25:51<2:23:11,  9.62s/it]                                                       {'loss': 2.0473, 'learning_rate': 0.00037934348682207064, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1287/2180 [3:25:51<2:23:11,  9.62s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1288/2180 [3:26:01<2:22:53,  9.61s/it]                                                       {'loss': 2.1495, 'learning_rate': 0.00037862253465132306, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1288/2180 [3:26:01<2:22:53,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1289/2180 [3:26:10<2:22:45,  9.61s/it]                                                       {'loss': 1.9857, 'learning_rate': 0.00037790185053795245, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1289/2180 [3:26:10<2:22:45,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1290/2180 [3:26:20<2:22:34,  9.61s/it]                                                       {'loss': 1.9664, 'learning_rate': 0.0003771814360735616, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1290/2180 [3:26:20<2:22:34,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1291/2180 [3:26:30<2:22:24,  9.61s/it]                                                       {'loss': 2.1204, 'learning_rate': 0.00037646129284915755, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1291/2180 [3:26:30<2:22:24,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1292/2180 [3:26:39<2:22:03,  9.60s/it]                                                       {'loss': 2.0609, 'learning_rate': 0.00037574142245514825, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1292/2180 [3:26:39<2:22:03,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1293/2180 [3:26:49<2:22:00,  9.61s/it]                                                       {'loss': 2.1562, 'learning_rate': 0.0003750218264813393, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1293/2180 [3:26:49<2:22:00,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1294/2180 [3:26:58<2:21:42,  9.60s/it]                                                       {'loss': 1.9595, 'learning_rate': 0.0003743025065169305, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1294/2180 [3:26:58<2:21:42,  9.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1295/2180 [3:27:08<2:22:02,  9.63s/it]                                                       {'loss': 2.1006, 'learning_rate': 0.0003735834641505116, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1295/2180 [3:27:08<2:22:02,  9.63s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1296/2180 [3:27:18<2:21:35,  9.61s/it]                                                       {'loss': 2.0215, 'learning_rate': 0.00037286470097005954, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1296/2180 [3:27:18<2:21:35,  9.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1297/2180 [3:27:27<2:21:29,  9.61s/it]                                                       {'loss': 2.0277, 'learning_rate': 0.0003721462185629347, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1297/2180 [3:27:27<2:21:29,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1298/2180 [3:27:37<2:21:15,  9.61s/it]                                                       {'loss': 2.1243, 'learning_rate': 0.00037142801851587707, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1298/2180 [3:27:37<2:21:15,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1299/2180 [3:27:46<2:21:00,  9.60s/it]                                                       {'loss': 2.0634, 'learning_rate': 0.00037071010241500357, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1299/2180 [3:27:46<2:21:00,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1300/2180 [3:27:56<2:20:47,  9.60s/it]                                                       {'loss': 2.0619, 'learning_rate': 0.00036999247184580383, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1300/2180 [3:27:56<2:20:47,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1301/2180 [3:28:06<2:20:46,  9.61s/it]                                                       {'loss': 2.0417, 'learning_rate': 0.00036927512839313636, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1301/2180 [3:28:06<2:20:46,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1302/2180 [3:28:15<2:20:41,  9.61s/it]                                                       {'loss': 2.0306, 'learning_rate': 0.0003685580736412268, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1302/2180 [3:28:15<2:20:41,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1303/2180 [3:28:25<2:20:27,  9.61s/it]                                                       {'loss': 2.0118, 'learning_rate': 0.000367841309173662, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1303/2180 [3:28:25<2:20:27,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1304/2180 [3:28:34<2:20:08,  9.60s/it]                                                       {'loss': 2.1347, 'learning_rate': 0.0003671248365733883, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1304/2180 [3:28:34<2:20:08,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1305/2180 [3:28:44<2:19:55,  9.60s/it]                                                       {'loss': 2.0375, 'learning_rate': 0.0003664086574227075, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1305/2180 [3:28:44<2:19:55,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1306/2180 [3:28:54<2:20:16,  9.63s/it]                                                       {'loss': 2.0917, 'learning_rate': 0.000365692773303273, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1306/2180 [3:28:54<2:20:16,  9.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2180 [3:29:03<2:19:54,  9.62s/it]                                                       {'loss': 2.1242, 'learning_rate': 0.00036497718579608696, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2180 [3:29:03<2:19:54,  9.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1308/2180 [3:29:13<2:19:46,  9.62s/it]                                                       {'loss': 1.9939, 'learning_rate': 0.0003642618964814964, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1308/2180 [3:29:13<2:19:46,  9.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1309/2180 [3:29:23<2:19:33,  9.61s/it]                                                       {'loss': 2.0992, 'learning_rate': 0.00036354690693918946, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1309/2180 [3:29:23<2:19:33,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1310/2180 [3:29:32<2:19:10,  9.60s/it]                                                       {'loss': 1.9887, 'learning_rate': 0.00036283221874819284, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1310/2180 [3:29:32<2:19:10,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1311/2180 [3:29:42<2:19:03,  9.60s/it]                                                       {'loss': 2.1104, 'learning_rate': 0.0003621178334868672, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1311/2180 [3:29:42<2:19:03,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1312/2180 [3:29:51<2:18:52,  9.60s/it]                                                       {'loss': 2.0886, 'learning_rate': 0.00036140375273290476, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1312/2180 [3:29:51<2:18:52,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1313/2180 [3:30:01<2:18:37,  9.59s/it]                                                       {'loss': 2.0473, 'learning_rate': 0.0003606899780633245, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1313/2180 [3:30:01<2:18:37,  9.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1314/2180 [3:30:11<2:18:27,  9.59s/it]                                                       {'loss': 2.0698, 'learning_rate': 0.0003599765110544699, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1314/2180 [3:30:11<2:18:27,  9.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1315/2180 [3:30:20<2:18:28,  9.61s/it]                                                       {'loss': 2.057, 'learning_rate': 0.0003592633532820052, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1315/2180 [3:30:20<2:18:28,  9.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1316/2180 [3:30:30<2:18:06,  9.59s/it]                                                       {'loss': 2.024, 'learning_rate': 0.0003585505063209109, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1316/2180 [3:30:30<2:18:06,  9.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1317/2180 [3:30:39<2:18:03,  9.60s/it]                                                       {'loss': 2.1332, 'learning_rate': 0.00035783797174548194, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1317/2180 [3:30:39<2:18:03,  9.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1318/2180 [3:30:49<2:17:46,  9.59s/it]                                                       {'loss': 2.0754, 'learning_rate': 0.00035712575112932277, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1318/2180 [3:30:49<2:17:46,  9.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1319/2180 [3:30:59<2:17:36,  9.59s/it]                                                       {'loss': 2.0424, 'learning_rate': 0.000356413846045345, 'epoch': 0.6}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1319/2180 [3:30:59<2:17:36,  9.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1320/2180 [3:31:08<2:17:42,  9.61s/it]                                                       {'loss': 1.9993, 'learning_rate': 0.000355702258065763, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1320/2180 [3:31:08<2:17:42,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1321/2180 [3:31:18<2:17:37,  9.61s/it]                                                       {'loss': 2.071, 'learning_rate': 0.0003549909887620909, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1321/2180 [3:31:18<2:17:37,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1322/2180 [3:31:27<2:17:41,  9.63s/it]                                                       {'loss': 1.9754, 'learning_rate': 0.00035428003970513914, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1322/2180 [3:31:27<2:17:41,  9.63s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1323/2180 [3:31:37<2:17:31,  9.63s/it]                                                       {'loss': 2.0483, 'learning_rate': 0.00035356941246501085, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1323/2180 [3:31:37<2:17:31,  9.63s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1324/2180 [3:31:47<2:17:06,  9.61s/it]                                                       {'loss': 1.9989, 'learning_rate': 0.0003528591086110984, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1324/2180 [3:31:47<2:17:06,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1325/2180 [3:31:56<2:16:55,  9.61s/it]                                                       {'loss': 2.0845, 'learning_rate': 0.00035214912971208, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1325/2180 [3:31:56<2:16:55,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1326/2180 [3:32:06<2:16:46,  9.61s/it]                                                       {'loss': 2.0531, 'learning_rate': 0.0003514394773359163, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1326/2180 [3:32:06<2:16:46,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1327/2180 [3:32:15<2:16:39,  9.61s/it]                                                       {'loss': 2.1375, 'learning_rate': 0.0003507301530498469, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1327/2180 [3:32:15<2:16:39,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1328/2180 [3:32:25<2:16:27,  9.61s/it]                                                       {'loss': 2.0653, 'learning_rate': 0.00035002115842038646, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1328/2180 [3:32:25<2:16:27,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1329/2180 [3:32:35<2:16:13,  9.60s/it]                                                       {'loss': 2.0589, 'learning_rate': 0.00034931249501332195, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1329/2180 [3:32:35<2:16:13,  9.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1330/2180 [3:32:44<2:16:02,  9.60s/it]                                                       {'loss': 2.1316, 'learning_rate': 0.00034860416439370885, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1330/2180 [3:32:44<2:16:02,  9.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1331/2180 [3:32:54<2:15:49,  9.60s/it]                                                       {'loss': 2.1219, 'learning_rate': 0.0003478961681258674, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1331/2180 [3:32:54<2:15:49,  9.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1332/2180 [3:33:03<2:15:45,  9.61s/it]                                                       {'loss': 2.0328, 'learning_rate': 0.0003471885077733796, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1332/2180 [3:33:03<2:15:45,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1333/2180 [3:33:13<2:15:36,  9.61s/it]                                                       {'loss': 2.0956, 'learning_rate': 0.0003464811848990859, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1333/2180 [3:33:13<2:15:36,  9.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1334/2180 [3:33:23<2:15:24,  9.60s/it]                                                       {'loss': 2.0647, 'learning_rate': 0.00034577420106508063, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1334/2180 [3:33:23<2:15:24,  9.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2180 [3:33:32<2:15:58,  9.65s/it]                                                       {'loss': 1.9819, 'learning_rate': 0.0003450675578327105, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2180 [3:33:32<2:15:58,  9.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1336/2180 [3:33:42<2:15:44,  9.65s/it]                                                       {'loss': 2.0781, 'learning_rate': 0.000344361256762569, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1336/2180 [3:33:42<2:15:44,  9.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1337/2180 [3:33:52<2:15:15,  9.63s/it]                                                       {'loss': 1.9775, 'learning_rate': 0.00034365529941449456, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1337/2180 [3:33:52<2:15:15,  9.63s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1338/2180 [3:34:01<2:15:05,  9.63s/it]                                                       {'loss': 2.0603, 'learning_rate': 0.0003429496873475664, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1338/2180 [3:34:01<2:15:05,  9.63s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1339/2180 [3:34:11<2:14:50,  9.62s/it]                                                       {'loss': 2.0484, 'learning_rate': 0.0003422444221201009, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1339/2180 [3:34:11<2:14:50,  9.62s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1340/2180 [3:34:21<2:14:41,  9.62s/it]                                                       {'loss': 2.0784, 'learning_rate': 0.0003415395052896487, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1340/2180 [3:34:21<2:14:41,  9.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1341/2180 [3:34:30<2:14:23,  9.61s/it]                                                       {'loss': 1.9919, 'learning_rate': 0.0003408349384129912, 'epoch': 0.61}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1341/2180 [3:34:30<2:14:23,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1342/2180 [3:34:40<2:14:09,  9.61s/it]                                                       {'loss': 2.0915, 'learning_rate': 0.00034013072304613643, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1342/2180 [3:34:40<2:14:09,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1343/2180 [3:34:49<2:14:05,  9.61s/it]                                                       {'loss': 2.0498, 'learning_rate': 0.00033942686074431674, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1343/2180 [3:34:49<2:14:05,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1344/2180 [3:34:59<2:13:38,  9.59s/it]                                                       {'loss': 2.1122, 'learning_rate': 0.0003387233530619843, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1344/2180 [3:34:59<2:13:38,  9.59s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1345/2180 [3:35:08<2:13:26,  9.59s/it]                                                       {'loss': 2.0544, 'learning_rate': 0.0003380202015528084, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1345/2180 [3:35:08<2:13:26,  9.59s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1346/2180 [3:35:18<2:13:26,  9.60s/it]                                                       {'loss': 2.0161, 'learning_rate': 0.0003373174077696715, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1346/2180 [3:35:18<2:13:26,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1347/2180 [3:35:28<2:13:27,  9.61s/it]                                                       {'loss': 2.0484, 'learning_rate': 0.0003366149732646661, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1347/2180 [3:35:28<2:13:27,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1348/2180 [3:35:37<2:13:14,  9.61s/it]                                                       {'loss': 2.0754, 'learning_rate': 0.00033591289958909143, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1348/2180 [3:35:37<2:13:14,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1349/2180 [3:35:47<2:12:57,  9.60s/it]                                                       {'loss': 2.0053, 'learning_rate': 0.00033521118829344954, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1349/2180 [3:35:47<2:12:57,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1350/2180 [3:35:57<2:12:49,  9.60s/it]                                                       {'loss': 2.0186, 'learning_rate': 0.0003345098409274423, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1350/2180 [3:35:57<2:12:49,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1351/2180 [3:36:06<2:12:42,  9.60s/it]                                                       {'loss': 2.0869, 'learning_rate': 0.00033380885903996796, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1351/2180 [3:36:06<2:12:42,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1352/2180 [3:36:16<2:12:30,  9.60s/it]                                                       {'loss': 2.0911, 'learning_rate': 0.00033310824417911766, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1352/2180 [3:36:16<2:12:30,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1353/2180 [3:36:25<2:12:05,  9.58s/it]                                                       {'loss': 2.0216, 'learning_rate': 0.00033240799789217184, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1353/2180 [3:36:25<2:12:05,  9.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1354/2180 [3:36:35<2:12:11,  9.60s/it]                                                       {'loss': 2.0285, 'learning_rate': 0.00033170812172559694, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1354/2180 [3:36:35<2:12:11,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1355/2180 [3:36:45<2:12:04,  9.61s/it]                                                       {'loss': 2.0587, 'learning_rate': 0.000331008617225042, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1355/2180 [3:36:45<2:12:04,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1356/2180 [3:36:54<2:11:47,  9.60s/it]                                                       {'loss': 2.1461, 'learning_rate': 0.0003303094859353355, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1356/2180 [3:36:54<2:11:47,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1357/2180 [3:37:04<2:11:56,  9.62s/it]                                                       {'loss': 1.9968, 'learning_rate': 0.0003296107294004812, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1357/2180 [3:37:04<2:11:56,  9.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1358/2180 [3:37:13<2:11:41,  9.61s/it]                                                       {'loss': 1.9995, 'learning_rate': 0.0003289123491636559, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1358/2180 [3:37:13<2:11:41,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1359/2180 [3:37:23<2:11:21,  9.60s/it]                                                       {'loss': 2.0162, 'learning_rate': 0.00032821434676720443, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1359/2180 [3:37:23<2:11:21,  9.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1360/2180 [3:37:32<2:10:59,  9.58s/it]                                                       {'loss': 2.0278, 'learning_rate': 0.00032751672375263836, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1360/2180 [3:37:32<2:10:59,  9.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1361/2180 [3:37:42<2:11:12,  9.61s/it]                                                       {'loss': 2.0611, 'learning_rate': 0.0003268194816606305, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1361/2180 [3:37:42<2:11:12,  9.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1362/2180 [3:37:52<2:10:58,  9.61s/it]                                                       {'loss': 2.0863, 'learning_rate': 0.00032612262203101267, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1362/2180 [3:37:52<2:10:58,  9.61s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1363/2180 [3:38:01<2:10:39,  9.60s/it]                                                       {'loss': 2.1025, 'learning_rate': 0.00032542614640277225, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1363/2180 [3:38:01<2:10:39,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1364/2180 [3:38:11<2:10:29,  9.60s/it]                                                       {'loss': 2.0525, 'learning_rate': 0.0003247300563140481, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1364/2180 [3:38:11<2:10:29,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1365/2180 [3:38:21<2:10:21,  9.60s/it]                                                       {'loss': 2.0575, 'learning_rate': 0.00032403435330212807, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1365/2180 [3:38:21<2:10:21,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1366/2180 [3:38:30<2:10:13,  9.60s/it]                                                       {'loss': 2.2341, 'learning_rate': 0.00032333903890344515, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1366/2180 [3:38:30<2:10:13,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1367/2180 [3:38:40<2:09:59,  9.59s/it]                                                       {'loss': 2.0619, 'learning_rate': 0.00032264411465357333, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1367/2180 [3:38:40<2:09:59,  9.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1368/2180 [3:38:49<2:09:49,  9.59s/it]                                                       {'loss': 2.0332, 'learning_rate': 0.00032194958208722654, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1368/2180 [3:38:49<2:09:49,  9.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1369/2180 [3:38:59<2:09:41,  9.59s/it]                                                       {'loss': 1.9822, 'learning_rate': 0.00032125544273825204, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1369/2180 [3:38:59<2:09:41,  9.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1370/2180 [3:39:08<2:09:32,  9.60s/it]                                                       {'loss': 1.9955, 'learning_rate': 0.0003205616981396297, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1370/2180 [3:39:08<2:09:32,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1371/2180 [3:39:18<2:09:22,  9.60s/it]                                                       {'loss': 1.9498, 'learning_rate': 0.00031986834982346713, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1371/2180 [3:39:18<2:09:22,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1372/2180 [3:39:28<2:09:07,  9.59s/it]                                                       {'loss': 2.1223, 'learning_rate': 0.00031917539932099694, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1372/2180 [3:39:28<2:09:07,  9.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1373/2180 [3:39:37<2:08:52,  9.58s/it]                                                       {'loss': 2.0917, 'learning_rate': 0.00031848284816257336, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1373/2180 [3:39:37<2:08:52,  9.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1374/2180 [3:39:47<2:08:32,  9.57s/it]                                                       {'loss': 2.0067, 'learning_rate': 0.0003177906978776682, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1374/2180 [3:39:47<2:08:32,  9.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1375/2180 [3:39:56<2:08:25,  9.57s/it]                                                       {'loss': 2.0472, 'learning_rate': 0.0003170989499948683, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1375/2180 [3:39:56<2:08:25,  9.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1376/2180 [3:40:06<2:08:13,  9.57s/it]                                                       {'loss': 2.0758, 'learning_rate': 0.0003164076060418719, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1376/2180 [3:40:06<2:08:13,  9.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1377/2180 [3:40:15<2:08:07,  9.57s/it]                                                       {'loss': 2.1032, 'learning_rate': 0.000315716667545485, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1377/2180 [3:40:15<2:08:07,  9.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1378/2180 [3:40:25<2:07:58,  9.57s/it]                                                       {'loss': 2.0306, 'learning_rate': 0.00031502613603161836, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1378/2180 [3:40:25<2:07:58,  9.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1379/2180 [3:40:35<2:07:54,  9.58s/it]                                                       {'loss': 2.1007, 'learning_rate': 0.00031433601302528335, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1379/2180 [3:40:35<2:07:54,  9.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1380/2180 [3:40:44<2:07:42,  9.58s/it]                                                       {'loss': 2.0847, 'learning_rate': 0.00031364630005058995, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1380/2180 [3:40:44<2:07:42,  9.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1381/2180 [3:40:54<2:07:37,  9.58s/it]                                                       {'loss': 2.0449, 'learning_rate': 0.0003129569986307422, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1381/2180 [3:40:54<2:07:37,  9.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1382/2180 [3:41:03<2:07:41,  9.60s/it]                                                       {'loss': 2.1032, 'learning_rate': 0.00031226811028803515, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1382/2180 [3:41:03<2:07:41,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1383/2180 [3:41:13<2:07:31,  9.60s/it]                                                       {'loss': 2.0552, 'learning_rate': 0.00031157963654385173, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1383/2180 [3:41:13<2:07:31,  9.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1384/2180 [3:41:23<2:07:18,  9.60s/it]                                                       {'loss': 2.0283, 'learning_rate': 0.0003108915789186592, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1384/2180 [3:41:23<2:07:18,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1385/2180 [3:41:32<2:06:57,  9.58s/it]                                                       {'loss': 1.9357, 'learning_rate': 0.00031020393893200604, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1385/2180 [3:41:32<2:06:57,  9.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1386/2180 [3:41:42<2:06:47,  9.58s/it]                                                       {'loss': 2.0722, 'learning_rate': 0.00030951671810251823, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1386/2180 [3:41:42<2:06:47,  9.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1387/2180 [3:41:51<2:06:31,  9.57s/it]                                                       {'loss': 2.1126, 'learning_rate': 0.0003088299179478959, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1387/2180 [3:41:51<2:06:31,  9.57s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1388/2180 [3:42:01<2:06:33,  9.59s/it]                                                       {'loss': 2.044, 'learning_rate': 0.0003081435399849104, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1388/2180 [3:42:01<2:06:33,  9.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1389/2180 [3:42:11<2:06:26,  9.59s/it]                                                       {'loss': 2.0084, 'learning_rate': 0.0003074575857294004, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1389/2180 [3:42:11<2:06:26,  9.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2180 [3:42:20<2:06:17,  9.59s/it]                                                       {'loss': 2.1056, 'learning_rate': 0.0003067720566962691, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2180 [3:42:20<2:06:17,  9.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2180 [3:42:30<2:05:59,  9.58s/it]                                                       {'loss': 2.093, 'learning_rate': 0.0003060869543994806, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2180 [3:42:30<2:05:59,  9.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2180 [3:42:39<2:05:41,  9.57s/it]                                                       {'loss': 2.0252, 'learning_rate': 0.0003054022803520562, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2180 [3:42:39<2:05:41,  9.57s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2180 [3:42:49<2:05:40,  9.58s/it]                                                       {'loss': 2.0588, 'learning_rate': 0.0003047180360660721, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2180 [3:42:49<2:05:40,  9.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2180 [3:42:59<2:05:43,  9.60s/it]                                                       {'loss': 2.0463, 'learning_rate': 0.00030403422305265475, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2180 [3:42:59<2:05:43,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2180 [3:43:08<2:05:39,  9.60s/it]                                                       {'loss': 2.142, 'learning_rate': 0.0003033508428219785, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2180 [3:43:08<2:05:39,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2180 [3:43:18<2:05:27,  9.60s/it]                                                       {'loss': 2.0175, 'learning_rate': 0.00030266789688326184, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2180 [3:43:18<2:05:27,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1397/2180 [3:43:27<2:05:21,  9.61s/it]                                                       {'loss': 2.0634, 'learning_rate': 0.00030198538674476393, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1397/2180 [3:43:27<2:05:21,  9.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1398/2180 [3:43:37<2:05:00,  9.59s/it]                                                       {'loss': 2.0631, 'learning_rate': 0.00030130331391378185, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1398/2180 [3:43:37<2:05:00,  9.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1399/2180 [3:43:47<2:05:02,  9.61s/it]                                                       {'loss': 2.0489, 'learning_rate': 0.0003006216798966468, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1399/2180 [3:43:47<2:05:02,  9.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2180 [3:43:56<2:04:52,  9.61s/it]                                                       {'loss': 2.0747, 'learning_rate': 0.00029994048619872034, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2180 [3:43:56<2:04:52,  9.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1401/2180 [3:44:06<2:04:32,  9.59s/it]                                                       {'loss': 1.9798, 'learning_rate': 0.0002992597343243927, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1401/2180 [3:44:06<2:04:32,  9.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1402/2180 [3:44:15<2:04:27,  9.60s/it]                                                       {'loss': 2.0831, 'learning_rate': 0.0002985794257770773, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1402/2180 [3:44:15<2:04:27,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1403/2180 [3:44:25<2:04:18,  9.60s/it]                                                       {'loss': 2.0453, 'learning_rate': 0.0002978995620592092, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1403/2180 [3:44:25<2:04:18,  9.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1404/2180 [3:44:35<2:04:18,  9.61s/it]                                                       {'loss': 2.0616, 'learning_rate': 0.0002972201446722405, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1404/2180 [3:44:35<2:04:18,  9.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1405/2180 [3:44:44<2:04:12,  9.62s/it]                                                       {'loss': 2.0065, 'learning_rate': 0.00029654117511663803, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1405/2180 [3:44:44<2:04:12,  9.62s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1406/2180 [3:44:54<2:03:49,  9.60s/it]                                                       {'loss': 2.0891, 'learning_rate': 0.0002958626548918795, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1406/2180 [3:44:54<2:03:49,  9.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1407/2180 [3:45:03<2:03:35,  9.59s/it]                                                       {'loss': 2.0506, 'learning_rate': 0.00029518458549645014, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1407/2180 [3:45:03<2:03:35,  9.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1408/2180 [3:45:13<2:03:26,  9.59s/it]                                                       {'loss': 2.0644, 'learning_rate': 0.00029450696842783954, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1408/2180 [3:45:13<2:03:26,  9.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1409/2180 [3:45:23<2:03:30,  9.61s/it]                                                       {'loss': 1.9952, 'learning_rate': 0.00029382980518253865, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1409/2180 [3:45:23<2:03:30,  9.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1410/2180 [3:45:32<2:03:17,  9.61s/it]                                                       {'loss': 2.016, 'learning_rate': 0.00029315309725603595, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1410/2180 [3:45:32<2:03:17,  9.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1411/2180 [3:45:42<2:03:03,  9.60s/it]                                                       {'loss': 2.0901, 'learning_rate': 0.00029247684614281446, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1411/2180 [3:45:42<2:03:03,  9.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1412/2180 [3:45:51<2:02:49,  9.60s/it]                                                       {'loss': 2.0913, 'learning_rate': 0.0002918010533363481, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1412/2180 [3:45:51<2:02:49,  9.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1413/2180 [3:46:01<2:02:36,  9.59s/it]                                                       {'loss': 1.9902, 'learning_rate': 0.0002911257203290987, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1413/2180 [3:46:01<2:02:36,  9.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1414/2180 [3:46:11<2:02:39,  9.61s/it]                                                       {'loss': 2.0937, 'learning_rate': 0.00029045084861251314, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1414/2180 [3:46:11<2:02:39,  9.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1415/2180 [3:46:20<2:02:27,  9.60s/it]                                                       {'loss': 2.0547, 'learning_rate': 0.00028977643967701897, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1415/2180 [3:46:20<2:02:27,  9.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1416/2180 [3:46:30<2:02:04,  9.59s/it]                                                       {'loss': 1.9857, 'learning_rate': 0.00028910249501202156, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1416/2180 [3:46:30<2:02:04,  9.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1417/2180 [3:46:39<2:01:50,  9.58s/it]                                                       {'loss': 2.0674, 'learning_rate': 0.00028842901610590165, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1417/2180 [3:46:39<2:01:50,  9.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1418/2180 [3:46:49<2:01:35,  9.57s/it]                                                       {'loss': 2.0338, 'learning_rate': 0.00028775600444601123, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1418/2180 [3:46:49<2:01:35,  9.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1419/2180 [3:46:59<2:02:01,  9.62s/it]                                                       {'loss': 2.0406, 'learning_rate': 0.00028708346151866973, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1419/2180 [3:46:59<2:02:01,  9.62s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1420/2180 [3:47:08<2:01:34,  9.60s/it]                                                       {'loss': 2.0412, 'learning_rate': 0.0002864113888091622, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1420/2180 [3:47:08<2:01:34,  9.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1421/2180 [3:47:18<2:01:21,  9.59s/it]                                                       {'loss': 2.062, 'learning_rate': 0.0002857397878017348, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1421/2180 [3:47:18<2:01:21,  9.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1422/2180 [3:47:27<2:01:10,  9.59s/it]                                                       {'loss': 2.1767, 'learning_rate': 0.00028506865997959173, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1422/2180 [3:47:27<2:01:10,  9.59s/it]