
================================================= Thu Apr 11 06:50:16 AM UTC 2024 =========================================================

[2024-04-11 02:50:18,987] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:21,568] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 02:50:21,568] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 02:50:23,589] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:25,252] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 02:50:25,252] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 02:50:25,252] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 02:50:25,252] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 02:50:25,252] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 02:50:28,979] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:29,018] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:29,020] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:29,071] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 02:50:30,081] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:50:30,197] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:50:30,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 02:50:30,232] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 02:50:30,313] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.53s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.43s/it]
Creating scaler
Creating scaler
Creating scaler
Creating scaler
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Formatting inputs...Skip in lazy mode
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240411_025155-l5imi2en
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mve-clip-dino-pretrain-7b
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/l5imi2en
  0%|          | 0/2180 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2180 [00:21<13:05:38, 21.63s/it]                                                   {'loss': 9.0856, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:21<13:05:38, 21.63s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 2/2180 [00:31<8:44:20, 14.44s/it]                                                   {'loss': 8.9896, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:31<8:44:20, 14.44s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 3/2180 [00:40<7:21:56, 12.18s/it]                                                  {'loss': 8.0933, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [00:40<7:21:56, 12.18s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 4/2180 [00:50<6:43:20, 11.12s/it]                                                  {'loss': 6.7571, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [00:50<6:43:20, 11.12s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 5/2180 [00:59<6:21:24, 10.52s/it]                                                  {'loss': 6.0926, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [00:59<6:21:24, 10.52s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 6/2180 [01:08<6:08:09, 10.16s/it]                                                  {'loss': 5.6733, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [01:08<6:08:09, 10.16s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
[2024-04-11 02:53:11,481] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3092427
[2024-04-11 02:53:11,482] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3092428
[2024-04-11 02:53:12,355] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3092429
[2024-04-11 02:53:13,256] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3092430
[2024-04-11 02:53:14,224] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'plain', '--data_path', '/data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json', '--image_folder', '/data/data1/akane/LLaVA/data/blip_laion_558k', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--resampler_grid_size', '24', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--tune_mm_resampler', 'True', '--scaled_clip_residual', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/data/data1/akane/mve-clip-dino-scaler-pretrain/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'mve-clip-dino-pretrain-7b'] exits with return code = -15
\n================================================= Thu Apr 11 04:54:53 PM UTC 2024 =========================================================\n
[2024-04-11 12:54:55,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/bin/deepspeed", line 3, in <module>
    from deepspeed.launcher.runner import main
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 22, in <module>
    from . import module_inject
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/__init__.py", line 6, in <module>
    from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py", line 590, in <module>
    from ..pipe import PipelineModule
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/pipe/__init__.py", line 6, in <module>
    from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/pipe/__init__.py", line 6, in <module>
    from .module import PipelineModule, LayerSpec, TiedLayerSpec
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/pipe/module.py", line 19, in <module>
    from ..activation_checkpointing import checkpointing
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 26, in <module>
    from deepspeed.runtime.config import DeepSpeedConfig
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 29, in <module>
    from .zero.config import get_zero_config, ZeroStageEnum
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/__init__.py", line 6, in <module>
    from .partition_parameters import ZeroParamType
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 37, in <module>
    from deepspeed.inference.quantization.utils import _quantize_param, WEIGHT_QUANTIZATION_LAYERS, wrap_quantized_functional, wrap_load_from_state_dict
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/inference/quantization/utils.py", line 15, in <module>
    device = get_accelerator().device_name() if get_accelerator().is_available() else 'cpu'
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 202, in is_available
    return torch.cuda.is_available()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py", line 138, in is_available
    return torch._C._cuda_getDeviceCount() > 0
KeyboardInterrupt
\n================================================= Thu Apr 11 04:55:17 PM UTC 2024 =========================================================\n
[2024-04-11 12:55:19,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:21,732] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 12:55:21,732] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain-debug/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 12:55:23,870] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:25,573] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 12:55:25,573] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 12:55:25,573] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 12:55:25,573] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 12:55:25,573] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 12:55:29,228] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:29,313] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:29,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:29,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:55:30,420] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:55:30,420] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 12:55:30,589] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:55:30,649] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:55:30,992] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.45s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([576, 1024]), 589824
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([3072, 1024]), 3145728
Resampler param: torch.Size([3072]), 3072
Resampler param: torch.Size([1024, 1024]), 1048576
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param: torch.Size([1024]), 1024
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: False
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412480
frozen=7346291713
##### Scaler learnt value
False
##### Scaler learnt value
False
##### Scaler learnt value
False
Formatting inputs...Skip in lazy mode
##### Scaler learnt value
False
[2024-04-11 12:56:52,047] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1261592
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
            return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)


  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
            work = default_pg.broadcast([tensor], opts)work = default_pg.broadcast([tensor], opts)work = default_pg.broadcast([tensor], opts)


RuntimeErrorRuntimeError: RuntimeError: [2] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at ../torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fe71d392617 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7fe71d34da56 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x32c (0x7fe6d8e4e11c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7fe6d8e4f2a2 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0x55 (0x7fe6d8e4f6c5 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fe6d8e06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fe6d8e06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7fe6d8e06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xb2 (0x7fe6a8eca812 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #9: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x203 (0x7fe6a8ed01e3 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::broadcast(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::BroadcastOptions const&) + 0x41c (0x7fe6a8ee320c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0x557c910 (0x7fe6d8dfb910 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x5585342 (0x7fe6d8e04342 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x55853e9 (0x7fe6d8e043e9 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4bb157b (0x7fe6d843057b in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x4baf55c (0x7fe6d842e55c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x1904ae8 (0x7fe6d5183ae8 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x558be12 (0x7fe6d8e0ae12 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x559bfb7 (0x7fe6d8e1afb7 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xc43a45 (0x7fe6eb3daa45 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x3eebb4 (0x7fe6eab85bb4 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: /home/akane38/miniconda3/envs/llava/bin/python() [0x4fc697]
frame #22: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #23: /home/akane38/miniconda3/envs/llava/bin/python() [0x50819f]
frame #24: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #25: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #27: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #29: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #31: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x31f (0x4ed2bf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #33: /home/akane38/miniconda3/envs/llava/bin/python() [0x50f5bf]
frame #34: /home/akane38/miniconda3/envs/llava/bin/python() [0x55a375]
frame #35: _PyEval_EvalFrameDefault + 0x9fb (0x4ed99b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #36: /home/akane38/miniconda3/envs/llava/bin/python() [0x507eae]
frame #37: PyObject_Call + 0xb8 (0x508858 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #38: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c5298]
frame #39: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ed6d1 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #43: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #45: /home/akane38/miniconda3/envs/llava/bin/python() [0x591d92]
frame #46: PyEval_EvalCode + 0x87 (0x591cd7 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #47: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c2967]
frame #48: /home/akane38/miniconda3/envs/llava/bin/python() [0x5bdad0]
frame #49: /home/akane38/miniconda3/envs/llava/bin/python() [0x45956b]
frame #50: _PyRun_SimpleFileObject + 0x19f (0x5b805f in /home/akane38/miniconda3/envs/llava/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5b7dc3 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #52: Py_RunMain + 0x38d (0x5b4b7d in /home/akane38/miniconda3/envs/llava/bin/python)
frame #53: Py_BytesMain + 0x39 (0x584e49 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fe7299d5d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fe7299d5e40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: /home/akane38/miniconda3/envs/llava/bin/python() [0x584cfe]
. This may indicate a possible application crash on rank 0 or a network set up issue.: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at ../torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7ff06e10f617 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7ff06e0caa56 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x32c (0x7ff029c4e11c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7ff029c4f2a2 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0x55 (0x7ff029c4f6c5 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff029c06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff029c06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff029c06f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xb2 (0x7feff9cca812 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #9: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x203 (0x7feff9cd01e3 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::broadcast(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::BroadcastOptions const&) + 0x41c (0x7feff9ce320c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0x557c910 (0x7ff029bfb910 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x5585342 (0x7ff029c04342 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x55853e9 (0x7ff029c043e9 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4bb157b (0x7ff02923057b in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x4baf55c (0x7ff02922e55c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x1904ae8 (0x7ff025f83ae8 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x558be12 (0x7ff029c0ae12 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x559bfb7 (0x7ff029c1afb7 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xc43a45 (0x7ff03c1daa45 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x3eebb4 (0x7ff03b985bb4 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: /home/akane38/miniconda3/envs/llava/bin/python() [0x4fc697]
frame #22: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #23: /home/akane38/miniconda3/envs/llava/bin/python() [0x50819f]
frame #24: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #25: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #27: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #29: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #31: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x31f (0x4ed2bf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #33: /home/akane38/miniconda3/envs/llava/bin/python() [0x50f5bf]
frame #34: /home/akane38/miniconda3/envs/llava/bin/python() [0x55a375]
frame #35: _PyEval_EvalFrameDefault + 0x9fb (0x4ed99b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #36: /home/akane38/miniconda3/envs/llava/bin/python() [0x507eae]
frame #37: PyObject_Call + 0xb8 (0x508858 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #38: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c5298]
frame #39: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ed6d1 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #43: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #45: /home/akane38/miniconda3/envs/llava/bin/python() [0x591d92]
frame #46: PyEval_EvalCode + 0x87 (0x591cd7 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #47: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c2967]
frame #48: /home/akane38/miniconda3/envs/llava/bin/python() [0x5bdad0]
frame #49: /home/akane38/miniconda3/envs/llava/bin/python() [0x45956b]
frame #50: _PyRun_SimpleFileObject + 0x19f (0x5b805f in /home/akane38/miniconda3/envs/llava/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5b7dc3 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #52: Py_RunMain + 0x38d (0x5b4b7d in /home/akane38/miniconda3/envs/llava/bin/python)
frame #53: Py_BytesMain + 0x39 (0x584e49 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7ff07a6f4d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7ff07a6f4e40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: /home/akane38/miniconda3/envs/llava/bin/python() [0x584cfe]
. This may indicate a possible application crash on rank 0 or a network set up issue.
[1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at ../torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7ff466b92617 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7ff466b4da56 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x32c (0x7ff42264e11c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7ff42264f2a2 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0x55 (0x7ff42264f6c5 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff422606f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff422606f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7ff422606f11 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xb2 (0x7ff3f26ca812 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #9: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x203 (0x7ff3f26d01e3 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::broadcast(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::BroadcastOptions const&) + 0x41c (0x7ff3f26e320c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0x557c910 (0x7ff4225fb910 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x5585342 (0x7ff422604342 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x55853e9 (0x7ff4226043e9 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4bb157b (0x7ff421c3057b in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x4baf55c (0x7ff421c2e55c in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x1904ae8 (0x7ff41e983ae8 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x558be12 (0x7ff42260ae12 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x559bfb7 (0x7ff42261afb7 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xc43a45 (0x7ff434bdaa45 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x3eebb4 (0x7ff434385bb4 in /home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: /home/akane38/miniconda3/envs/llava/bin/python() [0x4fc697]
frame #22: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #23: /home/akane38/miniconda3/envs/llava/bin/python() [0x50819f]
frame #24: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #25: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #27: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x4b26 (0x4f1ac6 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #29: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #31: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x31f (0x4ed2bf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #33: /home/akane38/miniconda3/envs/llava/bin/python() [0x50f5bf]
frame #34: /home/akane38/miniconda3/envs/llava/bin/python() [0x55a375]
frame #35: _PyEval_EvalFrameDefault + 0x9fb (0x4ed99b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #36: /home/akane38/miniconda3/envs/llava/bin/python() [0x507eae]
frame #37: PyObject_Call + 0xb8 (0x508858 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #38: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c5298]
frame #39: _PyObject_MakeTpCall + 0x25b (0x4f614b in /home/akane38/miniconda3/envs/llava/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ed6d1 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #43: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /home/akane38/miniconda3/envs/llava/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #45: /home/akane38/miniconda3/envs/llava/bin/python() [0x591d92]
frame #46: PyEval_EvalCode + 0x87 (0x591cd7 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #47: /home/akane38/miniconda3/envs/llava/bin/python() [0x5c2967]
frame #48: /home/akane38/miniconda3/envs/llava/bin/python() [0x5bdad0]
frame #49: /home/akane38/miniconda3/envs/llava/bin/python() [0x45956b]
frame #50: _PyRun_SimpleFileObject + 0x19f (0x5b805f in /home/akane38/miniconda3/envs/llava/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5b7dc3 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #52: Py_RunMain + 0x38d (0x5b4b7d in /home/akane38/miniconda3/envs/llava/bin/python)
frame #53: Py_BytesMain + 0x39 (0x584e49 in /home/akane38/miniconda3/envs/llava/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7ff473146d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7ff473146e40 in /lib/x86_64-linux-gnu/libc.so.6)
frame #56: /home/akane38/miniconda3/envs/llava/bin/python() [0x584cfe]
. This may indicate a possible application crash on rank 0 or a network set up issue.

During handling of the above exception, another exception occurred:



During handling of the above exception, another exception occurred:

Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
        train(attn_implementation="flash_attention_2")    train(attn_implementation="flash_attention_2")
train(attn_implementation="flash_attention_2")

  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1199, in train
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1199, in train
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1199, in train
            trainer.train()trainer.train()trainer.train()


  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
            return inner_training_loop(return inner_training_loop(return inner_training_loop(


  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
            for step, inputs in enumerate(epoch_iterator):for step, inputs in enumerate(epoch_iterator):for step, inputs in enumerate(epoch_iterator):


  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 379, in __iter__
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 379, in __iter__
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 379, in __iter__
            synchronize_rng_states(self.rng_types, self.synchronized_generator)synchronize_rng_states(self.rng_types, self.synchronized_generator)synchronize_rng_states(self.rng_types, self.synchronized_generator)


  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 111, in synchronize_rng_states
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 111, in synchronize_rng_states
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 111, in synchronize_rng_states
            synchronize_rng_state(RNGType(rng_type), generator=generator)synchronize_rng_state(RNGType(rng_type), generator=generator)synchronize_rng_state(RNGType(rng_type), generator=generator)


  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 89, in synchronize_rng_state
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 89, in synchronize_rng_state
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/random.py", line 89, in synchronize_rng_state
            torch.distributed.broadcast(rng_state, 0)torch.distributed.broadcast(rng_state, 0)torch.distributed.broadcast(rng_state, 0)


  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
            return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)


KeyboardInterruptKeyboardInterruptKeyboardInterrupt


Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/bin/deepspeed", line 6, in <module>
    main()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/launcher/runner.py", line 589, in main
    result.wait()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
[2024-04-11 12:56:52,550] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1261593
[2024-04-11 12:56:53,098] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1261594
[2024-04-11 12:56:53,532] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1261595
[2024-04-11 12:56:53,791] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting
\n================================================= Thu Apr 11 04:58:25 PM UTC 2024 =========================================================\n
[2024-04-11 12:58:27,923] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:30,486] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 12:58:30,486] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain-debug/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 12:58:32,487] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:34,204] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 12:58:34,204] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 12:58:34,204] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 12:58:34,204] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 12:58:34,204] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 12:58:37,790] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:38,050] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:38,194] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:38,241] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 12:58:38,939] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:58:39,359] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:58:39,444] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 12:58:39,444] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 12:58:39,495] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.39s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1128, in train
    model.get_model().clip_residual_scaler.requires_grad_(mode=True)
TypeError: requires_grad_() got an unexpected keyword argument 'mode'
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1128, in train
    model.get_model().clip_residual_scaler.requires_grad_(mode=True)
TypeError: requires_grad_() got an unexpected keyword argument 'mode'
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1128, in train
    model.get_model().clip_residual_scaler.requires_grad_(mode=True)
TypeError: requires_grad_() got an unexpected keyword argument 'mode'
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1128, in train
    model.get_model().clip_residual_scaler.requires_grad_(mode=True)
TypeError: requires_grad_() got an unexpected keyword argument 'mode'
[2024-04-11 12:59:37,293] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1275894
[2024-04-11 12:59:37,645] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1275895
[2024-04-11 12:59:37,715] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1275896
[2024-04-11 12:59:37,784] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1275897
[2024-04-11 12:59:37,785] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'plain', '--data_path', '/data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json', '--image_folder', '/data/data1/akane/LLaVA/data/blip_laion_558k', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--resampler_grid_size', '24', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--tune_mm_resampler', 'True', '--scaled_clip_residual', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/data/data1/akane/mve-clip-dino-scaler-pretrain-debug/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'mve-clip-dino-pretrain-7b'] exits with return code = 1
\n================================================= Thu Apr 11 05:00:35 PM UTC 2024 =========================================================\n
[2024-04-11 13:00:37,592] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:40,450] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 13:00:40,450] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain-debug/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 13:00:42,594] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:44,288] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 13:00:44,289] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 13:00:44,289] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 13:00:44,289] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 13:00:44,289] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 13:00:48,011] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:48,066] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:48,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:48,267] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:00:49,235] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:00:49,281] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:00:49,393] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:00:49,394] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-11 13:00:49,624] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
##### Scaler learnt value
True
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
##### Scaler learnt value
True
Formatting inputs...Skip in lazy mode
##### Scaler learnt value
True
##### Scaler learnt value
True
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240411_130207-cfn1ldbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mve-clip-dino-pretrain-7b
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/cfn1ldbb
  0%|          | 0/2180 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2180 [00:21<13:11:37, 21.80s/it]                                                   {'loss': 9.2888, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:21<13:11:37, 21.80s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 2/2180 [00:31<8:47:16, 14.53s/it]                                                   {'loss': 9.148, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:31<8:47:16, 14.53s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 3/2180 [00:40<7:24:41, 12.26s/it]                                                  {'loss': 8.5366, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [00:40<7:24:41, 12.26s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 4/2180 [00:50<6:46:01, 11.20s/it]                                                  {'loss': 6.5021, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [00:50<6:46:01, 11.20s/it]Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 5/2180 [00:59<6:23:58, 10.59s/it]                                                  {'loss': 5.9218, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [00:59<6:23:58, 10.59s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 6/2180 [01:09<6:10:52, 10.24s/it]                                                  {'loss': 5.6589, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [01:09<6:10:52, 10.24s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 7/2180 [01:18<6:02:37, 10.01s/it]                                                  {'loss': 5.4403, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2180 [01:18<6:02:37, 10.01s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
  0%|          | 8/2180 [01:28<5:57:08,  9.87s/it]                                                  {'loss': 5.1664, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2180 [01:28<5:57:08,  9.87s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
[2024-04-11 13:03:42,267] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1285427
Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/bin/deepspeed", line 6, in <module>
    main()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/launcher/runner.py", line 589, in main
    result.wait()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
[2024-04-11 13:03:43,042] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1285428
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1204, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2795, in training_step
    self.accelerator.backward(loss)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1955, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2019, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1204, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2795, in training_step
    self.accelerator.backward(loss)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1955, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2019, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[2024-04-11 13:03:44,011] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1285429
\n================================================= Thu Apr 11 05:03:44 PM UTC 2024 =========================================================\n
[2024-04-11 13:03:44,876] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1285430
[2024-04-11 13:03:45,554] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting
[2024-04-11 13:03:46,620] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:48,811] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=4,5,6,7: setting --include=localhost:4,5,6,7
[2024-04-11 13:03:48,811] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data/data1/akane/LLaVA/data/blip_laion_cc_sbu_558k.json --image_folder /data/data1/akane/LLaVA/data/blip_laion_558k --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --resampler_grid_size 24 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --tune_mm_resampler True --scaled_clip_residual True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/data1/akane/mve-clip-dino-scaler-pretrain-debug/checkpoints --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name mve-clip-dino-pretrain-7b
[2024-04-11 13:03:50,984] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:52,654] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}
[2024-04-11 13:03:52,654] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-11 13:03:52,654] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-11 13:03:52,654] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-11 13:03:52,654] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7
[2024-04-11 13:03:56,303] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:56,516] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:56,646] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:56,655] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 13:03:57,599] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:03:57,731] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:03:57,865] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:03:57,918] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 13:03:57,918] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Projector param: torch.Size([4096, 1024]), 4194304
Projector param: torch.Size([4096]), 4096
Projector param: torch.Size([4096, 4096]), 16777216
Projector param: torch.Size([4096]), 4096
Resampler param is trainable: True
Projector param is trainable: True
LLM param is trainable: True
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=27412481
frozen=7346291712
Formatting inputs...Skip in lazy mode
##### Scaler learnt value
True
##### Scaler learnt value
True
##### Scaler learnt value
True
##### Scaler learnt value
True
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240411_130521-sw3cu2wo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mve-clip-dino-pretrain-7b
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/sw3cu2wo
  0%|          | 0/2180 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])

Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2180 [00:21<13:07:45, 21.69s/it]                                                   {'loss': 10.0946, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:21<13:07:45, 21.69s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 2/2180 [00:31<8:46:39, 14.51s/it]                                                   {'loss': 9.8077, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:31<8:46:39, 14.51s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 3/2180 [00:40<7:24:34, 12.25s/it]                                                  {'loss': 8.6629, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [00:40<7:24:34, 12.25s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 4/2180 [00:50<6:46:15, 11.20s/it]                                                  {'loss': 7.3016, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [00:50<6:46:15, 11.20s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 5/2180 [00:59<6:24:23, 10.60s/it]                                                  {'loss': 6.1121, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [00:59<6:24:23, 10.60s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 6/2180 [01:09<6:10:56, 10.24s/it]                                                  {'loss': 6.0149, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [01:09<6:10:56, 10.24s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 7/2180 [01:18<6:02:48, 10.02s/it]                                                  {'loss': 5.8665, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2180 [01:18<6:02:48, 10.02s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 8/2180 [01:28<5:57:22,  9.87s/it]                                                  {'loss': 5.632, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2180 [01:28<5:57:22,  9.87s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])Found CLIP image features: torch.Size([32, 576, 1024])

Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
  0%|          | 9/2180 [01:38<5:53:39,  9.77s/it]                                                  {'loss': 5.4299, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2180 [01:38<5:53:39,  9.77s/it]Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Found CLIP image features: torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
Adding CLIP image features: torch.Size([32, 576, 1024]) + torch.Size([32, 576, 1024])
[2024-04-11 13:07:09,614] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1300979
Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1209, in wait
[2024-04-11 13:07:09,808] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1300979
    return self._wait(timeout=timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/bin/deepspeed", line 6, in <module>
    main()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/launcher/runner.py", line 589, in main
    result.wait()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
[2024-04-11 13:07:10,508] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1300980
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1204, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2795, in training_step
    self.accelerator.backward(loss)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1955, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2019, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1204, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2795, in training_step
    self.accelerator.backward(loss)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1847, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1955, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2019, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[2024-04-11 13:07:11,623] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1300981
[2024-04-11 13:07:12,374] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1300982
[2024-04-11 13:07:13,078] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting
