[2024-03-14 19:22:55,519] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
tensor([[    1,   450,  1967,  1401,   919, 29879,   263,   767, 13407,   373,
           263,   900,  8497,  1591,   297,   278,  1250,   310,   263, 13328,
          8818, 29875,  7776, 29889,   940,   338, 13587,   263,  7254,   528,
          2728, 29892, 10075, 10223,   292,   304, 13958,   372,   714,   310,
           278,  3474,   304, 15589, 29889,   450,  8818, 29875,   338, 19500,
          1623,   263, 19587, 11952, 29892,   411,   916, 24413,  1316,   408,
           263,   534,  2707,   322,   263,  1559,  7962,   297,   278,  9088,
         29889,   450,   767, 29915, 29879,  5412,  2602,   373,   278,   900,
          8497,  1591, 12778,   385,  8031,  1543,   304,   278,  1967, 29892,
          4332,  3864,   278,  8570,   310,  1209,   414,  1609, 29889,     2]],
       device='cuda:0')
torch.Size([1, 100])
The image depicts a man standing on a folding table in the back of a yellow taxi cab. He is holding a blue shirt, possibly preparing to hang it out of the window to dry. The taxi is driving down a busy street, with other vehicles such as a truck and a car visible in the scene. The man's unique position on the folding table adds an interesting element to the image, capturing the attention of passersby.
