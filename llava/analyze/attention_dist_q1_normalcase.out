
[2024-03-25 17:03:06,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>
What is odd about this image? ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline attached to a yellow car, which is driving down a busy street. It is unusual to see someone sitting on a clothesline, especially while the car is in motion. This scene is not only unconventional but also potentially dangerous, as the man's position on the clothesline could pose a risk to his safety and the safety of others on the road.
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.006081814236111111
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.05876736111111111
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.06848068237304683
Total image attn by gen tokens (avged per gen query) for layer 0: 0.6018204159206815
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2709315405951606

****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.010796440972222222
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.16710069444444445
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.1355932235717774
Total image attn by gen tokens (avged per gen query) for layer 1: 0.2557687335544162
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.441537348429362

****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.3269097222222222
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.8291666666666667
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.008572207556830462
Total image attn by gen tokens (avged per gen query) for layer 2: 0.050034056769476996
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.11222706900702582

****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.396875
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.8888888888888888
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.006822024451361815
Total image attn by gen tokens (avged per gen query) for layer 3: 0.02220638593037923
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.08208270072937011

****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.39166666666666666
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.8451388888888889
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.013249169455634233
Total image attn by gen tokens (avged per gen query) for layer 4: 0.02208434740702311
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.11952759424845377

****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.384375
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.7958333333333333
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.016901630825466623
Total image attn by gen tokens (avged per gen query) for layer 5: 0.030278958214653862
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.15698607762654623

****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.39895833333333336
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.8090277777777778
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.026496423615349644
Total image attn by gen tokens (avged per gen query) for layer 6: 0.031633332040574814
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13284246656629775

****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.39131944444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.7826388888888889
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.02464135752783883
Total image attn by gen tokens (avged per gen query) for layer 7: 0.03378717369503445
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15893257988823783

****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.36215277777777777
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.7263888888888889
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.03384123908148874
Total image attn by gen tokens (avged per gen query) for layer 8: 0.041338814629448786
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1984310574001736

****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.3486111111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.678125
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.03054312600029841
Total image attn by gen tokens (avged per gen query) for layer 9: 0.04938607745700412
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.2419457965426975

****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.31059027777777776
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.5989583333333334
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.0476372083028157
Total image attn by gen tokens (avged per gen query) for layer 10: 0.052620580461290145
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.3007838779025608

****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.33663194444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.6451388888888889
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.040090253618028385
Total image attn by gen tokens (avged per gen query) for layer 11: 0.06766157150268555
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.24710928599039714

****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.31753472222222223
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.6072916666666667
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.04151282310485841
Total image attn by gen tokens (avged per gen query) for layer 12: 0.049049064848158096
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.3021464453803168

****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.3220486111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.6177083333333333
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.046731652153862865
Total image attn by gen tokens (avged per gen query) for layer 13: 0.05759809282090929
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27796192169189454

****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.2798611111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.5392361111111111
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.04478432337443031
Total image attn by gen tokens (avged per gen query) for layer 14: 0.08552204767862956
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.330457517835829

****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.265625
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.5267361111111111
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.048622110154893705
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07064454820421007
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.35399723052978516

****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.2899305555555556
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.5729166666666666
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.04497527016533752
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07054893705579969
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.31155912611219616

****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.328125
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.6642361111111111
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.02841650644938147
Total image attn by gen tokens (avged per gen query) for layer 17: 0.07345089382595486
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.23389648861355253

****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.3611111111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.7270833333333333
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.024721336364746126
Total image attn by gen tokens (avged per gen query) for layer 18: 0.051807996961805554
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.19638733334011502

****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.36701388888888886
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.7340277777777777
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.019828383127848376
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06471113628811306
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.18143270280626084

****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.3645833333333333
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.7263888888888889
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.03129937383863664
Total image attn by gen tokens (avged per gen query) for layer 20: 0.08337450557284885
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.15893723169962565

****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.4059027777777778
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.8013888888888889
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.013828033871120832
Total image attn by gen tokens (avged per gen query) for layer 21: 0.06831540001763238
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.11646767722235786

****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.41215277777777776
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.7958333333333333
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.012557840347290086
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06667208671569824
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.12493673960367839

****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.4378472222222222
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.8326388888888889
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.010768432087368392
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04375033113691542
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.11284234788682726

****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.41180555555555554
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.7770833333333333
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.02444846895005967
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07684861289130317
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.12161958482530381

****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.4791666666666667
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.8979166666666667
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.012754782040913876
Total image attn by gen tokens (avged per gen query) for layer 25: 0.035640030437045625
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.0536885208553738

****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.42743055555555554
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.7930555555555555
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.019944037331475206
Total image attn by gen tokens (avged per gen query) for layer 26: 0.057569392522176105
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.12943101459079318

****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.4857638888888889
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.8763888888888889
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.009501474433475078
Total image attn by gen tokens (avged per gen query) for layer 27: 0.018230431609683566
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09587920506795247

****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.49444444444444446
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.8625
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.015614567862616599
Total image attn by gen tokens (avged per gen query) for layer 28: 0.03065037727355957
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.09123505486382379

****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.48368055555555556
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.8131944444444444
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.020557774437798396
Total image attn by gen tokens (avged per gen query) for layer 29: 0.058725028567843965
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.1075227525499132

****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.5267361111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.8548611111111111
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.014926674630906887
Total image attn by gen tokens (avged per gen query) for layer 30: 0.02733306090037028
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.10287915335761176

****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.32881944444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.6149305555555555
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.03889678319295252
Total image attn by gen tokens (avged per gen query) for layer 31: 0.09327959484524197
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.25289306640625

####### Avg image attn for all layers: 0.07632318805489274
####### Avg gen text attn for all layers: 0.19123464094267956
####### Avg prompt attn for all layers: 0.7018934461805554
####### Avg question attn for all layers: 0.030548724821872195




[2024-03-25 17:10:29,934] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:11:51,289] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:18:19,703] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
torch.Size([1, 74, 576])
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:19:32,966] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009]], dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06]], dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06]], dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06]], dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06]], dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06]], dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06]], dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06]], dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06]], dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05]], dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05]], dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05]], dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05]], dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05]], dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05]], dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05]], dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05]], dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05]], dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05]], dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05]], dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06]], dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06]], dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05]], dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05]], dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06]], dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05]], dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05]], dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05]], dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05]], dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05]], dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04]], dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:21:56,838] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:38:16,065] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
[2024-03-25 17:38:25,792] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range
[2024-03-25 17:39:31,431] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[1]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:40:11,772] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[1]
[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 209, in eval_model
    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
  File "/home/akane38/LLaVA/llava/mm_utils.py", line 190, in tokenizer_image_token
    print(prompt_chunks[2])
IndexError: list index out of range
[2024-03-25 17:43:04,981] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 209, in eval_model
    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
  File "/home/akane38/LLaVA/llava/mm_utils.py", line 195, in tokenizer_image_token
    print(insert_separator(prompt_chunks, [image_token_index] * (offset + 1)))
UnboundLocalError: local variable 'offset' referenced before assignment
[2024-03-25 17:43:35,158] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range
[2024-03-25 17:44:40,741] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:45:28,334] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[1, -200, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:46:04,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range
[2024-03-25 17:47:45,963] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range
[2024-03-25 17:49:24,630] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1], [-200, -200], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 270, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/analyze/record_attns.py", line 221, in eval_model
    output_ids = model.generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 160, in generate
    return super().generate(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 1479, in generate
    return self.greedy_search(
  File "/home/akane38/LLaVA/transformers/src/transformers/generation/utils.py", line 2316, in greedy_search
    outputs = self(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 94, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 385, in prepare_inputs_labels_for_multimodal
    inputs_emb_modalities[example_idx].append({"text" : 1})
IndexError: list index out of range
[2024-03-25 17:50:27,365] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1], [-200, -200], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:52:57,637] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1], [-200, -200], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[1, -200, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:56:14,681] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
<zip object at 0x7ffa30817e40>
[[1], [-200, -200], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
<zip object at 0x7ffa3057e680>
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
<zip object at 0x7ffa3057e680>
[1, -200, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 17:56:42,704] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[[1], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]

[([1], [-200, -200]), ([1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901], [-200, -200])]
[[1], [-200, -200], [1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]

[([1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901], [-200, -200])]
[[1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]]
[([1], [-200, -200]), ([1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901], [-200, -200])]
[1, -200, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 18:00:44,982] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
[-200, 1, 259, 13, 11889, 29901, 1724, 338, 7736, 1048, 445, 1967, 29973, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 319, 1799, 9047, 13566, 29901]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The oddity in this image is that there is a person sitting in front of a computer, but instead of using a keyboard and mouse, they are using a cell phone. This is considered odd because most people would use a computer for tasks that require typing and precision, but in this case, the person has chosen to use a cell phone.

tensor([[0.0008, 0.0010, 0.0010, 0.0011, 0.0010, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0009, 0.0011, 0.0012, 0.0013, 0.0012, 0.0021, 0.0010, 0.0010, 0.0010,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0013, 0.0012, 0.0022, 0.0010, 0.0010, 0.0010,
         0.0009],
        [0.0006, 0.0008, 0.0010, 0.0011, 0.0011, 0.0019, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0008, 0.0009, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0008, 0.0010, 0.0010, 0.0011, 0.0010, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0009, 0.0011, 0.0012, 0.0013, 0.0012, 0.0021, 0.0010, 0.0010, 0.0010,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0013, 0.0012, 0.0022, 0.0010, 0.0010, 0.0010,
         0.0009],
        [0.0006, 0.0008, 0.0010, 0.0011, 0.0011, 0.0019, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0008, 0.0009, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0035170403079710145
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5814333929531816
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.05829905081486353
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1324728260869565
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2277947301449983


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0026, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0028, 0.0004, 0.0003, 0.0002,
         0.0001],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0028, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0025, 0.0003, 0.0003, 0.0002,
         0.0001]], dtype=torch.float16)

tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0026, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0028, 0.0004, 0.0003, 0.0002,
         0.0001],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0028, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0025, 0.0003, 0.0003, 0.0002,
         0.0001]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.0010243291440217392
Total image attn by gen tokens (avged per gen query) for layer 1: 0.2428669307542884
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.1333134692648183
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.25407608695652173
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.3697435130243716


tensor([[6.7353e-06, 1.7345e-05, 3.3975e-06, 7.9870e-06, 8.1658e-06, 2.7714e-03,
         6.4135e-05, 4.2915e-05, 9.5963e-06, 7.8678e-06],
        [7.3314e-06, 1.8597e-05, 3.6359e-06, 8.2850e-06, 7.8082e-06, 2.6836e-03,
         5.7697e-05, 3.7491e-05, 9.2983e-06, 8.4043e-06],
        [3.0398e-06, 8.1062e-06, 1.4305e-06, 3.8147e-06, 3.5167e-06, 1.8120e-03,
         2.7955e-05, 1.6689e-05, 3.3975e-06, 3.4571e-06],
        [2.9802e-06, 7.9870e-06, 1.6093e-06, 4.1127e-06, 4.1723e-06, 1.7328e-03,
         3.3259e-05, 2.0683e-05, 4.5300e-06, 4.5896e-06],
        [3.6955e-06, 1.0073e-05, 2.0266e-06, 4.5896e-06, 4.4703e-06, 1.8120e-03,
         3.6955e-05, 2.2590e-05, 5.9009e-06, 6.6161e-06]], dtype=torch.float16)

tensor([[6.7353e-06, 1.7345e-05, 3.3975e-06, 7.9870e-06, 8.1658e-06, 2.7714e-03,
         6.4135e-05, 4.2915e-05, 9.5963e-06, 7.8678e-06],
        [7.3314e-06, 1.8597e-05, 3.6359e-06, 8.2850e-06, 7.8082e-06, 2.6836e-03,
         5.7697e-05, 3.7491e-05, 9.2983e-06, 8.4043e-06],
        [3.0398e-06, 8.1062e-06, 1.4305e-06, 3.8147e-06, 3.5167e-06, 1.8120e-03,
         2.7955e-05, 1.6689e-05, 3.3975e-06, 3.4571e-06],
        [2.9802e-06, 7.9870e-06, 1.6093e-06, 4.1127e-06, 4.1723e-06, 1.7328e-03,
         3.3259e-05, 2.0683e-05, 4.5300e-06, 4.5896e-06],
        [3.6955e-06, 1.0073e-05, 2.0266e-06, 4.5896e-06, 4.4703e-06, 1.8120e-03,
         3.6955e-05, 2.2590e-05, 5.9009e-06, 6.6161e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 5.2935835243999094e-05
Total image attn by gen tokens (avged per gen query) for layer 2: 0.0822632278221241
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.6912641939909561
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.022701539855072464
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.20377103833184726


tensor([[1.1921e-07, 2.3842e-07, 1.7881e-07, 5.3644e-07, 4.7684e-07, 7.3290e-04,
         2.2054e-06, 1.1325e-06, 3.5763e-07, 3.5763e-07],
        [2.3842e-07, 4.1723e-07, 2.3842e-07, 7.1526e-07, 6.5565e-07, 7.8487e-04,
         2.5630e-06, 1.4901e-06, 4.7684e-07, 4.7684e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 1.7881e-07, 1.7881e-07, 4.6682e-04,
         1.1325e-06, 7.7486e-07, 1.7881e-07, 1.7881e-07],
        [1.1921e-07, 2.3842e-07, 1.7881e-07, 4.7684e-07, 4.7684e-07, 7.0143e-04,
         2.3842e-06, 1.8477e-06, 2.9802e-07, 2.9802e-07],
        [1.1921e-07, 2.3842e-07, 1.1921e-07, 3.5763e-07, 3.5763e-07, 6.8569e-04,
         1.9670e-06, 1.0133e-06, 2.3842e-07, 2.9802e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 2.3842e-07, 1.7881e-07, 5.3644e-07, 4.7684e-07, 7.3290e-04,
         2.2054e-06, 1.1325e-06, 3.5763e-07, 3.5763e-07],
        [2.3842e-07, 4.1723e-07, 2.3842e-07, 7.1526e-07, 6.5565e-07, 7.8487e-04,
         2.5630e-06, 1.4901e-06, 4.7684e-07, 4.7684e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 1.7881e-07, 1.7881e-07, 4.6682e-04,
         1.1325e-06, 7.7486e-07, 1.7881e-07, 1.7881e-07],
        [1.1921e-07, 2.3842e-07, 1.7881e-07, 4.7684e-07, 4.7684e-07, 7.0143e-04,
         2.3842e-06, 1.8477e-06, 2.9802e-07, 2.9802e-07],
        [1.1921e-07, 2.3842e-07, 1.1921e-07, 3.5763e-07, 3.5763e-07, 6.8569e-04,
         1.9670e-06, 1.0133e-06, 2.3842e-07, 2.9802e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 1.933263695758322e-06
Total image attn by gen tokens (avged per gen query) for layer 3: 0.017366167427836986
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8600981408271237
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.009638247282608696
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.11289744446243065


tensor([[5.9605e-08, 1.7881e-07, 5.9605e-08, 1.7881e-07, 1.7881e-07, 5.5695e-04,
         1.0729e-06, 7.1526e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 2.3842e-07, 5.9605e-08, 2.3842e-07, 1.7881e-07, 6.5613e-04,
         1.0729e-06, 7.1526e-07, 2.9802e-07, 1.7881e-07],
        [5.9605e-08, 1.7881e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 4.4584e-04,
         6.5565e-07, 4.7684e-07, 2.3842e-07, 1.1921e-07],
        [0.0000e+00, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 3.5882e-04,
         5.3644e-07, 3.5763e-07, 1.7881e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.2568e-04,
         4.1723e-07, 2.9802e-07, 1.1921e-07, 5.9605e-08]], dtype=torch.float16)

tensor([[5.9605e-08, 1.7881e-07, 5.9605e-08, 1.7881e-07, 1.7881e-07, 5.5695e-04,
         1.0729e-06, 7.1526e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 2.3842e-07, 5.9605e-08, 2.3842e-07, 1.7881e-07, 6.5613e-04,
         1.0729e-06, 7.1526e-07, 2.9802e-07, 1.7881e-07],
        [5.9605e-08, 1.7881e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 4.4584e-04,
         6.5565e-07, 4.7684e-07, 2.3842e-07, 1.1921e-07],
        [0.0000e+00, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 3.5882e-04,
         5.3644e-07, 3.5763e-07, 1.7881e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.2568e-04,
         4.1723e-07, 2.9802e-07, 1.1921e-07, 5.9605e-08]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 4.2500703231148096e-07
Total image attn by gen tokens (avged per gen query) for layer 4: 0.011482358842656233
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.8335322409436322
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.010459125905797102
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.1445262743079144


tensor([[5.9605e-08, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 2.9016e-04,
         5.9605e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07],
        [1.7881e-07, 3.5763e-07, 1.7881e-07, 5.3644e-07, 4.7684e-07, 5.7793e-04,
         1.9670e-06, 1.1325e-06, 4.1723e-07, 2.9802e-07],
        [2.9802e-07, 5.9605e-07, 3.5763e-07, 1.0133e-06, 9.5367e-07, 8.3494e-04,
         3.2783e-06, 1.8477e-06, 7.7486e-07, 5.9605e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 3.5810e-04,
         7.7486e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.4652e-04,
         4.1723e-07, 2.3842e-07, 1.1921e-07, 5.9605e-08]], dtype=torch.float16)

tensor([[5.9605e-08, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 2.9016e-04,
         5.9605e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07],
        [1.7881e-07, 3.5763e-07, 1.7881e-07, 5.3644e-07, 4.7684e-07, 5.7793e-04,
         1.9670e-06, 1.1325e-06, 4.1723e-07, 2.9802e-07],
        [2.9802e-07, 5.9605e-07, 3.5763e-07, 1.0133e-06, 9.5367e-07, 8.3494e-04,
         3.2783e-06, 1.8477e-06, 7.7486e-07, 5.9605e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 3.5810e-04,
         7.7486e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.4652e-04,
         4.1723e-07, 2.3842e-07, 1.1921e-07, 5.9605e-08]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 5.571738533351732e-07
Total image attn by gen tokens (avged per gen query) for layer 5: 0.010047391704891039
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7934113887773044
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.017252604166666668
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1792886153511379


tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6631e-04,
         5.3644e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6679e-04,
         4.7684e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.1805e-04,
         5.9605e-07, 2.9802e-07, 1.1921e-07, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1648e-04,
         2.9802e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 1.7881e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.0208e-04,
         7.7486e-07, 3.5763e-07, 2.9802e-07, 1.7881e-07]], dtype=torch.float16)

tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6631e-04,
         5.3644e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6679e-04,
         4.7684e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.1805e-04,
         5.9605e-07, 2.9802e-07, 1.1921e-07, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1648e-04,
         2.9802e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 1.7881e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.0208e-04,
         7.7486e-07, 3.5763e-07, 2.9802e-07, 1.7881e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 2.211418704710145e-07
Total image attn by gen tokens (avged per gen query) for layer 6: 0.006766952466273653
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.7929504807444586
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.03535439311594203
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.16492817367332568


tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.5538e-04,
         1.7881e-06, 1.4305e-06, 2.3842e-07, 2.3842e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1720e-04,
         2.9802e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6035e-04,
         5.3644e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3854e-04,
         2.3842e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6785e-04,
         1.7881e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08]], dtype=torch.float16)

tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.5538e-04,
         1.7881e-06, 1.4305e-06, 2.3842e-07, 2.3842e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1720e-04,
         2.9802e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6035e-04,
         5.3644e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3854e-04,
         2.3842e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6785e-04,
         1.7881e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 1.0884326437245244e-07
Total image attn by gen tokens (avged per gen query) for layer 7: 0.005214465701061747
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7845197527304939
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.027414515398550724
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.18285126616989356


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8239e-04,
         5.9605e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.1768e-04,
         8.3447e-07, 4.1723e-07, 2.3842e-07, 2.9802e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00, 2.6298e-04,
         1.3113e-06, 7.7486e-07, 2.3842e-07, 2.9802e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4913e-04,
         4.1723e-07, 2.3842e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1450e-04,
         2.9802e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8239e-04,
         5.9605e-07, 2.9802e-07, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.1768e-04,
         8.3447e-07, 4.1723e-07, 2.3842e-07, 2.9802e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00, 2.6298e-04,
         1.3113e-06, 7.7486e-07, 2.3842e-07, 2.9802e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4913e-04,
         4.1723e-07, 2.3842e-07, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1450e-04,
         2.9802e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 1.3130298559216486e-07
Total image attn by gen tokens (avged per gen query) for layer 8: 0.005680567112521849
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7301473073337389
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.031533061594202896
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.23263906395953635


tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2187e-04,
         4.7684e-07, 3.5763e-07, 1.7881e-07, 1.7881e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9159e-04,
         5.3644e-07, 3.5763e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.9983e-04,
         9.5367e-07, 6.5565e-07, 3.5763e-07, 2.9802e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6608e-04,
         5.3644e-07, 3.5763e-07, 1.7881e-07, 1.7881e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5105e-04,
         3.5763e-07, 2.3842e-07, 1.7881e-07, 1.1921e-07]], dtype=torch.float16)

tensor([[5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2187e-04,
         4.7684e-07, 3.5763e-07, 1.7881e-07, 1.7881e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9159e-04,
         5.3644e-07, 3.5763e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.9983e-04,
         9.5367e-07, 6.5565e-07, 3.5763e-07, 2.9802e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6608e-04,
         5.3644e-07, 3.5763e-07, 1.7881e-07, 1.7881e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5105e-04,
         3.5763e-07, 2.3842e-07, 1.7881e-07, 1.1921e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 1.4339668163354846e-07
Total image attn by gen tokens (avged per gen query) for layer 9: 0.007712566334268321
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6717701562936755
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.042544157608695655
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.27797311976336053


tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.1509e-04,
         1.1325e-06, 7.7486e-07, 5.3644e-07, 4.1723e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6393e-04,
         6.5565e-07, 4.1723e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.2902e-04,
         9.5367e-07, 5.9605e-07, 4.1723e-07, 3.5763e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.7847e-04,
         7.7486e-07, 4.1723e-07, 4.1723e-07, 3.5763e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6035e-04,
         8.9407e-07, 5.3644e-07, 3.5763e-07, 2.9802e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.1509e-04,
         1.1325e-06, 7.7486e-07, 5.3644e-07, 4.1723e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.6393e-04,
         6.5565e-07, 4.1723e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.2902e-04,
         9.5367e-07, 5.9605e-07, 4.1723e-07, 3.5763e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.7847e-04,
         7.7486e-07, 4.1723e-07, 4.1723e-07, 3.5763e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6035e-04,
         8.9407e-07, 5.3644e-07, 3.5763e-07, 2.9802e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 3.0234240103459015e-07
Total image attn by gen tokens (avged per gen query) for layer 10: 0.009033577165741852
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.5993027747541234
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.045657835144927536
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.3460058129352072


tensor([[1.7881e-07, 2.3842e-07, 1.1921e-07, 1.7881e-07, 1.7881e-07, 5.6934e-04,
         2.7418e-06, 2.4438e-06, 1.3113e-06, 9.5367e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.2987e-04,
         2.5630e-06, 1.9073e-06, 1.4305e-06, 9.5367e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.0518e-04,
         1.3113e-06, 8.9407e-07, 6.5565e-07, 3.5763e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2566e-04,
         5.9605e-07, 3.5763e-07, 2.9802e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1018e-04,
         1.3709e-06, 9.5367e-07, 7.1526e-07, 4.1723e-07]], dtype=torch.float16)

tensor([[1.7881e-07, 2.3842e-07, 1.1921e-07, 1.7881e-07, 1.7881e-07, 5.6934e-04,
         2.7418e-06, 2.4438e-06, 1.3113e-06, 9.5367e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.2987e-04,
         2.5630e-06, 1.9073e-06, 1.4305e-06, 9.5367e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.0518e-04,
         1.3113e-06, 8.9407e-07, 6.5565e-07, 3.5763e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2566e-04,
         5.9605e-07, 3.5763e-07, 2.9802e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1018e-04,
         1.3709e-06, 9.5367e-07, 7.1526e-07, 4.1723e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 4.327815511952276e-07
Total image attn by gen tokens (avged per gen query) for layer 11: 0.019000041312065678
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6668004834133645
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.041072237318840576
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2731272379557292


tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.3167e-04,
         1.1325e-06, 8.9407e-07, 5.3644e-07, 2.9802e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.4131e-04,
         8.9407e-07, 5.9605e-07, 5.9605e-07, 3.5763e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7742e-04,
         5.9605e-07, 3.5763e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4618e-04,
         5.3644e-07, 3.5763e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.4465e-04,
         8.9407e-07, 5.9605e-07, 4.1723e-07, 2.3842e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.3167e-04,
         1.1325e-06, 8.9407e-07, 5.3644e-07, 2.9802e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.4131e-04,
         8.9407e-07, 5.9605e-07, 5.9605e-07, 3.5763e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7742e-04,
         5.9605e-07, 3.5763e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4618e-04,
         5.3644e-07, 3.5763e-07, 3.5763e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.4465e-04,
         8.9407e-07, 5.9605e-07, 4.1723e-07, 2.3842e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 2.099120098611583e-07
Total image attn by gen tokens (avged per gen query) for layer 12: 0.015934754109037094
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6188152803890947
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.045459692028985504
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.3197902734728827


tensor([[1.7881e-07, 2.3842e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 6.7663e-04,
         2.9802e-06, 1.8477e-06, 8.3447e-07, 4.7684e-07],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.7207e-04,
         1.1921e-06, 7.7486e-07, 7.7486e-07, 5.3644e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00, 3.9959e-04,
         8.3447e-07, 4.7684e-07, 4.1723e-07, 2.9802e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.4322e-04,
         1.1325e-06, 6.5565e-07, 5.9605e-07, 5.3644e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.6945e-04,
         1.4901e-06, 8.3447e-07, 6.5565e-07, 4.7684e-07]], dtype=torch.float16)

tensor([[1.7881e-07, 2.3842e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 6.7663e-04,
         2.9802e-06, 1.8477e-06, 8.3447e-07, 4.7684e-07],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 4.7207e-04,
         1.1921e-06, 7.7486e-07, 7.7486e-07, 5.3644e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00, 3.9959e-04,
         8.3447e-07, 4.7684e-07, 4.1723e-07, 2.9802e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.4322e-04,
         1.1325e-06, 6.5565e-07, 5.9605e-07, 5.3644e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.6945e-04,
         1.4901e-06, 8.3447e-07, 6.5565e-07, 4.7684e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 7.195749144623245e-07
Total image attn by gen tokens (avged per gen query) for layer 13: 0.019620090291120003
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.5895248668781224
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.05225317028985507
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.3386018725409024


tensor([[6.5565e-07, 6.5565e-07, 3.5763e-07, 7.1526e-07, 9.5367e-07, 1.0700e-03,
         4.8280e-06, 4.1723e-06, 1.8477e-06, 1.5497e-06],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9462e-04,
         1.0729e-06, 7.1526e-07, 9.5367e-07, 8.3447e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 5.1928e-04,
         1.4305e-06, 8.9407e-07, 1.0133e-06, 8.3447e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.1257e-04,
         6.5565e-07, 4.1723e-07, 6.5565e-07, 6.5565e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 4.2510e-04,
         8.9407e-07, 7.7486e-07, 7.1526e-07, 5.3644e-07]], dtype=torch.float16)

tensor([[6.5565e-07, 6.5565e-07, 3.5763e-07, 7.1526e-07, 9.5367e-07, 1.0700e-03,
         4.8280e-06, 4.1723e-06, 1.8477e-06, 1.5497e-06],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9462e-04,
         1.0729e-06, 7.1526e-07, 9.5367e-07, 8.3447e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 5.1928e-04,
         1.4305e-06, 8.9407e-07, 1.0133e-06, 8.3447e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.1257e-04,
         6.5565e-07, 4.1723e-07, 6.5565e-07, 6.5565e-07],
        [1.1921e-07, 1.1921e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 4.2510e-04,
         8.9407e-07, 7.7486e-07, 7.1526e-07, 5.3644e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 6.254168524258379e-07
Total image attn by gen tokens (avged per gen query) for layer 14: 0.029615630274233612
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5398349692856056
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.06261322463768115
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.3679361758024796


tensor([[1.7881e-07, 2.3842e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 4.6611e-04,
         7.7486e-07, 7.1526e-07, 1.0729e-06, 6.5565e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5490e-04,
         4.1723e-07, 2.9802e-07, 6.5565e-07, 4.7684e-07],
        [1.1921e-07, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.8290e-04,
         7.1526e-07, 7.1526e-07, 1.1921e-06, 1.1921e-06],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.7537e-04,
         4.1723e-07, 2.3842e-07, 5.9605e-07, 4.7684e-07],
        [1.7881e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.5824e-04,
         1.1325e-06, 7.7486e-07, 1.3709e-06, 8.9407e-07]], dtype=torch.float16)

tensor([[1.7881e-07, 2.3842e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 4.6611e-04,
         7.7486e-07, 7.1526e-07, 1.0729e-06, 6.5565e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5490e-04,
         4.1723e-07, 2.9802e-07, 6.5565e-07, 4.7684e-07],
        [1.1921e-07, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.8290e-04,
         7.1526e-07, 7.1526e-07, 1.1921e-06, 1.1921e-06],
        [5.9605e-08, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 2.7537e-04,
         4.1723e-07, 2.3842e-07, 5.9605e-07, 4.7684e-07],
        [1.7881e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.5824e-04,
         1.1325e-06, 7.7486e-07, 1.3709e-06, 8.9407e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 6.815661554751189e-07
Total image attn by gen tokens (avged per gen query) for layer 15: 0.022754508516062862
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5397831408873848
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.0675384963768116
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.3699238542197407


tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 5.5599e-04,
         8.9407e-07, 6.5565e-07, 1.2517e-06, 7.1526e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5872e-04,
         2.9802e-07, 1.7881e-07, 5.3644e-07, 4.1723e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3246e-04,
         1.1921e-07, 5.9605e-08, 2.9802e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1114e-04,
         1.7881e-07, 1.1921e-07, 3.5763e-07, 3.5763e-07],
        [1.1921e-07, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.8457e-04,
         2.9802e-07, 2.3842e-07, 5.9605e-07, 4.1723e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 1.1921e-07, 5.5599e-04,
         8.9407e-07, 6.5565e-07, 1.2517e-06, 7.1526e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5872e-04,
         2.9802e-07, 1.7881e-07, 5.3644e-07, 4.1723e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3246e-04,
         1.1921e-07, 5.9605e-08, 2.9802e-07, 2.3842e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1114e-04,
         1.7881e-07, 1.1921e-07, 3.5763e-07, 3.5763e-07],
        [1.1921e-07, 1.1921e-07, 0.0000e+00, 5.9605e-08, 5.9605e-08, 3.8457e-04,
         2.9802e-07, 2.3842e-07, 5.9605e-07, 4.1723e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 2.280525539232337e-07
Total image attn by gen tokens (avged per gen query) for layer 16: 0.018820679706075916
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5742626051971877
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.054064764492753624
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.35285195060398267


tensor([[2.3842e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 3.7837e-04,
         1.3113e-06, 1.0133e-06, 2.6226e-06, 1.4901e-06],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.7847e-04,
         3.5763e-07, 2.3842e-07, 6.5565e-07, 4.7684e-07],
        [1.1921e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.3329e-04,
         3.5763e-07, 2.3842e-07, 8.9407e-07, 7.7486e-07],
        [2.3842e-07, 2.9802e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 2.3186e-04,
         4.7684e-07, 4.7684e-07, 1.4305e-06, 1.1325e-06],
        [5.3644e-07, 5.9605e-07, 1.7881e-07, 1.7881e-07, 2.3842e-07, 2.9945e-04,
         1.3113e-06, 1.5497e-06, 3.4571e-06, 2.2054e-06]], dtype=torch.float16)

tensor([[2.3842e-07, 3.5763e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 3.7837e-04,
         1.3113e-06, 1.0133e-06, 2.6226e-06, 1.4901e-06],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.7847e-04,
         3.5763e-07, 2.3842e-07, 6.5565e-07, 4.7684e-07],
        [1.1921e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.3329e-04,
         3.5763e-07, 2.3842e-07, 8.9407e-07, 7.7486e-07],
        [2.3842e-07, 2.9802e-07, 5.9605e-08, 1.1921e-07, 1.1921e-07, 2.3186e-04,
         4.7684e-07, 4.7684e-07, 1.4305e-06, 1.1325e-06],
        [5.3644e-07, 5.9605e-07, 1.7881e-07, 1.7881e-07, 2.3842e-07, 2.9945e-04,
         1.3113e-06, 1.5497e-06, 3.4571e-06, 2.2054e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 1.4236007911571558e-06
Total image attn by gen tokens (avged per gen query) for layer 17: 0.03709454812865327
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6274572282597638
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.07620018115942029
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2592480424521626


tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9312e-04,
         7.7486e-07, 6.5565e-07, 1.3113e-06, 6.5565e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 0.0000e+00, 2.1160e-04,
         3.5763e-07, 2.3842e-07, 4.7684e-07, 3.5763e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 0.0000e+00, 0.0000e+00, 1.3411e-04,
         1.7881e-07, 1.1921e-07, 4.1723e-07, 2.9802e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.2529e-04,
         1.1921e-07, 1.1921e-07, 4.1723e-07, 2.9802e-07],
        [3.5763e-07, 5.9605e-07, 1.7881e-07, 1.1921e-07, 1.1921e-07, 1.4555e-04,
         4.7684e-07, 4.7684e-07, 1.0729e-06, 8.3447e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9312e-04,
         7.7486e-07, 6.5565e-07, 1.3113e-06, 6.5565e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 0.0000e+00, 2.1160e-04,
         3.5763e-07, 2.3842e-07, 4.7684e-07, 3.5763e-07],
        [5.9605e-08, 1.1921e-07, 5.9605e-08, 0.0000e+00, 0.0000e+00, 1.3411e-04,
         1.7881e-07, 1.1921e-07, 4.1723e-07, 2.9802e-07],
        [1.1921e-07, 1.7881e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.2529e-04,
         1.1921e-07, 1.1921e-07, 4.1723e-07, 2.9802e-07],
        [3.5763e-07, 5.9605e-07, 1.7881e-07, 1.1921e-07, 1.1921e-07, 1.4555e-04,
         4.7684e-07, 4.7684e-07, 1.0729e-06, 8.3447e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 2.08011571911798e-06
Total image attn by gen tokens (avged per gen query) for layer 18: 0.024125914642776268
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.702080464017564
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.051856884057971016
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.22193673728168875


tensor([[1.1921e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.6417e-04,
         2.9802e-07, 2.9802e-07, 1.0133e-06, 4.7684e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0444e-04,
         1.1921e-07, 1.1921e-07, 2.3842e-07, 1.7881e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0711e-04,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2804e-05,
         0.0000e+00, 0.0000e+00, 1.1921e-07, 1.1921e-07],
        [1.7881e-07, 2.9802e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.0444e-04,
         4.7684e-07, 6.5565e-07, 1.4305e-06, 7.7486e-07]], dtype=torch.float16)

tensor([[1.1921e-07, 2.3842e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.6417e-04,
         2.9802e-07, 2.9802e-07, 1.0133e-06, 4.7684e-07],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0444e-04,
         1.1921e-07, 1.1921e-07, 2.3842e-07, 1.7881e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0711e-04,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2804e-05,
         0.0000e+00, 0.0000e+00, 1.1921e-07, 1.1921e-07],
        [1.7881e-07, 2.9802e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 2.0444e-04,
         4.7684e-07, 6.5565e-07, 1.4305e-06, 7.7486e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 7.342601167982903e-07
Total image attn by gen tokens (avged per gen query) for layer 19: 0.02245155970255534
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7128298731817716
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.07104846014492754
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.19367010697074558


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3625e-05,
         1.1921e-07, 5.9605e-08, 2.3842e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1718e-04,
         1.7881e-07, 1.1921e-07, 2.9802e-07, 1.7881e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9366e-05,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4419e-05,
         5.9605e-08, 0.0000e+00, 1.1921e-07, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9288e-05,
         1.7881e-07, 1.1921e-07, 2.3842e-07, 1.1921e-07]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3625e-05,
         1.1921e-07, 5.9605e-08, 2.3842e-07, 1.1921e-07],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1718e-04,
         1.7881e-07, 1.1921e-07, 2.9802e-07, 1.7881e-07],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9366e-05,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 5.9605e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4419e-05,
         5.9605e-08, 0.0000e+00, 1.1921e-07, 5.9605e-08],
        [5.9605e-08, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9288e-05,
         1.7881e-07, 1.1921e-07, 2.3842e-07, 1.1921e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 4.20687855153844e-07
Total image attn by gen tokens (avged per gen query) for layer 20: 0.019089410270469776
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.727039072824561
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.06974637681159421
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.18412514009337494


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0169e-04,
         5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4712e-05,
         5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7624e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8174e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3685e-05,
         1.1921e-07, 1.1921e-07, 1.1921e-07, 5.9605e-08]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0169e-04,
         5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4712e-05,
         5.9605e-08, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7624e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8174e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 5.9605e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3685e-05,
         1.1921e-07, 1.1921e-07, 1.1921e-07, 5.9605e-08]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 3.109807553498641e-08
Total image attn by gen tokens (avged per gen query) for layer 21: 0.011379699776138084
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.8005622487137284
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.0625
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.1255580515101336


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0155e-05,
         0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8009e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0426e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1081e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4300e-05,
         0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0155e-05,
         0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8009e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0426e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1081e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4300e-05,
         0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 4.4055607007897416e-08
Total image attn by gen tokens (avged per gen query) for layer 22: 0.010198636331420013
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.786748511203821
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.057829483695652176
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.14522336876910666


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0160e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3557e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3975e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9147e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2723e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0160e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3557e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3975e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9147e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2723e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 1.9868214925130207e-08
Total image attn by gen tokens (avged per gen query) for layer 23: 0.008106900298077127
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8037433572437451
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.06006567028985507
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.12808407216832257


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8001e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7583e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2577e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1504e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2994e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8001e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7583e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2577e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1504e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2994e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 8.638354315274004e-10
Total image attn by gen tokens (avged per gen query) for layer 24: 0.005379996437957321
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7931535399478415
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.06442481884057971
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.1370416447736215


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6630e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5963e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5367e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6093e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3292e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6630e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5963e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5367e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6093e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3292e-05,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 8.638354315274004e-10
Total image attn by gen tokens (avged per gen query) for layer 25: 0.0048786488132200375
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8649108271667922
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.059329710144927536
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.07088081387506015


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5698e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8147e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5034e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5300e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6492e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5698e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8147e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5034e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5300e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6492e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.0
Total image attn by gen tokens (avged per gen query) for layer 26: 0.006171696427939594
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.7837043015853218
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.06538722826086957
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.144736773725869


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1127e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2054e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5167e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1590e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3246e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1127e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2054e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5167e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1590e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3246e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.0
Total image attn by gen tokens (avged per gen query) for layer 27: 0.0026847962020100026
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8297967262890028
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.061311141304347824
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.10620733620463937


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9670e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9407e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3447e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3113e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9670e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9407e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3447e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3113e-06,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.0
Total image attn by gen tokens (avged per gen query) for layer 28: 0.0030580810878587804
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8177471229995507
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.07370923913043478
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.10548555678215579


tensor([[1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.1723e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9802e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[1.1921e-07, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 4.1723e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9802e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3842e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 7.774518883746603e-09
Total image attn by gen tokens (avged per gen query) for layer 29: 0.008394917716150698
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7956170767977618
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.06906702898550725
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.12692097650058026


tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-08, 3.5763e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7881e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float16)

tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-08, 3.5763e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7881e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.0
Total image attn by gen tokens (avged per gen query) for layer 30: 0.007061325985452403
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.7834832253663436
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.08197463768115942
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.12748081096704456


tensor([[2.3842e-07, 2.3842e-07, 2.3842e-07, 2.3842e-07, 2.3842e-07, 2.8610e-06,
         1.1921e-07, 1.1921e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9670e-06,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9073e-06,
         2.9802e-07, 2.3842e-07, 9.5367e-07, 6.5565e-07],
        [2.3842e-07, 2.3842e-07, 1.7881e-07, 1.7881e-07, 1.1921e-07, 3.1590e-06,
         4.1723e-07, 2.3842e-07, 1.7881e-07, 1.1921e-07],
        [1.1921e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 2.7418e-06,
         1.1921e-07, 1.1921e-07, 1.7881e-07, 1.7881e-07]], dtype=torch.float16)

tensor([[2.3842e-07, 2.3842e-07, 2.3842e-07, 2.3842e-07, 2.3842e-07, 2.8610e-06,
         1.1921e-07, 1.1921e-07, 2.3842e-07, 1.7881e-07],
        [5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9670e-06,
         5.9605e-08, 5.9605e-08, 1.1921e-07, 1.1921e-07],
        [5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 5.9605e-08, 1.9073e-06,
         2.9802e-07, 2.3842e-07, 9.5367e-07, 6.5565e-07],
        [2.3842e-07, 2.3842e-07, 1.7881e-07, 1.7881e-07, 1.1921e-07, 3.1590e-06,
         4.1723e-07, 2.3842e-07, 1.7881e-07, 1.1921e-07],
        [1.1921e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 1.1921e-07, 2.7418e-06,
         1.1921e-07, 1.1921e-07, 1.7881e-07, 1.7881e-07]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 6.651532822760983e-07
Total image attn by gen tokens (avged per gen query) for layer 31: 0.10098404124163199
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.34036520944125415
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.28917572463768115
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.26947502467943274

####### Avg image attn for all layers: 0.04302104611111725
####### Avg gen text attn for all layers: 0.21283515229605243
####### Avg prompt attn for all layers: 0.06767914260643117
####### Avg question attn for all layers: 0.6764646589863992
[2024-03-25 18:20:48,188] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.

tensor([[0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0010,
         0.0008],
        [0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011, 0.0011,
         0.0010],
        [0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010, 0.0010,
         0.0009],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008],
        [0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010, 0.0009,
         0.0008]], dtype=torch.float16)

tensor([[0.0022, 0.0008, 0.0010, 0.0010, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,
         0.0010],
        [0.0024, 0.0009, 0.0011, 0.0012, 0.0012, 0.0012, 0.0021, 0.0010, 0.0011,
         0.0011],
        [0.0025, 0.0008, 0.0010, 0.0012, 0.0012, 0.0012, 0.0020, 0.0009, 0.0010,
         0.0010],
        [0.0023, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009],
        [0.0024, 0.0007, 0.0009, 0.0010, 0.0011, 0.0011, 0.0020, 0.0010, 0.0010,
         0.0009]], device='cuda:0', dtype=torch.float16)
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204


tensor([[0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003, 0.0002,
         0.0002],
        [0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004, 0.0003,
         0.0002],
        [0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003, 0.0002,
         0.0002]], dtype=torch.float16)

tensor([[0.0065, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0028, 0.0005, 0.0004,
         0.0003],
        [0.0073, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0031, 0.0004, 0.0003,
         0.0002],
        [0.0065, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0030, 0.0005, 0.0004,
         0.0003],
        [0.0076, 0.0001, 0.0003, 0.0001, 0.0002, 0.0002, 0.0027, 0.0004, 0.0003,
         0.0002]], device='cuda:0', dtype=torch.float16)
****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405


tensor([[1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05, 2.2964e-03,
         7.5340e-05, 4.5121e-05, 1.1027e-05, 9.0599e-06],
        [1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06, 2.2049e-03,
         7.2956e-05, 4.5955e-05, 1.3292e-05, 1.1384e-05],
        [1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06, 1.8883e-03,
         5.4359e-05, 3.8147e-05, 1.0133e-05, 8.9407e-06],
        [4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06, 1.2321e-03,
         3.3021e-05, 2.1040e-05, 4.2915e-06, 3.8147e-06],
        [5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06, 1.2665e-03,
         3.2246e-05, 2.1815e-05, 6.4969e-06, 6.0797e-06]], dtype=torch.float16)

tensor([[7.1729e-01, 1.3590e-05, 3.0458e-05, 4.2915e-06, 1.0014e-05, 1.0371e-05,
         2.2964e-03, 7.5340e-05, 4.5121e-05, 1.1027e-05],
        [7.3779e-01, 1.7047e-05, 3.9458e-05, 5.0068e-06, 1.0490e-05, 9.8944e-06,
         2.2049e-03, 7.2956e-05, 4.5955e-05, 1.3292e-05],
        [7.3584e-01, 1.1623e-05, 2.6464e-05, 3.5763e-06, 7.6890e-06, 7.0333e-06,
         1.8883e-03, 5.4359e-05, 3.8147e-05, 1.0133e-05],
        [6.7871e-01, 4.7088e-06, 1.0967e-05, 1.6093e-06, 4.0531e-06, 3.8743e-06,
         1.2321e-03, 3.3021e-05, 2.1040e-05, 4.2915e-06],
        [7.2754e-01, 5.9605e-06, 1.3411e-05, 1.7881e-06, 4.1723e-06, 3.9935e-06,
         1.2665e-03, 3.2246e-05, 2.1815e-05, 6.4969e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853


tensor([[4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06, 7.7963e-04,
         1.1146e-05, 6.9141e-06, 1.9670e-06, 1.7881e-06],
        [5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06, 8.3256e-04,
         1.1861e-05, 7.1526e-06, 3.6359e-06, 3.3975e-06],
        [4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06, 7.7057e-04,
         9.0003e-06, 6.7949e-06, 2.2054e-06, 2.2054e-06],
        [6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06, 9.5320e-04,
         1.1802e-05, 1.2398e-05, 2.8014e-06, 3.2187e-06],
        [3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06, 8.1873e-04,
         6.6757e-06, 4.8280e-06, 1.5497e-06, 1.6689e-06]], dtype=torch.float16)

tensor([[9.2773e-01, 4.7684e-06, 7.1526e-06, 1.1325e-06, 2.3246e-06, 2.1458e-06,
         7.7963e-04, 1.1146e-05, 6.9141e-06, 1.9670e-06],
        [8.8574e-01, 5.1260e-06, 1.1623e-05, 1.1325e-06, 2.6226e-06, 2.5034e-06,
         8.3256e-04, 1.1861e-05, 7.1526e-06, 3.6359e-06],
        [9.1748e-01, 4.3511e-06, 7.5698e-06, 7.7486e-07, 1.7881e-06, 1.7285e-06,
         7.7057e-04, 9.0003e-06, 6.7949e-06, 2.2054e-06],
        [8.5449e-01, 6.1989e-06, 7.5698e-06, 1.2517e-06, 2.8014e-06, 2.6226e-06,
         9.5320e-04, 1.1802e-05, 1.2398e-05, 2.8014e-06],
        [8.6084e-01, 3.8743e-06, 5.9605e-06, 7.1526e-07, 1.5497e-06, 1.4305e-06,
         8.1873e-04, 6.6757e-06, 4.8280e-06, 1.5497e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552


tensor([[6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06, 6.5088e-04,
         1.1861e-05, 7.4506e-06, 4.0531e-06, 2.3842e-06],
        [3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06, 7.5197e-04,
         9.0599e-06, 5.9605e-06, 2.6226e-06, 1.7285e-06],
        [5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06, 7.4148e-04,
         1.2040e-05, 9.4771e-06, 5.0664e-06, 2.5034e-06],
        [2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07, 4.2009e-04,
         4.7684e-06, 3.6359e-06, 1.7285e-06, 8.9407e-07],
        [3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06, 5.6839e-04,
         6.3181e-06, 4.3511e-06, 3.3379e-06, 1.4901e-06]], dtype=torch.float16)

tensor([[8.9502e-01, 6.0201e-06, 9.1195e-06, 9.5367e-07, 1.8477e-06, 1.7881e-06,
         6.5088e-04, 1.1861e-05, 7.4506e-06, 4.0531e-06],
        [8.7158e-01, 3.9339e-06, 6.9737e-06, 7.7486e-07, 1.5497e-06, 1.4901e-06,
         7.5197e-04, 9.0599e-06, 5.9605e-06, 2.6226e-06],
        [8.0957e-01, 5.3644e-06, 1.1325e-05, 1.3709e-06, 2.7418e-06, 2.5630e-06,
         7.4148e-04, 1.2040e-05, 9.4771e-06, 5.0664e-06],
        [8.0371e-01, 2.2054e-06, 4.0531e-06, 4.7684e-07, 9.5367e-07, 9.5367e-07,
         4.2009e-04, 4.7684e-06, 3.6359e-06, 1.7285e-06],
        [8.3350e-01, 3.0398e-06, 6.6757e-06, 5.9605e-07, 1.2517e-06, 1.1325e-06,
         5.6839e-04, 6.3181e-06, 4.3511e-06, 3.3379e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227


tensor([[7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06, 4.8518e-04,
         1.4365e-05, 9.0003e-06, 5.0664e-06, 2.6822e-06],
        [9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06, 8.4543e-04,
         3.0458e-05, 1.8716e-05, 8.2850e-06, 4.1723e-06],
        [6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06, 5.7936e-04,
         1.6809e-05, 1.0610e-05, 5.1856e-06, 2.5630e-06],
        [5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06, 5.6410e-04,
         1.5259e-05, 8.9407e-06, 4.5300e-06, 2.3246e-06],
        [5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06, 5.0545e-04,
         1.2100e-05, 7.3910e-06, 3.8743e-06, 2.0862e-06]], dtype=torch.float16)

tensor([[8.4961e-01, 7.7486e-06, 1.3709e-05, 1.4305e-06, 2.7418e-06, 2.5630e-06,
         4.8518e-04, 1.4365e-05, 9.0003e-06, 5.0664e-06],
        [7.5049e-01, 9.0003e-06, 1.9491e-05, 2.5034e-06, 4.8876e-06, 4.6492e-06,
         8.4543e-04, 3.0458e-05, 1.8716e-05, 8.2850e-06],
        [7.9053e-01, 6.0797e-06, 1.3292e-05, 1.3113e-06, 2.6226e-06, 2.5034e-06,
         5.7936e-04, 1.6809e-05, 1.0610e-05, 5.1856e-06],
        [7.6953e-01, 5.4836e-06, 1.0610e-05, 1.1325e-06, 2.3842e-06, 2.2650e-06,
         5.6410e-04, 1.5259e-05, 8.9407e-06, 4.5300e-06],
        [8.2568e-01, 5.2452e-06, 1.0371e-05, 1.0133e-06, 2.0266e-06, 1.9670e-06,
         5.0545e-04, 1.2100e-05, 7.3910e-06, 3.8743e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825


tensor([[4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06, 5.3835e-04,
         1.9431e-05, 1.0371e-05, 3.5167e-06, 1.8477e-06],
        [5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06, 5.4646e-04,
         2.2888e-05, 1.1086e-05, 6.1393e-06, 2.2650e-06],
        [5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06, 5.6458e-04,
         1.8477e-05, 8.9407e-06, 5.1260e-06, 2.3842e-06],
        [5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06, 6.5899e-04,
         1.5914e-05, 7.6890e-06, 4.5300e-06, 2.0266e-06],
        [8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06, 6.7520e-04,
         2.6107e-05, 1.2994e-05, 8.7619e-06, 3.5167e-06]], dtype=torch.float16)

tensor([[7.8076e-01, 4.0531e-06, 1.0252e-05, 5.9605e-07, 1.0729e-06, 1.0133e-06,
         5.3835e-04, 1.9431e-05, 1.0371e-05, 3.5167e-06],
        [7.3486e-01, 5.2452e-06, 1.3888e-05, 7.1526e-07, 1.4305e-06, 1.3113e-06,
         5.4646e-04, 2.2888e-05, 1.1086e-05, 6.1393e-06],
        [7.6221e-01, 5.1260e-06, 1.1146e-05, 7.1526e-07, 1.3709e-06, 1.2517e-06,
         5.6458e-04, 1.8477e-05, 8.9407e-06, 5.1260e-06],
        [7.5195e-01, 5.3644e-06, 1.0729e-05, 7.1526e-07, 1.2517e-06, 1.1921e-06,
         6.5899e-04, 1.5914e-05, 7.6890e-06, 4.5300e-06],
        [7.2803e-01, 8.8811e-06, 2.2769e-05, 7.7486e-07, 1.4305e-06, 1.3113e-06,
         6.7520e-04, 2.6107e-05, 1.2994e-05, 8.7619e-06]], device='cuda:0',
       dtype=torch.float16)
****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345


tensor([[2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06, 1.0338e-03,
         4.1902e-05, 2.1517e-05, 1.3769e-05, 1.1146e-05],
        [1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06, 5.3930e-04,
         1.9372e-05, 9.2387e-06, 9.1195e-06, 5.4836e-06],
        [2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06, 6.5613e-04,
         1.8001e-05, 9.9540e-06, 1.0490e-05, 7.3910e-06],
        [1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07, 6.3896e-04,
         1.3947e-05, 6.0201e-06, 5.8413e-06, 4.4703e-06],
        [1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07, 5.0974e-04,
         1.2994e-05, 6.0201e-06, 1.0252e-05, 5.4240e-06]], dtype=torch.float16)

tensor([[8.0371e-01, 2.9743e-05, 3.0398e-05, 1.7881e-06, 2.6226e-06, 2.4438e-06,
         1.0338e-03, 4.1902e-05, 2.1517e-05, 1.3769e-05],
        [7.7002e-01, 1.1683e-05, 1.9550e-05, 8.3447e-07, 1.3113e-06, 1.1921e-06,
         5.3930e-04, 1.9372e-05, 9.2387e-06, 9.1195e-06],
        [7.6270e-01, 2.1279e-05, 2.2054e-05, 1.0133e-06, 1.5497e-06, 1.4305e-06,
         6.5613e-04, 1.8001e-05, 9.9540e-06, 1.0490e-05],
        [7.7100e-01, 1.5497e-05, 1.5199e-05, 5.9605e-07, 9.5367e-07, 8.9407e-07,
         6.3896e-04, 1.3947e-05, 6.0201e-06, 5.8413e-06],
        [7.6855e-01, 1.6212e-05, 2.9147e-05, 5.9605e-07, 8.9407e-07, 8.9407e-07,
         5.0974e-04, 1.2994e-05, 6.0201e-06, 1.0252e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032


tensor([[2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06, 5.4026e-04,
         9.2804e-05, 2.2113e-05, 1.3351e-05, 1.0431e-05],
        [2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06, 5.8556e-04,
         5.2094e-05, 1.3351e-05, 1.6332e-05, 1.2457e-05],
        [2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07, 3.6764e-04,
         2.6822e-05, 8.3447e-06, 1.3053e-05, 8.2254e-06],
        [1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07, 3.7527e-04,
         2.5570e-05, 6.9737e-06, 7.5698e-06, 5.2452e-06],
        [3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06, 5.1022e-04,
         6.1214e-05, 1.6928e-05, 1.8597e-05, 1.1921e-05]], dtype=torch.float16)

tensor([[7.8320e-01, 2.8729e-05, 4.0531e-05, 1.4305e-06, 1.7881e-06, 1.6093e-06,
         5.4026e-04, 9.2804e-05, 2.2113e-05, 1.3351e-05],
        [6.9971e-01, 2.9981e-05, 4.8339e-05, 1.3709e-06, 2.2054e-06, 2.0862e-06,
         5.8556e-04, 5.2094e-05, 1.3351e-05, 1.6332e-05],
        [7.4854e-01, 2.0623e-05, 3.4511e-05, 5.9605e-07, 8.9407e-07, 8.3447e-07,
         3.6764e-04, 2.6822e-05, 8.3447e-06, 1.3053e-05],
        [7.4170e-01, 1.4424e-05, 1.9968e-05, 4.7684e-07, 7.1526e-07, 6.5565e-07,
         3.7527e-04, 2.5570e-05, 6.9737e-06, 7.5698e-06],
        [7.5293e-01, 3.0756e-05, 4.7207e-05, 1.5497e-06, 2.0862e-06, 1.9670e-06,
         5.1022e-04, 6.1214e-05, 1.6928e-05, 1.8597e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157


tensor([[3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06, 8.9598e-04,
         8.3447e-05, 1.9372e-05, 2.2471e-05, 9.8348e-06],
        [2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06, 7.1907e-04,
         6.6400e-05, 1.2994e-05, 1.5676e-05, 7.5698e-06],
        [2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06, 6.2799e-04,
         4.8161e-05, 1.1027e-05, 1.7226e-05, 7.6294e-06],
        [2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06, 6.5422e-04,
         6.1929e-05, 1.3530e-05, 1.2577e-05, 7.2122e-06],
        [3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06, 1.0376e-03,
         1.3113e-04, 3.1471e-05, 2.5094e-05, 1.3888e-05]], dtype=torch.float16)

tensor([[6.9141e-01, 3.2008e-05, 6.5982e-05, 2.2650e-06, 2.9802e-06, 2.8014e-06,
         8.9598e-04, 8.3447e-05, 1.9372e-05, 2.2471e-05],
        [7.6318e-01, 2.6882e-05, 5.2512e-05, 1.4305e-06, 2.0862e-06, 1.9670e-06,
         7.1907e-04, 6.6400e-05, 1.2994e-05, 1.5676e-05],
        [7.9004e-01, 2.6762e-05, 4.9591e-05, 1.3113e-06, 1.6689e-06, 1.5497e-06,
         6.2799e-04, 4.8161e-05, 1.1027e-05, 1.7226e-05],
        [7.3291e-01, 2.6941e-05, 3.8028e-05, 1.1325e-06, 1.4901e-06, 1.3709e-06,
         6.5422e-04, 6.1929e-05, 1.3530e-05, 1.2577e-05],
        [6.3574e-01, 3.8624e-05, 8.4817e-05, 3.3975e-06, 4.4107e-06, 4.1723e-06,
         1.0376e-03, 1.3113e-04, 3.1471e-05, 2.5094e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025


tensor([[9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06, 1.0843e-03,
         1.7679e-04, 4.2140e-05, 5.1022e-05, 2.8372e-05],
        [6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06, 6.6853e-04,
         1.1623e-04, 2.0504e-05, 2.6822e-05, 1.4186e-05],
        [8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06, 5.2404e-04,
         6.9201e-05, 1.4782e-05, 3.9995e-05, 2.3842e-05],
        [6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06, 6.6519e-04,
         1.0192e-04, 2.2352e-05, 3.7372e-05, 2.2292e-05],
        [7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06, 8.2445e-04,
         1.1599e-04, 2.2411e-05, 4.5300e-05, 2.5928e-05]], dtype=torch.float16)

tensor([[6.0645e-01, 9.5546e-05, 1.1176e-04, 5.7817e-06, 7.4506e-06, 7.0333e-06,
         1.0843e-03, 1.7679e-04, 4.2140e-05, 5.1022e-05],
        [6.8896e-01, 6.5029e-05, 7.5102e-05, 2.5630e-06, 3.5763e-06, 3.2187e-06,
         6.6853e-04, 1.1623e-04, 2.0504e-05, 2.6822e-05],
        [7.4609e-01, 8.1658e-05, 8.4162e-05, 2.2054e-06, 2.9802e-06, 2.5034e-06,
         5.2404e-04, 6.9201e-05, 1.4782e-05, 3.9995e-05],
        [6.5771e-01, 6.5386e-05, 7.1347e-05, 2.2054e-06, 3.3379e-06, 3.1590e-06,
         6.6519e-04, 1.0192e-04, 2.2352e-05, 3.7372e-05],
        [5.5664e-01, 7.5936e-05, 1.0532e-04, 3.1590e-06, 4.2319e-06, 4.0531e-06,
         8.2445e-04, 1.1599e-04, 2.2411e-05, 4.5300e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655


tensor([[1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05, 1.4458e-03,
         1.8156e-04, 3.3796e-05, 5.1081e-05, 2.6703e-05],
        [1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05, 1.1215e-03,
         1.6725e-04, 3.2365e-05, 6.3658e-05, 3.5226e-05],
        [8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06, 7.3576e-04,
         4.8935e-05, 8.9407e-06, 2.8491e-05, 1.6332e-05],
        [7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06, 7.2384e-04,
         7.4804e-05, 1.3113e-05, 1.9610e-05, 1.1444e-05],
        [1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05, 1.2503e-03,
         1.4484e-04, 2.9862e-05, 4.1604e-05, 2.3961e-05]], dtype=torch.float16)

tensor([[6.6064e-01, 1.3220e-04, 1.3888e-04, 9.6560e-06, 1.3530e-05, 1.2636e-05,
         1.4458e-03, 1.8156e-04, 3.3796e-05, 5.1081e-05],
        [6.7090e-01, 1.4937e-04, 1.7023e-04, 9.4771e-06, 1.2934e-05, 1.2040e-05,
         1.1215e-03, 1.6725e-04, 3.2365e-05, 6.3658e-05],
        [7.0312e-01, 8.6010e-05, 8.5771e-05, 4.2915e-06, 6.0797e-06, 5.5432e-06,
         7.3576e-04, 4.8935e-05, 8.9407e-06, 2.8491e-05],
        [6.8701e-01, 7.2896e-05, 6.6042e-05, 2.6822e-06, 3.9339e-06, 3.6359e-06,
         7.2384e-04, 7.4804e-05, 1.3113e-05, 1.9610e-05],
        [6.3770e-01, 1.1259e-04, 1.3113e-04, 9.0599e-06, 1.0967e-05, 1.0192e-05,
         1.2503e-03, 1.4484e-04, 2.9862e-05, 4.1604e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589


tensor([[8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06, 1.2093e-03,
         8.3685e-05, 1.1861e-05, 3.0696e-05, 1.7881e-05],
        [1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06, 1.0958e-03,
         9.3997e-05, 1.5438e-05, 3.8862e-05, 2.0802e-05],
        [9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06, 8.3113e-04,
         4.8459e-05, 6.7949e-06, 2.7061e-05, 1.8239e-05],
        [8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06, 8.2874e-04,
         5.1200e-05, 7.8678e-06, 2.7180e-05, 1.9670e-05],
        [1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06, 1.1501e-03,
         1.0091e-04, 1.8775e-05, 3.2902e-05, 2.8074e-05]], dtype=torch.float16)

tensor([[6.0791e-01, 8.7976e-05, 7.5161e-05, 6.4969e-06, 8.2850e-06, 7.9870e-06,
         1.2093e-03, 8.3685e-05, 1.1861e-05, 3.0696e-05],
        [6.2207e-01, 1.1003e-04, 1.2565e-04, 6.6161e-06, 8.4043e-06, 7.8678e-06,
         1.0958e-03, 9.3997e-05, 1.5438e-05, 3.8862e-05],
        [6.3818e-01, 9.5606e-05, 1.0151e-04, 5.0068e-06, 5.7817e-06, 5.6028e-06,
         8.3113e-04, 4.8459e-05, 6.7949e-06, 2.7061e-05],
        [6.0693e-01, 8.8573e-05, 9.3758e-05, 3.2187e-06, 3.4571e-06, 3.2783e-06,
         8.2874e-04, 5.1200e-05, 7.8678e-06, 2.7180e-05],
        [6.3184e-01, 1.0753e-04, 9.5010e-05, 7.2718e-06, 8.4043e-06, 7.9274e-06,
         1.1501e-03, 1.0091e-04, 1.8775e-05, 3.2902e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984


tensor([[2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06, 1.2960e-03,
         1.2994e-04, 2.0802e-05, 5.5730e-05, 3.5465e-05],
        [2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06, 1.0548e-03,
         8.4162e-05, 1.5795e-05, 6.7651e-05, 4.0531e-05],
        [3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06, 8.3447e-04,
         5.2273e-05, 8.2254e-06, 7.7426e-05, 4.5359e-05],
        [3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06, 9.1696e-04,
         7.4685e-05, 1.4842e-05, 7.7546e-05, 4.8518e-05],
        [3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06, 1.0242e-03,
         7.8321e-05, 1.6153e-05, 7.2539e-05, 4.9412e-05]], dtype=torch.float16)

tensor([[5.2734e-01, 2.6608e-04, 1.1224e-04, 5.6028e-06, 7.2122e-06, 6.4969e-06,
         1.2960e-03, 1.2994e-04, 2.0802e-05, 5.5730e-05],
        [6.5723e-01, 2.6822e-04, 1.6153e-04, 4.3511e-06, 5.1260e-06, 4.6492e-06,
         1.0548e-03, 8.4162e-05, 1.5795e-05, 6.7651e-05],
        [5.4395e-01, 3.2210e-04, 1.3912e-04, 5.6624e-06, 5.9009e-06, 5.0664e-06,
         8.3447e-04, 5.2273e-05, 8.2254e-06, 7.7426e-05],
        [5.2734e-01, 3.3474e-04, 1.4985e-04, 6.3777e-06, 6.9141e-06, 6.1393e-06,
         9.1696e-04, 7.4685e-05, 1.4842e-05, 7.7546e-05],
        [5.2588e-01, 3.1257e-04, 1.5700e-04, 7.8082e-06, 9.1195e-06, 8.2850e-06,
         1.0242e-03, 7.8321e-05, 1.6153e-05, 7.2539e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786


tensor([[3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05, 1.9245e-03,
         3.9029e-04, 4.1127e-05, 1.7405e-04, 6.7890e-05],
        [3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06, 1.4286e-03,
         1.7893e-04, 1.6987e-05, 1.3530e-04, 5.5194e-05],
        [4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06, 9.3746e-04,
         8.0943e-05, 6.8545e-06, 1.4627e-04, 7.3254e-05],
        [3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06, 6.5184e-04,
         5.0187e-05, 5.5432e-06, 9.3997e-05, 4.6730e-05],
        [3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06, 9.8324e-04,
         1.1760e-04, 1.7643e-05, 1.5199e-04, 5.2929e-05]], dtype=torch.float16)

tensor([[4.4507e-01, 3.4952e-04, 1.5521e-04, 1.5259e-05, 1.6928e-05, 1.5080e-05,
         1.9245e-03, 3.9029e-04, 4.1127e-05, 1.7405e-04],
        [6.1377e-01, 3.6049e-04, 1.4639e-04, 7.8082e-06, 8.7619e-06, 7.6294e-06,
         1.4286e-03, 1.7893e-04, 1.6987e-05, 1.3530e-04],
        [5.6055e-01, 4.0889e-04, 1.2577e-04, 1.0133e-05, 9.5963e-06, 7.6890e-06,
         9.3746e-04, 8.0943e-05, 6.8545e-06, 1.4627e-04],
        [5.0391e-01, 3.0851e-04, 1.1504e-04, 4.7684e-06, 4.5896e-06, 3.7551e-06,
         6.5184e-04, 5.0187e-05, 5.5432e-06, 9.3997e-05],
        [6.4355e-01, 3.3307e-04, 1.6284e-04, 7.0930e-06, 7.3314e-06, 6.6161e-06,
         9.8324e-04, 1.1760e-04, 1.7643e-05, 1.5199e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136


tensor([[1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06, 1.0462e-03,
         8.8215e-05, 1.1563e-05, 8.4400e-05, 2.8014e-05],
        [1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06, 1.0643e-03,
         7.7605e-05, 9.6560e-06, 8.2910e-05, 3.1888e-05],
        [1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06, 7.6437e-04,
         3.6001e-05, 4.4107e-06, 1.1569e-04, 4.5419e-05],
        [1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06, 5.8222e-04,
         2.8253e-05, 3.6955e-06, 7.8738e-05, 3.0220e-05],
        [2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06, 8.5115e-04,
         7.0035e-05, 1.2696e-05, 1.6630e-04, 5.0247e-05]], dtype=torch.float16)

tensor([[4.8120e-01, 1.4424e-04, 9.4771e-05, 5.7817e-06, 8.1062e-06, 6.9737e-06,
         1.0462e-03, 8.8215e-05, 1.1563e-05, 8.4400e-05],
        [6.3086e-01, 1.5390e-04, 9.8050e-05, 5.6624e-06, 6.5565e-06, 5.7220e-06,
         1.0643e-03, 7.7605e-05, 9.6560e-06, 8.2910e-05],
        [5.5566e-01, 1.8263e-04, 1.0097e-04, 6.9737e-06, 6.7353e-06, 5.6624e-06,
         7.6437e-04, 3.6001e-05, 4.4107e-06, 1.1569e-04],
        [5.6299e-01, 1.3554e-04, 8.0526e-05, 3.7551e-06, 3.4571e-06, 2.9802e-06,
         5.8222e-04, 2.8253e-05, 3.6955e-06, 7.8738e-05],
        [5.0781e-01, 2.1887e-04, 1.3614e-04, 7.0333e-06, 7.3314e-06, 6.3181e-06,
         8.5115e-04, 7.0035e-05, 1.2696e-05, 1.6630e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293


tensor([[1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06, 9.8610e-04,
         1.9431e-04, 1.9550e-05, 9.9719e-05, 2.2054e-05],
        [1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06, 1.1234e-03,
         1.6522e-04, 1.9729e-05, 1.3781e-04, 3.0816e-05],
        [1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06, 7.6866e-04,
         6.8903e-05, 8.8215e-06, 1.9348e-04, 4.2140e-05],
        [1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06, 7.0810e-04,
         1.5068e-04, 2.6584e-05, 1.0622e-04, 2.5868e-05],
        [1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06, 7.4768e-04,
         1.6987e-04, 3.4153e-05, 1.2267e-04, 2.8193e-05]], dtype=torch.float16)

tensor([[4.2603e-01, 1.2189e-04, 7.1347e-05, 3.0398e-06, 6.3181e-06, 5.4836e-06,
         9.8610e-04, 1.9431e-04, 1.9550e-05, 9.9719e-05],
        [5.8008e-01, 1.7202e-04, 9.0539e-05, 3.5167e-06, 5.8413e-06, 4.8876e-06,
         1.1234e-03, 1.6522e-04, 1.9729e-05, 1.3781e-04],
        [4.9097e-01, 1.7953e-04, 1.0532e-04, 3.6359e-06, 5.1856e-06, 4.2915e-06,
         7.6866e-04, 6.8903e-05, 8.8215e-06, 1.9348e-04],
        [4.6606e-01, 1.2517e-04, 8.9645e-05, 2.9802e-06, 4.1127e-06, 3.3975e-06,
         7.0810e-04, 1.5068e-04, 2.6584e-05, 1.0622e-04],
        [4.7144e-01, 1.2338e-04, 8.0585e-05, 3.8147e-06, 5.8413e-06, 4.9472e-06,
         7.4768e-04, 1.6987e-04, 3.4153e-05, 1.2267e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175


tensor([[2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05, 6.0844e-04,
         1.0848e-04, 1.4186e-05, 2.3282e-04, 4.0948e-05],
        [2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05, 5.2309e-04,
         8.7440e-05, 1.0729e-05, 1.8144e-04, 4.3213e-05],
        [2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05, 4.8280e-04,
         5.6088e-05, 8.0466e-06, 2.4748e-04, 5.6744e-05],
        [2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05, 4.2844e-04,
         8.7321e-05, 1.4007e-05, 1.7202e-04, 3.2127e-05],
        [3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05, 4.6396e-04,
         8.6308e-05, 2.7001e-05, 2.1434e-04, 3.8326e-05]], dtype=torch.float16)

tensor([[5.0293e-01, 2.7227e-04, 1.9598e-04, 1.4722e-05, 1.7345e-05, 1.3888e-05,
         6.0844e-04, 1.0848e-04, 1.4186e-05, 2.3282e-04],
        [7.0215e-01, 2.4438e-04, 1.4281e-04, 1.2696e-05, 1.5020e-05, 1.2159e-05,
         5.2309e-04, 8.7440e-05, 1.0729e-05, 1.8144e-04],
        [6.8604e-01, 2.7871e-04, 1.6248e-04, 1.5020e-05, 1.8477e-05, 1.5736e-05,
         4.8280e-04, 5.6088e-05, 8.0466e-06, 2.4748e-04],
        [6.4307e-01, 2.1875e-04, 1.3661e-04, 1.0610e-05, 1.3649e-05, 1.0729e-05,
         4.2844e-04, 8.7321e-05, 1.4007e-05, 1.7202e-04],
        [6.3574e-01, 3.8338e-04, 2.0087e-04, 1.4663e-05, 1.7524e-05, 1.4067e-05,
         4.6396e-04, 8.6308e-05, 2.7001e-05, 2.1434e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129


tensor([[9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06, 4.0627e-04,
         9.9599e-05, 1.3947e-05, 7.4148e-05, 1.5378e-05],
        [9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06, 5.0068e-04,
         8.5473e-05, 1.3888e-05, 6.6042e-05, 1.4186e-05],
        [1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06, 3.6001e-04,
         5.0068e-05, 9.0599e-06, 5.2452e-05, 1.5736e-05],
        [8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06, 2.9826e-04,
         5.6684e-05, 1.3292e-05, 4.9055e-05, 9.7156e-06],
        [1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06, 3.5262e-04,
         7.6354e-05, 2.7716e-05, 9.8467e-05, 2.6405e-05]], dtype=torch.float16)

tensor([[5.9814e-01, 9.1910e-05, 7.1883e-05, 4.3511e-06, 5.1260e-06, 4.1127e-06,
         4.0627e-04, 9.9599e-05, 1.3947e-05, 7.4148e-05],
        [6.9385e-01, 9.2864e-05, 6.6936e-05, 5.4836e-06, 5.8413e-06, 4.3511e-06,
         5.0068e-04, 8.5473e-05, 1.3888e-05, 6.6042e-05],
        [6.7529e-01, 1.0097e-04, 5.2929e-05, 7.4506e-06, 8.0466e-06, 5.7817e-06,
         3.6001e-04, 5.0068e-05, 9.0599e-06, 5.2452e-05],
        [6.9580e-01, 8.6129e-05, 5.1022e-05, 2.8014e-06, 3.3975e-06, 2.5034e-06,
         2.9826e-04, 5.6684e-05, 1.3292e-05, 4.9055e-05],
        [6.2256e-01, 1.1146e-04, 9.5129e-05, 7.5698e-06, 8.6427e-06, 6.6161e-06,
         3.5262e-04, 7.6354e-05, 2.7716e-05, 9.8467e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669


tensor([[6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06, 5.6267e-04,
         5.9962e-05, 1.2040e-05, 1.8942e-04, 1.4782e-05],
        [5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06, 5.0163e-04,
         3.7909e-05, 8.2254e-06, 1.5199e-04, 9.4771e-06],
        [7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06, 4.4680e-04,
         2.7418e-05, 6.4969e-06, 1.4484e-04, 1.6451e-05],
        [5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06, 2.7609e-04,
         2.9087e-05, 6.5565e-06, 1.0550e-04, 7.5102e-06],
        [1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06, 4.2415e-04,
         6.5744e-05, 3.1710e-05, 1.9586e-04, 1.5557e-05]], dtype=torch.float16)

tensor([[6.1328e-01, 6.7353e-05, 1.3602e-04, 4.4703e-06, 7.2718e-06, 5.6624e-06,
         5.6267e-04, 5.9962e-05, 1.2040e-05, 1.8942e-04],
        [7.6367e-01, 5.4240e-05, 8.5831e-05, 3.0398e-06, 5.1260e-06, 3.9935e-06,
         5.0163e-04, 3.7909e-05, 8.2254e-06, 1.5199e-04],
        [7.4902e-01, 7.5758e-05, 8.2314e-05, 4.5896e-06, 5.9605e-06, 4.5896e-06,
         4.4680e-04, 2.7418e-05, 6.4969e-06, 1.4484e-04],
        [7.6709e-01, 5.6028e-05, 6.4254e-05, 1.5497e-06, 2.3246e-06, 1.7285e-06,
         2.7609e-04, 2.9087e-05, 6.5565e-06, 1.0550e-04],
        [6.3281e-01, 1.2219e-04, 2.1768e-04, 5.4836e-06, 8.5235e-06, 6.4969e-06,
         4.2415e-04, 6.5744e-05, 3.1710e-05, 1.9586e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789


tensor([[6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06, 2.2745e-04,
         9.7275e-05, 1.5199e-05, 3.5381e-04, 1.1802e-05],
        [1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05, 3.8528e-04,
         8.5413e-05, 1.5378e-05, 3.3450e-04, 2.1815e-05],
        [1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06, 3.0541e-04,
         4.0889e-05, 7.6890e-06, 3.5000e-04, 2.1756e-05],
        [8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06, 2.0456e-04,
         6.1989e-05, 1.0252e-05, 2.6202e-04, 9.1791e-06],
        [1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06, 2.2984e-04,
         7.9691e-05, 2.6345e-05, 3.0923e-04, 1.1861e-05]], dtype=torch.float16)

tensor([[5.7568e-01, 6.4850e-05, 2.7728e-04, 3.7551e-06, 7.9870e-06, 5.9605e-06,
         2.2745e-04, 9.7275e-05, 1.5199e-05, 3.5381e-04],
        [6.7676e-01, 1.3924e-04, 2.3985e-04, 7.3314e-06, 1.5378e-05, 1.2398e-05,
         3.8528e-04, 8.5413e-05, 1.5378e-05, 3.3450e-04],
        [7.8076e-01, 1.1647e-04, 1.7774e-04, 7.3910e-06, 1.0908e-05, 8.6427e-06,
         3.0541e-04, 4.0889e-05, 7.6890e-06, 3.5000e-04],
        [7.6172e-01, 8.6367e-05, 1.5712e-04, 3.0398e-06, 5.3048e-06, 3.9935e-06,
         2.0456e-04, 6.1989e-05, 1.0252e-05, 2.6202e-04],
        [6.2842e-01, 1.0455e-04, 2.4772e-04, 3.5167e-06, 6.1393e-06, 4.5896e-06,
         2.2984e-04, 7.9691e-05, 2.6345e-05, 3.0923e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807


tensor([[3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06, 3.4904e-04,
         9.8109e-05, 1.0908e-05, 2.2471e-04, 3.0994e-06],
        [5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06, 4.6587e-04,
         1.0037e-04, 1.3232e-05, 2.4462e-04, 7.8678e-06],
        [3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06, 2.8253e-04,
         3.3915e-05, 5.0068e-06, 1.7619e-04, 4.9472e-06],
        [4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06, 2.8920e-04,
         1.0753e-04, 1.4842e-05, 2.0993e-04, 3.5763e-06],
        [5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06, 2.7299e-04,
         9.8944e-05, 3.2723e-05, 3.7980e-04, 4.1723e-06]], dtype=torch.float16)

tensor([[7.1973e-01, 3.0160e-05, 1.0645e-04, 2.3246e-06, 4.2319e-06, 3.0994e-06,
         3.4904e-04, 9.8109e-05, 1.0908e-05, 2.2471e-04],
        [7.8906e-01, 5.4002e-05, 1.1718e-04, 3.1590e-06, 6.5565e-06, 5.0068e-06,
         4.6587e-04, 1.0037e-04, 1.3232e-05, 2.4462e-04],
        [8.4912e-01, 3.6001e-05, 9.0599e-05, 2.2054e-06, 4.0531e-06, 3.0398e-06,
         2.8253e-04, 3.3915e-05, 5.0068e-06, 1.7619e-04],
        [8.3154e-01, 4.6194e-05, 1.1146e-04, 3.1590e-06, 6.0797e-06, 4.5300e-06,
         2.8920e-04, 1.0753e-04, 1.4842e-05, 2.0993e-04],
        [7.0166e-01, 5.3525e-05, 1.8692e-04, 2.8610e-06, 6.3181e-06, 4.6492e-06,
         2.7299e-04, 9.8944e-05, 3.2723e-05, 3.7980e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668


tensor([[3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06, 2.8014e-04,
         1.4138e-04, 2.4557e-05, 1.5724e-04, 5.9605e-06],
        [3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06, 3.7861e-04,
         1.0842e-04, 2.2829e-05, 1.2958e-04, 6.6757e-06],
        [5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06, 2.6393e-04,
         4.6909e-05, 1.4663e-05, 1.5485e-04, 7.3314e-06],
        [2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06, 2.2662e-04,
         1.4722e-04, 2.5868e-05, 1.1349e-04, 3.8147e-06],
        [3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06, 2.2519e-04,
         1.1498e-04, 4.7326e-05, 2.1005e-04, 3.9935e-06]], dtype=torch.float16)

tensor([[6.6455e-01, 3.5644e-05, 7.2896e-05, 1.7285e-06, 3.4571e-06, 2.3246e-06,
         2.8014e-04, 1.4138e-04, 2.4557e-05, 1.5724e-04],
        [8.2129e-01, 3.9220e-05, 6.1512e-05, 2.3842e-06, 4.1723e-06, 2.9802e-06,
         3.7861e-04, 1.0842e-04, 2.2829e-05, 1.2958e-04],
        [8.6963e-01, 5.2094e-05, 7.8797e-05, 3.9339e-06, 5.7220e-06, 3.9339e-06,
         2.6393e-04, 4.6909e-05, 1.4663e-05, 1.5485e-04],
        [8.0615e-01, 2.5809e-05, 7.0214e-05, 1.9670e-06, 3.4571e-06, 2.3246e-06,
         2.2662e-04, 1.4722e-04, 2.5868e-05, 1.1349e-04],
        [7.2998e-01, 3.0756e-05, 9.9421e-05, 1.4305e-06, 3.3379e-06, 2.4438e-06,
         2.2519e-04, 1.1498e-04, 4.7326e-05, 2.1005e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194


tensor([[5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06, 2.6965e-04,
         5.7220e-05, 1.5676e-05, 8.8394e-05, 6.7353e-06],
        [5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06, 3.5977e-04,
         6.9916e-05, 1.8716e-05, 1.2505e-04, 7.8082e-06],
        [4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06, 3.5644e-04,
         5.0545e-05, 1.5736e-05, 7.0214e-05, 1.0788e-05],
        [5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06, 2.3925e-04,
         9.5367e-05, 2.8729e-05, 5.8115e-05, 5.6028e-06],
        [1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06, 3.1829e-04,
         8.1122e-05, 3.8505e-05, 1.0622e-04, 5.1856e-06]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8711e-05, 4.2737e-05, 3.3379e-06, 4.6492e-06, 3.2783e-06,
         2.6965e-04, 5.7220e-05, 1.5676e-05, 8.8394e-05],
        [8.4912e-01, 5.7995e-05, 5.7042e-05, 5.6624e-06, 9.2983e-06, 6.9141e-06,
         3.5977e-04, 6.9916e-05, 1.8716e-05, 1.2505e-04],
        [8.1738e-01, 4.9770e-05, 5.3346e-05, 5.1856e-06, 8.4639e-06, 6.6757e-06,
         3.5644e-04, 5.0545e-05, 1.5736e-05, 7.0214e-05],
        [8.1738e-01, 5.8532e-05, 3.3736e-05, 2.6226e-06, 5.0068e-06, 3.5167e-06,
         2.3925e-04, 9.5367e-05, 2.8729e-05, 5.8115e-05],
        [8.1641e-01, 1.1152e-04, 6.1929e-05, 3.3975e-06, 6.1393e-06, 4.5896e-06,
         3.1829e-04, 8.1122e-05, 3.8505e-05, 1.0622e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737


tensor([[5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05, 2.1780e-04,
         1.2243e-04, 1.8060e-05, 1.2505e-04, 4.7088e-06],
        [1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05, 3.1662e-04,
         1.1116e-04, 2.0742e-05, 1.0705e-04, 1.1027e-05],
        [7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05, 2.4319e-04,
         6.0260e-05, 1.3828e-05, 1.0729e-04, 1.1802e-05],
        [5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05, 1.8144e-04,
         2.0838e-04, 4.2796e-05, 8.6308e-05, 3.0994e-06],
        [7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05, 1.8227e-04,
         1.3614e-04, 7.5221e-05, 1.6499e-04, 4.3511e-06]], dtype=torch.float16)

tensor([[6.6797e-01, 5.0128e-05, 4.4346e-05, 7.2718e-06, 1.3947e-05, 1.2100e-05,
         2.1780e-04, 1.2243e-04, 1.8060e-05, 1.2505e-04],
        [7.8760e-01, 1.1367e-04, 6.6221e-05, 1.4544e-05, 2.0146e-05, 1.3173e-05,
         3.1662e-04, 1.1116e-04, 2.0742e-05, 1.0705e-04],
        [8.5059e-01, 7.9215e-05, 8.1420e-05, 1.2577e-05, 2.8014e-05, 2.2948e-05,
         2.4319e-04, 6.0260e-05, 1.3828e-05, 1.0729e-04],
        [7.4463e-01, 5.6624e-05, 4.7803e-05, 6.2585e-06, 1.4007e-05, 1.1504e-05,
         1.8144e-04, 2.0838e-04, 4.2796e-05, 8.6308e-05],
        [6.7236e-01, 7.1883e-05, 8.4043e-05, 7.5102e-06, 1.5318e-05, 1.2577e-05,
         1.8227e-04, 1.3614e-04, 7.5221e-05, 1.6499e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761


tensor([[9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05, 4.1080e-04,
         1.7917e-04, 4.5955e-05, 9.6202e-05, 1.0014e-05],
        [1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05, 4.3440e-04,
         1.3471e-04, 4.0114e-05, 1.0222e-04, 1.4126e-05],
        [1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05, 4.0054e-04,
         1.0413e-04, 3.4034e-05, 9.7632e-05, 9.9540e-06],
        [2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05, 4.8590e-04,
         3.2473e-04, 9.7156e-05, 2.4045e-04, 2.0444e-05],
        [1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05, 4.2391e-04,
         2.3758e-04, 1.0061e-04, 1.6046e-04, 1.1802e-05]], dtype=torch.float16)

tensor([[8.5352e-01, 9.6560e-05, 7.5579e-05, 1.8358e-05, 2.8074e-05, 2.2650e-05,
         4.1080e-04, 1.7917e-04, 4.5955e-05, 9.6202e-05],
        [9.0674e-01, 1.2034e-04, 7.6652e-05, 1.3947e-05, 1.9133e-05, 1.6093e-05,
         4.3440e-04, 1.3471e-04, 4.0114e-05, 1.0222e-04],
        [8.9893e-01, 1.0026e-04, 9.2089e-05, 1.6212e-05, 1.9610e-05, 1.5676e-05,
         4.0054e-04, 1.0413e-04, 3.4034e-05, 9.7632e-05],
        [8.5791e-01, 2.0015e-04, 1.5724e-04, 2.3365e-05, 2.6524e-05, 2.0802e-05,
         4.8590e-04, 3.2473e-04, 9.7156e-05, 2.4045e-04],
        [8.3936e-01, 1.3876e-04, 9.9778e-05, 1.6510e-05, 2.8133e-05, 2.3484e-05,
         4.2391e-04, 2.3758e-04, 1.0061e-04, 1.6046e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032


tensor([[1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05, 2.7084e-04,
         5.6553e-04, 1.0997e-04, 2.3806e-04, 1.7107e-05],
        [1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06, 2.8133e-04,
         4.1890e-04, 8.2254e-05, 1.5199e-04, 1.1623e-05],
        [9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06, 3.0470e-04,
         1.8752e-04, 4.2021e-05, 1.8978e-04, 1.5140e-05],
        [1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05, 2.7561e-04,
         4.8542e-04, 1.1748e-04, 2.2531e-04, 1.9610e-05],
        [1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05, 2.1958e-04,
         4.9448e-04, 2.4939e-04, 3.5334e-04, 1.7762e-05]], dtype=torch.float16)

tensor([[6.3330e-01, 1.5259e-04, 1.8573e-04, 1.6510e-05, 2.1756e-05, 1.8954e-05,
         2.7084e-04, 5.6553e-04, 1.0997e-04, 2.3806e-04],
        [7.9785e-01, 1.1587e-04, 1.8358e-04, 8.7023e-06, 1.0908e-05, 9.0599e-06,
         2.8133e-04, 4.1890e-04, 8.2254e-05, 1.5199e-04],
        [8.5840e-01, 9.5427e-05, 1.8752e-04, 1.0788e-05, 1.2398e-05, 9.6560e-06,
         3.0470e-04, 1.8752e-04, 4.2021e-05, 1.8978e-04],
        [7.6172e-01, 1.4162e-04, 1.7715e-04, 1.1802e-05, 1.4842e-05, 1.1563e-05,
         2.7561e-04, 4.8542e-04, 1.1748e-04, 2.2531e-04],
        [6.7822e-01, 1.5986e-04, 3.0875e-04, 1.2159e-05, 1.9193e-05, 1.5140e-05,
         2.1958e-04, 4.9448e-04, 2.4939e-04, 3.5334e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352


tensor([[5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06, 2.8276e-04,
         6.3276e-04, 2.0552e-04, 4.8220e-05, 7.2718e-06],
        [7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06, 2.6608e-04,
         4.8518e-04, 1.7393e-04, 4.2200e-05, 8.8811e-06],
        [1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05, 3.0065e-04,
         2.9373e-04, 1.1539e-04, 9.0659e-05, 1.3828e-05],
        [1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06, 2.9683e-04,
         4.1342e-04, 1.3769e-04, 7.0333e-05, 1.3053e-05],
        [7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06, 2.6512e-04,
         6.1512e-04, 2.3937e-04, 5.7936e-05, 8.4043e-06]], dtype=torch.float16)

tensor([[8.4326e-01, 5.5194e-05, 5.9783e-05, 5.3048e-06, 7.3314e-06, 6.0201e-06,
         2.8276e-04, 6.3276e-04, 2.0552e-04, 4.8220e-05],
        [8.6768e-01, 7.0333e-05, 5.8711e-05, 5.0068e-06, 6.5565e-06, 5.1856e-06,
         2.6608e-04, 4.8518e-04, 1.7393e-04, 4.2200e-05],
        [8.0322e-01, 1.3912e-04, 1.0270e-04, 1.1325e-05, 1.4424e-05, 1.1265e-05,
         3.0065e-04, 2.9373e-04, 1.1539e-04, 9.0659e-05],
        [8.4033e-01, 1.0639e-04, 7.7367e-05, 9.6560e-06, 1.1265e-05, 8.9407e-06,
         2.9683e-04, 4.1342e-04, 1.3769e-04, 7.0333e-05],
        [8.4033e-01, 7.4804e-05, 6.9022e-05, 5.0664e-06, 6.9141e-06, 5.4240e-06,
         2.6512e-04, 6.1512e-04, 2.3937e-04, 5.7936e-05]], device='cuda:0',
       dtype=torch.float16)
****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044


tensor([[8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05, 3.0780e-04,
         4.3058e-04, 1.6236e-04, 1.5581e-04, 1.5855e-05],
        [9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05, 2.4962e-04,
         2.0599e-04, 7.5817e-05, 9.8705e-05, 1.0371e-05],
        [9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05, 3.2401e-04,
         2.6155e-04, 1.0300e-04, 1.8704e-04, 1.9372e-05],
        [1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06, 2.7847e-04,
         5.0449e-04, 2.2399e-04, 1.5306e-04, 1.9908e-05],
        [1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05, 3.0518e-04,
         4.0126e-04, 2.3103e-04, 1.5581e-04, 1.8835e-05]], dtype=torch.float16)

tensor([[8.1348e-01, 8.8871e-05, 1.2165e-04, 1.1742e-05, 1.4126e-05, 1.1563e-05,
         3.0780e-04, 4.3058e-04, 1.6236e-04, 1.5581e-04],
        [8.3594e-01, 9.2328e-05, 6.1214e-05, 1.1742e-05, 1.3947e-05, 1.3411e-05,
         2.4962e-04, 2.0599e-04, 7.5817e-05, 9.8705e-05],
        [8.7109e-01, 9.2208e-05, 1.3268e-04, 1.6272e-05, 1.3173e-05, 1.0312e-05,
         3.2401e-04, 2.6155e-04, 1.0300e-04, 1.8704e-04],
        [7.8174e-01, 1.1677e-04, 1.3006e-04, 1.0312e-05, 1.1265e-05, 8.9407e-06,
         2.7847e-04, 5.0449e-04, 2.2399e-04, 1.5306e-04],
        [7.6221e-01, 1.0610e-04, 1.4007e-04, 1.1206e-05, 1.5259e-05, 1.1921e-05,
         3.0518e-04, 4.0126e-04, 2.3103e-04, 1.5581e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511


tensor([[2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05, 2.1315e-04,
         3.6001e-04, 9.8407e-05, 2.0874e-04, 1.0729e-05],
        [2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05, 1.8895e-04,
         2.3127e-04, 7.0333e-05, 1.7738e-04, 9.7752e-06],
        [3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05, 2.4319e-04,
         3.6263e-04, 1.1939e-04, 4.7708e-04, 3.0458e-05],
        [2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05, 2.2173e-04,
         5.7793e-04, 1.8883e-04, 3.0470e-04, 2.2769e-05],
        [2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05, 2.1076e-04,
         3.9077e-04, 1.7166e-04, 3.1352e-04, 1.2398e-05]], dtype=torch.float16)

tensor([[7.6074e-01, 2.5129e-04, 1.2553e-04, 1.9908e-05, 2.4974e-05, 2.2113e-05,
         2.1315e-04, 3.6001e-04, 9.8407e-05, 2.0874e-04],
        [8.0615e-01, 2.1064e-04, 1.0651e-04, 1.8537e-05, 2.2650e-05, 1.9550e-05,
         1.8895e-04, 2.3127e-04, 7.0333e-05, 1.7738e-04],
        [7.6611e-01, 3.8147e-04, 4.0913e-04, 3.9637e-05, 4.9055e-05, 3.9399e-05,
         2.4319e-04, 3.6263e-04, 1.1939e-04, 4.7708e-04],
        [7.4658e-01, 2.6488e-04, 2.0456e-04, 2.9683e-05, 3.1054e-05, 2.3901e-05,
         2.2173e-04, 5.7793e-04, 1.8883e-04, 3.0470e-04],
        [7.2656e-01, 2.5129e-04, 1.9050e-04, 2.1040e-05, 2.5630e-05, 2.0742e-05,
         2.1076e-04, 3.9077e-04, 1.7166e-04, 3.1352e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832


tensor([[5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05, 2.6131e-04,
         2.7061e-04, 8.6486e-05, 1.5211e-04, 1.3411e-05],
        [5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06, 1.6379e-04,
         1.6272e-04, 5.6207e-05, 1.2398e-04, 9.1791e-06],
        [7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05, 2.2233e-04,
         2.6870e-04, 1.1104e-04, 2.3341e-04, 1.5914e-05],
        [5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05, 1.5986e-04,
         2.6155e-04, 8.4817e-05, 1.5306e-04, 1.1981e-05],
        [7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05, 1.8954e-04,
         2.2936e-04, 9.2328e-05, 2.4295e-04, 1.0550e-05]], dtype=torch.float16)

tensor([[8.0518e-01, 5.8353e-05, 8.1956e-05, 1.4305e-05, 2.3246e-05, 2.0802e-05,
         2.6131e-04, 2.7061e-04, 8.6486e-05, 1.5211e-04],
        [8.6133e-01, 5.7161e-05, 7.9751e-05, 8.6427e-06, 1.0192e-05, 8.1062e-06,
         1.6379e-04, 1.6272e-04, 5.6207e-05, 1.2398e-04],
        [8.1787e-01, 7.4446e-05, 1.3793e-04, 2.0206e-05, 2.8133e-05, 2.3425e-05,
         2.2233e-04, 2.6870e-04, 1.1104e-04, 2.3341e-04],
        [7.8857e-01, 5.4359e-05, 6.1154e-05, 9.5367e-06, 1.4305e-05, 1.0788e-05,
         1.5986e-04, 2.6155e-04, 8.4817e-05, 1.5306e-04],
        [8.0566e-01, 7.0453e-05, 9.6321e-05, 1.1086e-05, 1.5020e-05, 1.2279e-05,
         1.8954e-04, 2.2936e-04, 9.2328e-05, 2.4295e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871


tensor([[6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05, 9.9945e-04,
         5.8365e-04, 2.4033e-04, 6.5613e-04, 5.9068e-05],
        [5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05, 1.0490e-03,
         5.1880e-04, 2.2340e-04, 6.9857e-04, 6.3360e-05],
        [6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05, 1.0986e-03,
         1.2436e-03, 5.3978e-04, 1.1797e-03, 1.2541e-04],
        [6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05, 9.7942e-04,
         1.0300e-03, 4.0317e-04, 7.9632e-04, 1.0717e-04],
        [7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05, 9.3460e-04,
         8.5497e-04, 3.0088e-04, 8.7690e-04, 9.1136e-05]], dtype=torch.float16)

tensor([[4.3774e-01, 6.5327e-04, 5.8794e-04, 7.1287e-05, 7.9453e-05, 6.8963e-05,
         9.9945e-04, 5.8365e-04, 2.4033e-04, 6.5613e-04],
        [4.7852e-01, 5.8794e-04, 6.0511e-04, 7.5579e-05, 8.9765e-05, 8.0347e-05,
         1.0490e-03, 5.1880e-04, 2.2340e-04, 6.9857e-04],
        [5.0439e-01, 6.7806e-04, 5.9414e-04, 8.8692e-05, 1.0628e-04, 8.7619e-05,
         1.0986e-03, 1.2436e-03, 5.3978e-04, 1.1797e-03],
        [4.8779e-01, 6.4802e-04, 4.4870e-04, 7.2539e-05, 8.7082e-05, 7.3969e-05,
         9.7942e-04, 1.0300e-03, 4.0317e-04, 7.9632e-04],
        [4.3774e-01, 7.2861e-04, 5.4169e-04, 7.0632e-05, 8.3148e-05, 7.0214e-05,
         9.3460e-04, 8.5497e-04, 3.0088e-04, 8.7690e-04]], device='cuda:0',
       dtype=torch.float16)
****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 18:23:56,768] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796


[2024-03-25 17:03:06,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>
What is odd about this image? ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline attached to a yellow car, which is driving down a busy street. It is unusual to see someone sitting on a clothesline, especially while the car is in motion. This scene is not only unconventional but also potentially dangerous, as the man's position on the clothesline could pose a risk to his safety and the safety of others on the road.
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.006081814236111111
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.05876736111111111
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.06848068237304683
Total image attn by gen tokens (avged per gen query) for layer 0: 0.6018204159206815
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2709315405951606

****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.010796440972222222
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.16710069444444445
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.1355932235717774
Total image attn by gen tokens (avged per gen query) for layer 1: 0.2557687335544162
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.441537348429362

****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.3269097222222222
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.8291666666666667
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.008572207556830462
Total image attn by gen tokens (avged per gen query) for layer 2: 0.050034056769476996
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.11222706900702582

****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.396875
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.8888888888888888
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.006822024451361815
Total image attn by gen tokens (avged per gen query) for layer 3: 0.02220638593037923
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.08208270072937011

****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.39166666666666666
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.8451388888888889
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.013249169455634233
Total image attn by gen tokens (avged per gen query) for layer 4: 0.02208434740702311
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.11952759424845377

****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.384375
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.7958333333333333
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.016901630825466623
Total image attn by gen tokens (avged per gen query) for layer 5: 0.030278958214653862
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.15698607762654623

****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.39895833333333336
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.8090277777777778
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.026496423615349644
Total image attn by gen tokens (avged per gen query) for layer 6: 0.031633332040574814
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13284246656629775

****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.39131944444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.7826388888888889
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.02464135752783883
Total image attn by gen tokens (avged per gen query) for layer 7: 0.03378717369503445
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15893257988823783

****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.36215277777777777
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.7263888888888889
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.03384123908148874
Total image attn by gen tokens (avged per gen query) for layer 8: 0.041338814629448786
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1984310574001736

****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.3486111111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.678125
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.03054312600029841
Total image attn by gen tokens (avged per gen query) for layer 9: 0.04938607745700412
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.2419457965426975

****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.31059027777777776
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.5989583333333334
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.0476372083028157
Total image attn by gen tokens (avged per gen query) for layer 10: 0.052620580461290145
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.3007838779025608

****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.33663194444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.6451388888888889
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.040090253618028385
Total image attn by gen tokens (avged per gen query) for layer 11: 0.06766157150268555
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.24710928599039714

****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.31753472222222223
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.6072916666666667
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.04151282310485841
Total image attn by gen tokens (avged per gen query) for layer 12: 0.049049064848158096
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.3021464453803168

****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.3220486111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.6177083333333333
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.046731652153862865
Total image attn by gen tokens (avged per gen query) for layer 13: 0.05759809282090929
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27796192169189454

****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.2798611111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.5392361111111111
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.04478432337443031
Total image attn by gen tokens (avged per gen query) for layer 14: 0.08552204767862956
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.330457517835829

****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.265625
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.5267361111111111
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.048622110154893705
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07064454820421007
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.35399723052978516

****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.2899305555555556
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.5729166666666666
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.04497527016533752
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07054893705579969
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.31155912611219616

****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.328125
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.6642361111111111
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.02841650644938147
Total image attn by gen tokens (avged per gen query) for layer 17: 0.07345089382595486
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.23389648861355253

****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.3611111111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.7270833333333333
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.024721336364746126
Total image attn by gen tokens (avged per gen query) for layer 18: 0.051807996961805554
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.19638733334011502

****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.36701388888888886
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.7340277777777777
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.019828383127848376
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06471113628811306
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.18143270280626084

****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.3645833333333333
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.7263888888888889
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.03129937383863664
Total image attn by gen tokens (avged per gen query) for layer 20: 0.08337450557284885
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.15893723169962565

****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.4059027777777778
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.8013888888888889
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.013828033871120832
Total image attn by gen tokens (avged per gen query) for layer 21: 0.06831540001763238
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.11646767722235786

****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.41215277777777776
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.7958333333333333
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.012557840347290086
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06667208671569824
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.12493673960367839

****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.4378472222222222
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.8326388888888889
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.010768432087368392
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04375033113691542
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.11284234788682726

****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.41180555555555554
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.7770833333333333
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.02444846895005967
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07684861289130317
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.12161958482530381

****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.4791666666666667
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.8979166666666667
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.012754782040913876
Total image attn by gen tokens (avged per gen query) for layer 25: 0.035640030437045625
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.0536885208553738

****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.42743055555555554
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.7930555555555555
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.019944037331475206
Total image attn by gen tokens (avged per gen query) for layer 26: 0.057569392522176105
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.12943101459079318

****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.4857638888888889
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.8763888888888889
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.009501474433475078
Total image attn by gen tokens (avged per gen query) for layer 27: 0.018230431609683566
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09587920506795247

****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.49444444444444446
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.8625
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.015614567862616599
Total image attn by gen tokens (avged per gen query) for layer 28: 0.03065037727355957
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.09123505486382379

****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.48368055555555556
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.8131944444444444
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.020557774437798396
Total image attn by gen tokens (avged per gen query) for layer 29: 0.058725028567843965
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.1075227525499132

****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.5267361111111111
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.8548611111111111
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.014926674630906887
Total image attn by gen tokens (avged per gen query) for layer 30: 0.02733306090037028
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.10287915335761176

****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.32881944444444444
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.6149305555555555
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.03889678319295252
Total image attn by gen tokens (avged per gen query) for layer 31: 0.09327959484524197
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.25289306640625

####### Avg image attn for all layers: 0.07632318805489274
####### Avg gen text attn for all layers: 0.19123464094267956
####### Avg prompt attn for all layers: 0.7018934461805554
####### Avg question attn for all layers: 0.030548724821872195


################## (NO Prompt) Image + Question + Gen #############################

[2024-03-25 16:32:40,089] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow car, a taxi, and a traffic light, which adds to the overall peculiarity of the image.
****** Layer 0
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.07531549960752082
Total image attn by gen tokens (avged per gen query) for layer 0: 0.6596094928210294
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.26507500757144975

****** Layer 1
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.1595076548902294
Total image attn by gen tokens (avged per gen query) for layer 1: 0.31318167191517504
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.5273106731945956

****** Layer 2
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7268661064437673
Total image attn by gen tokens (avged per gen query) for layer 2: 0.08061549029772795
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.19251840325850475

****** Layer 3
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8771224323707291
Total image attn by gen tokens (avged per gen query) for layer 3: 0.02564244330683841
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0972351243224325

****** Layer 4
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.8552794426302366
Total image attn by gen tokens (avged per gen query) for layer 4: 0.02047793171073817
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12424262565902516

****** Layer 5
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.8082483207123189
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02705193169509308
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.16469974759258801

****** Layer 6
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8382932264593582
Total image attn by gen tokens (avged per gen query) for layer 6: 0.025902476491807384
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13580429704883432

****** Layer 7
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.8134494431411163
Total image attn by gen tokens (avged per gen query) for layer 7: 0.029742162438887585
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15680839441999606

****** Layer 8
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7559279671198205
Total image attn by gen tokens (avged per gen query) for layer 8: 0.03761136984523339
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.2064606630349461

****** Layer 9
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.7074998360645922
Total image attn by gen tokens (avged per gen query) for layer 9: 0.04329267936416819
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24920748457123962

****** Layer 10
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6618919855431664
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04596789879134939
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2921401156654841

****** Layer 11
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.7010199027725413
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07145926318591155
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.22752083404154716

****** Layer 12
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6667976741549335
Total image attn by gen tokens (avged per gen query) for layer 12: 0.051584183415280115
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.2816181424297864

****** Layer 13
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6624599468858936
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06898273395586617
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.2685573191582402

****** Layer 14
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.6023464021803457
Total image attn by gen tokens (avged per gen query) for layer 14: 0.09726304042188427
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.30039055739777

****** Layer 15
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5952555559858492
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07791023616549335
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.3268342078486575

****** Layer 16
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.625887943219535
Total image attn by gen tokens (avged per gen query) for layer 16: 0.0808255762993535
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29328648048111156

****** Layer 17
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.693933631800398
Total image attn by gen tokens (avged per gen query) for layer 17: 0.09421015389357941
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.21185621430602256

****** Layer 18
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7493246839016299
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06840570063530645
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1822696154630637

****** Layer 19
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7501792666278309
Total image attn by gen tokens (avged per gen query) for layer 19: 0.07984122747107397
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16997950590109523

****** Layer 20
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7588858302635483
Total image attn by gen tokens (avged per gen query) for layer 20: 0.10699301128146015
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.1341211584549916

****** Layer 21
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.8155211859111544
Total image attn by gen tokens (avged per gen query) for layer 21: 0.08921124059942705
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.09526757348941851

****** Layer 22
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.8036343659026712
Total image attn by gen tokens (avged per gen query) for layer 22: 0.08198031896277319
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11438531513455548

****** Layer 23
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8461458954630019
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04980715015266515
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10404695438433297

****** Layer 24
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.808239031441604
Total image attn by gen tokens (avged per gen query) for layer 24: 0.09195984466166436
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.09980112389673161

****** Layer 25
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.9125051075899148
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03998470909987824
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.047510183310206934

****** Layer 26
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.792586447317389
Total image attn by gen tokens (avged per gen query) for layer 26: 0.07913393913945065
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.12827961354316036

****** Layer 27
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8850938021382199
Total image attn by gen tokens (avged per gen query) for layer 27: 0.02134683011453363
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09355936774724646

****** Layer 28
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8829502455795868
Total image attn by gen tokens (avged per gen query) for layer 28: 0.03328027000910119
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08376948441131206

****** Layer 29
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.8417239490943619
Total image attn by gen tokens (avged per gen query) for layer 29: 0.061825969551183003
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.0964500813544551

****** Layer 30
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8739517549925213
Total image attn by gen tokens (avged per gen query) for layer 30: 0.02795724023746539
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.09809100477001335

****** Layer 31
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.0
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.6463379437410378
Total image attn by gen tokens (avged per gen query) for layer 31: 0.10278597964516169
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.25087607661380046

####### Avg image attn for all layers: 0.08705763023676753
####### Avg gen text attn for all layers: 0.18812416720239422
####### Avg prompt attn for all layers: 0.0
####### Avg question attn for all layers: 0.7248182025608384

################## Image + Question + Prompt + Gen #############################

[2024-03-25 18:23:56,768] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` <image> 
USER: What is odd about this image? A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline, which is an unconventional and unusual place to sit. Typically, clotheslines are used for hanging clothes and are not meant for sitting or resting. The scene also includes a yellow taxi cab driving by, which adds to the overall peculiarity of the image.
****** Layer 0
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 0: 0.0045693887246621625
Total image attn by gen tokens (avged per gen query) for layer 0: 0.5827946534027925
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.055400951488597916
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.1305954391891892
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2312089559194204

****** Layer 1
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 1: 0.008254592483108109
Total image attn by gen tokens (avged per gen query) for layer 1: 0.24893897288554423
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.09931901983312663
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.26013513513513514
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.39160687214619405

****** Layer 2
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 2: 0.7124155405405406
Total image attn by gen tokens (avged per gen query) for layer 2: 0.07935331318829511
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.7223392757209572
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.01584934543918919
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.18245806565155853

****** Layer 3
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 3: 0.8682432432432432
Total image attn by gen tokens (avged per gen query) for layer 3: 0.023056608599585457
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.8743165586445784
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.00857131545608108
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.0940555172997552

****** Layer 4
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 4: 0.839527027027027
Total image attn by gen tokens (avged per gen query) for layer 4: 0.018457742961677345
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.848722256518699
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.008822054476351352
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.12399794604327227

****** Layer 5
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 5: 0.7850506756756757
Total image attn by gen tokens (avged per gen query) for layer 5: 0.02216833668786126
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.7976792084204184
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.015083931587837838
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.1650685233038825

****** Layer 6
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 6: 0.7901182432432432
Total image attn by gen tokens (avged per gen query) for layer 6: 0.020265762870376174
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.8182498281066481
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.02641997466216216
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13506443436081345

****** Layer 7
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 7: 0.7732263513513513
Total image attn by gen tokens (avged per gen query) for layer 7: 0.025594124922881257
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.7962767562350712
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.024348078547297296
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15378104029475032

****** Layer 8
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 8: 0.7065033783783784
Total image attn by gen tokens (avged per gen query) for layer 8: 0.032734928904352964
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.7412127997424152
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.026274809966216218
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1997774613870157

****** Layer 9
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 9: 0.6566722972972973
Total image attn by gen tokens (avged per gen query) for layer 9: 0.03966224193572998
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.6854208514497088
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.03225295608108108
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.24266395053348025

****** Layer 10
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 10: 0.5886824324324325
Total image attn by gen tokens (avged per gen query) for layer 10: 0.04195496842667863
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.6306351326607369
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.03610641891891892
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.2913034799936655

****** Layer 11
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 11: 0.6169763513513513
Total image attn by gen tokens (avged per gen query) for layer 11: 0.07173660639170054
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.6596092920045595
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.03322951858108108
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.2354245830226589

****** Layer 12
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 12: 0.5929054054054054
Total image attn by gen tokens (avged per gen query) for layer 12: 0.050836575997842325
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.6360138686927589
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.03317673141891892
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.27997282389047984

****** Layer 13
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 13: 0.5709459459459459
Total image attn by gen tokens (avged per gen query) for layer 13: 0.06742099813512854
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.6203648979599411
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.037901182432432436
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27431292147249786

****** Layer 14
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 14: 0.5194256756756757
Total image attn by gen tokens (avged per gen query) for layer 14: 0.0935615333350929
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.5646101719624288
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.04362858952702703
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.29819970517545136

****** Layer 15
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 15: 0.5092905405405406
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07488864176982157
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.5549025664458405
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.048221072635135136
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.32198771914920293

****** Layer 16
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 16: 0.551097972972973
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07366797086354848
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.5951307077665587
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.04104201858108108
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.29015930278881175

****** Layer 17
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 17: 0.6148648648648649
Total image attn by gen tokens (avged per gen query) for layer 17: 0.08374025370623614
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.6459303611033671
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.061127533783783786
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.2092018514066129

****** Layer 18
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 18: 0.7014358108108109
Total image attn by gen tokens (avged per gen query) for layer 18: 0.06102278425886824
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.7237656438672865
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.03560494087837838
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.1796066309954669

****** Layer 19
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 19: 0.6946790540540541
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06917524337768555
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.7135369584367082
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.04830025337837838
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.16898754480722789

****** Layer 20
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 20: 0.7005912162162162
Total image attn by gen tokens (avged per gen query) for layer 20: 0.0867544767018911
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.7258719624699774
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.046716638513513514
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.14065692231461807

****** Layer 21
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 21: 0.7761824324324325
Total image attn by gen tokens (avged per gen query) for layer 21: 0.07340377730292243
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.7892573459728344
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.037267736486486486
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.10007114023775668

****** Layer 22
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 22: 0.7660472972972973
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06959737313760293
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.777815393499426
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.040751689189189186
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.11183554417378194

****** Layer 23
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 23: 0.8036317567567568
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04325212659062566
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.8123953729062467
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.03784839527027027
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.10650410523285737

****** Layer 24
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 24: 0.7601351351351351
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07739579999769056
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.7772456504203178
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.04233530405405406
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.10302324552793761

****** Layer 25
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 25: 0.8690878378378378
Total image attn by gen tokens (avged per gen query) for layer 25: 0.03495886841335812
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.8787877108599688
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.0376636402027027
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.04858978052397032

****** Layer 26
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 26: 0.754222972972973
Total image attn by gen tokens (avged per gen query) for layer 26: 0.0632159323305697
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.772106628160219
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.045845650337837836
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.11883178917137352

****** Layer 27
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 27: 0.8391047297297297
Total image attn by gen tokens (avged per gen query) for layer 27: 0.019978201067125476
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.8469084726797569
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.0365551097972973
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09655821645582044

****** Layer 28
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 28: 0.8268581081081081
Total image attn by gen tokens (avged per gen query) for layer 28: 0.02994193257512273
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.8378691802153715
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.04542335304054054
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.08676553416896511

****** Layer 29
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 29: 0.7795608108108109
Total image attn by gen tokens (avged per gen query) for layer 29: 0.05604540335165488
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.7985431632480106
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.048247466216216214
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.09716396718411832

****** Layer 30
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 30: 0.8086993243243243
Total image attn by gen tokens (avged per gen query) for layer 30: 0.026393587524826463
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.8215498537630649
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.05326224662162162
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.0987943120904871

****** Layer 31
Total attn of first 4 tokens wrt gen tokens (avged per gen query) for layer 31: 0.4514358108108108
Total image attn by gen tokens (avged per gen query) for layer 31: 0.08035853102400496
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.4887808722418708
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.18401604729729729
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.24684454943682696

####### Avg image attn for all layers: 0.07632275851997172
####### Avg gen text attn for all layers: 0.1820149511300229
####### Avg prompt attn for all layers: 0.050707018053209464
####### Avg question attn for all layers: 0.690955272296796
[2024-03-25 19:44:05,903] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Granular tokens config not found, falling back to not using granular tokens.
Built vision tower and projector!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.granular_mm_projector.0.bias', 'model.granular_mm_projector.0.weight', 'model.granular_mm_projector.2.bias', 'model.granular_mm_projector.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded model
llava-v1.5-7b
Checking if llava in model name
in vision tower init code
#### Prompt: ` A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>
What is odd about this image? ASSISTANT: `
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The odd aspect of this image is that a man is sitting on a clothesline attached to a yellow car, which is driving down a busy street. It is unusual to see someone sitting on a clothesline, especially while the car is in motion. This scene is not only unconventional but also potentially dangerous, as the man's position on the clothesline could pose a risk to his safety and the safety of others on the road.
****** Layer 0
Total attn of first token wrt gen tokens (avged per gen query) for layer 0: 0.0020060221354166667
Total image attn by gen tokens (avged per gen query) for layer 0: 0.6018204159206815
Total question text attn by gen tokens (avged per gen query) for layer 0: 0.03764049212137858
Total prompt text attn by gen tokens (avged per gen query) for layer 0: 0.05759548611111111
Total gen text attn by gen tokens (avged per gen query) for layer 0: 0.2709315405951606

****** Layer 1
Total attn of first token wrt gen tokens (avged per gen query) for layer 1: 0.006852213541666667
Total image attn by gen tokens (avged per gen query) for layer 1: 0.2557687335544162
Total question text attn by gen tokens (avged per gen query) for layer 1: 0.043782371944851343
Total prompt text attn by gen tokens (avged per gen query) for layer 1: 0.16111111111111112
Total gen text attn by gen tokens (avged per gen query) for layer 1: 0.441537348429362

****** Layer 2
Total attn of first token wrt gen tokens (avged per gen query) for layer 2: 0.3269097222222222
Total image attn by gen tokens (avged per gen query) for layer 2: 0.050034056769476996
Total question text attn by gen tokens (avged per gen query) for layer 2: 0.004192874828974406
Total prompt text attn by gen tokens (avged per gen query) for layer 2: 0.5024305555555556
Total gen text attn by gen tokens (avged per gen query) for layer 2: 0.11222706900702582

****** Layer 3
Total attn of first token wrt gen tokens (avged per gen query) for layer 3: 0.396875
Total image attn by gen tokens (avged per gen query) for layer 3: 0.02220638593037923
Total question text attn by gen tokens (avged per gen query) for layer 3: 0.004306948184967041
Total prompt text attn by gen tokens (avged per gen query) for layer 3: 0.49201388888888886
Total gen text attn by gen tokens (avged per gen query) for layer 3: 0.08208270072937011

****** Layer 4
Total attn of first token wrt gen tokens (avged per gen query) for layer 4: 0.39166666666666666
Total image attn by gen tokens (avged per gen query) for layer 4: 0.02208434740702311
Total question text attn by gen tokens (avged per gen query) for layer 4: 0.008430980973773532
Total prompt text attn by gen tokens (avged per gen query) for layer 4: 0.4534722222222222
Total gen text attn by gen tokens (avged per gen query) for layer 4: 0.11952759424845377

****** Layer 5
Total attn of first token wrt gen tokens (avged per gen query) for layer 5: 0.384375
Total image attn by gen tokens (avged per gen query) for layer 5: 0.030278958214653862
Total question text attn by gen tokens (avged per gen query) for layer 5: 0.008855605125427246
Total prompt text attn by gen tokens (avged per gen query) for layer 5: 0.4114583333333333
Total gen text attn by gen tokens (avged per gen query) for layer 5: 0.15698607762654623

****** Layer 6
Total attn of first token wrt gen tokens (avged per gen query) for layer 6: 0.39861111111111114
Total image attn by gen tokens (avged per gen query) for layer 6: 0.031633332040574814
Total question text attn by gen tokens (avged per gen query) for layer 6: 0.01696852578057183
Total prompt text attn by gen tokens (avged per gen query) for layer 6: 0.4107638888888889
Total gen text attn by gen tokens (avged per gen query) for layer 6: 0.13284246656629775

****** Layer 7
Total attn of first token wrt gen tokens (avged per gen query) for layer 7: 0.3909722222222222
Total image attn by gen tokens (avged per gen query) for layer 7: 0.03378717369503445
Total question text attn by gen tokens (avged per gen query) for layer 7: 0.013210867510901557
Total prompt text attn by gen tokens (avged per gen query) for layer 7: 0.39166666666666666
Total gen text attn by gen tokens (avged per gen query) for layer 7: 0.15893257988823783

****** Layer 8
Total attn of first token wrt gen tokens (avged per gen query) for layer 8: 0.3614583333333333
Total image attn by gen tokens (avged per gen query) for layer 8: 0.041338814629448786
Total question text attn by gen tokens (avged per gen query) for layer 8: 0.019213742680019803
Total prompt text attn by gen tokens (avged per gen query) for layer 8: 0.3645833333333333
Total gen text attn by gen tokens (avged per gen query) for layer 8: 0.1984310574001736

****** Layer 9
Total attn of first token wrt gen tokens (avged per gen query) for layer 9: 0.3482638888888889
Total image attn by gen tokens (avged per gen query) for layer 9: 0.04938607745700412
Total question text attn by gen tokens (avged per gen query) for layer 9: 0.01392307546403673
Total prompt text attn by gen tokens (avged per gen query) for layer 9: 0.3296875
Total gen text attn by gen tokens (avged per gen query) for layer 9: 0.2419457965426975

****** Layer 10
Total attn of first token wrt gen tokens (avged per gen query) for layer 10: 0.3104166666666667
Total image attn by gen tokens (avged per gen query) for layer 10: 0.052620580461290145
Total question text attn by gen tokens (avged per gen query) for layer 10: 0.018076497978634306
Total prompt text attn by gen tokens (avged per gen query) for layer 10: 0.2887152777777778
Total gen text attn by gen tokens (avged per gen query) for layer 10: 0.3007838779025608

****** Layer 11
Total attn of first token wrt gen tokens (avged per gen query) for layer 11: 0.33611111111111114
Total image attn by gen tokens (avged per gen query) for layer 11: 0.06766157150268555
Total question text attn by gen tokens (avged per gen query) for layer 11: 0.01765060822168986
Total prompt text attn by gen tokens (avged per gen query) for layer 11: 0.30885416666666665
Total gen text attn by gen tokens (avged per gen query) for layer 11: 0.24710928599039714

****** Layer 12
Total attn of first token wrt gen tokens (avged per gen query) for layer 12: 0.3173611111111111
Total image attn by gen tokens (avged per gen query) for layer 12: 0.049049064848158096
Total question text attn by gen tokens (avged per gen query) for layer 12: 0.01757907337612576
Total prompt text attn by gen tokens (avged per gen query) for layer 12: 0.29010416666666666
Total gen text attn by gen tokens (avged per gen query) for layer 12: 0.3021464453803168

****** Layer 13
Total attn of first token wrt gen tokens (avged per gen query) for layer 13: 0.3217013888888889
Total image attn by gen tokens (avged per gen query) for layer 13: 0.05759809282090929
Total question text attn by gen tokens (avged per gen query) for layer 13: 0.01601779328452216
Total prompt text attn by gen tokens (avged per gen query) for layer 13: 0.29583333333333334
Total gen text attn by gen tokens (avged per gen query) for layer 13: 0.27796192169189454

****** Layer 14
Total attn of first token wrt gen tokens (avged per gen query) for layer 14: 0.2795138888888889
Total image attn by gen tokens (avged per gen query) for layer 14: 0.08552204767862956
Total question text attn by gen tokens (avged per gen query) for layer 14: 0.021508856614430746
Total prompt text attn by gen tokens (avged per gen query) for layer 14: 0.25972222222222224
Total gen text attn by gen tokens (avged per gen query) for layer 14: 0.330457517835829

****** Layer 15
Total attn of first token wrt gen tokens (avged per gen query) for layer 15: 0.26545138888888886
Total image attn by gen tokens (avged per gen query) for layer 15: 0.07064454820421007
Total question text attn by gen tokens (avged per gen query) for layer 15: 0.01937195195092095
Total prompt text attn by gen tokens (avged per gen query) for layer 15: 0.26145833333333335
Total gen text attn by gen tokens (avged per gen query) for layer 15: 0.35399723052978516

****** Layer 16
Total attn of first token wrt gen tokens (avged per gen query) for layer 16: 0.28975694444444444
Total image attn by gen tokens (avged per gen query) for layer 16: 0.07054893705579969
Total question text attn by gen tokens (avged per gen query) for layer 16: 0.01911824941635132
Total prompt text attn by gen tokens (avged per gen query) for layer 16: 0.2833333333333333
Total gen text attn by gen tokens (avged per gen query) for layer 16: 0.31155912611219616

****** Layer 17
Total attn of first token wrt gen tokens (avged per gen query) for layer 17: 0.32795138888888886
Total image attn by gen tokens (avged per gen query) for layer 17: 0.07345089382595486
Total question text attn by gen tokens (avged per gen query) for layer 17: 0.013330413235558404
Total prompt text attn by gen tokens (avged per gen query) for layer 17: 0.3362847222222222
Total gen text attn by gen tokens (avged per gen query) for layer 17: 0.23389648861355253

****** Layer 18
Total attn of first token wrt gen tokens (avged per gen query) for layer 18: 0.3611111111111111
Total image attn by gen tokens (avged per gen query) for layer 18: 0.051807996961805554
Total question text attn by gen tokens (avged per gen query) for layer 18: 0.011463403701782227
Total prompt text attn by gen tokens (avged per gen query) for layer 18: 0.3663194444444444
Total gen text attn by gen tokens (avged per gen query) for layer 18: 0.19638733334011502

****** Layer 19
Total attn of first token wrt gen tokens (avged per gen query) for layer 19: 0.36666666666666664
Total image attn by gen tokens (avged per gen query) for layer 19: 0.06471113628811306
Total question text attn by gen tokens (avged per gen query) for layer 19: 0.007008408175574409
Total prompt text attn by gen tokens (avged per gen query) for layer 19: 0.36736111111111114
Total gen text attn by gen tokens (avged per gen query) for layer 19: 0.18143270280626084

****** Layer 20
Total attn of first token wrt gen tokens (avged per gen query) for layer 20: 0.3645833333333333
Total image attn by gen tokens (avged per gen query) for layer 20: 0.08337450557284885
Total question text attn by gen tokens (avged per gen query) for layer 20: 0.013956413004133436
Total prompt text attn by gen tokens (avged per gen query) for layer 20: 0.36180555555555555
Total gen text attn by gen tokens (avged per gen query) for layer 20: 0.15893723169962565

****** Layer 21
Total attn of first token wrt gen tokens (avged per gen query) for layer 21: 0.4059027777777778
Total image attn by gen tokens (avged per gen query) for layer 21: 0.06831540001763238
Total question text attn by gen tokens (avged per gen query) for layer 21: 0.0077504396438598635
Total prompt text attn by gen tokens (avged per gen query) for layer 21: 0.3958333333333333
Total gen text attn by gen tokens (avged per gen query) for layer 21: 0.11646767722235786

****** Layer 22
Total attn of first token wrt gen tokens (avged per gen query) for layer 22: 0.41180555555555554
Total image attn by gen tokens (avged per gen query) for layer 22: 0.06667208671569824
Total question text attn by gen tokens (avged per gen query) for layer 22: 0.00563297536638048
Total prompt text attn by gen tokens (avged per gen query) for layer 22: 0.384375
Total gen text attn by gen tokens (avged per gen query) for layer 22: 0.12493673960367839

****** Layer 23
Total attn of first token wrt gen tokens (avged per gen query) for layer 23: 0.4375
Total image attn by gen tokens (avged per gen query) for layer 23: 0.04375033113691542
Total question text attn by gen tokens (avged per gen query) for layer 23: 0.005587430132759942
Total prompt text attn by gen tokens (avged per gen query) for layer 23: 0.3951388888888889
Total gen text attn by gen tokens (avged per gen query) for layer 23: 0.11284234788682726

****** Layer 24
Total attn of first token wrt gen tokens (avged per gen query) for layer 24: 0.41180555555555554
Total image attn by gen tokens (avged per gen query) for layer 24: 0.07684861289130317
Total question text attn by gen tokens (avged per gen query) for layer 24: 0.006824441088570489
Total prompt text attn by gen tokens (avged per gen query) for layer 24: 0.36527777777777776
Total gen text attn by gen tokens (avged per gen query) for layer 24: 0.12161958482530381

****** Layer 25
Total attn of first token wrt gen tokens (avged per gen query) for layer 25: 0.47881944444444446
Total image attn by gen tokens (avged per gen query) for layer 25: 0.035640030437045625
Total question text attn by gen tokens (avged per gen query) for layer 25: 0.00318944255510966
Total prompt text attn by gen tokens (avged per gen query) for layer 25: 0.41944444444444445
Total gen text attn by gen tokens (avged per gen query) for layer 25: 0.0536885208553738

****** Layer 26
Total attn of first token wrt gen tokens (avged per gen query) for layer 26: 0.4270833333333333
Total image attn by gen tokens (avged per gen query) for layer 26: 0.057569392522176105
Total question text attn by gen tokens (avged per gen query) for layer 26: 0.006861060857772827
Total prompt text attn by gen tokens (avged per gen query) for layer 26: 0.3659722222222222
Total gen text attn by gen tokens (avged per gen query) for layer 26: 0.12943101459079318

****** Layer 27
Total attn of first token wrt gen tokens (avged per gen query) for layer 27: 0.48541666666666666
Total image attn by gen tokens (avged per gen query) for layer 27: 0.018230431609683566
Total question text attn by gen tokens (avged per gen query) for layer 27: 0.002143615484237671
Total prompt text attn by gen tokens (avged per gen query) for layer 27: 0.39131944444444444
Total gen text attn by gen tokens (avged per gen query) for layer 27: 0.09587920506795247

****** Layer 28
Total attn of first token wrt gen tokens (avged per gen query) for layer 28: 0.49409722222222224
Total image attn by gen tokens (avged per gen query) for layer 28: 0.03065037727355957
Total question text attn by gen tokens (avged per gen query) for layer 28: 0.003932007153828939
Total prompt text attn by gen tokens (avged per gen query) for layer 28: 0.3684027777777778
Total gen text attn by gen tokens (avged per gen query) for layer 28: 0.09123505486382379

****** Layer 29
Total attn of first token wrt gen tokens (avged per gen query) for layer 29: 0.48333333333333334
Total image attn by gen tokens (avged per gen query) for layer 29: 0.058725028567843965
Total question text attn by gen tokens (avged per gen query) for layer 29: 0.007088520129521688
Total prompt text attn by gen tokens (avged per gen query) for layer 29: 0.33003472222222224
Total gen text attn by gen tokens (avged per gen query) for layer 29: 0.1075227525499132

****** Layer 30
Total attn of first token wrt gen tokens (avged per gen query) for layer 30: 0.5263888888888889
Total image attn by gen tokens (avged per gen query) for layer 30: 0.02733306090037028
Total question text attn by gen tokens (avged per gen query) for layer 30: 0.004005716244379679
Total prompt text attn by gen tokens (avged per gen query) for layer 30: 0.3284722222222222
Total gen text attn by gen tokens (avged per gen query) for layer 30: 0.10287915335761176

****** Layer 31
Total attn of first token wrt gen tokens (avged per gen query) for layer 31: 0.32743055555555556
Total image attn by gen tokens (avged per gen query) for layer 31: 0.09327959484524197
Total question text attn by gen tokens (avged per gen query) for layer 31: 0.016104523340861
Total prompt text attn by gen tokens (avged per gen query) for layer 31: 0.2876736111111111
Total gen text attn by gen tokens (avged per gen query) for layer 31: 0.25289306640625

####### Avg image attn for all layers: 0.07632318805489274
####### Avg gen text attn for all layers: 0.19123464094267956
####### Avg prompt attn for all layers: 0.3445787217881945
####### Avg question attn for all layers: 0.012960228924122122
