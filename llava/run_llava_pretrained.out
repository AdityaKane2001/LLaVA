config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]config.json: 100%|██████████| 1.04k/1.04k [00:00<00:00, 2.44MB/s]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/eval/run_llava.py", line 147, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/llava/eval/run_llava.py", line 57, in eval_model
    tokenizer, model, image_processor, context_len = load_pretrained_model(
  File "/home/akane38/LLaVA/llava/model/builder.py", line 116, in load_pretrained_model
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/auto/tokenization_auto.py", line 836, in from_pretrained
    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/tokenization_utils_base.py", line 2015, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.
