python: can't open file '/home/akane38/LLaVA/scripts/scripts/evaluate_mllm.py': [Errno 2] No such file or directory
[2024-04-19 17:14:53,225] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/akane38/LLaVA/scripts/evaluate_mllm.py:18: DeprecationWarning: 
Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),
(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)
but was not found to be installed on your system.
If this would cause problems for you,
please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466
        
  import pandas as pd
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/scripts/evaluate_mllm.py", line 135, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/scripts/evaluate_mllm.py", line 41, in eval_model
    tokenizer, model, image_processor, context_len = load_multi_ve_pretrained_model(model_path, args.model_base, model_name)
  File "/home/akane38/LLaVA/llava/model/builder.py", line 182, in load_multi_ve_pretrained_model
    return tokenizer, model, multiple_image_processors, context_len
UnboundLocalError: local variable 'multiple_image_processors' referenced before assignment
[2024-04-19 17:16:35,541] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/akane38/LLaVA/scripts/evaluate_mllm.py:18: DeprecationWarning: 
Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),
(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)
but was not found to be installed on your system.
If this would cause problems for you,
please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466
        
  import pandas as pd
projector_type='mlp2x_gelu'
projector_type='mlp2x_gelu'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
Some weights of the model checkpoint at /data/data1/akane/mve-clip-dino-router-finetune/checkpoints/ were not used when initializing MultiVELlavaLlamaForCausalLM: ['model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.class_embedding', 'model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.post_layernorm.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.post_layernorm.weight', 'model.multiple_vision_towers.0.vision_tower.vision_model.pre_layrnorm.bias', 'model.multiple_vision_towers.0.vision_tower.vision_model.pre_layrnorm.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.embeddings.cls_token', 'model.multiple_vision_towers.1.dummy_vision_tower.embeddings.mask_token', 'model.multiple_vision_towers.1.dummy_vision_tower.embeddings.patch_embeddings.projection.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.embeddings.patch_embeddings.projection.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.embeddings.position_embeddings', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.0.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.1.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.10.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.11.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.12.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.13.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.14.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.15.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.16.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.17.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.18.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.19.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.2.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.20.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.21.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.22.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.23.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.3.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.4.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.5.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.6.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.7.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.8.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.key.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.key.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.query.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.query.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.value.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.attention.value.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.output.dense.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.attention.output.dense.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.layer_scale1.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.layer_scale2.lambda1', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.mlp.fc1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.mlp.fc1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.mlp.fc2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.mlp.fc2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.norm1.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.norm1.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.norm2.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.encoder.layer.9.norm2.weight', 'model.multiple_vision_towers.1.dummy_vision_tower.layernorm.bias', 'model.multiple_vision_towers.1.dummy_vision_tower.layernorm.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.0.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.0.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.1.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.1.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.10.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.10.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.11.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.11.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.12.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.12.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.13.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.13.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.14.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.14.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.15.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.15.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.16.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.16.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.17.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.17.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.18.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.18.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.19.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.19.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.2.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.2.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.20.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.20.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.21.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.21.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.22.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.22.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.23.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.23.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.3.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.3.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.4.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.4.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.5.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.5.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.6.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.6.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.7.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.7.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.8.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.8.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.attn.proj.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.attn.proj.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.attn.qkv.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.attn.qkv.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.ls1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.ls2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.mlp.fc1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.mlp.fc1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.mlp.fc2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.mlp.fc2.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.norm1.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.norm1.weight', 'model.multiple_vision_towers.1.vision_tower.blocks.9.norm2.bias', 'model.multiple_vision_towers.1.vision_tower.blocks.9.norm2.weight', 'model.multiple_vision_towers.1.vision_tower.cls_token', 'model.multiple_vision_towers.1.vision_tower.mask_token', 'model.multiple_vision_towers.1.vision_tower.norm.bias', 'model.multiple_vision_towers.1.vision_tower.norm.weight', 'model.multiple_vision_towers.1.vision_tower.patch_embed.proj.bias', 'model.multiple_vision_towers.1.vision_tower.patch_embed.proj.weight', 'model.multiple_vision_towers.1.vision_tower.pos_embed']
- This IS expected if you are initializing MultiVELlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MultiVELlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Using cache found in /home/akane38/.cache/torch/hub/facebookresearch_dinov2_main
/home/akane38/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/akane38/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/akane38/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
##### In image init code
Device map auto successful!
Device map auto successful!
0it [00:00, ?it/s]0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/scripts/evaluate_mllm.py", line 135, in <module>
    eval_model(args)
  File "/home/akane38/LLaVA/scripts/evaluate_mllm.py", line 79, in eval_model
    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
AttributeError: 'list' object has no attribute 'preprocess'
