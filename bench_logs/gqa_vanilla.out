You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.93s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.08s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.10s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:08<00:04,  4.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.35s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  6.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.54s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/3143 [00:00<?, ?it/s]  0%|          | 1/3143 [00:02<2:13:14,  2.54s/it]  0%|          | 2/3143 [00:03<1:09:45,  1.33s/it]  0%|          | 3/3143 [00:03<52:05,  1.00it/s]    0%|          | 4/3143 [00:04<40:08,  1.30it/s]  0%|          | 5/3143 [00:04<34:55,  1.50it/s]  0%|          | 6/3143 [00:05<31:35,  1.66it/s]  0%|          | 7/3143 [00:05<28:41,  1.82it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.86s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  7.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.87s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  7.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.88s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 8/3143 [00:05<25:26,  2.05it/s]  0%|          | 0/3145 [00:00<?, ?it/s]  0%|          | 0/3145 [00:00<?, ?it/s]  0%|          | 0/3145 [00:00<?, ?it/s]  0%|          | 9/3143 [00:06<23:18,  2.24it/s]  0%|          | 10/3143 [00:06<23:01,  2.27it/s]  0%|          | 11/3143 [00:06<21:41,  2.41it/s]  0%|          | 1/3145 [00:01<1:07:48,  1.29s/it]  0%|          | 1/3145 [00:01<1:14:37,  1.42s/it]  0%|          | 1/3145 [00:01<1:17:37,  1.48s/it]  0%|          | 12/3143 [00:07<22:06,  2.36it/s]  0%|          | 2/3145 [00:01<39:26,  1.33it/s]    0%|          | 2/3145 [00:01<41:03,  1.28it/s]    0%|          | 2/3145 [00:01<43:08,  1.21it/s]    0%|          | 13/3143 [00:07<22:36,  2.31it/s]  0%|          | 3/3145 [00:02<30:01,  1.74it/s]  0%|          | 3/3145 [00:02<32:18,  1.62it/s]  0%|          | 3/3145 [00:02<32:40,  1.60it/s]  0%|          | 14/3143 [00:08<21:13,  2.46it/s]  0%|          | 4/3145 [00:02<24:46,  2.11it/s]  0%|          | 4/3145 [00:02<27:39,  1.89it/s]  0%|          | 4/3145 [00:02<27:06,  1.93it/s]  0%|          | 15/3143 [00:08<20:31,  2.54it/s]  0%|          | 5/3145 [00:02<23:07,  2.26it/s]  0%|          | 5/3145 [00:02<24:35,  2.13it/s]  0%|          | 5/3145 [00:02<26:18,  1.99it/s]  1%|          | 16/3143 [00:08<20:43,  2.52it/s]  0%|          | 6/3145 [00:03<21:37,  2.42it/s]  0%|          | 6/3145 [00:03<23:37,  2.21it/s]  0%|          | 6/3145 [00:03<24:58,  2.09it/s]  1%|          | 17/3143 [00:09<20:35,  2.53it/s]  0%|          | 7/3145 [00:03<20:41,  2.53it/s]  0%|          | 7/3145 [00:03<22:38,  2.31it/s]  0%|          | 7/3145 [00:03<23:34,  2.22it/s]  1%|          | 18/3143 [00:09<20:54,  2.49it/s]  0%|          | 8/3145 [00:03<20:01,  2.61it/s]  0%|          | 8/3145 [00:04<21:06,  2.48it/s]  0%|          | 8/3145 [00:04<22:13,  2.35it/s]  0%|          | 9/3145 [00:04<19:15,  2.71it/s]  1%|          | 19/3143 [00:10<21:16,  2.45it/s]  0%|          | 9/3145 [00:04<20:53,  2.50it/s]  0%|          | 9/3145 [00:04<20:53,  2.50it/s]  1%|          | 20/3143 [00:10<20:40,  2.52it/s]  0%|          | 10/3145 [00:04<20:17,  2.57it/s]  0%|          | 10/3145 [00:04<18:29,  2.83it/s]  0%|          | 10/3145 [00:04<20:30,  2.55it/s]  0%|          | 11/3145 [00:04<16:48,  3.11it/s]  1%|          | 21/3143 [00:10<20:39,  2.52it/s]  0%|          | 11/3145 [00:05<20:53,  2.50it/s]  0%|          | 11/3145 [00:05<20:34,  2.54it/s]  0%|          | 12/3145 [00:05<17:21,  3.01it/s]  1%|          | 22/3143 [00:11<20:01,  2.60it/s]  0%|          | 12/3145 [00:05<20:30,  2.55it/s]  0%|          | 13/3145 [00:05<15:57,  3.27it/s]  0%|          | 12/3145 [00:05<21:11,  2.46it/s]  1%|          | 23/3143 [00:11<20:17,  2.56it/s]  0%|          | 13/3145 [00:05<21:03,  2.48it/s]  0%|          | 14/3145 [00:05<17:25,  2.99it/s]  0%|          | 13/3145 [00:06<20:37,  2.53it/s]  1%|          | 24/3143 [00:12<20:13,  2.57it/s]  0%|          | 15/3145 [00:06<17:51,  2.92it/s]  0%|          | 14/3145 [00:06<21:51,  2.39it/s]  0%|          | 14/3145 [00:06<20:38,  2.53it/s]  1%|          | 25/3143 [00:12<20:34,  2.53it/s]  0%|          | 15/3145 [00:06<21:07,  2.47it/s]  1%|          | 16/3145 [00:06<19:16,  2.71it/s]  0%|          | 15/3145 [00:06<20:39,  2.52it/s]  1%|          | 26/3143 [00:12<21:09,  2.45it/s]  1%|          | 16/3145 [00:07<20:53,  2.50it/s]  1%|          | 16/3145 [00:07<20:01,  2.60it/s]  1%|          | 17/3145 [00:07<20:39,  2.52it/s]  1%|          | 17/3145 [00:07<18:08,  2.87it/s]  1%|          | 27/3143 [00:13<20:25,  2.54it/s]  1%|          | 18/3145 [00:07<20:03,  2.60it/s]  1%|          | 18/3145 [00:07<16:48,  3.10it/s]  1%|          | 17/3145 [00:07<21:06,  2.47it/s]  1%|          | 28/3143 [00:13<20:56,  2.48it/s]  1%|          | 19/3145 [00:07<20:04,  2.59it/s]  1%|          | 18/3145 [00:08<21:03,  2.47it/s]  1%|          | 19/3145 [00:08<18:41,  2.79it/s]  1%|          | 29/3143 [00:14<21:13,  2.45it/s]  1%|          | 19/3145 [00:08<19:44,  2.64it/s]  1%|          | 20/3145 [00:08<18:40,  2.79it/s]  1%|          | 20/3145 [00:08<21:39,  2.41it/s]  1%|          | 30/3143 [00:14<20:25,  2.54it/s]  1%|          | 20/3145 [00:08<19:27,  2.68it/s]  1%|          | 21/3145 [00:08<19:00,  2.74it/s]  1%|          | 21/3145 [00:09<24:12,  2.15it/s]  1%|          | 21/3145 [00:09<20:07,  2.59it/s]  1%|          | 22/3145 [00:09<19:29,  2.67it/s]  1%|          | 31/3143 [00:15<23:34,  2.20it/s]  1%|          | 22/3145 [00:09<22:37,  2.30it/s]  1%|          | 22/3145 [00:09<20:49,  2.50it/s]  1%|          | 23/3145 [00:09<19:37,  2.65it/s]  1%|          | 32/3143 [00:15<23:13,  2.23it/s]  1%|          | 23/3145 [00:09<22:39,  2.30it/s]  1%|          | 23/3145 [00:10<21:12,  2.45it/s]  1%|          | 24/3145 [00:10<20:40,  2.52it/s]  1%|          | 33/3143 [00:16<23:45,  2.18it/s]  1%|          | 24/3145 [00:10<22:05,  2.35it/s]  1%|          | 24/3145 [00:10<21:28,  2.42it/s]  1%|          | 25/3145 [00:10<20:42,  2.51it/s]  1%|          | 34/3143 [00:16<23:44,  2.18it/s]  1%|          | 25/3145 [00:10<21:43,  2.39it/s]  1%|          | 25/3145 [00:10<20:38,  2.52it/s]  1%|          | 26/3145 [00:10<20:40,  2.51it/s]  1%|          | 35/3143 [00:16<22:49,  2.27it/s]  1%|          | 26/3145 [00:11<21:11,  2.45it/s]  1%|          | 26/3145 [00:11<20:57,  2.48it/s]  1%|          | 27/3145 [00:11<20:55,  2.48it/s]  1%|          | 36/3143 [00:17<22:46,  2.27it/s]  1%|          | 27/3145 [00:11<20:56,  2.48it/s]  1%|          | 28/3145 [00:11<20:17,  2.56it/s]  1%|          | 27/3145 [00:11<21:45,  2.39it/s]  1%|          | 37/3143 [00:17<21:39,  2.39it/s]  1%|          | 28/3145 [00:11<21:32,  2.41it/s]  1%|          | 29/3145 [00:12<20:25,  2.54it/s]  1%|          | 28/3145 [00:12<22:11,  2.34it/s]  1%|          | 38/3143 [00:18<21:20,  2.42it/s]  1%|          | 29/3145 [00:12<22:07,  2.35it/s]  1%|          | 30/3145 [00:12<21:02,  2.47it/s]  1%|          | 29/3145 [00:12<21:46,  2.39it/s]  1%|          | 39/3143 [00:18<21:08,  2.45it/s]  1%|          | 30/3145 [00:12<21:31,  2.41it/s]  1%|          | 30/3145 [00:12<21:18,  2.44it/s]  1%|          | 31/3145 [00:12<21:57,  2.36it/s]  1%|▏         | 40/3143 [00:18<21:13,  2.44it/s]  1%|          | 31/3145 [00:13<21:18,  2.44it/s]  1%|          | 31/3145 [00:13<21:58,  2.36it/s]  1%|          | 32/3145 [00:13<22:58,  2.26it/s]  1%|▏         | 41/3143 [00:19<21:40,  2.39it/s]  1%|          | 32/3145 [00:13<20:58,  2.47it/s]  1%|          | 32/3145 [00:13<21:37,  2.40it/s]  1%|          | 33/3145 [00:13<22:47,  2.27it/s]  1%|          | 33/3145 [00:13<20:54,  2.48it/s]  1%|▏         | 42/3143 [00:19<23:59,  2.15it/s]  1%|          | 33/3145 [00:14<22:12,  2.34it/s]  1%|          | 34/3145 [00:14<22:28,  2.31it/s]  1%|          | 34/3145 [00:14<21:19,  2.43it/s]  1%|▏         | 43/3143 [00:20<24:20,  2.12it/s]  1%|          | 34/3145 [00:14<21:41,  2.39it/s]  1%|          | 35/3145 [00:14<21:07,  2.45it/s]  1%|          | 35/3145 [00:14<20:45,  2.50it/s]  1%|▏         | 44/3143 [00:20<22:47,  2.27it/s]  1%|          | 35/3145 [00:14<21:06,  2.46it/s]  1%|          | 36/3145 [00:15<21:03,  2.46it/s]  1%|          | 36/3145 [00:15<20:18,  2.55it/s]  1%|▏         | 45/3143 [00:21<21:57,  2.35it/s]  1%|          | 37/3145 [00:15<17:39,  2.93it/s]  1%|          | 36/3145 [00:15<20:52,  2.48it/s]  1%|          | 37/3145 [00:15<21:48,  2.38it/s]  1%|          | 38/3145 [00:15<17:54,  2.89it/s]  1%|▏         | 46/3143 [00:21<24:16,  2.13it/s]  1%|          | 37/3145 [00:15<22:06,  2.34it/s]  1%|          | 38/3145 [00:15<22:46,  2.27it/s]  1%|          | 39/3145 [00:16<19:12,  2.69it/s]  1%|          | 38/3145 [00:16<21:49,  2.37it/s]  1%|▏         | 47/3143 [00:22<23:41,  2.18it/s]  1%|          | 39/3145 [00:16<21:44,  2.38it/s]  1%|▏         | 40/3145 [00:16<20:09,  2.57it/s]  2%|▏         | 48/3143 [00:22<22:30,  2.29it/s]  1%|▏         | 40/3145 [00:16<21:04,  2.46it/s]  1%|          | 39/3145 [00:16<22:58,  2.25it/s]  2%|▏         | 49/3143 [00:22<19:20,  2.66it/s]  1%|▏         | 41/3145 [00:16<20:10,  2.56it/s]  1%|▏         | 41/3145 [00:17<20:07,  2.57it/s]  1%|▏         | 40/3145 [00:17<23:16,  2.22it/s]  2%|▏         | 50/3143 [00:23<20:12,  2.55it/s]  1%|▏         | 42/3145 [00:17<21:01,  2.46it/s]  1%|▏         | 42/3145 [00:17<20:33,  2.52it/s]  2%|▏         | 51/3143 [00:23<17:53,  2.88it/s]  1%|▏         | 41/3145 [00:17<22:28,  2.30it/s]  1%|▏         | 43/3145 [00:17<21:39,  2.39it/s]  1%|▏         | 43/3145 [00:17<20:35,  2.51it/s]  2%|▏         | 52/3143 [00:23<18:38,  2.76it/s]  1%|▏         | 42/3145 [00:18<22:52,  2.26it/s]  1%|▏         | 44/3145 [00:18<20:30,  2.52it/s]  1%|▏         | 44/3145 [00:18<19:58,  2.59it/s]  2%|▏         | 53/3143 [00:24<19:11,  2.68it/s]  1%|▏         | 43/3145 [00:18<22:18,  2.32it/s]  1%|▏         | 45/3145 [00:18<20:23,  2.53it/s]  2%|▏         | 54/3143 [00:24<17:09,  3.00it/s]  1%|▏         | 45/3145 [00:18<20:13,  2.56it/s]  1%|▏         | 44/3145 [00:18<21:36,  2.39it/s]  1%|▏         | 46/3145 [00:18<20:01,  2.58it/s]  1%|▏         | 46/3145 [00:19<20:15,  2.55it/s]  2%|▏         | 55/3143 [00:24<19:06,  2.69it/s]  1%|▏         | 45/3145 [00:19<21:50,  2.37it/s]  1%|▏         | 47/3145 [00:19<21:03,  2.45it/s]  1%|▏         | 47/3145 [00:19<20:58,  2.46it/s]  2%|▏         | 56/3143 [00:25<19:37,  2.62it/s]  1%|▏         | 46/3145 [00:19<22:04,  2.34it/s]  2%|▏         | 48/3145 [00:19<20:59,  2.46it/s]  2%|▏         | 48/3145 [00:19<20:13,  2.55it/s]  2%|▏         | 57/3143 [00:25<19:48,  2.60it/s]  1%|▏         | 47/3145 [00:20<22:09,  2.33it/s]  2%|▏         | 49/3145 [00:20<20:55,  2.47it/s]  2%|▏         | 49/3145 [00:20<20:25,  2.53it/s]  2%|▏         | 58/3143 [00:26<20:57,  2.45it/s]  2%|▏         | 48/3145 [00:20<22:12,  2.32it/s]  2%|▏         | 50/3145 [00:20<20:35,  2.50it/s]  2%|▏         | 50/3145 [00:20<22:07,  2.33it/s]  2%|▏         | 59/3143 [00:26<20:19,  2.53it/s]  2%|▏         | 60/3143 [00:26<17:55,  2.87it/s]  2%|▏         | 51/3145 [00:21<20:37,  2.50it/s]  2%|▏         | 49/3145 [00:21<22:18,  2.31it/s]  2%|▏         | 51/3145 [00:21<21:43,  2.37it/s]  2%|▏         | 61/3143 [00:27<18:31,  2.77it/s]  2%|▏         | 52/3145 [00:21<20:36,  2.50it/s]  2%|▏         | 50/3145 [00:21<22:34,  2.28it/s]  2%|▏         | 52/3145 [00:21<22:20,  2.31it/s]  2%|▏         | 62/3143 [00:27<18:58,  2.71it/s]  2%|▏         | 53/3145 [00:21<20:53,  2.47it/s]  2%|▏         | 53/3145 [00:21<21:54,  2.35it/s]  2%|▏         | 51/3145 [00:22<25:00,  2.06it/s]  2%|▏         | 63/3143 [00:28<21:06,  2.43it/s]  2%|▏         | 54/3145 [00:22<21:19,  2.42it/s]  2%|▏         | 54/3145 [00:22<21:28,  2.40it/s]  2%|▏         | 52/3145 [00:22<23:28,  2.20it/s]  2%|▏         | 64/3143 [00:28<21:23,  2.40it/s]  2%|▏         | 55/3145 [00:22<20:58,  2.45it/s]  2%|▏         | 55/3145 [00:22<20:39,  2.49it/s]  2%|▏         | 53/3145 [00:22<23:28,  2.20it/s]  2%|▏         | 56/3145 [00:23<20:23,  2.53it/s]  2%|▏         | 65/3143 [00:28<21:06,  2.43it/s]  2%|▏         | 56/3145 [00:23<20:28,  2.52it/s]  2%|▏         | 54/3145 [00:23<22:34,  2.28it/s]  2%|▏         | 57/3145 [00:23<20:58,  2.45it/s]  2%|▏         | 66/3143 [00:29<21:20,  2.40it/s]  2%|▏         | 57/3145 [00:23<21:23,  2.41it/s]  2%|▏         | 55/3145 [00:23<21:26,  2.40it/s]  2%|▏         | 67/3143 [00:29<19:52,  2.58it/s]  2%|▏         | 58/3145 [00:23<21:18,  2.41it/s]  2%|▏         | 58/3145 [00:23<20:47,  2.47it/s]  2%|▏         | 56/3145 [00:24<20:55,  2.46it/s]  2%|▏         | 68/3143 [00:30<20:41,  2.48it/s]  2%|▏         | 59/3145 [00:24<20:35,  2.50it/s]  2%|▏         | 59/3145 [00:24<23:10,  2.22it/s]  2%|▏         | 57/3145 [00:24<20:36,  2.50it/s]  2%|▏         | 60/3145 [00:24<18:08,  2.83it/s]  2%|▏         | 69/3143 [00:30<20:50,  2.46it/s]  2%|▏         | 61/3145 [00:24<16:31,  3.11it/s]  2%|▏         | 58/3145 [00:24<20:09,  2.55it/s]  2%|▏         | 60/3145 [00:24<22:47,  2.26it/s]  2%|▏         | 70/3143 [00:30<20:28,  2.50it/s]  2%|▏         | 62/3145 [00:25<17:57,  2.86it/s]  2%|▏         | 59/3145 [00:25<20:44,  2.48it/s]  2%|▏         | 61/3145 [00:25<22:54,  2.24it/s]  2%|▏         | 63/3145 [00:25<16:24,  3.13it/s]  2%|▏         | 71/3143 [00:31<21:24,  2.39it/s]  2%|▏         | 60/3145 [00:25<21:39,  2.37it/s]  2%|▏         | 62/3145 [00:25<23:05,  2.23it/s]  2%|▏         | 64/3145 [00:25<17:28,  2.94it/s]  2%|▏         | 72/3143 [00:31<21:35,  2.37it/s]  2%|▏         | 61/3145 [00:26<20:55,  2.46it/s]  2%|▏         | 63/3145 [00:26<21:58,  2.34it/s]  2%|▏         | 65/3145 [00:26<17:54,  2.87it/s]  2%|▏         | 73/3143 [00:32<21:14,  2.41it/s]  2%|▏         | 62/3145 [00:26<21:23,  2.40it/s]  2%|▏         | 64/3145 [00:26<21:35,  2.38it/s]  2%|▏         | 66/3145 [00:26<19:51,  2.58it/s]  2%|▏         | 74/3143 [00:32<20:48,  2.46it/s]  2%|▏         | 65/3145 [00:26<21:52,  2.35it/s]  2%|▏         | 63/3145 [00:27<22:05,  2.32it/s]  2%|▏         | 75/3143 [00:32<20:20,  2.51it/s]  2%|▏         | 67/3145 [00:27<20:11,  2.54it/s]  2%|▏         | 64/3145 [00:27<21:15,  2.42it/s]  2%|▏         | 66/3145 [00:27<21:24,  2.40it/s]  2%|▏         | 76/3143 [00:33<19:51,  2.57it/s]  2%|▏         | 68/3145 [00:27<20:15,  2.53it/s]  2%|▏         | 67/3145 [00:27<21:47,  2.35it/s]  2%|▏         | 65/3145 [00:27<22:02,  2.33it/s]  2%|▏         | 77/3143 [00:33<20:32,  2.49it/s]  2%|▏         | 69/3145 [00:27<20:51,  2.46it/s]  2%|▏         | 66/3145 [00:28<18:59,  2.70it/s]  2%|▏         | 68/3145 [00:28<21:10,  2.42it/s]  2%|▏         | 78/3143 [00:34<20:11,  2.53it/s]  2%|▏         | 70/3145 [00:28<20:56,  2.45it/s]  2%|▏         | 67/3145 [00:28<19:13,  2.67it/s]  2%|▏         | 69/3145 [00:28<20:46,  2.47it/s]  3%|▎         | 79/3143 [00:34<20:46,  2.46it/s]  2%|▏         | 71/3145 [00:28<20:03,  2.55it/s]  2%|▏         | 68/3145 [00:28<19:50,  2.59it/s]  2%|▏         | 70/3145 [00:29<21:09,  2.42it/s]  2%|▏         | 72/3145 [00:29<19:39,  2.61it/s]  3%|▎         | 80/3143 [00:34<20:40,  2.47it/s]  2%|▏         | 69/3145 [00:29<19:58,  2.57it/s]  2%|▏         | 71/3145 [00:29<20:52,  2.45it/s]  2%|▏         | 73/3145 [00:29<19:54,  2.57it/s]  3%|▎         | 81/3143 [00:35<22:13,  2.30it/s]  2%|▏         | 70/3145 [00:29<20:42,  2.48it/s]  2%|▏         | 72/3145 [00:29<20:39,  2.48it/s]  2%|▏         | 74/3145 [00:29<19:53,  2.57it/s]  3%|▎         | 82/3143 [00:35<21:01,  2.43it/s]  2%|▏         | 71/3145 [00:30<20:33,  2.49it/s]  2%|▏         | 73/3145 [00:30<21:03,  2.43it/s]  2%|▏         | 75/3145 [00:30<19:55,  2.57it/s]  3%|▎         | 83/3143 [00:36<21:21,  2.39it/s]  2%|▏         | 72/3145 [00:30<20:53,  2.45it/s]  2%|▏         | 74/3145 [00:30<20:39,  2.48it/s]  2%|▏         | 76/3145 [00:30<20:03,  2.55it/s]  3%|▎         | 84/3143 [00:36<21:50,  2.33it/s]  2%|▏         | 73/3145 [00:30<21:04,  2.43it/s]  2%|▏         | 75/3145 [00:31<20:31,  2.49it/s]  2%|▏         | 77/3145 [00:31<20:01,  2.55it/s]  3%|▎         | 85/3143 [00:37<21:58,  2.32it/s]  2%|▏         | 74/3145 [00:31<20:37,  2.48it/s]  2%|▏         | 78/3145 [00:31<20:31,  2.49it/s]  2%|▏         | 76/3145 [00:31<22:09,  2.31it/s]  3%|▎         | 86/3143 [00:37<21:18,  2.39it/s]  2%|▏         | 75/3145 [00:31<21:38,  2.36it/s]  3%|▎         | 79/3145 [00:31<20:08,  2.54it/s]  2%|▏         | 77/3145 [00:31<21:39,  2.36it/s]  3%|▎         | 87/3143 [00:38<21:32,  2.36it/s]  2%|▏         | 76/3145 [00:32<21:18,  2.40it/s]  2%|▏         | 78/3145 [00:32<19:53,  2.57it/s]  3%|▎         | 80/3145 [00:32<21:08,  2.42it/s]  3%|▎         | 88/3143 [00:38<20:53,  2.44it/s]  2%|▏         | 77/3145 [00:32<21:37,  2.36it/s]  3%|▎         | 79/3145 [00:32<20:55,  2.44it/s]  3%|▎         | 81/3145 [00:32<21:52,  2.33it/s]  3%|▎         | 89/3143 [00:38<21:16,  2.39it/s]  2%|▏         | 78/3145 [00:33<20:38,  2.48it/s]  3%|▎         | 80/3145 [00:33<21:21,  2.39it/s]  3%|▎         | 82/3145 [00:33<21:45,  2.35it/s]  3%|▎         | 90/3143 [00:39<20:03,  2.54it/s]  3%|▎         | 79/3145 [00:33<19:57,  2.56it/s]  3%|▎         | 83/3145 [00:33<21:13,  2.41it/s]  3%|▎         | 81/3145 [00:33<22:00,  2.32it/s]  3%|▎         | 80/3145 [00:33<17:41,  2.89it/s]  3%|▎         | 91/3143 [00:39<20:06,  2.53it/s]  3%|▎         | 84/3145 [00:33<20:20,  2.51it/s]  3%|▎         | 82/3145 [00:34<21:27,  2.38it/s]  3%|▎         | 92/3143 [00:39<19:37,  2.59it/s]  3%|▎         | 81/3145 [00:34<19:36,  2.60it/s]  3%|▎         | 85/3145 [00:34<17:36,  2.90it/s]  3%|▎         | 83/3145 [00:34<20:23,  2.50it/s]  3%|▎         | 93/3143 [00:40<19:47,  2.57it/s]  3%|▎         | 82/3145 [00:34<20:40,  2.47it/s]  3%|▎         | 86/3145 [00:34<18:57,  2.69it/s]  3%|▎         | 94/3143 [00:40<17:23,  2.92it/s]  3%|▎         | 84/3145 [00:34<20:24,  2.50it/s]  3%|▎         | 83/3145 [00:34<17:54,  2.85it/s]  3%|▎         | 87/3145 [00:34<19:38,  2.60it/s]  3%|▎         | 95/3143 [00:40<17:52,  2.84it/s]  3%|▎         | 85/3145 [00:35<20:27,  2.49it/s]  3%|▎         | 84/3145 [00:35<18:47,  2.72it/s]  3%|▎         | 96/3143 [00:41<18:03,  2.81it/s]  3%|▎         | 88/3145 [00:35<20:36,  2.47it/s]  3%|▎         | 85/3145 [00:35<19:48,  2.58it/s]  3%|▎         | 86/3145 [00:35<21:18,  2.39it/s]  3%|▎         | 97/3143 [00:41<16:33,  3.07it/s]  3%|▎         | 89/3145 [00:35<21:06,  2.41it/s]  3%|▎         | 98/3143 [00:41<15:37,  3.25it/s]  3%|▎         | 86/3145 [00:35<19:38,  2.60it/s]  3%|▎         | 87/3145 [00:36<21:48,  2.34it/s]  3%|▎         | 99/3143 [00:42<14:22,  3.53it/s]  3%|▎         | 90/3145 [00:36<22:00,  2.31it/s]  3%|▎         | 87/3145 [00:36<20:10,  2.53it/s]  3%|▎         | 88/3145 [00:36<21:51,  2.33it/s]  3%|▎         | 100/3143 [00:42<15:41,  3.23it/s]  3%|▎         | 88/3145 [00:36<20:16,  2.51it/s]  3%|▎         | 91/3145 [00:36<22:24,  2.27it/s]  3%|▎         | 101/3143 [00:42<17:04,  2.97it/s]  3%|▎         | 89/3145 [00:36<22:50,  2.23it/s]  3%|▎         | 102/3143 [00:43<16:05,  3.15it/s]  3%|▎         | 92/3145 [00:37<21:11,  2.40it/s]  3%|▎         | 89/3145 [00:37<20:16,  2.51it/s]  3%|▎         | 90/3145 [00:37<21:19,  2.39it/s]  3%|▎         | 103/3143 [00:43<16:00,  3.17it/s]  3%|▎         | 90/3145 [00:37<19:43,  2.58it/s]  3%|▎         | 93/3145 [00:37<21:53,  2.32it/s]  3%|▎         | 104/3143 [00:43<15:38,  3.24it/s]  3%|▎         | 91/3145 [00:37<21:54,  2.32it/s]  3%|▎         | 91/3145 [00:37<19:54,  2.56it/s]  3%|▎         | 94/3145 [00:38<21:40,  2.35it/s]  3%|▎         | 92/3145 [00:38<21:22,  2.38it/s]  3%|▎         | 105/3143 [00:44<17:45,  2.85it/s]  3%|▎         | 92/3145 [00:38<19:50,  2.56it/s]  3%|▎         | 95/3145 [00:38<21:46,  2.33it/s]  3%|▎         | 93/3145 [00:38<21:05,  2.41it/s]  3%|▎         | 106/3143 [00:44<18:59,  2.66it/s]  3%|▎         | 93/3145 [00:38<19:28,  2.61it/s]  3%|▎         | 96/3145 [00:38<21:20,  2.38it/s]  3%|▎         | 107/3143 [00:44<17:34,  2.88it/s]  3%|▎         | 94/3145 [00:38<20:16,  2.51it/s]  3%|▎         | 94/3145 [00:39<19:39,  2.59it/s]  3%|▎         | 97/3145 [00:39<20:45,  2.45it/s]  3%|▎         | 108/3143 [00:45<18:22,  2.75it/s]  3%|▎         | 95/3145 [00:39<20:19,  2.50it/s]  3%|▎         | 95/3145 [00:39<19:31,  2.60it/s]  3%|▎         | 98/3145 [00:39<20:08,  2.52it/s]  3%|▎         | 109/3143 [00:45<18:31,  2.73it/s]  3%|▎         | 96/3145 [00:39<21:03,  2.41it/s]  3%|▎         | 96/3145 [00:39<20:23,  2.49it/s]  3%|▎         | 99/3145 [00:40<20:00,  2.54it/s]  3%|▎         | 110/3143 [00:46<19:53,  2.54it/s]  3%|▎         | 97/3145 [00:40<20:52,  2.43it/s]  3%|▎         | 97/3145 [00:40<21:14,  2.39it/s]  3%|▎         | 100/3145 [00:40<19:59,  2.54it/s]  4%|▎         | 111/3143 [00:46<18:03,  2.80it/s]  3%|▎         | 98/3145 [00:40<20:11,  2.51it/s]  4%|▎         | 112/3143 [00:46<16:46,  3.01it/s]  3%|▎         | 98/3145 [00:40<20:51,  2.43it/s]  3%|▎         | 101/3145 [00:40<20:19,  2.50it/s]  3%|▎         | 99/3145 [00:40<20:15,  2.51it/s]  3%|▎         | 99/3145 [00:41<21:11,  2.40it/s]  4%|▎         | 113/3143 [00:47<20:26,  2.47it/s]  3%|▎         | 102/3145 [00:41<21:13,  2.39it/s]  3%|▎         | 100/3145 [00:41<19:49,  2.56it/s]  3%|▎         | 100/3145 [00:41<20:46,  2.44it/s]  3%|▎         | 101/3145 [00:41<19:42,  2.57it/s]  4%|▎         | 114/3143 [00:47<21:12,  2.38it/s]  3%|▎         | 103/3145 [00:41<22:00,  2.30it/s]  3%|▎         | 101/3145 [00:42<21:29,  2.36it/s]  3%|▎         | 102/3145 [00:42<19:29,  2.60it/s]  3%|▎         | 104/3145 [00:42<21:16,  2.38it/s]  4%|▎         | 115/3143 [00:48<23:39,  2.13it/s]  3%|▎         | 103/3145 [00:42<20:01,  2.53it/s]  3%|▎         | 102/3145 [00:42<22:47,  2.23it/s]  3%|▎         | 105/3145 [00:42<21:30,  2.36it/s]  4%|▎         | 116/3143 [00:48<22:33,  2.24it/s]  3%|▎         | 104/3145 [00:42<20:13,  2.51it/s]  3%|▎         | 106/3145 [00:42<20:19,  2.49it/s]  3%|▎         | 103/3145 [00:43<22:35,  2.24it/s]  4%|▎         | 117/3143 [00:49<21:20,  2.36it/s]  3%|▎         | 107/3145 [00:43<20:01,  2.53it/s]  3%|▎         | 105/3145 [00:43<20:29,  2.47it/s]  3%|▎         | 104/3145 [00:43<22:24,  2.26it/s]  4%|▍         | 118/3143 [00:49<21:49,  2.31it/s]  3%|▎         | 106/3145 [00:43<19:46,  2.56it/s]  3%|▎         | 108/3145 [00:43<19:50,  2.55it/s]  3%|▎         | 105/3145 [00:43<21:38,  2.34it/s]  4%|▍         | 119/3143 [00:49<21:55,  2.30it/s]  3%|▎         | 109/3145 [00:44<20:04,  2.52it/s]  3%|▎         | 107/3145 [00:44<20:38,  2.45it/s]  3%|▎         | 106/3145 [00:44<21:04,  2.40it/s]  4%|▍         | 120/3143 [00:50<20:36,  2.45it/s]  3%|▎         | 108/3145 [00:44<19:54,  2.54it/s]  3%|▎         | 107/3145 [00:44<20:47,  2.44it/s]  3%|▎         | 110/3145 [00:44<21:47,  2.32it/s]  4%|▍         | 121/3143 [00:50<20:26,  2.46it/s]  3%|▎         | 109/3145 [00:44<20:30,  2.47it/s]  4%|▎         | 111/3145 [00:45<21:54,  2.31it/s]  3%|▎         | 108/3145 [00:45<21:34,  2.35it/s]  4%|▍         | 122/3143 [00:50<19:28,  2.59it/s]  3%|▎         | 110/3145 [00:45<20:35,  2.46it/s]  4%|▎         | 112/3145 [00:45<21:12,  2.38it/s]  3%|▎         | 109/3145 [00:45<22:05,  2.29it/s]  4%|▍         | 123/3143 [00:51<20:37,  2.44it/s]  4%|▎         | 111/3145 [00:45<21:39,  2.34it/s]  4%|▎         | 113/3145 [00:45<20:59,  2.41it/s]  3%|▎         | 110/3145 [00:45<21:57,  2.30it/s]  4%|▍         | 124/3143 [00:51<21:31,  2.34it/s]  4%|▎         | 112/3145 [00:46<21:30,  2.35it/s]  4%|▎         | 114/3145 [00:46<21:34,  2.34it/s]  4%|▎         | 111/3145 [00:46<22:04,  2.29it/s]  4%|▍         | 125/3143 [00:52<21:40,  2.32it/s]  4%|▎         | 113/3145 [00:46<20:19,  2.49it/s]  4%|▎         | 115/3145 [00:46<21:40,  2.33it/s]  4%|▎         | 112/3145 [00:46<22:05,  2.29it/s]  4%|▍         | 126/3143 [00:52<22:03,  2.28it/s]  4%|▎         | 114/3145 [00:46<19:32,  2.59it/s]  4%|▎         | 116/3145 [00:47<19:34,  2.58it/s]  4%|▎         | 113/3145 [00:47<22:02,  2.29it/s]  4%|▍         | 127/3143 [00:53<21:22,  2.35it/s]  4%|▎         | 115/3145 [00:47<20:15,  2.49it/s]  4%|▎         | 117/3145 [00:47<19:44,  2.56it/s]  4%|▎         | 114/3145 [00:47<21:59,  2.30it/s]  4%|▍         | 128/3143 [00:53<21:07,  2.38it/s]  4%|▎         | 116/3145 [00:47<20:06,  2.51it/s]  4%|▍         | 118/3145 [00:47<20:27,  2.47it/s]  4%|▍         | 129/3143 [00:54<21:34,  2.33it/s]  4%|▎         | 117/3145 [00:48<21:19,  2.37it/s]  4%|▎         | 115/3145 [00:48<24:15,  2.08it/s]  4%|▍         | 119/3145 [00:48<21:21,  2.36it/s]  4%|▍         | 130/3143 [00:54<21:35,  2.33it/s]  4%|▎         | 116/3145 [00:48<22:47,  2.22it/s]  4%|▍         | 118/3145 [00:48<21:31,  2.34it/s]  4%|▍         | 120/3145 [00:48<21:05,  2.39it/s]  4%|▍         | 131/3143 [00:54<20:42,  2.42it/s]  4%|▍         | 119/3145 [00:49<20:57,  2.41it/s]  4%|▎         | 117/3145 [00:49<22:05,  2.28it/s]  4%|▍         | 121/3145 [00:49<21:50,  2.31it/s]  4%|▍         | 120/3145 [00:49<21:16,  2.37it/s]  4%|▍         | 132/3143 [00:55<22:55,  2.19it/s]  4%|▍         | 118/3145 [00:49<22:09,  2.28it/s]  4%|▍         | 122/3145 [00:49<23:59,  2.10it/s]  4%|▍         | 119/3145 [00:49<21:40,  2.33it/s]  4%|▍         | 121/3145 [00:49<21:47,  2.31it/s]  4%|▍         | 133/3143 [00:55<23:09,  2.17it/s]  4%|▍         | 123/3145 [00:50<23:26,  2.15it/s]  4%|▍         | 134/3143 [00:56<20:48,  2.41it/s]  4%|▍         | 120/3145 [00:50<20:58,  2.40it/s]  4%|▍         | 122/3145 [00:50<21:14,  2.37it/s]  4%|▍         | 135/3143 [00:56<18:09,  2.76it/s]  4%|▍         | 121/3145 [00:50<19:00,  2.65it/s]  4%|▍         | 124/3145 [00:50<22:04,  2.28it/s]  4%|▍         | 123/3145 [00:50<20:46,  2.42it/s]  4%|▍         | 136/3143 [00:56<19:12,  2.61it/s]  4%|▍         | 125/3145 [00:51<22:22,  2.25it/s]  4%|▍         | 122/3145 [00:51<20:23,  2.47it/s]  4%|▍         | 124/3145 [00:51<20:10,  2.50it/s]  4%|▍         | 137/3143 [00:57<17:07,  2.93it/s]  4%|▍         | 123/3145 [00:51<20:02,  2.51it/s]  4%|▍         | 126/3145 [00:51<22:22,  2.25it/s]  4%|▍         | 125/3145 [00:51<20:07,  2.50it/s]  4%|▍         | 138/3143 [00:57<18:47,  2.66it/s]  4%|▍         | 124/3145 [00:51<19:32,  2.58it/s]  4%|▍         | 126/3145 [00:51<19:38,  2.56it/s]  4%|▍         | 127/3145 [00:51<22:26,  2.24it/s]  4%|▍         | 139/3143 [00:58<19:32,  2.56it/s]  4%|▍         | 128/3145 [00:52<19:46,  2.54it/s]  4%|▍         | 125/3145 [00:52<20:23,  2.47it/s]  4%|▍         | 127/3145 [00:52<19:55,  2.52it/s]  4%|▍         | 129/3145 [00:52<19:23,  2.59it/s]  4%|▍         | 126/3145 [00:52<19:36,  2.57it/s]  4%|▍         | 128/3145 [00:52<19:12,  2.62it/s]  4%|▍         | 140/3143 [00:58<22:48,  2.19it/s]  4%|▍         | 130/3145 [00:53<19:57,  2.52it/s]  4%|▍         | 141/3143 [00:58<21:14,  2.35it/s]  4%|▍         | 127/3145 [00:53<20:28,  2.46it/s]  4%|▍         | 129/3145 [00:53<19:54,  2.53it/s]  4%|▍         | 130/3145 [00:53<18:47,  2.67it/s]  4%|▍         | 131/3145 [00:53<20:41,  2.43it/s]  4%|▍         | 128/3145 [00:53<20:57,  2.40it/s]  5%|▍         | 142/3143 [00:59<21:46,  2.30it/s]  4%|▍         | 131/3145 [00:53<19:07,  2.63it/s]  4%|▍         | 132/3145 [00:53<20:00,  2.51it/s]  4%|▍         | 129/3145 [00:53<20:04,  2.50it/s]  5%|▍         | 143/3143 [00:59<22:10,  2.25it/s]  4%|▍         | 132/3145 [00:54<18:50,  2.66it/s]  4%|▍         | 133/3145 [00:54<19:46,  2.54it/s]  4%|▍         | 130/3145 [00:54<19:45,  2.54it/s]  5%|▍         | 144/3143 [01:00<21:32,  2.32it/s]  4%|▍         | 134/3145 [00:54<19:17,  2.60it/s]  4%|▍         | 133/3145 [00:54<20:02,  2.50it/s]  4%|▍         | 131/3145 [00:54<19:48,  2.54it/s]  5%|▍         | 145/3143 [01:00<21:03,  2.37it/s]  4%|▍         | 134/3145 [00:54<18:29,  2.71it/s]  4%|▍         | 135/3145 [00:54<19:02,  2.63it/s]  4%|▍         | 132/3145 [00:55<19:30,  2.57it/s]  5%|▍         | 146/3143 [01:01<20:57,  2.38it/s]  4%|▍         | 135/3145 [00:55<18:55,  2.65it/s]  4%|▍         | 136/3145 [00:55<19:18,  2.60it/s]  4%|▍         | 133/3145 [00:55<19:29,  2.57it/s]  5%|▍         | 147/3143 [01:01<20:10,  2.47it/s]  4%|▍         | 137/3145 [00:55<17:15,  2.91it/s]  4%|▍         | 136/3145 [00:55<18:41,  2.68it/s]  4%|▍         | 134/3145 [00:55<20:17,  2.47it/s]  4%|▍         | 138/3145 [00:55<17:01,  2.94it/s]  5%|▍         | 148/3143 [01:01<20:07,  2.48it/s]  4%|▍         | 137/3145 [00:56<19:06,  2.62it/s]  4%|▍         | 135/3145 [00:56<20:25,  2.46it/s]  4%|▍         | 139/3145 [00:56<17:31,  2.86it/s]  5%|▍         | 149/3143 [01:02<19:35,  2.55it/s]  4%|▍         | 138/3145 [00:56<19:20,  2.59it/s]  4%|▍         | 140/3145 [00:56<17:20,  2.89it/s]  4%|▍         | 136/3145 [00:56<19:59,  2.51it/s]  5%|▍         | 150/3143 [01:02<19:08,  2.61it/s]  4%|▍         | 139/3145 [00:56<19:30,  2.57it/s]  4%|▍         | 137/3145 [00:57<19:46,  2.53it/s]  4%|▍         | 141/3145 [00:57<18:21,  2.73it/s]  5%|▍         | 151/3143 [01:03<19:30,  2.56it/s]  4%|▍         | 138/3145 [00:57<18:25,  2.72it/s]  4%|▍         | 140/3145 [00:57<21:53,  2.29it/s]  5%|▍         | 142/3145 [00:57<19:50,  2.52it/s]  5%|▍         | 152/3143 [01:03<20:17,  2.46it/s]  4%|▍         | 139/3145 [00:57<19:50,  2.53it/s]  4%|▍         | 141/3145 [00:57<21:29,  2.33it/s]  5%|▍         | 143/3145 [00:57<19:45,  2.53it/s]  5%|▍         | 142/3145 [00:58<18:23,  2.72it/s]  5%|▍         | 153/3143 [01:04<22:35,  2.21it/s]  4%|▍         | 140/3145 [00:58<19:34,  2.56it/s]  5%|▍         | 144/3145 [00:58<18:58,  2.64it/s]  5%|▍         | 154/3143 [01:04<21:36,  2.31it/s]  5%|▍         | 143/3145 [00:58<20:04,  2.49it/s]  4%|▍         | 141/3145 [00:58<19:38,  2.55it/s]  5%|▍         | 145/3145 [00:58<19:18,  2.59it/s]  5%|▍         | 155/3143 [01:04<20:51,  2.39it/s]  5%|▍         | 144/3145 [00:58<20:31,  2.44it/s]  5%|▍         | 142/3145 [00:59<20:19,  2.46it/s]  5%|▍         | 146/3145 [00:59<19:57,  2.50it/s]  5%|▍         | 156/3143 [01:05<19:52,  2.50it/s]  5%|▍         | 145/3145 [00:59<20:23,  2.45it/s]  5%|▍         | 143/3145 [00:59<20:42,  2.42it/s]  5%|▍         | 147/3145 [00:59<19:56,  2.50it/s]  5%|▍         | 157/3143 [01:05<20:49,  2.39it/s]  5%|▍         | 146/3145 [00:59<20:08,  2.48it/s]  5%|▍         | 144/3145 [00:59<21:03,  2.38it/s]  5%|▍         | 148/3145 [00:59<19:55,  2.51it/s]  5%|▌         | 158/3143 [01:06<21:21,  2.33it/s]  5%|▍         | 147/3145 [01:00<20:24,  2.45it/s]  5%|▍         | 149/3145 [01:00<19:29,  2.56it/s]  5%|▍         | 145/3145 [01:00<21:05,  2.37it/s]  5%|▌         | 159/3143 [01:06<20:52,  2.38it/s]  5%|▍         | 148/3145 [01:00<20:32,  2.43it/s]  5%|▍         | 150/3145 [01:00<19:21,  2.58it/s]  5%|▍         | 146/3145 [01:00<20:29,  2.44it/s]  5%|▌         | 160/3143 [01:06<18:06,  2.75it/s]  5%|▍         | 149/3145 [01:01<20:28,  2.44it/s]  5%|▍         | 151/3145 [01:01<19:15,  2.59it/s]  5%|▍         | 147/3145 [01:01<19:58,  2.50it/s]  5%|▌         | 161/3143 [01:07<19:15,  2.58it/s]  5%|▍         | 150/3145 [01:01<19:28,  2.56it/s]  5%|▍         | 152/3145 [01:01<19:32,  2.55it/s]  5%|▍         | 148/3145 [01:01<20:27,  2.44it/s]  5%|▌         | 162/3143 [01:07<19:54,  2.50it/s]  5%|▍         | 151/3145 [01:01<19:35,  2.55it/s]  5%|▍         | 149/3145 [01:01<19:40,  2.54it/s]  5%|▍         | 153/3145 [01:01<20:35,  2.42it/s]  5%|▌         | 163/3143 [01:07<19:41,  2.52it/s]  5%|▍         | 152/3145 [01:02<20:14,  2.46it/s]  5%|▍         | 150/3145 [01:02<19:31,  2.56it/s]  5%|▍         | 154/3145 [01:02<21:15,  2.35it/s]  5%|▍         | 153/3145 [01:02<18:44,  2.66it/s]  5%|▌         | 164/3143 [01:08<22:17,  2.23it/s]  5%|▍         | 151/3145 [01:02<20:09,  2.48it/s]  5%|▍         | 155/3145 [01:02<20:47,  2.40it/s]  5%|▍         | 154/3145 [01:02<18:52,  2.64it/s]  5%|▍         | 152/3145 [01:03<20:32,  2.43it/s]  5%|▌         | 165/3143 [01:09<23:34,  2.11it/s]  5%|▍         | 156/3145 [01:03<21:03,  2.37it/s]  5%|▍         | 155/3145 [01:03<19:10,  2.60it/s]  5%|▍         | 153/3145 [01:03<20:49,  2.39it/s]  5%|▌         | 166/3143 [01:09<22:15,  2.23it/s]  5%|▍         | 157/3145 [01:03<21:05,  2.36it/s]  5%|▍         | 156/3145 [01:03<18:58,  2.62it/s]  5%|▌         | 167/3143 [01:09<20:54,  2.37it/s]  5%|▍         | 154/3145 [01:03<20:22,  2.45it/s]  5%|▌         | 158/3145 [01:03<20:34,  2.42it/s]  5%|▍         | 157/3145 [01:04<18:44,  2.66it/s]  5%|▌         | 158/3145 [01:04<16:59,  2.93it/s]  5%|▌         | 168/3143 [01:10<20:47,  2.39it/s]  5%|▌         | 159/3145 [01:04<19:54,  2.50it/s]  5%|▍         | 155/3145 [01:04<21:32,  2.31it/s]  5%|▌         | 159/3145 [01:04<17:27,  2.85it/s]  5%|▌         | 169/3143 [01:10<20:41,  2.40it/s]  5%|▌         | 160/3145 [01:04<20:13,  2.46it/s]  5%|▍         | 156/3145 [01:04<23:15,  2.14it/s]  5%|▌         | 160/3145 [01:05<18:05,  2.75it/s]  5%|▌         | 170/3143 [01:10<19:44,  2.51it/s]  5%|▌         | 161/3145 [01:05<21:02,  2.36it/s]  5%|▍         | 157/3145 [01:05<22:12,  2.24it/s]  5%|▌         | 161/3145 [01:05<20:24,  2.44it/s]  5%|▌         | 171/3143 [01:11<21:49,  2.27it/s]  5%|▌         | 162/3145 [01:05<20:41,  2.40it/s]  5%|▌         | 158/3145 [01:05<22:06,  2.25it/s]  5%|▌         | 172/3143 [01:11<20:42,  2.39it/s]  5%|▌         | 162/3145 [01:06<21:04,  2.36it/s]  5%|▌         | 163/3145 [01:06<21:39,  2.30it/s]  5%|▌         | 159/3145 [01:06<21:02,  2.37it/s]  6%|▌         | 173/3143 [01:12<19:58,  2.48it/s]  5%|▌         | 163/3145 [01:06<20:01,  2.48it/s]  5%|▌         | 164/3145 [01:06<20:46,  2.39it/s]  5%|▌         | 160/3145 [01:06<21:07,  2.36it/s]  5%|▌         | 164/3145 [01:06<19:22,  2.57it/s]  6%|▌         | 174/3143 [01:12<20:22,  2.43it/s]  5%|▌         | 165/3145 [01:06<20:20,  2.44it/s]  5%|▌         | 165/3145 [01:07<19:25,  2.56it/s]  5%|▌         | 161/3145 [01:07<23:01,  2.16it/s]  6%|▌         | 175/3143 [01:13<20:15,  2.44it/s]  5%|▌         | 166/3145 [01:07<20:20,  2.44it/s]  5%|▌         | 166/3145 [01:07<19:14,  2.58it/s]  6%|▌         | 176/3143 [01:13<19:40,  2.51it/s]  5%|▌         | 162/3145 [01:07<23:25,  2.12it/s]  5%|▌         | 167/3145 [01:07<20:10,  2.46it/s]  5%|▌         | 167/3145 [01:07<19:22,  2.56it/s]  5%|▌         | 168/3145 [01:07<17:41,  2.80it/s]  6%|▌         | 177/3143 [01:13<19:39,  2.51it/s]  5%|▌         | 163/3145 [01:08<23:22,  2.13it/s]  5%|▌         | 169/3145 [01:08<17:34,  2.82it/s]  5%|▌         | 168/3145 [01:08<19:29,  2.55it/s]  6%|▌         | 178/3143 [01:14<19:19,  2.56it/s]  5%|▌         | 164/3145 [01:08<22:44,  2.19it/s]  5%|▌         | 170/3145 [01:08<17:57,  2.76it/s]  5%|▌         | 169/3145 [01:08<20:05,  2.47it/s]  6%|▌         | 179/3143 [01:14<19:25,  2.54it/s]  5%|▌         | 165/3145 [01:08<22:20,  2.22it/s]  5%|▌         | 170/3145 [01:09<19:24,  2.55it/s]  6%|▌         | 180/3143 [01:15<19:15,  2.56it/s]  5%|▌         | 171/3145 [01:09<19:16,  2.57it/s]  5%|▌         | 166/3145 [01:09<22:07,  2.24it/s]  5%|▌         | 171/3145 [01:09<20:03,  2.47it/s]  6%|▌         | 181/3143 [01:15<20:06,  2.45it/s]  5%|▌         | 172/3145 [01:09<20:26,  2.42it/s]  5%|▌         | 167/3145 [01:09<21:14,  2.34it/s]  5%|▌         | 172/3145 [01:09<19:20,  2.56it/s]  6%|▌         | 182/3143 [01:15<20:20,  2.43it/s]  6%|▌         | 173/3145 [01:10<20:57,  2.36it/s]  5%|▌         | 168/3145 [01:10<21:33,  2.30it/s]  6%|▌         | 173/3145 [01:10<19:57,  2.48it/s]  6%|▌         | 183/3143 [01:16<21:01,  2.35it/s]  6%|▌         | 174/3145 [01:10<21:20,  2.32it/s]  5%|▌         | 169/3145 [01:10<21:25,  2.31it/s]  6%|▌         | 174/3145 [01:10<20:21,  2.43it/s]  6%|▌         | 184/3143 [01:16<21:16,  2.32it/s]  6%|▌         | 175/3145 [01:11<22:46,  2.17it/s]  5%|▌         | 170/3145 [01:11<20:21,  2.44it/s]  6%|▌         | 175/3145 [01:11<20:42,  2.39it/s]  6%|▌         | 185/3143 [01:17<20:46,  2.37it/s]  6%|▌         | 176/3145 [01:11<20:55,  2.36it/s]  5%|▌         | 171/3145 [01:11<19:47,  2.51it/s]  6%|▌         | 176/3145 [01:11<20:07,  2.46it/s]  6%|▌         | 186/3143 [01:17<20:25,  2.41it/s]  6%|▌         | 177/3145 [01:11<20:31,  2.41it/s]  5%|▌         | 172/3145 [01:11<19:42,  2.51it/s]  6%|▌         | 177/3145 [01:11<19:59,  2.47it/s]  6%|▌         | 187/3143 [01:17<19:12,  2.56it/s]  6%|▌         | 173/3145 [01:12<19:32,  2.54it/s]  6%|▌         | 178/3145 [01:12<21:19,  2.32it/s]  6%|▌         | 178/3145 [01:12<19:52,  2.49it/s]  6%|▌         | 188/3143 [01:18<18:48,  2.62it/s]  6%|▌         | 174/3145 [01:12<18:59,  2.61it/s]  6%|▌         | 179/3145 [01:12<20:58,  2.36it/s]  6%|▌         | 179/3145 [01:12<19:47,  2.50it/s]  6%|▌         | 189/3143 [01:18<19:46,  2.49it/s]  6%|▌         | 175/3145 [01:12<19:30,  2.54it/s]  6%|▌         | 180/3145 [01:13<20:42,  2.39it/s]  6%|▌         | 180/3145 [01:13<20:24,  2.42it/s]  6%|▌         | 190/3143 [01:19<20:12,  2.44it/s]  6%|▌         | 176/3145 [01:13<19:47,  2.50it/s]  6%|▌         | 181/3145 [01:13<21:02,  2.35it/s]  6%|▌         | 181/3145 [01:13<20:39,  2.39it/s]  6%|▌         | 191/3143 [01:19<20:52,  2.36it/s]  6%|▌         | 177/3145 [01:13<19:50,  2.49it/s]  6%|▌         | 182/3145 [01:13<21:30,  2.30it/s]  6%|▌         | 182/3145 [01:14<19:47,  2.50it/s]  6%|▌         | 192/3143 [01:20<20:32,  2.39it/s]  6%|▌         | 178/3145 [01:14<19:43,  2.51it/s]  6%|▌         | 183/3145 [01:14<21:31,  2.29it/s]  6%|▌         | 183/3145 [01:14<20:49,  2.37it/s]  6%|▌         | 193/3143 [01:20<19:45,  2.49it/s]  6%|▌         | 179/3145 [01:14<20:14,  2.44it/s]  6%|▌         | 194/3143 [01:20<17:26,  2.82it/s]  6%|▌         | 184/3145 [01:14<20:59,  2.35it/s]  6%|▌         | 184/3145 [01:14<20:12,  2.44it/s]  6%|▌         | 180/3145 [01:15<20:53,  2.37it/s]  6%|▌         | 195/3143 [01:20<17:32,  2.80it/s]  6%|▌         | 185/3145 [01:15<20:00,  2.46it/s]  6%|▌         | 185/3145 [01:15<19:50,  2.49it/s]  6%|▌         | 181/3145 [01:15<20:28,  2.41it/s]  6%|▌         | 196/3143 [01:21<18:06,  2.71it/s]  6%|▌         | 186/3145 [01:15<19:27,  2.54it/s]  6%|▌         | 186/3145 [01:15<19:24,  2.54it/s]  6%|▋         | 197/3143 [01:21<16:29,  2.98it/s]  6%|▌         | 182/3145 [01:15<19:48,  2.49it/s]  6%|▌         | 187/3145 [01:15<19:44,  2.50it/s]  6%|▌         | 187/3145 [01:15<19:12,  2.57it/s]  6%|▋         | 198/3143 [01:21<15:34,  3.15it/s]  6%|▌         | 183/3145 [01:16<20:02,  2.46it/s]  6%|▌         | 188/3145 [01:16<19:38,  2.51it/s]  6%|▌         | 188/3145 [01:16<19:48,  2.49it/s]  6%|▋         | 199/3143 [01:22<17:48,  2.76it/s]  6%|▌         | 184/3145 [01:16<20:42,  2.38it/s]  6%|▌         | 189/3145 [01:16<19:42,  2.50it/s]  6%|▌         | 189/3145 [01:16<20:08,  2.45it/s]  6%|▋         | 200/3143 [01:22<18:49,  2.60it/s]  6%|▌         | 190/3145 [01:17<19:06,  2.58it/s]  6%|▌         | 185/3145 [01:17<20:31,  2.40it/s]  6%|▌         | 190/3145 [01:17<19:22,  2.54it/s]  6%|▋         | 201/3143 [01:23<19:35,  2.50it/s]  6%|▌         | 191/3145 [01:17<19:13,  2.56it/s]  6%|▌         | 186/3145 [01:17<20:22,  2.42it/s]  6%|▋         | 202/3143 [01:23<19:34,  2.50it/s]  6%|▌         | 191/3145 [01:17<22:06,  2.23it/s]  6%|▌         | 192/3145 [01:17<19:19,  2.55it/s]  6%|▌         | 187/3145 [01:17<20:42,  2.38it/s]  6%|▋         | 203/3143 [01:24<19:31,  2.51it/s]  6%|▌         | 192/3145 [01:18<21:56,  2.24it/s]  6%|▌         | 188/3145 [01:18<20:00,  2.46it/s]  6%|▌         | 193/3145 [01:18<21:11,  2.32it/s]  6%|▋         | 204/3143 [01:24<19:23,  2.53it/s]  6%|▌         | 193/3145 [01:18<21:18,  2.31it/s]  6%|▌         | 189/3145 [01:18<19:24,  2.54it/s]  6%|▌         | 194/3145 [01:18<21:16,  2.31it/s]  7%|▋         | 205/3143 [01:24<19:22,  2.53it/s]  6%|▌         | 190/3145 [01:19<19:24,  2.54it/s]  6%|▌         | 194/3145 [01:19<22:21,  2.20it/s]  6%|▌         | 195/3145 [01:19<20:19,  2.42it/s]  7%|▋         | 206/3143 [01:25<19:19,  2.53it/s]  6%|▌         | 191/3145 [01:19<19:51,  2.48it/s]  6%|▌         | 195/3145 [01:19<21:46,  2.26it/s]  6%|▌         | 196/3145 [01:19<20:16,  2.42it/s]  7%|▋         | 207/3143 [01:25<19:52,  2.46it/s]  6%|▌         | 192/3145 [01:19<20:36,  2.39it/s]  6%|▌         | 196/3145 [01:19<21:36,  2.27it/s]  6%|▋         | 197/3145 [01:20<20:31,  2.39it/s]  7%|▋         | 208/3143 [01:26<20:12,  2.42it/s]  6%|▌         | 193/3145 [01:20<20:07,  2.44it/s]  6%|▋         | 197/3145 [01:20<21:10,  2.32it/s]  6%|▋         | 198/3145 [01:20<20:16,  2.42it/s]  7%|▋         | 209/3143 [01:26<20:15,  2.41it/s]  6%|▌         | 194/3145 [01:20<19:41,  2.50it/s]  6%|▋         | 198/3145 [01:20<21:12,  2.32it/s]  6%|▋         | 199/3145 [01:20<20:11,  2.43it/s]  7%|▋         | 210/3143 [01:26<20:27,  2.39it/s]  6%|▌         | 195/3145 [01:21<19:29,  2.52it/s]  6%|▋         | 199/3145 [01:21<20:21,  2.41it/s]  6%|▋         | 200/3145 [01:21<19:44,  2.49it/s]  6%|▌         | 196/3145 [01:21<18:51,  2.61it/s]  7%|▋         | 211/3143 [01:27<21:02,  2.32it/s]  6%|▋         | 200/3145 [01:21<20:37,  2.38it/s]  6%|▋         | 201/3145 [01:21<19:51,  2.47it/s]  6%|▋         | 197/3145 [01:21<18:53,  2.60it/s]  7%|▋         | 212/3143 [01:27<21:24,  2.28it/s]  6%|▋         | 202/3145 [01:22<19:57,  2.46it/s]  6%|▋         | 201/3145 [01:22<20:50,  2.35it/s]  6%|▋         | 198/3145 [01:22<18:29,  2.66it/s]  7%|▋         | 213/3143 [01:28<20:39,  2.36it/s]  6%|▋         | 202/3145 [01:22<19:56,  2.46it/s]  6%|▋         | 203/3145 [01:22<21:18,  2.30it/s]  6%|▋         | 199/3145 [01:22<18:31,  2.65it/s]  7%|▋         | 214/3143 [01:28<20:16,  2.41it/s]  6%|▋         | 203/3145 [01:22<19:45,  2.48it/s]  6%|▋         | 204/3145 [01:23<21:47,  2.25it/s]  6%|▋         | 200/3145 [01:23<19:32,  2.51it/s]  7%|▋         | 215/3143 [01:29<19:25,  2.51it/s]  6%|▋         | 204/3145 [01:23<19:30,  2.51it/s]  6%|▋         | 201/3145 [01:23<18:48,  2.61it/s]  7%|▋         | 205/3145 [01:23<21:11,  2.31it/s]  7%|▋         | 216/3143 [01:29<19:04,  2.56it/s]  7%|▋         | 205/3145 [01:23<20:19,  2.41it/s]  7%|▋         | 206/3145 [01:23<19:57,  2.45it/s]  6%|▋         | 202/3145 [01:23<18:59,  2.58it/s]  7%|▋         | 217/3143 [01:29<19:20,  2.52it/s]  7%|▋         | 206/3145 [01:24<20:04,  2.44it/s]  7%|▋         | 207/3145 [01:24<19:25,  2.52it/s]  6%|▋         | 203/3145 [01:24<19:36,  2.50it/s]  7%|▋         | 218/3143 [01:30<18:49,  2.59it/s]  7%|▋         | 207/3145 [01:24<20:26,  2.40it/s]  6%|▋         | 204/3145 [01:24<18:59,  2.58it/s]  7%|▋         | 208/3145 [01:24<19:59,  2.45it/s]  7%|▋         | 219/3143 [01:30<18:40,  2.61it/s]  7%|▋         | 208/3145 [01:24<19:59,  2.45it/s]  7%|▋         | 209/3145 [01:24<20:15,  2.42it/s]  7%|▋         | 205/3145 [01:25<19:46,  2.48it/s]  7%|▋         | 220/3143 [01:30<18:50,  2.58it/s]  7%|▋         | 209/3145 [01:25<19:17,  2.54it/s]  7%|▋         | 210/3145 [01:25<20:01,  2.44it/s]  7%|▋         | 221/3143 [01:31<19:38,  2.48it/s]  7%|▋         | 206/3145 [01:25<20:56,  2.34it/s]  7%|▋         | 210/3145 [01:25<19:38,  2.49it/s]  7%|▋         | 211/3145 [01:25<19:59,  2.45it/s]  7%|▋         | 207/3145 [01:25<21:05,  2.32it/s]  7%|▋         | 222/3143 [01:31<20:33,  2.37it/s]  7%|▋         | 211/3145 [01:26<20:12,  2.42it/s]  7%|▋         | 212/3145 [01:26<20:40,  2.36it/s]  7%|▋         | 223/3143 [01:32<19:36,  2.48it/s]  7%|▋         | 208/3145 [01:26<20:52,  2.35it/s]  7%|▋         | 212/3145 [01:26<19:46,  2.47it/s]  7%|▋         | 213/3145 [01:26<20:11,  2.42it/s]  7%|▋         | 224/3143 [01:32<20:07,  2.42it/s]  7%|▋         | 209/3145 [01:26<20:17,  2.41it/s]  7%|▋         | 213/3145 [01:26<19:48,  2.47it/s]  7%|▋         | 214/3145 [01:27<19:53,  2.45it/s]  7%|▋         | 210/3145 [01:27<19:27,  2.51it/s]  7%|▋         | 225/3143 [01:33<20:08,  2.41it/s]  7%|▋         | 214/3145 [01:27<19:41,  2.48it/s]  7%|▋         | 215/3145 [01:27<20:18,  2.40it/s]  7%|▋         | 211/3145 [01:27<20:17,  2.41it/s]  7%|▋         | 226/3143 [01:33<21:19,  2.28it/s]  7%|▋         | 215/3145 [01:27<20:27,  2.39it/s]  7%|▋         | 216/3145 [01:27<20:02,  2.44it/s]  7%|▋         | 212/3145 [01:27<20:02,  2.44it/s]  7%|▋         | 227/3143 [01:33<21:24,  2.27it/s]  7%|▋         | 217/3145 [01:28<18:17,  2.67it/s]  7%|▋         | 213/3145 [01:28<19:05,  2.56it/s]  7%|▋         | 216/3145 [01:28<22:24,  2.18it/s]  7%|▋         | 228/3143 [01:34<20:37,  2.35it/s]  7%|▋         | 218/3145 [01:28<18:08,  2.69it/s]  7%|▋         | 217/3145 [01:28<21:26,  2.28it/s]  7%|▋         | 229/3143 [01:34<18:10,  2.67it/s]  7%|▋         | 214/3145 [01:28<19:50,  2.46it/s]  7%|▋         | 218/3145 [01:28<18:22,  2.65it/s]  7%|▋         | 219/3145 [01:28<18:57,  2.57it/s]  7%|▋         | 215/3145 [01:29<19:13,  2.54it/s]  7%|▋         | 230/3143 [01:35<18:33,  2.62it/s]  7%|▋         | 219/3145 [01:29<19:00,  2.57it/s]  7%|▋         | 220/3145 [01:29<18:50,  2.59it/s]  7%|▋         | 231/3143 [01:35<18:47,  2.58it/s]  7%|▋         | 216/3145 [01:29<19:56,  2.45it/s]  7%|▋         | 220/3145 [01:29<19:00,  2.56it/s]  7%|▋         | 221/3145 [01:29<19:11,  2.54it/s]  7%|▋         | 217/3145 [01:29<19:54,  2.45it/s]  7%|▋         | 232/3143 [01:35<19:37,  2.47it/s]  7%|▋         | 221/3145 [01:30<19:02,  2.56it/s]  7%|▋         | 222/3145 [01:30<19:16,  2.53it/s]  7%|▋         | 218/3145 [01:30<20:35,  2.37it/s]  7%|▋         | 233/3143 [01:36<20:41,  2.34it/s]  7%|▋         | 223/3145 [01:30<19:16,  2.53it/s]  7%|▋         | 222/3145 [01:30<21:41,  2.25it/s]  7%|▋         | 219/3145 [01:30<19:40,  2.48it/s]  7%|▋         | 234/3143 [01:36<20:15,  2.39it/s]  7%|▋         | 224/3145 [01:30<19:08,  2.54it/s]  7%|▋         | 223/3145 [01:31<20:52,  2.33it/s]  7%|▋         | 235/3143 [01:37<19:15,  2.52it/s]  7%|▋         | 220/3145 [01:31<22:26,  2.17it/s]  7%|▋         | 225/3145 [01:31<19:39,  2.48it/s]  7%|▋         | 224/3145 [01:31<20:48,  2.34it/s]  7%|▋         | 226/3145 [01:31<17:10,  2.83it/s]  8%|▊         | 236/3143 [01:37<19:35,  2.47it/s]  7%|▋         | 221/3145 [01:31<20:56,  2.33it/s]  8%|▊         | 237/3143 [01:37<17:00,  2.85it/s]  7%|▋         | 225/3145 [01:31<21:19,  2.28it/s]  7%|▋         | 227/3145 [01:31<17:54,  2.72it/s]  7%|▋         | 222/3145 [01:32<20:29,  2.38it/s]  7%|▋         | 228/3145 [01:32<18:03,  2.69it/s]  8%|▊         | 238/3143 [01:38<19:53,  2.43it/s]  7%|▋         | 226/3145 [01:32<21:15,  2.29it/s]  7%|▋         | 223/3145 [01:32<20:23,  2.39it/s]  8%|▊         | 239/3143 [01:38<19:33,  2.48it/s]  7%|▋         | 229/3145 [01:32<18:34,  2.62it/s]  7%|▋         | 227/3145 [01:32<21:16,  2.29it/s]  7%|▋         | 224/3145 [01:32<20:17,  2.40it/s]  8%|▊         | 240/3143 [01:39<19:58,  2.42it/s]  7%|▋         | 230/3145 [01:33<19:37,  2.48it/s]  7%|▋         | 228/3145 [01:33<20:46,  2.34it/s]  7%|▋         | 225/3145 [01:33<19:34,  2.49it/s]  7%|▋         | 231/3145 [01:33<19:10,  2.53it/s]  8%|▊         | 241/3143 [01:39<20:01,  2.41it/s]  7%|▋         | 226/3145 [01:33<19:28,  2.50it/s]  7%|▋         | 229/3145 [01:33<21:24,  2.27it/s]  8%|▊         | 242/3143 [01:39<20:15,  2.39it/s]  7%|▋         | 227/3145 [01:34<19:16,  2.52it/s]  7%|▋         | 232/3145 [01:34<20:18,  2.39it/s]  7%|▋         | 230/3145 [01:34<20:47,  2.34it/s]  8%|▊         | 243/3143 [01:40<21:53,  2.21it/s]  7%|▋         | 233/3145 [01:34<24:25,  1.99it/s]  7%|▋         | 231/3145 [01:34<24:25,  1.99it/s]  7%|▋         | 228/3145 [01:34<24:24,  1.99it/s]  8%|▊         | 244/3143 [01:40<21:18,  2.27it/s]  7%|▋         | 234/3145 [01:35<22:25,  2.16it/s]  7%|▋         | 232/3145 [01:35<23:06,  2.10it/s]  7%|▋         | 229/3145 [01:35<24:30,  1.98it/s]  7%|▋         | 235/3145 [01:35<23:02,  2.10it/s]  8%|▊         | 245/3143 [01:41<25:34,  1.89it/s]  7%|▋         | 233/3145 [01:35<24:20,  1.99it/s]  7%|▋         | 230/3145 [01:35<24:29,  1.98it/s]  8%|▊         | 236/3145 [01:36<21:26,  2.26it/s]  7%|▋         | 234/3145 [01:36<22:39,  2.14it/s]  8%|▊         | 246/3143 [01:42<24:53,  1.94it/s]  7%|▋         | 231/3145 [01:36<23:09,  2.10it/s]  8%|▊         | 237/3145 [01:36<20:51,  2.32it/s]  8%|▊         | 247/3143 [01:42<22:26,  2.15it/s]  7%|▋         | 232/3145 [01:36<21:55,  2.21it/s]  7%|▋         | 235/3145 [01:36<23:12,  2.09it/s]  8%|▊         | 238/3145 [01:36<20:24,  2.37it/s]  8%|▊         | 248/3143 [01:42<22:04,  2.19it/s]  7%|▋         | 233/3145 [01:37<20:49,  2.33it/s]  8%|▊         | 236/3145 [01:37<24:03,  2.02it/s]  8%|▊         | 239/3145 [01:37<20:30,  2.36it/s]  8%|▊         | 249/3143 [01:43<20:55,  2.31it/s]  7%|▋         | 234/3145 [01:37<19:54,  2.44it/s]  8%|▊         | 240/3145 [01:37<20:04,  2.41it/s]  8%|▊         | 237/3145 [01:37<24:19,  1.99it/s]  7%|▋         | 235/3145 [01:37<19:47,  2.45it/s]  8%|▊         | 250/3143 [01:43<21:02,  2.29it/s]  8%|▊         | 241/3145 [01:38<20:33,  2.35it/s]  8%|▊         | 236/3145 [01:38<20:07,  2.41it/s]  8%|▊         | 238/3145 [01:38<24:43,  1.96it/s]  8%|▊         | 251/3143 [01:44<20:57,  2.30it/s]  8%|▊         | 242/3145 [01:38<21:47,  2.22it/s]  8%|▊         | 237/3145 [01:38<19:52,  2.44it/s]  8%|▊         | 252/3143 [01:44<20:29,  2.35it/s]  8%|▊         | 239/3145 [01:38<25:09,  1.93it/s]  8%|▊         | 240/3145 [01:39<20:52,  2.32it/s]  8%|▊         | 253/3143 [01:44<19:58,  2.41it/s]  8%|▊         | 243/3145 [01:39<21:44,  2.22it/s]  8%|▊         | 238/3145 [01:39<20:24,  2.37it/s]  8%|▊         | 241/3145 [01:39<20:02,  2.41it/s]  8%|▊         | 254/3143 [01:45<19:39,  2.45it/s]  8%|▊         | 244/3145 [01:39<21:07,  2.29it/s]  8%|▊         | 239/3145 [01:39<20:41,  2.34it/s]  8%|▊         | 242/3145 [01:39<20:36,  2.35it/s]  8%|▊         | 245/3145 [01:39<20:35,  2.35it/s]  8%|▊         | 255/3143 [01:45<20:12,  2.38it/s]  8%|▊         | 240/3145 [01:39<20:42,  2.34it/s]  8%|▊         | 243/3145 [01:40<20:11,  2.40it/s]  8%|▊         | 246/3145 [01:40<20:36,  2.34it/s]  8%|▊         | 256/3143 [01:46<20:20,  2.36it/s]  8%|▊         | 241/3145 [01:40<20:37,  2.35it/s]  8%|▊         | 244/3145 [01:40<19:45,  2.45it/s]  8%|▊         | 247/3145 [01:40<20:48,  2.32it/s]  8%|▊         | 242/3145 [01:40<20:01,  2.42it/s]  8%|▊         | 257/3143 [01:46<20:38,  2.33it/s]  8%|▊         | 245/3145 [01:41<20:15,  2.39it/s]  8%|▊         | 248/3145 [01:41<20:23,  2.37it/s]  8%|▊         | 258/3143 [01:47<20:41,  2.32it/s]  8%|▊         | 243/3145 [01:41<23:08,  2.09it/s]  8%|▊         | 246/3145 [01:41<19:53,  2.43it/s]  8%|▊         | 249/3145 [01:41<20:06,  2.40it/s]  8%|▊         | 259/3143 [01:47<20:14,  2.38it/s]  8%|▊         | 247/3145 [01:41<17:53,  2.70it/s]  8%|▊         | 244/3145 [01:41<22:14,  2.17it/s]  8%|▊         | 250/3145 [01:41<19:45,  2.44it/s]  8%|▊         | 260/3143 [01:47<19:34,  2.45it/s]  8%|▊         | 248/3145 [01:42<18:54,  2.55it/s]  8%|▊         | 245/3145 [01:42<22:34,  2.14it/s]  8%|▊         | 251/3145 [01:42<20:16,  2.38it/s]  8%|▊         | 261/3143 [01:48<19:31,  2.46it/s]  8%|▊         | 249/3145 [01:42<19:04,  2.53it/s]  8%|▊         | 252/3145 [01:42<19:45,  2.44it/s]  8%|▊         | 262/3143 [01:48<19:25,  2.47it/s]  8%|▊         | 246/3145 [01:42<23:21,  2.07it/s]  8%|▊         | 250/3145 [01:42<18:20,  2.63it/s]  8%|▊         | 263/3143 [01:49<19:18,  2.49it/s]  8%|▊         | 247/3145 [01:43<22:58,  2.10it/s]  8%|▊         | 253/3145 [01:43<21:19,  2.26it/s]  8%|▊         | 251/3145 [01:43<18:37,  2.59it/s]  8%|▊         | 264/3143 [01:49<19:41,  2.44it/s]  8%|▊         | 254/3145 [01:43<20:34,  2.34it/s]  8%|▊         | 248/3145 [01:43<22:49,  2.12it/s]  8%|▊         | 252/3145 [01:43<19:06,  2.52it/s]  8%|▊         | 255/3145 [01:43<17:49,  2.70it/s]  8%|▊         | 265/3143 [01:49<20:03,  2.39it/s]  8%|▊         | 249/3145 [01:44<21:37,  2.23it/s]  8%|▊         | 253/3145 [01:44<18:57,  2.54it/s]  8%|▊         | 256/3145 [01:44<18:13,  2.64it/s]  8%|▊         | 254/3145 [01:44<18:13,  2.64it/s]  8%|▊         | 266/3143 [01:50<20:34,  2.33it/s]  8%|▊         | 257/3145 [01:44<16:07,  2.98it/s]  8%|▊         | 250/3145 [01:44<21:52,  2.21it/s]  8%|▊         | 255/3145 [01:44<19:18,  2.49it/s]  8%|▊         | 267/3143 [01:50<20:41,  2.32it/s]  8%|▊         | 258/3145 [01:44<17:27,  2.76it/s]  8%|▊         | 251/3145 [01:45<21:42,  2.22it/s]  9%|▊         | 268/3143 [01:51<18:08,  2.64it/s]  8%|▊         | 256/3145 [01:45<19:06,  2.52it/s]  8%|▊         | 259/3145 [01:45<18:31,  2.60it/s]  8%|▊         | 252/3145 [01:45<20:50,  2.31it/s]  8%|▊         | 257/3145 [01:45<17:46,  2.71it/s]  9%|▊         | 269/3143 [01:51<20:02,  2.39it/s]  8%|▊         | 260/3145 [01:45<19:12,  2.50it/s]  8%|▊         | 253/3145 [01:45<21:01,  2.29it/s]  8%|▊         | 258/3145 [01:46<18:29,  2.60it/s]  9%|▊         | 270/3143 [01:51<19:37,  2.44it/s]  8%|▊         | 254/3145 [01:46<19:57,  2.41it/s]  8%|▊         | 261/3145 [01:46<19:17,  2.49it/s]  8%|▊         | 259/3145 [01:46<16:07,  2.98it/s]  9%|▊         | 271/3143 [01:52<20:13,  2.37it/s]  8%|▊         | 255/3145 [01:46<19:34,  2.46it/s]  8%|▊         | 262/3145 [01:46<19:55,  2.41it/s]  8%|▊         | 260/3145 [01:46<17:55,  2.68it/s]  9%|▊         | 272/3143 [01:52<20:26,  2.34it/s]  8%|▊         | 256/3145 [01:47<19:15,  2.50it/s]  8%|▊         | 261/3145 [01:47<18:35,  2.59it/s]  8%|▊         | 263/3145 [01:47<20:26,  2.35it/s]  9%|▊         | 273/3143 [01:53<19:59,  2.39it/s]  8%|▊         | 257/3145 [01:47<20:04,  2.40it/s]  8%|▊         | 262/3145 [01:47<18:07,  2.65it/s]  8%|▊         | 264/3145 [01:47<20:27,  2.35it/s]  9%|▊         | 274/3143 [01:53<20:17,  2.36it/s]  8%|▊         | 258/3145 [01:47<20:16,  2.37it/s]  8%|▊         | 263/3145 [01:47<18:56,  2.54it/s]  8%|▊         | 265/3145 [01:48<20:50,  2.30it/s]  8%|▊         | 259/3145 [01:48<19:25,  2.48it/s]  9%|▊         | 275/3143 [01:54<20:35,  2.32it/s]  8%|▊         | 264/3145 [01:48<18:51,  2.55it/s]  8%|▊         | 266/3145 [01:48<20:51,  2.30it/s]  9%|▉         | 276/3143 [01:54<20:38,  2.31it/s]  8%|▊         | 260/3145 [01:48<20:07,  2.39it/s]  8%|▊         | 265/3145 [01:48<19:04,  2.52it/s]  8%|▊         | 267/3145 [01:48<20:21,  2.36it/s]  8%|▊         | 266/3145 [01:49<19:00,  2.53it/s]  8%|▊         | 261/3145 [01:49<20:34,  2.34it/s]  9%|▉         | 277/3143 [01:55<20:55,  2.28it/s]  9%|▊         | 268/3145 [01:49<19:46,  2.42it/s]  9%|▉         | 278/3143 [01:55<19:48,  2.41it/s]  8%|▊         | 262/3145 [01:49<19:50,  2.42it/s]  8%|▊         | 267/3145 [01:49<19:31,  2.46it/s]  8%|▊         | 263/3145 [01:49<17:33,  2.74it/s]  9%|▊         | 269/3145 [01:49<21:36,  2.22it/s]  9%|▉         | 279/3143 [01:55<20:06,  2.37it/s]  9%|▊         | 268/3145 [01:49<19:24,  2.47it/s]  9%|▊         | 270/3145 [01:50<20:32,  2.33it/s]  8%|▊         | 264/3145 [01:50<17:50,  2.69it/s]  9%|▉         | 280/3143 [01:56<19:38,  2.43it/s]  9%|▊         | 269/3145 [01:50<19:20,  2.48it/s]  8%|▊         | 265/3145 [01:50<18:51,  2.55it/s]  9%|▊         | 270/3145 [01:50<19:24,  2.47it/s]  9%|▊         | 271/3145 [01:50<23:11,  2.07it/s]  9%|▉         | 281/3143 [01:56<22:33,  2.11it/s]  8%|▊         | 266/3145 [01:51<18:59,  2.53it/s]  9%|▊         | 271/3145 [01:51<17:21,  2.76it/s]  9%|▊         | 272/3145 [01:51<22:35,  2.12it/s]  9%|▊         | 272/3145 [01:51<15:42,  3.05it/s]  9%|▉         | 282/3143 [01:57<22:02,  2.16it/s]  8%|▊         | 267/3145 [01:51<18:55,  2.53it/s]  9%|▊         | 273/3145 [01:51<21:59,  2.18it/s]  9%|▊         | 268/3145 [01:51<16:52,  2.84it/s]  9%|▉         | 283/3143 [01:57<21:30,  2.22it/s]  9%|▊         | 273/3145 [01:51<19:15,  2.48it/s]  9%|▊         | 274/3145 [01:52<16:51,  2.84it/s]  9%|▊         | 274/3145 [01:52<21:43,  2.20it/s]  9%|▊         | 269/3145 [01:52<18:35,  2.58it/s]  9%|▉         | 284/3143 [01:58<21:33,  2.21it/s]  9%|▊         | 275/3145 [01:52<17:31,  2.73it/s]  9%|▊         | 275/3145 [01:52<21:38,  2.21it/s]  9%|▊         | 270/3145 [01:52<19:25,  2.47it/s]  9%|▉         | 285/3143 [01:58<21:24,  2.22it/s]  9%|▊         | 271/3145 [01:52<18:33,  2.58it/s]  9%|▉         | 276/3145 [01:52<21:05,  2.27it/s]  9%|▉         | 276/3145 [01:53<19:34,  2.44it/s]  9%|▉         | 286/3143 [01:59<21:31,  2.21it/s]  9%|▊         | 272/3145 [01:53<19:08,  2.50it/s]  9%|▉         | 277/3145 [01:53<20:36,  2.32it/s]  9%|▉         | 277/3145 [01:53<19:55,  2.40it/s]  9%|▉         | 287/3143 [01:59<20:46,  2.29it/s]  9%|▊         | 273/3145 [01:53<18:52,  2.54it/s]  9%|▉         | 278/3145 [01:53<20:06,  2.38it/s]  9%|▉         | 278/3145 [01:53<19:08,  2.50it/s]  9%|▉         | 288/3143 [01:59<20:13,  2.35it/s]  9%|▉         | 279/3145 [01:54<18:09,  2.63it/s]  9%|▊         | 274/3145 [01:54<18:53,  2.53it/s]  9%|▉         | 279/3145 [01:54<18:52,  2.53it/s]  9%|▉         | 280/3145 [01:54<17:40,  2.70it/s]  9%|▉         | 289/3143 [02:00<20:25,  2.33it/s]  9%|▊         | 275/3145 [01:54<19:01,  2.51it/s]  9%|▉         | 280/3145 [01:54<18:54,  2.52it/s]  9%|▉         | 281/3145 [01:54<18:36,  2.57it/s]  9%|▉         | 276/3145 [01:54<19:19,  2.47it/s]  9%|▉         | 281/3145 [01:55<19:28,  2.45it/s]  9%|▉         | 290/3143 [02:00<22:53,  2.08it/s]  9%|▉         | 282/3145 [01:55<18:36,  2.56it/s]  9%|▉         | 277/3145 [01:55<19:32,  2.45it/s]  9%|▉         | 282/3145 [01:55<19:38,  2.43it/s]  9%|▉         | 291/3143 [02:01<24:38,  1.93it/s]  9%|▉         | 283/3145 [01:55<19:42,  2.42it/s]  9%|▉         | 278/3145 [01:55<19:27,  2.46it/s]  9%|▉         | 283/3145 [01:55<19:31,  2.44it/s]  9%|▉         | 292/3143 [02:01<22:03,  2.15it/s]  9%|▉         | 284/3145 [01:56<19:37,  2.43it/s]  9%|▉         | 279/3145 [01:56<18:55,  2.52it/s]  9%|▉         | 284/3145 [01:56<20:42,  2.30it/s]  9%|▉         | 293/3143 [02:02<21:30,  2.21it/s]  9%|▉         | 285/3145 [01:56<20:04,  2.37it/s]  9%|▉         | 280/3145 [01:56<18:55,  2.52it/s]  9%|▉         | 294/3143 [02:02<20:34,  2.31it/s]  9%|▉         | 285/3145 [01:56<21:02,  2.27it/s]  9%|▉         | 281/3145 [01:56<19:21,  2.47it/s]  9%|▉         | 286/3145 [01:57<20:52,  2.28it/s]  9%|▉         | 295/3143 [02:03<19:44,  2.40it/s]  9%|▉         | 286/3145 [01:57<20:29,  2.32it/s]  9%|▉         | 282/3145 [01:57<19:33,  2.44it/s]  9%|▉         | 287/3145 [01:57<20:13,  2.36it/s]  9%|▉         | 296/3143 [02:03<19:27,  2.44it/s]  9%|▉         | 287/3145 [01:57<20:04,  2.37it/s]  9%|▉         | 283/3145 [01:57<18:38,  2.56it/s]  9%|▉         | 288/3145 [01:57<19:51,  2.40it/s]  9%|▉         | 288/3145 [01:57<17:37,  2.70it/s]  9%|▉         | 297/3143 [02:03<19:48,  2.39it/s]  9%|▉         | 289/3145 [01:58<15:50,  3.01it/s]  9%|▉         | 284/3145 [01:58<19:06,  2.50it/s]  9%|▉         | 289/3145 [01:58<19:37,  2.42it/s]  9%|▉         | 298/3143 [02:04<18:56,  2.50it/s]  9%|▉         | 290/3145 [01:58<17:16,  2.75it/s]  9%|▉         | 285/3145 [01:58<18:55,  2.52it/s]  9%|▉         | 290/3145 [01:58<19:12,  2.48it/s] 10%|▉         | 299/3143 [02:04<19:25,  2.44it/s]  9%|▉         | 286/3145 [01:58<18:58,  2.51it/s]  9%|▉         | 291/3145 [01:58<18:17,  2.60it/s]  9%|▉         | 291/3145 [01:59<19:34,  2.43it/s] 10%|▉         | 300/3143 [02:05<18:40,  2.54it/s]  9%|▉         | 287/3145 [01:59<18:38,  2.55it/s]  9%|▉         | 292/3145 [01:59<18:52,  2.52it/s]  9%|▉         | 292/3145 [01:59<20:01,  2.37it/s] 10%|▉         | 301/3143 [02:05<18:10,  2.61it/s]  9%|▉         | 288/3145 [01:59<19:23,  2.46it/s]  9%|▉         | 293/3145 [01:59<19:07,  2.49it/s] 10%|▉         | 302/3143 [02:05<18:28,  2.56it/s]  9%|▉         | 293/3145 [01:59<20:41,  2.30it/s]  9%|▉         | 294/3145 [02:00<18:36,  2.55it/s]  9%|▉         | 289/3145 [02:00<19:25,  2.45it/s] 10%|▉         | 303/3143 [02:06<18:04,  2.62it/s]  9%|▉         | 294/3145 [02:00<19:58,  2.38it/s]  9%|▉         | 290/3145 [02:00<19:18,  2.46it/s]  9%|▉         | 295/3145 [02:00<19:33,  2.43it/s] 10%|▉         | 304/3143 [02:06<18:09,  2.60it/s]  9%|▉         | 295/3145 [02:00<20:04,  2.37it/s]  9%|▉         | 291/3145 [02:00<16:52,  2.82it/s]  9%|▉         | 296/3145 [02:01<19:58,  2.38it/s] 10%|▉         | 305/3143 [02:06<19:03,  2.48it/s]  9%|▉         | 296/3145 [02:01<19:17,  2.46it/s]  9%|▉         | 292/3145 [02:01<19:40,  2.42it/s]  9%|▉         | 297/3145 [02:01<19:39,  2.41it/s]  9%|▉         | 297/3145 [02:01<18:35,  2.55it/s] 10%|▉         | 306/3143 [02:07<19:23,  2.44it/s]  9%|▉         | 293/3145 [02:01<19:15,  2.47it/s]  9%|▉         | 298/3145 [02:01<18:55,  2.51it/s]  9%|▉         | 298/3145 [02:01<18:26,  2.57it/s] 10%|▉         | 307/3143 [02:07<18:47,  2.51it/s] 10%|▉         | 299/3145 [02:02<16:48,  2.82it/s] 10%|▉         | 299/3145 [02:02<18:27,  2.57it/s]  9%|▉         | 294/3145 [02:02<20:38,  2.30it/s] 10%|▉         | 308/3143 [02:08<18:39,  2.53it/s] 10%|▉         | 300/3145 [02:02<17:29,  2.71it/s] 10%|▉         | 300/3145 [02:02<18:01,  2.63it/s]  9%|▉         | 295/3145 [02:02<19:24,  2.45it/s] 10%|▉         | 309/3143 [02:08<18:35,  2.54it/s] 10%|▉         | 301/3145 [02:02<17:54,  2.65it/s]  9%|▉         | 296/3145 [02:03<19:07,  2.48it/s] 10%|▉         | 301/3145 [02:03<19:12,  2.47it/s] 10%|▉         | 310/3143 [02:08<18:45,  2.52it/s] 10%|▉         | 302/3145 [02:03<19:40,  2.41it/s] 10%|▉         | 302/3145 [02:03<19:08,  2.47it/s]  9%|▉         | 297/3145 [02:03<20:01,  2.37it/s] 10%|▉         | 311/3143 [02:09<18:53,  2.50it/s] 10%|▉         | 312/3143 [02:09<17:56,  2.63it/s] 10%|▉         | 303/3145 [02:03<20:06,  2.36it/s] 10%|▉         | 303/3145 [02:03<20:27,  2.32it/s]  9%|▉         | 298/3145 [02:03<21:08,  2.24it/s] 10%|▉         | 313/3143 [02:10<17:58,  2.62it/s] 10%|▉         | 299/3145 [02:04<19:59,  2.37it/s] 10%|▉         | 304/3145 [02:04<20:31,  2.31it/s] 10%|▉         | 304/3145 [02:04<22:35,  2.10it/s] 10%|▉         | 314/3143 [02:10<18:40,  2.53it/s] 10%|▉         | 300/3145 [02:04<19:09,  2.47it/s] 10%|▉         | 305/3145 [02:04<21:11,  2.23it/s] 10%|▉         | 305/3145 [02:05<23:21,  2.03it/s] 10%|█         | 315/3143 [02:10<18:30,  2.55it/s] 10%|▉         | 301/3145 [02:05<19:35,  2.42it/s] 10%|▉         | 306/3145 [02:05<21:02,  2.25it/s] 10%|█         | 316/3143 [02:11<18:28,  2.55it/s] 10%|▉         | 302/3145 [02:05<18:37,  2.54it/s] 10%|▉         | 306/3145 [02:05<24:48,  1.91it/s] 10%|▉         | 307/3145 [02:05<20:20,  2.33it/s] 10%|▉         | 303/3145 [02:05<16:51,  2.81it/s] 10%|█         | 317/3143 [02:11<19:06,  2.46it/s] 10%|▉         | 307/3145 [02:06<23:29,  2.01it/s] 10%|▉         | 308/3145 [02:06<20:22,  2.32it/s] 10%|▉         | 304/3145 [02:06<17:32,  2.70it/s] 10%|█         | 318/3143 [02:12<18:40,  2.52it/s] 10%|▉         | 308/3145 [02:06<21:48,  2.17it/s] 10%|▉         | 309/3145 [02:06<20:16,  2.33it/s] 10%|▉         | 305/3145 [02:06<18:01,  2.63it/s] 10%|█         | 319/3143 [02:12<18:38,  2.53it/s] 10%|▉         | 309/3145 [02:06<20:59,  2.25it/s] 10%|▉         | 310/3145 [02:06<20:03,  2.36it/s] 10%|█         | 320/3143 [02:12<18:19,  2.57it/s] 10%|▉         | 306/3145 [02:07<19:13,  2.46it/s] 10%|▉         | 310/3145 [02:07<20:34,  2.30it/s] 10%|█         | 321/3143 [02:13<17:38,  2.67it/s] 10%|▉         | 311/3145 [02:07<19:42,  2.40it/s] 10%|▉         | 307/3145 [02:07<19:44,  2.40it/s] 10%|▉         | 311/3145 [02:07<19:45,  2.39it/s] 10%|█         | 322/3143 [02:13<17:34,  2.68it/s] 10%|▉         | 312/3145 [02:07<19:57,  2.37it/s] 10%|▉         | 312/3145 [02:07<17:01,  2.77it/s] 10%|▉         | 308/3145 [02:07<19:46,  2.39it/s] 10%|█         | 323/3143 [02:14<18:25,  2.55it/s] 10%|▉         | 313/3145 [02:08<17:05,  2.76it/s] 10%|▉         | 313/3145 [02:08<20:17,  2.33it/s] 10%|▉         | 309/3145 [02:08<19:18,  2.45it/s] 10%|█         | 324/3143 [02:14<18:23,  2.56it/s] 10%|▉         | 314/3145 [02:08<16:39,  2.83it/s] 10%|▉         | 310/3145 [02:08<17:30,  2.70it/s] 10%|▉         | 314/3145 [02:08<19:04,  2.47it/s] 10%|█         | 315/3145 [02:08<17:05,  2.76it/s] 10%|█         | 315/3145 [02:08<18:34,  2.54it/s] 10%|█         | 325/3143 [02:14<19:08,  2.45it/s] 10%|▉         | 311/3145 [02:09<18:35,  2.54it/s] 10%|█         | 316/3145 [02:09<18:36,  2.53it/s] 10%|█         | 316/3145 [02:09<18:03,  2.61it/s] 10%|▉         | 312/3145 [02:09<18:35,  2.54it/s] 10%|█         | 326/3143 [02:15<19:31,  2.41it/s] 10%|█         | 317/3145 [02:09<18:01,  2.61it/s] 10%|█         | 327/3143 [02:15<17:47,  2.64it/s] 10%|█         | 317/3145 [02:09<18:10,  2.59it/s] 10%|▉         | 313/3145 [02:09<18:04,  2.61it/s] 10%|█         | 318/3145 [02:09<16:00,  2.94it/s] 10%|█         | 328/3143 [02:16<18:29,  2.54it/s] 10%|▉         | 314/3145 [02:10<18:20,  2.57it/s] 10%|█         | 318/3145 [02:10<18:36,  2.53it/s] 10%|█         | 319/3145 [02:10<17:10,  2.74it/s] 10%|█         | 315/3145 [02:10<17:12,  2.74it/s] 10%|█         | 329/3143 [02:16<18:15,  2.57it/s] 10%|█         | 319/3145 [02:10<18:43,  2.51it/s] 10%|█         | 320/3145 [02:10<18:08,  2.59it/s] 10%|█         | 316/3145 [02:10<17:44,  2.66it/s] 10%|█         | 330/3143 [02:16<18:49,  2.49it/s] 10%|█         | 320/3145 [02:10<18:59,  2.48it/s] 10%|█         | 321/3145 [02:11<15:56,  2.95it/s] 10%|█         | 317/3145 [02:11<17:54,  2.63it/s] 11%|█         | 331/3143 [02:17<19:31,  2.40it/s] 10%|█         | 321/3145 [02:11<19:15,  2.44it/s] 10%|█         | 322/3145 [02:11<17:23,  2.71it/s] 10%|█         | 318/3145 [02:11<18:10,  2.59it/s] 11%|█         | 332/3143 [02:17<17:48,  2.63it/s] 10%|█         | 322/3145 [02:11<19:09,  2.46it/s] 10%|█         | 323/3145 [02:11<17:43,  2.65it/s] 11%|█         | 333/3143 [02:17<17:31,  2.67it/s] 10%|█         | 319/3145 [02:12<18:17,  2.57it/s] 10%|█         | 324/3145 [02:12<17:31,  2.68it/s] 10%|█         | 323/3145 [02:12<19:32,  2.41it/s] 10%|█         | 320/3145 [02:12<18:39,  2.52it/s] 11%|█         | 334/3143 [02:18<18:35,  2.52it/s] 10%|█         | 325/3145 [02:12<18:28,  2.55it/s] 10%|█         | 324/3145 [02:12<19:52,  2.37it/s] 10%|█         | 321/3145 [02:12<18:33,  2.54it/s] 11%|█         | 335/3143 [02:18<19:07,  2.45it/s] 10%|█         | 326/3145 [02:13<18:52,  2.49it/s] 10%|█         | 322/3145 [02:13<18:30,  2.54it/s] 10%|█         | 325/3145 [02:13<22:05,  2.13it/s] 11%|█         | 336/3143 [02:19<18:49,  2.49it/s] 10%|█         | 327/3145 [02:13<18:38,  2.52it/s] 10%|█         | 323/3145 [02:13<18:35,  2.53it/s] 10%|█         | 326/3145 [02:13<21:11,  2.22it/s] 11%|█         | 337/3143 [02:19<18:46,  2.49it/s] 10%|█         | 328/3145 [02:13<18:57,  2.48it/s] 10%|█         | 327/3145 [02:13<19:11,  2.45it/s] 10%|█         | 324/3145 [02:14<18:56,  2.48it/s] 11%|█         | 338/3143 [02:20<19:09,  2.44it/s] 10%|█         | 329/3145 [02:14<18:57,  2.47it/s] 10%|█         | 325/3145 [02:14<18:44,  2.51it/s] 10%|█         | 328/3145 [02:14<21:29,  2.18it/s] 11%|█         | 339/3143 [02:20<19:37,  2.38it/s] 10%|█         | 330/3145 [02:14<18:59,  2.47it/s] 10%|█         | 326/3145 [02:14<19:10,  2.45it/s] 10%|█         | 329/3145 [02:14<20:29,  2.29it/s] 11%|█         | 340/3143 [02:20<19:56,  2.34it/s] 11%|█         | 331/3145 [02:15<19:27,  2.41it/s] 10%|█         | 327/3145 [02:15<19:49,  2.37it/s] 10%|█         | 330/3145 [02:15<20:26,  2.30it/s] 11%|█         | 341/3143 [02:21<19:34,  2.39it/s] 11%|█         | 332/3145 [02:15<19:45,  2.37it/s] 11%|█         | 331/3145 [02:15<18:27,  2.54it/s] 10%|█         | 328/3145 [02:15<19:25,  2.42it/s] 11%|█         | 342/3143 [02:21<19:21,  2.41it/s] 11%|█         | 333/3145 [02:15<19:17,  2.43it/s] 11%|█         | 332/3145 [02:16<18:56,  2.48it/s] 10%|█         | 329/3145 [02:16<19:13,  2.44it/s] 11%|█         | 343/3143 [02:22<19:32,  2.39it/s] 11%|█         | 334/3145 [02:16<18:24,  2.55it/s] 11%|█         | 333/3145 [02:16<19:13,  2.44it/s] 10%|█         | 330/3145 [02:16<19:31,  2.40it/s] 11%|█         | 344/3143 [02:22<19:13,  2.43it/s] 11%|█         | 335/3145 [02:16<18:04,  2.59it/s] 11%|█         | 334/3145 [02:16<17:11,  2.72it/s] 11%|█         | 331/3145 [02:16<18:32,  2.53it/s] 11%|█         | 336/3145 [02:17<18:22,  2.55it/s] 11%|█         | 345/3143 [02:23<19:43,  2.36it/s] 11%|█         | 335/3145 [02:17<17:41,  2.65it/s] 11%|█         | 332/3145 [02:17<18:21,  2.55it/s] 11%|█         | 336/3145 [02:17<16:32,  2.83it/s] 11%|█         | 346/3143 [02:23<19:25,  2.40it/s] 11%|█         | 337/3145 [02:17<19:04,  2.45it/s] 11%|█         | 333/3145 [02:17<18:15,  2.57it/s] 11%|█         | 347/3143 [02:23<18:58,  2.46it/s] 11%|█         | 337/3145 [02:17<17:42,  2.64it/s] 11%|█         | 338/3145 [02:17<19:02,  2.46it/s] 11%|█         | 334/3145 [02:18<18:11,  2.58it/s] 11%|█         | 338/3145 [02:18<17:22,  2.69it/s] 11%|█         | 348/3143 [02:24<18:54,  2.46it/s] 11%|█         | 339/3145 [02:18<19:31,  2.40it/s] 11%|█         | 335/3145 [02:18<18:25,  2.54it/s] 11%|█         | 349/3143 [02:24<18:49,  2.47it/s] 11%|█         | 339/3145 [02:18<18:18,  2.55it/s] 11%|█         | 340/3145 [02:18<19:07,  2.44it/s] 11%|█         | 336/3145 [02:18<18:57,  2.47it/s] 11%|█         | 350/3143 [02:25<18:55,  2.46it/s] 11%|█         | 341/3145 [02:19<19:31,  2.39it/s] 11%|█         | 340/3145 [02:19<20:19,  2.30it/s] 11%|█         | 337/3145 [02:19<18:19,  2.55it/s] 11%|█         | 351/3143 [02:25<18:57,  2.46it/s] 11%|█         | 342/3145 [02:19<18:33,  2.52it/s] 11%|█         | 341/3145 [02:19<19:38,  2.38it/s] 11%|█         | 338/3145 [02:19<18:23,  2.54it/s] 11%|█         | 352/3143 [02:25<19:15,  2.42it/s] 11%|█         | 343/3145 [02:19<18:46,  2.49it/s] 11%|█         | 339/3145 [02:20<17:46,  2.63it/s] 11%|█         | 342/3145 [02:20<20:18,  2.30it/s] 11%|█         | 353/3143 [02:26<19:13,  2.42it/s] 11%|█         | 340/3145 [02:20<18:27,  2.53it/s] 11%|█         | 344/3145 [02:20<19:54,  2.35it/s] 11%|█         | 343/3145 [02:20<20:08,  2.32it/s] 11%|█         | 344/3145 [02:20<17:14,  2.71it/s] 11%|█▏        | 354/3143 [02:26<18:55,  2.46it/s] 11%|█         | 345/3145 [02:20<19:26,  2.40it/s] 11%|█         | 341/3145 [02:20<19:01,  2.46it/s] 11%|█▏        | 355/3143 [02:27<18:59,  2.45it/s] 11%|█         | 342/3145 [02:21<19:14,  2.43it/s] 11%|█         | 346/3145 [02:21<19:55,  2.34it/s] 11%|█         | 345/3145 [02:21<20:11,  2.31it/s] 11%|█▏        | 356/3143 [02:27<19:06,  2.43it/s] 11%|█         | 343/3145 [02:21<19:31,  2.39it/s] 11%|█         | 347/3145 [02:21<20:13,  2.31it/s] 11%|█         | 346/3145 [02:21<20:57,  2.23it/s] 11%|█▏        | 357/3143 [02:27<18:46,  2.47it/s] 11%|█         | 344/3145 [02:22<19:04,  2.45it/s] 11%|█         | 348/3145 [02:22<19:29,  2.39it/s] 11%|█         | 347/3145 [02:22<20:22,  2.29it/s] 11%|█▏        | 358/3143 [02:28<18:46,  2.47it/s] 11%|█         | 349/3145 [02:22<19:19,  2.41it/s] 11%|█         | 345/3145 [02:22<19:28,  2.40it/s] 11%|█         | 348/3145 [02:22<19:46,  2.36it/s] 11%|█▏        | 359/3143 [02:28<18:21,  2.53it/s] 11%|█         | 350/3145 [02:22<18:41,  2.49it/s] 11%|█         | 346/3145 [02:22<19:07,  2.44it/s] 11%|█         | 349/3145 [02:23<19:24,  2.40it/s] 11%|█▏        | 360/3143 [02:29<18:53,  2.46it/s] 11%|█         | 351/3145 [02:23<19:11,  2.43it/s] 11%|█         | 350/3145 [02:23<19:11,  2.43it/s] 11%|█▏        | 361/3143 [02:29<16:29,  2.81it/s] 11%|█         | 347/3145 [02:23<20:14,  2.30it/s] 11%|█         | 352/3145 [02:23<19:27,  2.39it/s] 11%|█         | 351/3145 [02:23<18:37,  2.50it/s] 11%|█         | 348/3145 [02:23<19:44,  2.36it/s] 12%|█▏        | 362/3143 [02:29<17:57,  2.58it/s] 11%|█         | 353/3145 [02:24<19:06,  2.43it/s] 11%|█         | 349/3145 [02:24<19:16,  2.42it/s] 11%|█         | 352/3145 [02:24<19:10,  2.43it/s] 12%|█▏        | 363/3143 [02:30<20:51,  2.22it/s] 11%|█▏        | 354/3145 [02:24<18:30,  2.51it/s] 11%|█         | 353/3145 [02:24<18:21,  2.53it/s] 11%|█         | 350/3145 [02:24<19:20,  2.41it/s] 12%|█▏        | 364/3143 [02:30<19:32,  2.37it/s] 11%|█▏        | 354/3145 [02:24<18:14,  2.55it/s] 11%|█▏        | 355/3145 [02:24<19:26,  2.39it/s] 11%|█         | 351/3145 [02:25<18:59,  2.45it/s] 12%|█▏        | 365/3143 [02:31<18:56,  2.44it/s] 11%|█         | 352/3145 [02:25<18:30,  2.52it/s] 11%|█▏        | 356/3145 [02:25<19:31,  2.38it/s] 11%|█▏        | 355/3145 [02:25<19:15,  2.42it/s] 12%|█▏        | 366/3143 [02:31<18:08,  2.55it/s] 12%|█▏        | 367/3143 [02:31<16:07,  2.87it/s] 11%|█         | 353/3145 [02:25<19:24,  2.40it/s] 11%|█▏        | 357/3145 [02:25<20:10,  2.30it/s] 11%|█▏        | 356/3145 [02:25<20:22,  2.28it/s] 11%|█▏        | 354/3145 [02:26<18:56,  2.46it/s] 12%|█▏        | 368/3143 [02:32<17:23,  2.66it/s] 11%|█▏        | 358/3145 [02:26<19:28,  2.39it/s] 11%|█▏        | 357/3145 [02:26<19:38,  2.37it/s] 12%|█▏        | 369/3143 [02:32<15:29,  2.98it/s] 11%|█▏        | 355/3145 [02:26<18:20,  2.54it/s] 11%|█▏        | 359/3145 [02:26<18:35,  2.50it/s] 11%|█▏        | 358/3145 [02:26<19:15,  2.41it/s] 12%|█▏        | 370/3143 [02:32<16:51,  2.74it/s] 11%|█▏        | 356/3145 [02:27<18:18,  2.54it/s] 11%|█▏        | 360/3145 [02:27<18:35,  2.50it/s] 11%|█▏        | 359/3145 [02:27<19:32,  2.38it/s] 12%|█▏        | 371/3143 [02:33<17:25,  2.65it/s] 11%|█▏        | 357/3145 [02:27<18:27,  2.52it/s] 11%|█▏        | 361/3145 [02:27<18:55,  2.45it/s] 11%|█▏        | 360/3145 [02:27<18:40,  2.49it/s] 12%|█▏        | 372/3143 [02:33<15:56,  2.90it/s] 11%|█▏        | 358/3145 [02:27<18:01,  2.58it/s] 12%|█▏        | 362/3145 [02:27<18:50,  2.46it/s] 11%|█▏        | 361/3145 [02:27<18:57,  2.45it/s] 12%|█▏        | 373/3143 [02:33<17:18,  2.67it/s] 12%|█▏        | 363/3145 [02:28<18:42,  2.48it/s] 11%|█▏        | 359/3145 [02:28<19:39,  2.36it/s] 12%|█▏        | 362/3145 [02:28<18:47,  2.47it/s] 12%|█▏        | 374/3143 [02:34<17:36,  2.62it/s] 12%|█▏        | 364/3145 [02:28<18:31,  2.50it/s] 12%|█▏        | 363/3145 [02:28<18:14,  2.54it/s] 11%|█▏        | 360/3145 [02:28<19:47,  2.34it/s] 12%|█▏        | 375/3143 [02:34<17:44,  2.60it/s] 12%|█▏        | 364/3145 [02:29<18:16,  2.54it/s] 12%|█▏        | 365/3145 [02:29<19:03,  2.43it/s] 11%|█▏        | 361/3145 [02:29<19:52,  2.33it/s] 12%|█▏        | 376/3143 [02:35<18:25,  2.50it/s] 12%|█▏        | 365/3145 [02:29<16:26,  2.82it/s] 12%|█▏        | 366/3145 [02:29<20:00,  2.31it/s] 12%|█▏        | 362/3145 [02:29<19:54,  2.33it/s] 12%|█▏        | 377/3143 [02:35<19:17,  2.39it/s] 12%|█▏        | 366/3145 [02:29<17:10,  2.70it/s] 12%|█▏        | 367/3145 [02:29<19:18,  2.40it/s] 12%|█▏        | 363/3145 [02:29<19:12,  2.41it/s] 12%|█▏        | 367/3145 [02:30<17:02,  2.72it/s] 12%|█▏        | 378/3143 [02:36<18:56,  2.43it/s] 12%|█▏        | 368/3145 [02:30<18:51,  2.45it/s] 12%|█▏        | 364/3145 [02:30<19:29,  2.38it/s] 12%|█▏        | 379/3143 [02:36<18:20,  2.51it/s] 12%|█▏        | 368/3145 [02:30<17:37,  2.63it/s] 12%|█▏        | 369/3145 [02:30<18:42,  2.47it/s] 12%|█▏        | 365/3145 [02:30<19:40,  2.36it/s] 12%|█▏        | 369/3145 [02:30<17:20,  2.67it/s] 12%|█▏        | 380/3143 [02:36<19:11,  2.40it/s] 12%|█▏        | 370/3145 [02:31<18:25,  2.51it/s] 12%|█▏        | 366/3145 [02:31<18:52,  2.45it/s] 12%|█▏        | 381/3143 [02:37<17:43,  2.60it/s] 12%|█▏        | 370/3145 [02:31<19:32,  2.37it/s] 12%|█▏        | 371/3145 [02:31<18:36,  2.49it/s] 12%|█▏        | 367/3145 [02:31<18:15,  2.54it/s] 12%|█▏        | 382/3143 [02:37<17:20,  2.65it/s] 12%|█▏        | 371/3145 [02:31<18:40,  2.48it/s] 12%|█▏        | 368/3145 [02:31<18:04,  2.56it/s] 12%|█▏        | 372/3145 [02:31<19:11,  2.41it/s] 12%|█▏        | 383/3143 [02:37<18:12,  2.53it/s] 12%|█▏        | 372/3145 [02:32<18:07,  2.55it/s] 12%|█▏        | 373/3145 [02:32<19:04,  2.42it/s] 12%|█▏        | 384/3143 [02:38<17:41,  2.60it/s] 12%|█▏        | 369/3145 [02:32<19:22,  2.39it/s] 12%|█▏        | 373/3145 [02:32<18:42,  2.47it/s] 12%|█▏        | 370/3145 [02:32<19:04,  2.42it/s] 12%|█▏        | 374/3145 [02:32<19:48,  2.33it/s] 12%|█▏        | 385/3143 [02:38<18:40,  2.46it/s] 12%|█▏        | 374/3145 [02:32<18:34,  2.49it/s] 12%|█▏        | 371/3145 [02:33<18:50,  2.45it/s] 12%|█▏        | 375/3145 [02:33<17:06,  2.70it/s] 12%|█▏        | 375/3145 [02:33<20:10,  2.29it/s] 12%|█▏        | 386/3143 [02:39<19:20,  2.38it/s] 12%|█▏        | 372/3145 [02:33<18:31,  2.50it/s] 12%|█▏        | 376/3145 [02:33<17:56,  2.57it/s] 12%|█▏        | 376/3145 [02:33<20:15,  2.28it/s] 12%|█▏        | 387/3143 [02:39<19:20,  2.38it/s] 12%|█▏        | 373/3145 [02:33<17:55,  2.58it/s] 12%|█▏        | 377/3145 [02:34<17:00,  2.71it/s] 12%|█▏        | 377/3145 [02:34<19:23,  2.38it/s] 12%|█▏        | 388/3143 [02:40<19:06,  2.40it/s] 12%|█▏        | 374/3145 [02:34<18:14,  2.53it/s] 12%|█▏        | 378/3145 [02:34<17:24,  2.65it/s] 12%|█▏        | 378/3145 [02:34<19:08,  2.41it/s] 12%|█▏        | 389/3143 [02:40<18:52,  2.43it/s] 12%|█▏        | 375/3145 [02:34<18:45,  2.46it/s] 12%|█▏        | 379/3145 [02:34<17:33,  2.63it/s] 12%|█▏        | 379/3145 [02:34<19:21,  2.38it/s] 12%|█▏        | 390/3143 [02:40<19:18,  2.38it/s] 12%|█▏        | 376/3145 [02:35<18:29,  2.50it/s] 12%|█▏        | 380/3145 [02:35<17:32,  2.63it/s] 12%|█▏        | 380/3145 [02:35<18:20,  2.51it/s] 12%|█▏        | 391/3143 [02:41<18:51,  2.43it/s] 12%|█▏        | 377/3145 [02:35<18:13,  2.53it/s] 12%|█▏        | 381/3145 [02:35<17:36,  2.62it/s] 12%|█▏        | 392/3143 [02:41<18:32,  2.47it/s] 12%|█▏        | 381/3145 [02:35<19:24,  2.37it/s] 12%|█▏        | 378/3145 [02:36<18:18,  2.52it/s] 12%|█▏        | 382/3145 [02:36<18:24,  2.50it/s] 13%|█▎        | 393/3143 [02:42<18:16,  2.51it/s] 12%|█▏        | 382/3145 [02:36<20:14,  2.27it/s] 12%|█▏        | 379/3145 [02:36<18:49,  2.45it/s] 12%|█▏        | 383/3145 [02:36<19:35,  2.35it/s] 13%|█▎        | 394/3143 [02:42<19:22,  2.36it/s] 12%|█▏        | 383/3145 [02:36<19:44,  2.33it/s] 12%|█▏        | 380/3145 [02:36<19:06,  2.41it/s] 12%|█▏        | 384/3145 [02:36<19:07,  2.41it/s] 12%|█▏        | 384/3145 [02:37<19:20,  2.38it/s] 13%|█▎        | 395/3143 [02:42<19:41,  2.33it/s] 12%|█▏        | 385/3145 [02:37<18:48,  2.45it/s] 12%|█▏        | 381/3145 [02:37<19:41,  2.34it/s] 13%|█▎        | 396/3143 [02:43<19:14,  2.38it/s] 12%|█▏        | 385/3145 [02:37<19:33,  2.35it/s] 12%|█▏        | 382/3145 [02:37<19:15,  2.39it/s] 12%|█▏        | 386/3145 [02:37<18:44,  2.45it/s] 13%|█▎        | 397/3143 [02:43<18:50,  2.43it/s] 12%|█▏        | 386/3145 [02:37<19:21,  2.38it/s] 12%|█▏        | 383/3145 [02:38<18:38,  2.47it/s] 12%|█▏        | 387/3145 [02:38<19:23,  2.37it/s] 13%|█▎        | 398/3143 [02:44<17:57,  2.55it/s] 12%|█▏        | 387/3145 [02:38<18:07,  2.54it/s] 12%|█▏        | 384/3145 [02:38<18:52,  2.44it/s] 12%|█▏        | 388/3145 [02:38<19:33,  2.35it/s] 12%|█▏        | 388/3145 [02:38<17:43,  2.59it/s] 13%|█▎        | 399/3143 [02:44<18:11,  2.51it/s] 12%|█▏        | 385/3145 [02:38<18:31,  2.48it/s] 12%|█▏        | 389/3145 [02:39<19:39,  2.34it/s] 13%|█▎        | 400/3143 [02:44<18:06,  2.53it/s] 12%|█▏        | 389/3145 [02:39<18:51,  2.44it/s] 12%|█▏        | 386/3145 [02:39<16:18,  2.82it/s] 12%|█▏        | 390/3145 [02:39<19:04,  2.41it/s] 13%|█▎        | 401/3143 [02:45<18:27,  2.48it/s] 12%|█▏        | 390/3145 [02:39<18:52,  2.43it/s] 12%|█▏        | 387/3145 [02:39<17:43,  2.59it/s] 12%|█▏        | 391/3145 [02:39<18:53,  2.43it/s] 13%|█▎        | 402/3143 [02:45<18:52,  2.42it/s] 12%|█▏        | 391/3145 [02:39<19:28,  2.36it/s] 12%|█▏        | 388/3145 [02:40<18:21,  2.50it/s] 12%|█▏        | 392/3145 [02:40<18:44,  2.45it/s] 13%|█▎        | 403/3143 [02:46<19:10,  2.38it/s] 12%|█▏        | 389/3145 [02:40<18:09,  2.53it/s] 12%|█▏        | 392/3145 [02:40<20:02,  2.29it/s] 13%|█▎        | 404/3143 [02:46<18:19,  2.49it/s] 12%|█▏        | 393/3145 [02:40<20:05,  2.28it/s] 12%|█▏        | 390/3145 [02:40<18:38,  2.46it/s] 12%|█▏        | 393/3145 [02:40<19:46,  2.32it/s] 13%|█▎        | 405/3143 [02:46<18:04,  2.53it/s] 13%|█▎        | 394/3145 [02:41<19:36,  2.34it/s] 12%|█▏        | 391/3145 [02:41<18:30,  2.48it/s] 13%|█▎        | 394/3145 [02:41<21:00,  2.18it/s] 13%|█▎        | 406/3143 [02:47<17:34,  2.60it/s] 13%|█▎        | 395/3145 [02:41<19:11,  2.39it/s] 12%|█▏        | 392/3145 [02:41<18:11,  2.52it/s] 13%|█▎        | 395/3145 [02:41<20:42,  2.21it/s] 13%|█▎        | 396/3145 [02:41<18:22,  2.49it/s] 13%|█▎        | 407/3143 [02:47<18:36,  2.45it/s] 12%|█▏        | 393/3145 [02:42<18:18,  2.50it/s] 13%|█▎        | 396/3145 [02:42<20:04,  2.28it/s] 13%|█▎        | 408/3143 [02:48<18:56,  2.41it/s] 13%|█▎        | 397/3145 [02:42<18:56,  2.42it/s] 13%|█▎        | 394/3145 [02:42<19:05,  2.40it/s] 13%|█▎        | 397/3145 [02:42<19:56,  2.30it/s] 13%|█▎        | 398/3145 [02:42<18:31,  2.47it/s] 13%|█▎        | 409/3143 [02:48<18:47,  2.42it/s] 13%|█▎        | 395/3145 [02:43<21:33,  2.13it/s] 13%|█▎        | 399/3145 [02:43<18:18,  2.50it/s] 13%|█▎        | 410/3143 [02:49<18:35,  2.45it/s] 13%|█▎        | 398/3145 [02:43<21:39,  2.11it/s] 13%|█▎        | 396/3145 [02:43<20:40,  2.22it/s] 13%|█▎        | 411/3143 [02:49<18:02,  2.52it/s] 13%|█▎        | 400/3145 [02:43<18:16,  2.50it/s] 13%|█▎        | 399/3145 [02:43<23:43,  1.93it/s] 13%|█▎        | 397/3145 [02:43<19:40,  2.33it/s] 13%|█▎        | 412/3143 [02:49<18:04,  2.52it/s] 13%|█▎        | 401/3145 [02:43<19:17,  2.37it/s] 13%|█▎        | 400/3145 [02:44<22:19,  2.05it/s] 13%|█▎        | 398/3145 [02:44<19:12,  2.38it/s] 13%|█▎        | 413/3143 [02:50<18:36,  2.44it/s] 13%|█▎        | 402/3145 [02:44<18:58,  2.41it/s] 13%|█▎        | 401/3145 [02:44<21:38,  2.11it/s] 13%|█▎        | 414/3143 [02:50<18:02,  2.52it/s] 13%|█▎        | 399/3145 [02:44<19:43,  2.32it/s] 13%|█▎        | 403/3145 [02:44<19:01,  2.40it/s] 13%|█▎        | 402/3145 [02:45<20:34,  2.22it/s] 13%|█▎        | 415/3143 [02:51<18:23,  2.47it/s] 13%|█▎        | 400/3145 [02:45<19:35,  2.34it/s] 13%|█▎        | 404/3145 [02:45<18:43,  2.44it/s] 13%|█▎        | 403/3145 [02:45<19:55,  2.29it/s] 13%|█▎        | 401/3145 [02:45<20:02,  2.28it/s] 13%|█▎        | 416/3143 [02:51<19:46,  2.30it/s] 13%|█▎        | 405/3145 [02:45<19:42,  2.32it/s] 13%|█▎        | 417/3143 [02:51<17:28,  2.60it/s] 13%|█▎        | 404/3145 [02:45<19:49,  2.30it/s] 13%|█▎        | 402/3145 [02:46<20:12,  2.26it/s] 13%|█▎        | 406/3145 [02:46<20:25,  2.23it/s] 13%|█▎        | 418/3143 [02:52<17:28,  2.60it/s] 13%|█▎        | 405/3145 [02:46<19:57,  2.29it/s] 13%|█▎        | 403/3145 [02:46<19:01,  2.40it/s] 13%|█▎        | 407/3145 [02:46<20:30,  2.23it/s] 13%|█▎        | 419/3143 [02:52<17:18,  2.62it/s] 13%|█▎        | 406/3145 [02:46<19:04,  2.39it/s] 13%|█▎        | 404/3145 [02:46<18:35,  2.46it/s] 13%|█▎        | 408/3145 [02:47<20:23,  2.24it/s] 13%|█▎        | 407/3145 [02:47<19:08,  2.38it/s] 13%|█▎        | 405/3145 [02:47<18:26,  2.48it/s] 13%|█▎        | 420/3143 [02:53<20:01,  2.27it/s] 13%|█▎        | 409/3145 [02:47<19:44,  2.31it/s] 13%|█▎        | 421/3143 [02:53<17:10,  2.64it/s] 13%|█▎        | 408/3145 [02:47<18:52,  2.42it/s] 13%|█▎        | 406/3145 [02:47<18:14,  2.50it/s] 13%|█▎        | 422/3143 [02:53<17:25,  2.60it/s] 13%|█▎        | 410/3145 [02:47<19:49,  2.30it/s] 13%|█▎        | 409/3145 [02:47<18:16,  2.50it/s] 13%|█▎        | 407/3145 [02:48<19:11,  2.38it/s] 13%|█▎        | 410/3145 [02:48<17:58,  2.54it/s] 13%|█▎        | 411/3145 [02:48<19:52,  2.29it/s] 13%|█▎        | 423/3143 [02:54<18:30,  2.45it/s] 13%|█▎        | 408/3145 [02:48<18:17,  2.49it/s] 13%|█▎        | 411/3145 [02:48<18:22,  2.48it/s] 13%|█▎        | 412/3145 [02:48<19:15,  2.37it/s] 13%|█▎        | 409/3145 [02:48<18:26,  2.47it/s] 13%|█▎        | 424/3143 [02:54<19:29,  2.33it/s] 13%|█▎        | 412/3145 [02:49<18:21,  2.48it/s] 14%|█▎        | 425/3143 [02:55<17:53,  2.53it/s] 13%|█▎        | 410/3145 [02:49<17:56,  2.54it/s] 13%|█▎        | 413/3145 [02:49<20:13,  2.25it/s] 14%|█▎        | 426/3143 [02:55<18:32,  2.44it/s] 13%|█▎        | 413/3145 [02:49<19:01,  2.39it/s] 13%|█▎        | 411/3145 [02:49<17:58,  2.53it/s] 13%|█▎        | 414/3145 [02:49<19:42,  2.31it/s] 13%|█▎        | 414/3145 [02:49<17:36,  2.59it/s] 14%|█▎        | 427/3143 [02:55<18:03,  2.51it/s] 13%|█▎        | 412/3145 [02:49<18:09,  2.51it/s] 13%|█▎        | 415/3145 [02:50<19:04,  2.39it/s] 13%|█▎        | 416/3145 [02:50<16:30,  2.76it/s] 13%|█▎        | 415/3145 [02:50<18:16,  2.49it/s] 14%|█▎        | 428/3143 [02:56<18:23,  2.46it/s] 13%|█▎        | 413/3145 [02:50<18:10,  2.51it/s] 13%|█▎        | 417/3145 [02:50<17:00,  2.67it/s] 13%|█▎        | 416/3145 [02:50<17:45,  2.56it/s] 14%|█▎        | 429/3143 [02:56<18:02,  2.51it/s] 13%|█▎        | 414/3145 [02:50<18:02,  2.52it/s] 13%|█▎        | 418/3145 [02:51<16:55,  2.69it/s] 13%|█▎        | 417/3145 [02:51<17:46,  2.56it/s] 14%|█▎        | 430/3143 [02:57<17:36,  2.57it/s] 13%|█▎        | 418/3145 [02:51<15:52,  2.86it/s] 13%|█▎        | 415/3145 [02:51<20:27,  2.22it/s] 13%|█▎        | 419/3145 [02:51<17:26,  2.60it/s] 14%|█▎        | 431/3143 [02:57<18:05,  2.50it/s] 13%|█▎        | 419/3145 [02:51<15:26,  2.94it/s] 13%|█▎        | 416/3145 [02:51<19:35,  2.32it/s] 13%|█▎        | 420/3145 [02:51<18:16,  2.49it/s] 14%|█▎        | 432/3143 [02:57<18:33,  2.43it/s] 13%|█▎        | 420/3145 [02:52<16:08,  2.82it/s] 13%|█▎        | 421/3145 [02:52<18:39,  2.43it/s] 13%|█▎        | 417/3145 [02:52<21:48,  2.08it/s] 14%|█▍        | 433/3143 [02:58<18:59,  2.38it/s] 13%|█▎        | 421/3145 [02:52<17:04,  2.66it/s] 13%|█▎        | 422/3145 [02:52<18:26,  2.46it/s] 13%|█▎        | 418/3145 [02:52<21:21,  2.13it/s] 14%|█▍        | 434/3143 [02:58<18:20,  2.46it/s] 13%|█▎        | 422/3145 [02:52<18:16,  2.48it/s] 13%|█▎        | 423/3145 [02:53<17:57,  2.53it/s] 13%|█▎        | 419/3145 [02:53<20:19,  2.23it/s] 14%|█▍        | 435/3143 [02:59<18:16,  2.47it/s] 13%|█▎        | 423/3145 [02:53<18:48,  2.41it/s] 13%|█▎        | 424/3145 [02:53<17:59,  2.52it/s] 13%|█▎        | 420/3145 [02:53<19:37,  2.31it/s] 14%|█▍        | 436/3143 [02:59<18:04,  2.49it/s] 13%|█▎        | 424/3145 [02:53<19:03,  2.38it/s] 14%|█▎        | 425/3145 [02:53<17:32,  2.59it/s] 13%|█▎        | 421/3145 [02:54<19:27,  2.33it/s] 14%|█▍        | 437/3143 [02:59<18:15,  2.47it/s] 14%|█▎        | 425/3145 [02:54<18:20,  2.47it/s] 14%|█▎        | 426/3145 [02:54<17:34,  2.58it/s] 13%|█▎        | 422/3145 [02:54<19:33,  2.32it/s] 14%|█▍        | 438/3143 [03:00<18:44,  2.41it/s] 14%|█▎        | 426/3145 [02:54<18:10,  2.49it/s] 14%|█▎        | 427/3145 [02:54<18:48,  2.41it/s] 13%|█▎        | 423/3145 [02:54<18:10,  2.50it/s] 14%|█▎        | 427/3145 [02:54<18:27,  2.45it/s] 14%|█▍        | 439/3143 [03:00<20:40,  2.18it/s] 13%|█▎        | 424/3145 [02:55<17:35,  2.58it/s] 14%|█▎        | 428/3145 [02:55<19:01,  2.38it/s] 14%|█▍        | 440/3143 [03:01<19:56,  2.26it/s] 14%|█▎        | 428/3145 [02:55<18:53,  2.40it/s] 14%|█▎        | 425/3145 [02:55<17:25,  2.60it/s] 14%|█▎        | 429/3145 [02:55<18:38,  2.43it/s] 14%|█▍        | 441/3143 [03:01<19:24,  2.32it/s] 14%|█▎        | 429/3145 [02:55<19:09,  2.36it/s] 14%|█▎        | 426/3145 [02:55<18:04,  2.51it/s] 14%|█▎        | 430/3145 [02:55<18:56,  2.39it/s] 14%|█▍        | 442/3143 [03:02<18:23,  2.45it/s] 14%|█▎        | 430/3145 [02:56<18:49,  2.40it/s] 14%|█▎        | 427/3145 [02:56<17:28,  2.59it/s] 14%|█▎        | 431/3145 [02:56<19:29,  2.32it/s] 14%|█▍        | 443/3143 [03:02<17:47,  2.53it/s] 14%|█▎        | 428/3145 [02:56<17:38,  2.57it/s] 14%|█▎        | 431/3145 [02:56<19:49,  2.28it/s] 14%|█▎        | 432/3145 [02:56<18:40,  2.42it/s] 14%|█▍        | 444/3143 [03:02<17:55,  2.51it/s] 14%|█▎        | 429/3145 [02:57<17:27,  2.59it/s] 14%|█▎        | 432/3145 [02:57<19:36,  2.31it/s] 14%|█▍        | 433/3145 [02:57<18:59,  2.38it/s] 14%|█▍        | 445/3143 [03:03<17:54,  2.51it/s] 14%|█▎        | 430/3145 [02:57<17:23,  2.60it/s] 14%|█▍        | 434/3145 [02:57<18:11,  2.48it/s] 14%|█▍        | 433/3145 [02:57<20:32,  2.20it/s] 14%|█▍        | 446/3143 [03:03<17:27,  2.57it/s] 14%|█▎        | 431/3145 [02:57<17:26,  2.59it/s] 14%|█▍        | 435/3145 [02:58<18:38,  2.42it/s] 14%|█▍        | 447/3143 [03:03<16:51,  2.66it/s] 14%|█▍        | 434/3145 [02:58<19:33,  2.31it/s] 14%|█▎        | 432/3145 [02:58<17:44,  2.55it/s] 14%|█▍        | 436/3145 [02:58<17:56,  2.52it/s] 14%|█▍        | 448/3143 [03:04<16:21,  2.75it/s] 14%|█▍        | 435/3145 [02:58<19:14,  2.35it/s] 14%|█▍        | 433/3145 [02:58<17:21,  2.60it/s] 14%|█▍        | 449/3143 [03:04<15:43,  2.86it/s] 14%|█▍        | 437/3145 [02:58<18:03,  2.50it/s] 14%|█▍        | 436/3145 [02:58<18:23,  2.45it/s] 14%|█▍        | 434/3145 [02:59<18:00,  2.51it/s] 14%|█▍        | 450/3143 [03:04<15:43,  2.85it/s] 14%|█▍        | 438/3145 [02:59<17:51,  2.53it/s] 14%|█▍        | 437/3145 [02:59<18:18,  2.47it/s] 14%|█▍        | 451/3143 [03:05<15:41,  2.86it/s] 14%|█▍        | 435/3145 [02:59<19:05,  2.37it/s] 14%|█▍        | 439/3145 [02:59<17:51,  2.53it/s] 14%|█▍        | 438/3145 [02:59<18:11,  2.48it/s] 14%|█▍        | 452/3143 [03:05<16:40,  2.69it/s] 14%|█▍        | 436/3145 [02:59<18:21,  2.46it/s] 14%|█▍        | 440/3145 [02:59<17:46,  2.54it/s] 14%|█▍        | 439/3145 [02:59<17:40,  2.55it/s] 14%|█▍        | 453/3143 [03:06<16:59,  2.64it/s] 14%|█▍        | 437/3145 [03:00<18:20,  2.46it/s] 14%|█▍        | 441/3145 [03:00<18:06,  2.49it/s] 14%|█▍        | 440/3145 [03:00<17:28,  2.58it/s] 14%|█▍        | 454/3143 [03:06<16:59,  2.64it/s] 14%|█▍        | 438/3145 [03:00<17:50,  2.53it/s] 14%|█▍        | 442/3145 [03:00<17:39,  2.55it/s] 14%|█▍        | 441/3145 [03:00<17:49,  2.53it/s] 14%|█▍        | 455/3143 [03:06<17:43,  2.53it/s] 14%|█▍        | 439/3145 [03:01<18:08,  2.49it/s] 14%|█▍        | 443/3145 [03:01<17:56,  2.51it/s] 14%|█▍        | 442/3145 [03:01<18:18,  2.46it/s] 14%|█▍        | 444/3145 [03:01<15:42,  2.87it/s] 15%|█▍        | 456/3143 [03:07<17:24,  2.57it/s] 14%|█▍        | 440/3145 [03:01<17:20,  2.60it/s] 14%|█▍        | 443/3145 [03:01<18:46,  2.40it/s] 15%|█▍        | 457/3143 [03:07<17:31,  2.55it/s] 14%|█▍        | 441/3145 [03:01<17:35,  2.56it/s] 14%|█▍        | 445/3145 [03:01<17:02,  2.64it/s] 14%|█▍        | 444/3145 [03:02<18:29,  2.43it/s] 15%|█▍        | 458/3143 [03:08<17:18,  2.58it/s] 14%|█▍        | 442/3145 [03:02<17:48,  2.53it/s] 14%|█▍        | 446/3145 [03:02<17:51,  2.52it/s] 14%|█▍        | 445/3145 [03:02<18:23,  2.45it/s] 15%|█▍        | 459/3143 [03:08<18:16,  2.45it/s] 14%|█▍        | 443/3145 [03:02<18:30,  2.43it/s] 14%|█▍        | 447/3145 [03:02<19:31,  2.30it/s] 14%|█▍        | 446/3145 [03:02<19:29,  2.31it/s] 15%|█▍        | 460/3143 [03:08<17:52,  2.50it/s] 14%|█▍        | 444/3145 [03:03<18:15,  2.47it/s] 14%|█▍        | 448/3145 [03:03<18:22,  2.45it/s] 14%|█▍        | 447/3145 [03:03<20:11,  2.23it/s] 15%|█▍        | 461/3143 [03:09<18:21,  2.43it/s] 14%|█▍        | 445/3145 [03:03<18:08,  2.48it/s] 14%|█▍        | 449/3145 [03:03<18:23,  2.44it/s] 14%|█▍        | 448/3145 [03:03<19:11,  2.34it/s] 15%|█▍        | 462/3143 [03:09<18:24,  2.43it/s] 14%|█▍        | 446/3145 [03:03<19:06,  2.35it/s] 14%|█▍        | 450/3145 [03:03<18:18,  2.45it/s] 14%|█▍        | 449/3145 [03:04<19:09,  2.35it/s] 14%|█▍        | 451/3145 [03:04<16:52,  2.66it/s] 15%|█▍        | 463/3143 [03:10<18:49,  2.37it/s] 14%|█▍        | 447/3145 [03:04<19:45,  2.27it/s] 14%|█▍        | 450/3145 [03:04<19:00,  2.36it/s] 14%|█▍        | 452/3145 [03:04<17:12,  2.61it/s] 15%|█▍        | 464/3143 [03:10<18:38,  2.39it/s] 14%|█▍        | 448/3145 [03:04<19:38,  2.29it/s] 14%|█▍        | 453/3145 [03:05<17:00,  2.64it/s] 14%|█▍        | 451/3145 [03:05<19:52,  2.26it/s] 15%|█▍        | 465/3143 [03:11<19:03,  2.34it/s] 14%|█▍        | 449/3145 [03:05<19:35,  2.29it/s] 14%|█▍        | 454/3145 [03:05<17:08,  2.62it/s] 15%|█▍        | 466/3143 [03:11<18:27,  2.42it/s] 14%|█▍        | 450/3145 [03:05<18:50,  2.38it/s] 14%|█▍        | 452/3145 [03:05<21:00,  2.14it/s] 14%|█▍        | 455/3145 [03:05<17:02,  2.63it/s] 15%|█▍        | 467/3143 [03:11<17:35,  2.54it/s] 14%|█▍        | 451/3145 [03:06<18:10,  2.47it/s] 14%|█▍        | 453/3145 [03:06<19:38,  2.28it/s] 14%|█▍        | 456/3145 [03:06<16:46,  2.67it/s] 15%|█▍        | 468/3143 [03:12<17:29,  2.55it/s] 14%|█▍        | 452/3145 [03:06<18:03,  2.48it/s] 15%|█▍        | 457/3145 [03:06<17:34,  2.55it/s] 14%|█▍        | 454/3145 [03:06<21:33,  2.08it/s] 15%|█▍        | 469/3143 [03:12<18:23,  2.42it/s] 14%|█▍        | 453/3145 [03:06<18:01,  2.49it/s] 15%|█▍        | 458/3145 [03:06<17:10,  2.61it/s] 14%|█▍        | 455/3145 [03:07<20:28,  2.19it/s] 15%|█▍        | 470/3143 [03:13<18:09,  2.45it/s] 14%|█▍        | 454/3145 [03:07<17:35,  2.55it/s] 15%|█▍        | 459/3145 [03:07<17:38,  2.54it/s] 14%|█▍        | 456/3145 [03:07<19:41,  2.28it/s] 15%|█▍        | 471/3143 [03:13<17:50,  2.50it/s] 14%|█▍        | 455/3145 [03:07<17:39,  2.54it/s] 15%|█▍        | 460/3145 [03:07<17:37,  2.54it/s] 15%|█▍        | 457/3145 [03:07<19:38,  2.28it/s] 15%|█▌        | 472/3143 [03:13<18:15,  2.44it/s] 14%|█▍        | 456/3145 [03:07<17:38,  2.54it/s] 15%|█▍        | 461/3145 [03:08<17:52,  2.50it/s] 15%|█▍        | 458/3145 [03:08<18:38,  2.40it/s] 15%|█▍        | 457/3145 [03:08<17:38,  2.54it/s] 15%|█▌        | 473/3143 [03:14<19:00,  2.34it/s] 15%|█▍        | 462/3145 [03:08<17:50,  2.51it/s] 15%|█▍        | 459/3145 [03:08<20:20,  2.20it/s] 15%|█▍        | 458/3145 [03:08<17:59,  2.49it/s] 15%|█▌        | 474/3143 [03:14<18:34,  2.39it/s] 15%|█▍        | 463/3145 [03:08<17:41,  2.53it/s] 15%|█▍        | 460/3145 [03:09<19:43,  2.27it/s] 15%|█▍        | 459/3145 [03:09<17:58,  2.49it/s] 15%|█▌        | 475/3143 [03:15<18:00,  2.47it/s] 15%|█▍        | 464/3145 [03:09<17:48,  2.51it/s] 15%|█▍        | 461/3145 [03:09<20:01,  2.23it/s] 15%|█▍        | 460/3145 [03:09<18:32,  2.41it/s] 15%|█▌        | 476/3143 [03:15<19:13,  2.31it/s] 15%|█▍        | 465/3145 [03:09<17:57,  2.49it/s] 15%|█▍        | 461/3145 [03:10<18:15,  2.45it/s] 15%|█▍        | 462/3145 [03:10<19:27,  2.30it/s] 15%|█▌        | 477/3143 [03:16<19:33,  2.27it/s] 15%|█▍        | 466/3145 [03:10<18:20,  2.44it/s] 15%|█▍        | 462/3145 [03:10<17:44,  2.52it/s] 15%|█▍        | 463/3145 [03:10<18:45,  2.38it/s] 15%|█▍        | 467/3145 [03:10<17:38,  2.53it/s] 15%|█▌        | 478/3143 [03:16<19:46,  2.25it/s] 15%|█▍        | 463/3145 [03:10<17:23,  2.57it/s] 15%|█▍        | 464/3145 [03:10<18:37,  2.40it/s] 15%|█▍        | 468/3145 [03:10<17:38,  2.53it/s] 15%|█▌        | 479/3143 [03:16<19:40,  2.26it/s] 15%|█▍        | 464/3145 [03:11<18:01,  2.48it/s] 15%|█▍        | 465/3145 [03:11<18:29,  2.42it/s] 15%|█▌        | 480/3143 [03:17<18:56,  2.34it/s] 15%|█▍        | 469/3145 [03:11<18:34,  2.40it/s] 15%|█▍        | 465/3145 [03:11<18:25,  2.42it/s] 15%|█▍        | 466/3145 [03:11<18:18,  2.44it/s] 15%|█▌        | 481/3143 [03:17<18:59,  2.34it/s] 15%|█▍        | 470/3145 [03:11<18:58,  2.35it/s] 15%|█▍        | 466/3145 [03:12<17:50,  2.50it/s] 15%|█▍        | 467/3145 [03:12<17:53,  2.49it/s] 15%|█▍        | 471/3145 [03:12<19:04,  2.34it/s] 15%|█▍        | 467/3145 [03:12<17:08,  2.60it/s] 15%|█▌        | 482/3143 [03:18<21:19,  2.08it/s] 15%|█▍        | 468/3145 [03:12<18:47,  2.37it/s] 15%|█▌        | 472/3145 [03:12<16:22,  2.72it/s] 15%|█▍        | 468/3145 [03:12<17:44,  2.51it/s] 15%|█▌        | 483/3143 [03:18<20:25,  2.17it/s] 15%|█▍        | 469/3145 [03:12<19:02,  2.34it/s] 15%|█▌        | 473/3145 [03:12<17:08,  2.60it/s] 15%|█▍        | 469/3145 [03:13<17:43,  2.52it/s] 15%|█▌        | 484/3143 [03:19<20:10,  2.20it/s] 15%|█▌        | 474/3145 [03:13<16:48,  2.65it/s] 15%|█▍        | 470/3145 [03:13<19:10,  2.33it/s] 15%|█▍        | 470/3145 [03:13<17:10,  2.60it/s] 15%|█▌        | 475/3145 [03:13<15:09,  2.94it/s] 15%|█▌        | 485/3143 [03:19<17:53,  2.48it/s] 15%|█▍        | 471/3145 [03:13<19:34,  2.28it/s] 15%|█▌        | 476/3145 [03:13<14:48,  3.00it/s] 15%|█▍        | 471/3145 [03:13<17:23,  2.56it/s] 15%|█▌        | 486/3143 [03:19<18:01,  2.46it/s] 15%|█▌        | 477/3145 [03:14<13:25,  3.31it/s] 15%|█▌        | 472/3145 [03:14<18:01,  2.47it/s] 15%|█▌        | 487/3143 [03:20<17:55,  2.47it/s] 15%|█▌        | 478/3145 [03:14<13:59,  3.18it/s] 15%|█▌        | 472/3145 [03:14<22:29,  1.98it/s] 15%|█▌        | 473/3145 [03:14<17:40,  2.52it/s] 16%|█▌        | 488/3143 [03:20<18:00,  2.46it/s] 15%|█▌        | 473/3145 [03:14<20:45,  2.15it/s] 15%|█▌        | 479/3145 [03:14<15:27,  2.87it/s] 16%|█▌        | 489/3143 [03:20<15:53,  2.78it/s] 15%|█▌        | 474/3145 [03:15<17:08,  2.60it/s] 15%|█▌        | 474/3145 [03:15<20:07,  2.21it/s] 15%|█▌        | 480/3145 [03:15<16:10,  2.75it/s] 16%|█▌        | 490/3143 [03:21<16:41,  2.65it/s] 15%|█▌        | 475/3145 [03:15<17:18,  2.57it/s] 15%|█▌        | 475/3145 [03:15<19:29,  2.28it/s] 15%|█▌        | 481/3145 [03:15<16:29,  2.69it/s] 15%|█▌        | 476/3145 [03:15<17:21,  2.56it/s] 16%|█▌        | 491/3143 [03:21<17:55,  2.47it/s] 15%|█▌        | 476/3145 [03:16<18:50,  2.36it/s] 15%|█▌        | 482/3145 [03:16<17:18,  2.56it/s] 15%|█▌        | 477/3145 [03:16<16:45,  2.65it/s] 16%|█▌        | 492/3143 [03:22<18:38,  2.37it/s] 15%|█▌        | 477/3145 [03:16<18:31,  2.40it/s] 15%|█▌        | 478/3145 [03:16<14:47,  3.01it/s] 15%|█▌        | 483/3145 [03:16<20:04,  2.21it/s] 15%|█▌        | 478/3145 [03:16<17:47,  2.50it/s] 16%|█▌        | 493/3143 [03:22<19:14,  2.30it/s] 15%|█▌        | 479/3145 [03:16<16:11,  2.74it/s] 15%|█▌        | 484/3145 [03:17<18:51,  2.35it/s] 15%|█▌        | 479/3145 [03:17<18:01,  2.47it/s] 16%|█▌        | 494/3143 [03:23<18:14,  2.42it/s] 15%|█▌        | 480/3145 [03:17<16:29,  2.69it/s] 15%|█▌        | 485/3145 [03:17<18:09,  2.44it/s] 16%|█▌        | 495/3143 [03:23<18:06,  2.44it/s] 15%|█▌        | 480/3145 [03:17<18:37,  2.38it/s] 15%|█▌        | 481/3145 [03:17<17:19,  2.56it/s] 15%|█▌        | 486/3145 [03:17<17:40,  2.51it/s] 16%|█▌        | 496/3143 [03:23<17:38,  2.50it/s] 15%|█▌        | 482/3145 [03:18<16:48,  2.64it/s] 15%|█▌        | 481/3145 [03:18<18:29,  2.40it/s] 16%|█▌        | 497/3143 [03:24<16:10,  2.73it/s] 15%|█▌        | 487/3145 [03:18<20:29,  2.16it/s] 15%|█▌        | 482/3145 [03:18<18:45,  2.37it/s] 16%|█▌        | 498/3143 [03:24<14:54,  2.96it/s] 15%|█▌        | 483/3145 [03:18<18:29,  2.40it/s] 16%|█▌        | 488/3145 [03:18<19:31,  2.27it/s] 15%|█▌        | 483/3145 [03:18<18:23,  2.41it/s] 15%|█▌        | 484/3145 [03:19<18:10,  2.44it/s] 16%|█▌        | 499/3143 [03:25<17:35,  2.51it/s] 16%|█▌        | 489/3145 [03:19<18:54,  2.34it/s] 15%|█▌        | 485/3145 [03:19<17:55,  2.47it/s] 15%|█▌        | 484/3145 [03:19<18:41,  2.37it/s] 16%|█▌        | 500/3143 [03:25<17:22,  2.53it/s] 16%|█▌        | 490/3145 [03:19<19:17,  2.29it/s] 15%|█▌        | 486/3145 [03:19<18:01,  2.46it/s] 15%|█▌        | 485/3145 [03:19<18:52,  2.35it/s] 16%|█▌        | 501/3143 [03:25<17:53,  2.46it/s] 16%|█▌        | 491/3145 [03:20<19:03,  2.32it/s] 15%|█▌        | 487/3145 [03:20<18:24,  2.41it/s] 15%|█▌        | 486/3145 [03:20<18:57,  2.34it/s] 16%|█▌        | 502/3143 [03:26<17:51,  2.46it/s] 16%|█▌        | 492/3145 [03:20<18:04,  2.45it/s] 16%|█▌        | 488/3145 [03:20<18:14,  2.43it/s] 15%|█▌        | 487/3145 [03:20<18:58,  2.34it/s] 16%|█▌        | 503/3143 [03:26<17:46,  2.48it/s] 16%|█▌        | 493/3145 [03:20<18:43,  2.36it/s] 16%|█▌        | 489/3145 [03:21<18:30,  2.39it/s] 16%|█▌        | 488/3145 [03:21<18:57,  2.34it/s] 16%|█▌        | 504/3143 [03:27<18:23,  2.39it/s] 16%|█▌        | 494/3145 [03:21<18:53,  2.34it/s] 16%|█▌        | 490/3145 [03:21<17:24,  2.54it/s] 16%|█▌        | 489/3145 [03:21<18:28,  2.40it/s] 16%|█▌        | 505/3143 [03:27<17:49,  2.47it/s] 16%|█▌        | 495/3145 [03:21<18:31,  2.38it/s] 16%|█▌        | 491/3145 [03:21<17:19,  2.55it/s] 16%|█▌        | 490/3145 [03:21<19:10,  2.31it/s] 16%|█▌        | 506/3143 [03:28<19:43,  2.23it/s] 16%|█▌        | 492/3145 [03:22<16:53,  2.62it/s] 16%|█▌        | 496/3145 [03:22<18:30,  2.39it/s] 16%|█▌        | 491/3145 [03:22<18:30,  2.39it/s] 16%|█▌        | 507/3143 [03:28<18:35,  2.36it/s] 16%|█▌        | 497/3145 [03:22<17:44,  2.49it/s] 16%|█▌        | 493/3145 [03:22<17:35,  2.51it/s] 16%|█▌        | 492/3145 [03:22<18:11,  2.43it/s] 16%|█▌        | 508/3143 [03:28<18:14,  2.41it/s] 16%|█▌        | 498/3145 [03:22<17:28,  2.52it/s] 16%|█▌        | 494/3145 [03:22<17:22,  2.54it/s] 16%|█▌        | 493/3145 [03:23<18:40,  2.37it/s] 16%|█▌        | 509/3143 [03:29<18:04,  2.43it/s] 16%|█▌        | 499/3145 [03:23<17:24,  2.53it/s] 16%|█▌        | 495/3145 [03:23<17:26,  2.53it/s] 16%|█▌        | 494/3145 [03:23<18:18,  2.41it/s] 16%|█▌        | 500/3145 [03:23<17:04,  2.58it/s] 16%|█▌        | 510/3143 [03:29<18:17,  2.40it/s] 16%|█▌        | 496/3145 [03:23<18:05,  2.44it/s] 16%|█▌        | 495/3145 [03:24<18:44,  2.36it/s] 16%|█▌        | 501/3145 [03:24<18:16,  2.41it/s] 16%|█▌        | 497/3145 [03:24<17:45,  2.49it/s] 16%|█▋        | 511/3143 [03:30<18:58,  2.31it/s] 16%|█▌        | 496/3145 [03:24<18:28,  2.39it/s] 16%|█▌        | 502/3145 [03:24<18:19,  2.40it/s] 16%|█▌        | 498/3145 [03:24<17:33,  2.51it/s] 16%|█▋        | 512/3143 [03:30<18:45,  2.34it/s] 16%|█▌        | 499/3145 [03:24<15:17,  2.88it/s] 16%|█▌        | 497/3145 [03:24<18:16,  2.41it/s] 16%|█▌        | 503/3145 [03:24<18:05,  2.43it/s] 16%|█▌        | 500/3145 [03:25<15:24,  2.86it/s] 16%|█▋        | 513/3143 [03:31<21:00,  2.09it/s] 16%|█▌        | 498/3145 [03:25<17:48,  2.48it/s] 16%|█▌        | 504/3145 [03:25<17:37,  2.50it/s] 16%|█▌        | 501/3145 [03:25<16:12,  2.72it/s] 16%|█▋        | 514/3143 [03:31<20:09,  2.17it/s] 16%|█▌        | 499/3145 [03:25<18:04,  2.44it/s] 16%|█▌        | 505/3145 [03:25<18:23,  2.39it/s] 16%|█▌        | 502/3145 [03:25<16:26,  2.68it/s] 16%|█▌        | 500/3145 [03:26<17:55,  2.46it/s] 16%|█▋        | 515/3143 [03:32<20:59,  2.09it/s] 16%|█▌        | 506/3145 [03:26<18:48,  2.34it/s] 16%|█▌        | 503/3145 [03:26<16:33,  2.66it/s] 16%|█▋        | 516/3143 [03:32<17:41,  2.47it/s] 16%|█▌        | 501/3145 [03:26<18:21,  2.40it/s] 16%|█▌        | 507/3145 [03:26<16:41,  2.63it/s] 16%|█▌        | 504/3145 [03:26<16:50,  2.61it/s] 16%|█▋        | 517/3143 [03:32<17:57,  2.44it/s] 16%|█▌        | 508/3145 [03:26<17:23,  2.53it/s] 16%|█▌        | 502/3145 [03:27<20:45,  2.12it/s] 16%|█▌        | 505/3145 [03:27<17:02,  2.58it/s] 16%|█▋        | 518/3143 [03:33<18:11,  2.40it/s] 16%|█▌        | 503/3145 [03:27<19:02,  2.31it/s] 16%|█▌        | 509/3145 [03:27<18:47,  2.34it/s] 16%|█▌        | 506/3145 [03:27<17:07,  2.57it/s] 17%|█▋        | 519/3143 [03:33<17:19,  2.52it/s] 16%|█▌        | 504/3145 [03:27<19:01,  2.31it/s] 16%|█▌        | 510/3145 [03:27<18:53,  2.32it/s] 16%|█▌        | 507/3145 [03:27<16:47,  2.62it/s] 17%|█▋        | 520/3143 [03:33<17:49,  2.45it/s] 16%|█▌        | 505/3145 [03:28<18:16,  2.41it/s] 16%|█▌        | 508/3145 [03:28<16:44,  2.63it/s] 16%|█▌        | 511/3145 [03:28<19:00,  2.31it/s] 17%|█▋        | 521/3143 [03:34<17:27,  2.50it/s] 16%|█▌        | 506/3145 [03:28<18:31,  2.37it/s] 16%|█▌        | 509/3145 [03:28<18:02,  2.43it/s] 16%|█▋        | 512/3145 [03:28<19:00,  2.31it/s] 17%|█▋        | 522/3143 [03:34<17:19,  2.52it/s] 16%|█▋        | 513/3145 [03:29<18:09,  2.42it/s] 16%|█▌        | 507/3145 [03:29<19:02,  2.31it/s] 17%|█▋        | 523/3143 [03:35<16:57,  2.58it/s] 16%|█▌        | 510/3145 [03:29<18:07,  2.42it/s] 16%|█▋        | 514/3145 [03:29<17:25,  2.52it/s] 16%|█▌        | 508/3145 [03:29<18:34,  2.37it/s] 17%|█▋        | 524/3143 [03:35<17:33,  2.49it/s] 16%|█▌        | 511/3145 [03:29<18:25,  2.38it/s] 16%|█▋        | 515/3145 [03:29<16:59,  2.58it/s] 16%|█▌        | 509/3145 [03:29<18:43,  2.35it/s] 17%|█▋        | 525/3143 [03:35<17:32,  2.49it/s] 16%|█▋        | 512/3145 [03:30<18:08,  2.42it/s] 16%|█▋        | 516/3145 [03:30<16:37,  2.64it/s] 17%|█▋        | 526/3143 [03:36<16:52,  2.59it/s] 16%|█▌        | 510/3145 [03:30<18:50,  2.33it/s] 16%|█▋        | 513/3145 [03:30<18:07,  2.42it/s] 16%|█▋        | 517/3145 [03:30<16:47,  2.61it/s] 17%|█▋        | 527/3143 [03:36<17:08,  2.54it/s] 16%|█▋        | 514/3145 [03:30<17:28,  2.51it/s] 16%|█▌        | 511/3145 [03:30<18:54,  2.32it/s] 16%|█▋        | 518/3145 [03:31<16:57,  2.58it/s] 16%|█▋        | 515/3145 [03:31<17:56,  2.44it/s] 16%|█▋        | 512/3145 [03:31<18:39,  2.35it/s] 17%|█▋        | 528/3143 [03:37<19:27,  2.24it/s] 17%|█▋        | 519/3145 [03:31<17:05,  2.56it/s] 16%|█▋        | 516/3145 [03:31<18:30,  2.37it/s] 17%|█▋        | 529/3143 [03:37<18:45,  2.32it/s] 16%|█▋        | 513/3145 [03:31<19:14,  2.28it/s] 17%|█▋        | 520/3145 [03:31<18:52,  2.32it/s] 16%|█▋        | 517/3145 [03:32<18:02,  2.43it/s] 17%|█▋        | 530/3143 [03:38<18:17,  2.38it/s] 16%|█▋        | 514/3145 [03:32<19:24,  2.26it/s] 17%|█▋        | 521/3145 [03:32<18:56,  2.31it/s] 16%|█▋        | 515/3145 [03:32<16:26,  2.67it/s] 16%|█▋        | 518/3145 [03:32<17:39,  2.48it/s] 17%|█▋        | 531/3143 [03:38<17:51,  2.44it/s] 17%|█▋        | 519/3145 [03:32<15:58,  2.74it/s] 16%|█▋        | 516/3145 [03:32<16:37,  2.64it/s] 17%|█▋        | 522/3145 [03:32<19:16,  2.27it/s] 17%|█▋        | 532/3143 [03:38<17:59,  2.42it/s] 16%|█▋        | 517/3145 [03:33<15:07,  2.90it/s] 17%|█▋        | 520/3145 [03:33<15:44,  2.78it/s] 17%|█▋        | 523/3145 [03:33<18:15,  2.39it/s] 17%|█▋        | 533/3143 [03:39<18:01,  2.41it/s] 17%|█▋        | 521/3145 [03:33<15:32,  2.81it/s] 16%|█▋        | 518/3145 [03:33<15:31,  2.82it/s] 17%|█▋        | 524/3145 [03:33<17:54,  2.44it/s] 17%|█▋        | 534/3143 [03:39<17:25,  2.49it/s] 17%|█▋        | 519/3145 [03:33<16:18,  2.68it/s] 17%|█▋        | 522/3145 [03:33<16:32,  2.64it/s] 17%|█▋        | 525/3145 [03:34<18:16,  2.39it/s] 17%|█▋        | 523/3145 [03:34<14:38,  2.98it/s] 17%|█▋        | 535/3143 [03:39<17:03,  2.55it/s] 17%|█▋        | 520/3145 [03:34<17:10,  2.55it/s] 17%|█▋        | 536/3143 [03:40<15:04,  2.88it/s] 17%|█▋        | 524/3145 [03:34<13:31,  3.23it/s] 17%|█▋        | 526/3145 [03:34<18:31,  2.36it/s] 17%|█▋        | 537/3143 [03:40<15:22,  2.82it/s] 17%|█▋        | 525/3145 [03:34<14:48,  2.95it/s] 17%|█▋        | 521/3145 [03:34<18:13,  2.40it/s] 17%|█▋        | 527/3145 [03:34<19:14,  2.27it/s] 17%|█▋        | 538/3143 [03:40<15:39,  2.77it/s] 17%|█▋        | 522/3145 [03:35<17:22,  2.52it/s] 17%|█▋        | 526/3145 [03:35<15:36,  2.80it/s] 17%|█▋        | 528/3145 [03:35<18:41,  2.33it/s] 17%|█▋        | 539/3143 [03:41<15:43,  2.76it/s] 17%|█▋        | 523/3145 [03:35<17:31,  2.49it/s] 17%|█▋        | 527/3145 [03:35<16:50,  2.59it/s] 17%|█▋        | 540/3143 [03:41<13:57,  3.11it/s] 17%|█▋        | 529/3145 [03:35<18:49,  2.32it/s] 17%|█▋        | 524/3145 [03:35<18:02,  2.42it/s] 17%|█▋        | 528/3145 [03:36<17:25,  2.50it/s] 17%|█▋        | 541/3143 [03:42<15:28,  2.80it/s] 17%|█▋        | 530/3145 [03:36<18:51,  2.31it/s] 17%|█▋        | 525/3145 [03:36<17:41,  2.47it/s] 17%|█▋        | 542/3143 [03:42<15:33,  2.79it/s] 17%|█▋        | 529/3145 [03:36<18:07,  2.41it/s] 17%|█▋        | 526/3145 [03:36<17:34,  2.48it/s] 17%|█▋        | 531/3145 [03:36<20:41,  2.10it/s] 17%|█▋        | 543/3143 [03:42<15:54,  2.72it/s] 17%|█▋        | 530/3145 [03:36<18:23,  2.37it/s] 17%|█▋        | 527/3145 [03:37<17:34,  2.48it/s] 17%|█▋        | 532/3145 [03:37<19:31,  2.23it/s] 17%|█▋        | 544/3143 [03:43<16:16,  2.66it/s] 17%|█▋        | 531/3145 [03:37<20:24,  2.13it/s] 17%|█▋        | 528/3145 [03:37<17:33,  2.48it/s] 17%|█▋        | 533/3145 [03:37<19:18,  2.26it/s] 17%|█▋        | 545/3143 [03:43<16:37,  2.60it/s] 17%|█▋        | 532/3145 [03:37<19:54,  2.19it/s] 17%|█▋        | 529/3145 [03:37<18:05,  2.41it/s] 17%|█▋        | 534/3145 [03:38<18:43,  2.32it/s] 17%|█▋        | 546/3143 [03:43<17:08,  2.53it/s] 17%|█▋        | 533/3145 [03:38<19:05,  2.28it/s] 17%|█▋        | 535/3145 [03:38<18:10,  2.39it/s] 17%|█▋        | 530/3145 [03:38<18:08,  2.40it/s] 17%|█▋        | 547/3143 [03:44<19:12,  2.25it/s] 17%|█▋        | 536/3145 [03:38<17:46,  2.45it/s] 17%|█▋        | 534/3145 [03:38<19:40,  2.21it/s] 17%|█▋        | 531/3145 [03:38<18:51,  2.31it/s] 17%|█▋        | 535/3145 [03:39<18:40,  2.33it/s] 17%|█▋        | 548/3143 [03:45<20:36,  2.10it/s] 17%|█▋        | 537/3145 [03:39<17:46,  2.45it/s] 17%|█▋        | 532/3145 [03:39<18:36,  2.34it/s] 17%|█▋        | 536/3145 [03:39<18:17,  2.38it/s] 17%|█▋        | 538/3145 [03:39<17:49,  2.44it/s] 17%|█▋        | 533/3145 [03:39<17:56,  2.43it/s] 17%|█▋        | 549/3143 [03:45<22:44,  1.90it/s] 17%|█▋        | 539/3145 [03:39<17:22,  2.50it/s] 17%|█▋        | 537/3145 [03:40<18:26,  2.36it/s] 17%|█▋        | 534/3145 [03:40<18:14,  2.39it/s] 17%|█▋        | 550/3143 [03:46<21:37,  2.00it/s] 17%|█▋        | 540/3145 [03:40<16:49,  2.58it/s] 17%|█▋        | 538/3145 [03:40<18:53,  2.30it/s] 17%|█▋        | 535/3145 [03:40<17:58,  2.42it/s] 18%|█▊        | 551/3143 [03:46<20:00,  2.16it/s] 17%|█▋        | 541/3145 [03:40<16:13,  2.67it/s] 17%|█▋        | 536/3145 [03:40<17:21,  2.51it/s] 17%|█▋        | 539/3145 [03:40<18:28,  2.35it/s] 17%|█▋        | 542/3145 [03:41<16:56,  2.56it/s] 18%|█▊        | 552/3143 [03:47<20:52,  2.07it/s] 17%|█▋        | 540/3145 [03:41<18:10,  2.39it/s] 17%|█▋        | 537/3145 [03:41<17:59,  2.41it/s] 17%|█▋        | 543/3145 [03:41<17:50,  2.43it/s] 18%|█▊        | 553/3143 [03:47<19:47,  2.18it/s] 17%|█▋        | 541/3145 [03:41<17:59,  2.41it/s] 17%|█▋        | 538/3145 [03:41<18:24,  2.36it/s] 17%|█▋        | 544/3145 [03:41<17:11,  2.52it/s] 18%|█▊        | 554/3143 [03:47<19:09,  2.25it/s] 17%|█▋        | 542/3145 [03:42<18:36,  2.33it/s] 17%|█▋        | 539/3145 [03:42<18:50,  2.31it/s] 17%|█▋        | 545/3145 [03:42<17:42,  2.45it/s] 18%|█▊        | 555/3143 [03:48<18:37,  2.32it/s] 17%|█▋        | 540/3145 [03:42<18:37,  2.33it/s] 17%|█▋        | 543/3145 [03:42<19:51,  2.18it/s] 17%|█▋        | 546/3145 [03:42<16:53,  2.57it/s] 18%|█▊        | 556/3143 [03:48<17:55,  2.41it/s] 18%|█▊        | 557/3143 [03:48<15:29,  2.78it/s] 17%|█▋        | 541/3145 [03:43<18:16,  2.37it/s] 17%|█▋        | 544/3145 [03:43<19:07,  2.27it/s] 17%|█▋        | 547/3145 [03:43<17:29,  2.47it/s] 17%|█▋        | 542/3145 [03:43<15:50,  2.74it/s] 18%|█▊        | 558/3143 [03:49<16:17,  2.65it/s] 17%|█▋        | 548/3145 [03:43<17:18,  2.50it/s] 17%|█▋        | 545/3145 [03:43<19:56,  2.17it/s] 17%|█▋        | 543/3145 [03:43<16:00,  2.71it/s] 18%|█▊        | 559/3143 [03:49<16:53,  2.55it/s] 17%|█▋        | 546/3145 [03:43<18:25,  2.35it/s] 17%|█▋        | 549/3145 [03:43<17:21,  2.49it/s] 17%|█▋        | 544/3145 [03:44<17:50,  2.43it/s] 18%|█▊        | 560/3143 [03:50<17:04,  2.52it/s] 17%|█▋        | 550/3145 [03:44<17:49,  2.43it/s] 17%|█▋        | 547/3145 [03:44<19:33,  2.21it/s] 17%|█▋        | 545/3145 [03:44<18:11,  2.38it/s] 18%|█▊        | 561/3143 [03:50<17:39,  2.44it/s] 18%|█▊        | 551/3145 [03:44<17:30,  2.47it/s] 17%|█▋        | 548/3145 [03:44<18:20,  2.36it/s] 17%|█▋        | 546/3145 [03:44<17:26,  2.48it/s] 18%|█▊        | 562/3143 [03:50<17:21,  2.48it/s] 18%|█▊        | 552/3145 [03:45<17:04,  2.53it/s] 17%|█▋        | 549/3145 [03:45<18:17,  2.37it/s] 17%|█▋        | 547/3145 [03:45<17:28,  2.48it/s] 18%|█▊        | 553/3145 [03:45<17:21,  2.49it/s] 17%|█▋        | 550/3145 [03:45<18:01,  2.40it/s] 18%|█▊        | 563/3143 [03:51<19:25,  2.21it/s] 17%|█▋        | 548/3145 [03:45<17:15,  2.51it/s] 18%|█▊        | 554/3145 [03:46<17:43,  2.44it/s] 18%|█▊        | 564/3143 [03:51<18:57,  2.27it/s] 18%|█▊        | 551/3145 [03:46<18:40,  2.32it/s] 17%|█▋        | 549/3145 [03:46<17:44,  2.44it/s] 18%|█▊        | 555/3145 [03:46<15:21,  2.81it/s] 18%|█▊        | 565/3143 [03:52<18:21,  2.34it/s] 18%|█▊        | 552/3145 [03:46<18:23,  2.35it/s] 18%|█▊        | 556/3145 [03:46<15:15,  2.83it/s] 17%|█▋        | 550/3145 [03:46<18:09,  2.38it/s] 18%|█▊        | 566/3143 [03:52<17:30,  2.45it/s] 18%|█▊        | 553/3145 [03:46<18:36,  2.32it/s] 18%|█▊        | 557/3145 [03:46<15:50,  2.72it/s] 18%|█▊        | 551/3145 [03:47<17:57,  2.41it/s] 18%|█▊        | 567/3143 [03:53<17:28,  2.46it/s] 18%|█▊        | 554/3145 [03:47<18:25,  2.34it/s] 18%|█▊        | 558/3145 [03:47<16:13,  2.66it/s] 18%|█▊        | 552/3145 [03:47<17:07,  2.52it/s] 18%|█▊        | 568/3143 [03:53<17:34,  2.44it/s] 18%|█▊        | 559/3145 [03:47<16:12,  2.66it/s] 18%|█▊        | 553/3145 [03:47<16:34,  2.61it/s] 18%|█▊        | 555/3145 [03:47<18:23,  2.35it/s] 18%|█▊        | 569/3143 [03:53<17:30,  2.45it/s] 18%|█▊        | 556/3145 [03:48<17:05,  2.52it/s] 18%|█▊        | 560/3145 [03:48<16:39,  2.59it/s] 18%|█▊        | 554/3145 [03:48<16:52,  2.56it/s] 18%|█▊        | 570/3143 [03:54<17:03,  2.51it/s] 18%|█▊        | 557/3145 [03:48<17:17,  2.49it/s] 18%|█▊        | 561/3145 [03:48<16:33,  2.60it/s] 18%|█▊        | 555/3145 [03:48<17:04,  2.53it/s] 18%|█▊        | 571/3143 [03:54<16:25,  2.61it/s] 18%|█▊        | 562/3145 [03:48<15:30,  2.78it/s] 18%|█▊        | 558/3145 [03:48<17:26,  2.47it/s] 18%|█▊        | 556/3145 [03:49<18:10,  2.37it/s] 18%|█▊        | 572/3143 [03:55<16:23,  2.61it/s] 18%|█▊        | 563/3145 [03:49<16:00,  2.69it/s] 18%|█▊        | 559/3145 [03:49<16:19,  2.64it/s] 18%|█▊        | 557/3145 [03:49<18:31,  2.33it/s] 18%|█▊        | 573/3143 [03:55<17:08,  2.50it/s] 18%|█▊        | 564/3145 [03:49<16:17,  2.64it/s] 18%|█▊        | 560/3145 [03:49<16:37,  2.59it/s] 18%|█▊        | 574/3143 [03:55<16:54,  2.53it/s] 18%|█▊        | 558/3145 [03:49<19:10,  2.25it/s] 18%|█▊        | 565/3145 [03:50<16:36,  2.59it/s] 18%|█▊        | 561/3145 [03:50<18:33,  2.32it/s] 18%|█▊        | 559/3145 [03:50<18:28,  2.33it/s] 18%|█▊        | 575/3143 [03:56<17:24,  2.46it/s] 18%|█▊        | 566/3145 [03:50<17:13,  2.50it/s] 18%|█▊        | 562/3145 [03:50<18:20,  2.35it/s] 18%|█▊        | 576/3143 [03:56<17:32,  2.44it/s] 18%|█▊        | 560/3145 [03:50<19:13,  2.24it/s] 18%|█▊        | 567/3145 [03:50<17:49,  2.41it/s] 18%|█▊        | 563/3145 [03:51<17:47,  2.42it/s] 18%|█▊        | 577/3143 [03:57<16:32,  2.58it/s] 18%|█▊        | 561/3145 [03:51<18:46,  2.29it/s] 18%|█▊        | 564/3145 [03:51<18:07,  2.37it/s] 18%|█▊        | 568/3145 [03:51<19:01,  2.26it/s] 18%|█▊        | 578/3143 [03:57<16:38,  2.57it/s] 18%|█▊        | 562/3145 [03:51<18:47,  2.29it/s] 18%|█▊        | 565/3145 [03:51<18:48,  2.29it/s] 18%|█▊        | 569/3145 [03:51<20:04,  2.14it/s] 18%|█▊        | 579/3143 [03:57<17:03,  2.50it/s] 18%|█▊        | 563/3145 [03:52<18:38,  2.31it/s] 18%|█▊        | 570/3145 [03:52<18:38,  2.30it/s] 18%|█▊        | 566/3145 [03:52<18:37,  2.31it/s] 18%|█▊        | 580/3143 [03:58<17:46,  2.40it/s] 18%|█▊        | 564/3145 [03:52<17:03,  2.52it/s] 18%|█▊        | 565/3145 [03:52<15:45,  2.73it/s] 18%|█▊        | 571/3145 [03:52<18:52,  2.27it/s] 18%|█▊        | 567/3145 [03:52<18:47,  2.29it/s] 18%|█▊        | 581/3143 [03:58<18:10,  2.35it/s] 18%|█▊        | 566/3145 [03:53<14:50,  2.90it/s] 18%|█▊        | 572/3145 [03:53<17:49,  2.41it/s] 18%|█▊        | 568/3145 [03:53<18:52,  2.27it/s] 19%|█▊        | 582/3143 [03:59<18:04,  2.36it/s] 18%|█▊        | 573/3145 [03:53<17:09,  2.50it/s] 18%|█▊        | 567/3145 [03:53<16:23,  2.62it/s] 18%|█▊        | 569/3145 [03:53<18:07,  2.37it/s] 19%|█▊        | 583/3143 [03:59<17:50,  2.39it/s] 18%|█▊        | 570/3145 [03:53<15:54,  2.70it/s] 18%|█▊        | 574/3145 [03:53<16:51,  2.54it/s] 18%|█▊        | 568/3145 [03:53<16:44,  2.56it/s] 19%|█▊        | 584/3143 [03:59<17:33,  2.43it/s] 18%|█▊        | 571/3145 [03:54<15:37,  2.75it/s] 18%|█▊        | 575/3145 [03:54<16:51,  2.54it/s] 18%|█▊        | 569/3145 [03:54<16:50,  2.55it/s] 19%|█▊        | 585/3143 [04:00<18:26,  2.31it/s] 18%|█▊        | 572/3145 [03:54<16:26,  2.61it/s] 18%|█▊        | 576/3145 [03:54<16:41,  2.57it/s] 18%|█▊        | 570/3145 [03:54<17:57,  2.39it/s] 19%|█▊        | 586/3143 [04:00<18:01,  2.37it/s] 18%|█▊        | 573/3145 [03:55<16:37,  2.58it/s] 18%|█▊        | 577/3145 [03:55<17:12,  2.49it/s] 18%|█▊        | 571/3145 [03:55<18:47,  2.28it/s] 18%|█▊        | 574/3145 [03:55<16:45,  2.56it/s] 19%|█▊        | 587/3143 [04:01<18:16,  2.33it/s] 18%|█▊        | 578/3145 [03:55<17:20,  2.47it/s] 18%|█▊        | 572/3145 [03:55<18:44,  2.29it/s] 18%|█▊        | 575/3145 [03:55<16:23,  2.61it/s] 19%|█▊        | 588/3143 [04:01<17:54,  2.38it/s] 18%|█▊        | 579/3145 [03:55<16:54,  2.53it/s] 18%|█▊        | 573/3145 [03:56<18:16,  2.34it/s] 18%|█▊        | 576/3145 [03:56<16:24,  2.61it/s] 18%|█▊        | 580/3145 [03:56<17:14,  2.48it/s] 19%|█▊        | 589/3143 [04:02<18:25,  2.31it/s] 18%|█▊        | 574/3145 [03:56<17:25,  2.46it/s] 18%|█▊        | 577/3145 [03:56<17:02,  2.51it/s] 19%|█▉        | 590/3143 [04:02<18:29,  2.30it/s] 18%|█▊        | 581/3145 [03:56<18:39,  2.29it/s] 18%|█▊        | 575/3145 [03:56<16:54,  2.53it/s] 18%|█▊        | 578/3145 [03:57<17:14,  2.48it/s] 19%|█▉        | 591/3143 [04:02<17:40,  2.41it/s] 18%|█▊        | 576/3145 [03:57<16:53,  2.54it/s] 19%|█▊        | 582/3145 [03:57<20:28,  2.09it/s] 18%|█▊        | 579/3145 [03:57<17:09,  2.49it/s] 19%|█▉        | 592/3143 [04:03<17:22,  2.45it/s] 18%|█▊        | 577/3145 [03:57<17:44,  2.41it/s] 19%|█▊        | 583/3145 [03:57<19:29,  2.19it/s] 18%|█▊        | 580/3145 [03:57<17:37,  2.43it/s] 19%|█▉        | 593/3143 [04:03<18:26,  2.30it/s] 18%|█▊        | 578/3145 [03:58<17:42,  2.42it/s] 19%|█▊        | 584/3145 [03:58<19:07,  2.23it/s] 18%|█▊        | 581/3145 [03:58<18:11,  2.35it/s] 19%|█▉        | 594/3143 [04:04<18:24,  2.31it/s] 18%|█▊        | 579/3145 [03:58<17:07,  2.50it/s] 19%|█▊        | 585/3145 [03:58<18:30,  2.31it/s] 19%|█▊        | 582/3145 [03:58<17:46,  2.40it/s] 19%|█▉        | 595/3143 [04:04<18:30,  2.29it/s] 18%|█▊        | 580/3145 [03:58<17:33,  2.44it/s] 19%|█▊        | 586/3145 [03:58<17:31,  2.43it/s] 19%|█▊        | 583/3145 [03:59<16:44,  2.55it/s] 19%|█▉        | 596/3143 [04:05<18:00,  2.36it/s] 19%|█▊        | 584/3145 [03:59<16:00,  2.67it/s] 18%|█▊        | 581/3145 [03:59<18:16,  2.34it/s] 19%|█▊        | 587/3145 [03:59<17:53,  2.38it/s] 19%|█▉        | 597/3143 [04:05<17:41,  2.40it/s] 19%|█▊        | 582/3145 [03:59<17:49,  2.40it/s] 19%|█▊        | 588/3145 [03:59<17:37,  2.42it/s] 19%|█▊        | 585/3145 [03:59<17:15,  2.47it/s] 19%|█▉        | 598/3143 [04:05<17:57,  2.36it/s] 19%|█▊        | 583/3145 [04:00<17:35,  2.43it/s] 19%|█▊        | 589/3145 [04:00<17:41,  2.41it/s] 19%|█▊        | 586/3145 [04:00<17:41,  2.41it/s] 19%|█▉        | 599/3143 [04:06<17:16,  2.45it/s] 19%|█▊        | 584/3145 [04:00<16:42,  2.56it/s] 19%|█▉        | 590/3145 [04:00<17:48,  2.39it/s] 19%|█▊        | 587/3145 [04:00<18:00,  2.37it/s] 19%|█▉        | 600/3143 [04:06<17:09,  2.47it/s] 19%|█▉        | 591/3145 [04:00<15:36,  2.73it/s] 19%|█▊        | 585/3145 [04:00<17:17,  2.47it/s] 19%|█▊        | 588/3145 [04:01<18:09,  2.35it/s] 19%|█▉        | 601/3143 [04:07<17:03,  2.48it/s] 19%|█▉        | 592/3145 [04:01<16:12,  2.63it/s] 19%|█▊        | 586/3145 [04:01<17:13,  2.48it/s] 19%|█▊        | 589/3145 [04:01<18:13,  2.34it/s] 19%|█▊        | 587/3145 [04:01<16:45,  2.54it/s] 19%|█▉        | 593/3145 [04:01<16:38,  2.56it/s] 19%|█▉        | 602/3143 [04:07<19:13,  2.20it/s] 19%|█▉        | 590/3145 [04:01<17:15,  2.47it/s] 19%|█▉        | 594/3145 [04:02<16:19,  2.61it/s] 19%|█▊        | 588/3145 [04:02<17:28,  2.44it/s] 19%|█▉        | 603/3143 [04:08<18:29,  2.29it/s] 19%|█▉        | 591/3145 [04:02<16:31,  2.58it/s] 19%|█▊        | 589/3145 [04:02<16:43,  2.55it/s] 19%|█▉        | 595/3145 [04:02<16:59,  2.50it/s] 19%|█▉        | 592/3145 [04:02<16:26,  2.59it/s] 19%|█▉        | 604/3143 [04:08<18:47,  2.25it/s] 19%|█▉        | 590/3145 [04:02<16:30,  2.58it/s] 19%|█▉        | 605/3143 [04:08<16:18,  2.59it/s] 19%|█▉        | 596/3145 [04:03<17:57,  2.37it/s] 19%|█▉        | 593/3145 [04:03<16:17,  2.61it/s] 19%|█▉        | 591/3145 [04:03<16:33,  2.57it/s] 19%|█▉        | 594/3145 [04:03<14:27,  2.94it/s] 19%|█▉        | 606/3143 [04:09<16:19,  2.59it/s] 19%|█▉        | 597/3145 [04:03<18:43,  2.27it/s] 19%|█▉        | 595/3145 [04:03<14:43,  2.89it/s] 19%|█▉        | 592/3145 [04:03<16:40,  2.55it/s] 19%|█▉        | 607/3143 [04:09<16:56,  2.49it/s] 19%|█▉        | 598/3145 [04:03<17:53,  2.37it/s] 19%|█▉        | 593/3145 [04:04<16:40,  2.55it/s] 19%|█▉        | 596/3145 [04:04<16:12,  2.62it/s] 19%|█▉        | 608/3143 [04:10<17:19,  2.44it/s] 19%|█▉        | 599/3145 [04:04<18:03,  2.35it/s] 19%|█▉        | 594/3145 [04:04<17:18,  2.46it/s] 19%|█▉        | 597/3145 [04:04<17:14,  2.46it/s] 19%|█▉        | 609/3143 [04:10<17:44,  2.38it/s] 19%|█▉        | 600/3145 [04:04<17:25,  2.43it/s] 19%|█▉        | 595/3145 [04:04<17:44,  2.40it/s] 19%|█▉        | 610/3143 [04:10<17:00,  2.48it/s] 19%|█▉        | 601/3145 [04:05<16:56,  2.50it/s] 19%|█▉        | 598/3145 [04:05<18:14,  2.33it/s] 19%|█▉        | 611/3143 [04:11<16:41,  2.53it/s] 19%|█▉        | 596/3145 [04:05<17:59,  2.36it/s] 19%|█▉        | 599/3145 [04:05<17:22,  2.44it/s] 19%|█▉        | 602/3145 [04:05<17:26,  2.43it/s] 19%|█▉        | 612/3143 [04:11<16:31,  2.55it/s] 19%|█▉        | 597/3145 [04:05<18:10,  2.34it/s] 19%|█▉        | 603/3145 [04:05<16:50,  2.52it/s] 19%|█▉        | 600/3145 [04:05<17:41,  2.40it/s] 20%|█▉        | 613/3143 [04:12<16:30,  2.56it/s] 19%|█▉        | 598/3145 [04:06<17:48,  2.38it/s] 19%|█▉        | 604/3145 [04:06<17:21,  2.44it/s] 19%|█▉        | 601/3145 [04:06<18:07,  2.34it/s] 20%|█▉        | 614/3143 [04:12<16:14,  2.59it/s] 19%|█▉        | 599/3145 [04:06<17:30,  2.42it/s] 19%|█▉        | 602/3145 [04:06<17:34,  2.41it/s] 19%|█▉        | 605/3145 [04:06<17:36,  2.40it/s] 20%|█▉        | 615/3143 [04:12<16:39,  2.53it/s] 19%|█▉        | 600/3145 [04:07<17:24,  2.44it/s] 19%|█▉        | 603/3145 [04:07<17:41,  2.39it/s] 19%|█▉        | 606/3145 [04:07<18:12,  2.32it/s] 20%|█▉        | 616/3143 [04:13<17:13,  2.44it/s] 19%|█▉        | 601/3145 [04:07<17:17,  2.45it/s] 19%|█▉        | 604/3145 [04:07<17:49,  2.38it/s] 19%|█▉        | 607/3145 [04:07<18:11,  2.32it/s] 19%|█▉        | 602/3145 [04:07<16:33,  2.56it/s] 20%|█▉        | 617/3143 [04:13<17:32,  2.40it/s] 19%|█▉        | 605/3145 [04:07<17:32,  2.41it/s] 19%|█▉        | 608/3145 [04:08<18:34,  2.28it/s] 19%|█▉        | 603/3145 [04:08<15:12,  2.79it/s] 20%|█▉        | 618/3143 [04:14<15:58,  2.63it/s] 19%|█▉        | 604/3145 [04:08<15:48,  2.68it/s] 19%|█▉        | 609/3145 [04:08<18:25,  2.29it/s] 19%|█▉        | 606/3145 [04:08<19:16,  2.19it/s] 20%|█▉        | 619/3143 [04:14<16:40,  2.52it/s] 19%|█▉        | 607/3145 [04:08<17:05,  2.48it/s] 19%|█▉        | 605/3145 [04:08<15:45,  2.69it/s] 20%|█▉        | 620/3143 [04:14<16:23,  2.57it/s] 19%|█▉        | 610/3145 [04:08<18:35,  2.27it/s] 19%|█▉        | 608/3145 [04:09<17:52,  2.37it/s] 19%|█▉        | 606/3145 [04:09<16:21,  2.59it/s] 20%|█▉        | 621/3143 [04:15<16:24,  2.56it/s] 19%|█▉        | 611/3145 [04:09<18:05,  2.33it/s] 19%|█▉        | 607/3145 [04:09<16:28,  2.57it/s] 20%|█▉        | 622/3143 [04:15<17:01,  2.47it/s] 19%|█▉        | 609/3145 [04:09<19:29,  2.17it/s] 19%|█▉        | 612/3145 [04:09<19:48,  2.13it/s] 19%|█▉        | 608/3145 [04:10<16:30,  2.56it/s] 20%|█▉        | 623/3143 [04:16<17:09,  2.45it/s] 19%|█▉        | 610/3145 [04:10<19:22,  2.18it/s] 19%|█▉        | 609/3145 [04:10<16:39,  2.54it/s] 19%|█▉        | 613/3145 [04:10<20:52,  2.02it/s] 20%|█▉        | 624/3143 [04:16<17:59,  2.33it/s] 19%|█▉        | 611/3145 [04:10<19:16,  2.19it/s] 19%|█▉        | 610/3145 [04:10<16:57,  2.49it/s] 20%|█▉        | 614/3145 [04:10<20:15,  2.08it/s] 19%|█▉        | 612/3145 [04:11<18:07,  2.33it/s] 20%|█▉        | 625/3143 [04:16<18:16,  2.30it/s] 19%|█▉        | 611/3145 [04:11<17:01,  2.48it/s] 20%|█▉        | 615/3145 [04:11<18:56,  2.23it/s] 19%|█▉        | 613/3145 [04:11<18:06,  2.33it/s] 20%|█▉        | 626/3143 [04:17<18:52,  2.22it/s] 20%|█▉        | 616/3145 [04:11<18:28,  2.28it/s] 19%|█▉        | 612/3145 [04:11<17:34,  2.40it/s] 20%|█▉        | 614/3145 [04:11<17:27,  2.42it/s] 20%|█▉        | 627/3143 [04:17<18:19,  2.29it/s] 19%|█▉        | 613/3145 [04:12<17:17,  2.44it/s] 20%|█▉        | 617/3145 [04:12<18:27,  2.28it/s] 20%|█▉        | 615/3145 [04:12<18:00,  2.34it/s] 20%|█▉        | 628/3143 [04:18<17:23,  2.41it/s] 20%|█▉        | 614/3145 [04:12<17:05,  2.47it/s] 20%|█▉        | 618/3145 [04:12<18:45,  2.25it/s] 20%|██        | 629/3143 [04:18<17:18,  2.42it/s] 20%|█▉        | 616/3145 [04:12<17:56,  2.35it/s] 20%|█▉        | 615/3145 [04:12<17:32,  2.40it/s] 20%|█▉        | 619/3145 [04:13<18:15,  2.31it/s] 20%|██        | 630/3143 [04:19<16:53,  2.48it/s] 20%|█▉        | 617/3145 [04:13<18:08,  2.32it/s] 20%|█▉        | 616/3145 [04:13<17:09,  2.46it/s] 20%|█▉        | 620/3145 [04:13<17:21,  2.42it/s] 20%|██        | 631/3143 [04:19<16:42,  2.51it/s] 20%|█▉        | 618/3145 [04:13<17:14,  2.44it/s] 20%|█▉        | 617/3145 [04:13<17:11,  2.45it/s] 20%|██        | 632/3143 [04:19<14:48,  2.83it/s] 20%|█▉        | 621/3145 [04:13<17:13,  2.44it/s] 20%|█▉        | 619/3145 [04:13<17:15,  2.44it/s] 20%|█▉        | 618/3145 [04:14<17:03,  2.47it/s] 20%|█▉        | 622/3145 [04:14<17:01,  2.47it/s] 20%|██        | 633/3143 [04:20<15:54,  2.63it/s] 20%|█▉        | 620/3145 [04:14<16:25,  2.56it/s] 20%|█▉        | 623/3145 [04:14<16:36,  2.53it/s] 20%|█▉        | 619/3145 [04:14<17:31,  2.40it/s] 20%|██        | 634/3143 [04:20<16:47,  2.49it/s] 20%|█▉        | 621/3145 [04:14<16:56,  2.48it/s] 20%|█▉        | 624/3145 [04:14<15:58,  2.63it/s] 20%|█▉        | 620/3145 [04:14<17:14,  2.44it/s] 20%|██        | 635/3143 [04:20<16:44,  2.50it/s] 20%|█▉        | 622/3145 [04:15<16:49,  2.50it/s] 20%|█▉        | 625/3145 [04:15<16:38,  2.52it/s] 20%|█▉        | 621/3145 [04:15<17:09,  2.45it/s] 20%|██        | 636/3143 [04:21<17:11,  2.43it/s] 20%|█▉        | 623/3145 [04:15<16:48,  2.50it/s] 20%|█▉        | 626/3145 [04:15<16:28,  2.55it/s] 20%|█▉        | 622/3145 [04:15<16:59,  2.48it/s] 20%|██        | 637/3143 [04:21<15:45,  2.65it/s] 20%|█▉        | 624/3145 [04:15<16:37,  2.53it/s] 20%|█▉        | 627/3145 [04:16<17:15,  2.43it/s] 20%|█▉        | 623/3145 [04:16<17:08,  2.45it/s] 20%|██        | 638/3143 [04:22<16:36,  2.51it/s] 20%|█▉        | 625/3145 [04:16<17:58,  2.34it/s] 20%|██        | 639/3143 [04:22<16:14,  2.57it/s] 20%|█▉        | 628/3145 [04:16<17:48,  2.35it/s] 20%|█▉        | 624/3145 [04:16<17:32,  2.39it/s] 20%|█▉        | 626/3145 [04:16<17:58,  2.33it/s] 20%|█▉        | 625/3145 [04:16<15:23,  2.73it/s] 20%|██        | 640/3143 [04:22<16:26,  2.54it/s] 20%|██        | 629/3145 [04:17<18:12,  2.30it/s] 20%|█▉        | 627/3145 [04:17<17:17,  2.43it/s] 20%|█▉        | 626/3145 [04:17<16:18,  2.57it/s] 20%|██        | 641/3143 [04:23<16:30,  2.53it/s] 20%|██        | 630/3145 [04:17<17:27,  2.40it/s] 20%|█▉        | 628/3145 [04:17<17:31,  2.39it/s] 20%|█▉        | 627/3145 [04:17<16:59,  2.47it/s] 20%|██        | 642/3143 [04:23<17:20,  2.40it/s] 20%|██        | 631/3145 [04:18<19:27,  2.15it/s] 20%|██        | 629/3145 [04:18<17:18,  2.42it/s] 20%|█▉        | 628/3145 [04:18<17:21,  2.42it/s] 20%|██        | 643/3143 [04:24<17:55,  2.32it/s] 20%|██        | 632/3145 [04:18<18:55,  2.21it/s] 20%|██        | 630/3145 [04:18<17:07,  2.45it/s] 20%|██        | 629/3145 [04:18<17:13,  2.43it/s] 20%|██        | 633/3145 [04:18<16:57,  2.47it/s] 20%|██        | 644/3143 [04:24<18:01,  2.31it/s] 20%|██        | 631/3145 [04:18<17:05,  2.45it/s] 20%|██        | 630/3145 [04:19<17:00,  2.46it/s] 20%|██        | 634/3145 [04:19<16:23,  2.55it/s] 21%|██        | 645/3143 [04:25<18:03,  2.31it/s] 20%|██        | 632/3145 [04:19<16:55,  2.47it/s] 20%|██        | 635/3145 [04:19<14:21,  2.91it/s] 20%|██        | 631/3145 [04:19<17:27,  2.40it/s] 21%|██        | 646/3143 [04:25<16:59,  2.45it/s] 20%|██        | 633/3145 [04:19<16:28,  2.54it/s] 20%|██        | 636/3145 [04:19<14:56,  2.80it/s] 20%|██        | 632/3145 [04:19<17:43,  2.36it/s] 21%|██        | 647/3143 [04:25<16:35,  2.51it/s] 20%|██        | 634/3145 [04:20<16:28,  2.54it/s] 20%|██        | 637/3145 [04:20<16:12,  2.58it/s] 20%|██        | 633/3145 [04:20<17:43,  2.36it/s] 21%|██        | 648/3143 [04:26<17:08,  2.42it/s] 20%|██        | 635/3145 [04:20<16:22,  2.55it/s] 20%|██        | 638/3145 [04:20<16:18,  2.56it/s] 20%|██        | 634/3145 [04:20<16:55,  2.47it/s] 21%|██        | 649/3143 [04:26<17:01,  2.44it/s] 20%|██        | 636/3145 [04:20<17:05,  2.45it/s] 20%|██        | 639/3145 [04:20<16:20,  2.56it/s] 20%|██        | 635/3145 [04:21<17:21,  2.41it/s] 21%|██        | 650/3143 [04:27<16:29,  2.52it/s] 20%|██        | 640/3145 [04:21<14:15,  2.93it/s] 20%|██        | 637/3145 [04:21<17:33,  2.38it/s] 20%|██        | 636/3145 [04:21<17:04,  2.45it/s] 21%|██        | 651/3143 [04:27<17:16,  2.41it/s] 20%|██        | 641/3145 [04:21<15:25,  2.71it/s] 20%|██        | 638/3145 [04:21<17:05,  2.45it/s] 20%|██        | 637/3145 [04:21<16:30,  2.53it/s] 21%|██        | 652/3143 [04:27<16:00,  2.59it/s] 20%|██        | 642/3145 [04:22<15:20,  2.72it/s] 20%|██        | 639/3145 [04:22<16:17,  2.56it/s] 20%|██        | 638/3145 [04:22<16:15,  2.57it/s] 21%|██        | 653/3143 [04:28<16:16,  2.55it/s] 20%|██        | 643/3145 [04:22<15:16,  2.73it/s] 20%|██        | 640/3145 [04:22<16:12,  2.58it/s] 20%|██        | 639/3145 [04:22<16:29,  2.53it/s] 21%|██        | 654/3143 [04:28<16:25,  2.53it/s] 20%|██        | 644/3145 [04:22<16:07,  2.58it/s] 20%|██        | 641/3145 [04:22<16:31,  2.53it/s] 20%|██        | 640/3145 [04:23<16:19,  2.56it/s] 21%|██        | 655/3143 [04:29<16:59,  2.44it/s] 20%|██        | 642/3145 [04:23<16:22,  2.55it/s] 21%|██        | 645/3145 [04:23<17:09,  2.43it/s] 20%|██        | 641/3145 [04:23<16:00,  2.61it/s] 20%|██        | 643/3145 [04:23<16:03,  2.60it/s] 21%|██        | 656/3143 [04:29<17:56,  2.31it/s] 21%|██        | 646/3145 [04:23<17:42,  2.35it/s] 20%|██        | 642/3145 [04:23<16:10,  2.58it/s] 20%|██        | 644/3145 [04:24<16:09,  2.58it/s] 21%|██        | 657/3143 [04:29<17:05,  2.42it/s] 21%|██        | 647/3145 [04:24<17:37,  2.36it/s] 20%|██        | 643/3145 [04:24<16:47,  2.48it/s] 21%|██        | 645/3145 [04:24<16:48,  2.48it/s] 21%|██        | 658/3143 [04:30<17:35,  2.35it/s] 21%|██        | 648/3145 [04:24<17:04,  2.44it/s] 20%|██        | 644/3145 [04:24<15:14,  2.73it/s] 21%|██        | 646/3145 [04:24<17:07,  2.43it/s] 21%|██        | 659/3143 [04:30<17:55,  2.31it/s] 21%|██        | 645/3145 [04:24<16:16,  2.56it/s] 21%|██        | 649/3145 [04:25<17:49,  2.33it/s] 21%|██        | 646/3145 [04:25<14:46,  2.82it/s] 21%|██        | 647/3145 [04:25<17:44,  2.35it/s] 21%|██        | 660/3143 [04:31<17:47,  2.33it/s] 21%|██        | 650/3145 [04:25<17:51,  2.33it/s] 21%|██        | 647/3145 [04:25<15:23,  2.71it/s] 21%|██        | 648/3145 [04:25<18:03,  2.30it/s] 21%|██        | 661/3143 [04:31<17:23,  2.38it/s] 21%|██        | 651/3145 [04:25<17:22,  2.39it/s] 21%|██        | 662/3143 [04:31<15:38,  2.64it/s] 21%|██        | 648/3145 [04:26<16:18,  2.55it/s] 21%|██        | 649/3145 [04:26<17:38,  2.36it/s] 21%|██        | 652/3145 [04:26<17:31,  2.37it/s] 21%|██        | 663/3143 [04:32<15:37,  2.65it/s] 21%|██        | 649/3145 [04:26<16:00,  2.60it/s] 21%|██        | 653/3145 [04:26<16:29,  2.52it/s] 21%|██        | 650/3145 [04:26<17:48,  2.33it/s] 21%|██        | 650/3145 [04:26<15:42,  2.65it/s] 21%|██        | 664/3143 [04:32<15:55,  2.60it/s] 21%|██        | 654/3145 [04:26<16:16,  2.55it/s] 21%|██        | 651/3145 [04:27<17:56,  2.32it/s] 21%|██        | 651/3145 [04:27<15:38,  2.66it/s] 21%|██        | 665/3143 [04:33<16:08,  2.56it/s] 21%|██        | 655/3145 [04:27<16:46,  2.47it/s] 21%|██        | 652/3145 [04:27<15:54,  2.61it/s] 21%|██        | 652/3145 [04:27<19:24,  2.14it/s] 21%|██        | 666/3143 [04:33<16:40,  2.48it/s] 21%|██        | 656/3145 [04:27<16:18,  2.54it/s] 21%|██        | 653/3145 [04:27<16:12,  2.56it/s] 21%|██        | 653/3145 [04:28<18:59,  2.19it/s] 21%|██        | 667/3143 [04:33<16:21,  2.52it/s] 21%|██        | 657/3145 [04:28<17:01,  2.43it/s] 21%|██        | 654/3145 [04:28<18:10,  2.29it/s] 21%|██        | 654/3145 [04:28<16:48,  2.47it/s] 21%|██▏       | 668/3143 [04:34<17:01,  2.42it/s] 21%|██        | 658/3145 [04:28<16:25,  2.52it/s] 21%|██        | 655/3145 [04:28<16:07,  2.57it/s] 21%|██        | 655/3145 [04:28<18:07,  2.29it/s] 21%|██▏       | 669/3143 [04:34<16:47,  2.46it/s] 21%|██        | 659/3145 [04:28<16:20,  2.54it/s] 21%|██        | 656/3145 [04:29<16:41,  2.48it/s] 21%|██        | 656/3145 [04:29<17:32,  2.36it/s] 21%|██▏       | 670/3143 [04:35<17:52,  2.31it/s] 21%|██        | 660/3145 [04:29<16:52,  2.45it/s] 21%|██        | 657/3145 [04:29<16:33,  2.51it/s] 21%|██        | 657/3145 [04:29<17:39,  2.35it/s] 21%|██▏       | 671/3143 [04:35<17:05,  2.41it/s] 21%|██        | 661/3145 [04:29<16:53,  2.45it/s] 21%|██        | 658/3145 [04:30<16:27,  2.52it/s] 21%|██        | 658/3145 [04:30<17:46,  2.33it/s] 21%|██▏       | 672/3143 [04:36<16:47,  2.45it/s] 21%|██        | 662/3145 [04:30<16:29,  2.51it/s] 21%|██        | 659/3145 [04:30<16:55,  2.45it/s] 21%|██        | 659/3145 [04:30<16:42,  2.48it/s] 21%|██▏       | 673/3143 [04:36<17:25,  2.36it/s] 21%|██        | 663/3145 [04:30<17:02,  2.43it/s] 21%|██        | 660/3145 [04:30<16:18,  2.54it/s] 21%|██        | 664/3145 [04:30<14:58,  2.76it/s] 21%|██        | 660/3145 [04:30<17:46,  2.33it/s] 21%|██▏       | 674/3143 [04:36<17:04,  2.41it/s] 21%|██        | 661/3145 [04:31<16:18,  2.54it/s] 21%|██▏       | 675/3143 [04:37<14:54,  2.76it/s] 21%|██        | 665/3145 [04:31<15:06,  2.74it/s] 21%|██        | 661/3145 [04:31<17:54,  2.31it/s] 21%|██        | 662/3145 [04:31<14:23,  2.88it/s] 22%|██▏       | 676/3143 [04:37<13:33,  3.03it/s] 21%|██        | 666/3145 [04:31<15:51,  2.60it/s] 21%|██        | 663/3145 [04:31<12:53,  3.21it/s] 22%|██▏       | 677/3143 [04:37<12:33,  3.27it/s] 21%|██        | 662/3145 [04:31<17:55,  2.31it/s] 22%|██▏       | 678/3143 [04:37<11:46,  3.49it/s] 21%|██        | 664/3145 [04:32<13:36,  3.04it/s] 21%|██        | 667/3145 [04:32<16:12,  2.55it/s] 21%|██        | 663/3145 [04:32<18:54,  2.19it/s] 22%|██▏       | 679/3143 [04:38<13:10,  3.12it/s] 21%|██        | 668/3145 [04:32<16:20,  2.53it/s] 21%|██        | 665/3145 [04:32<14:55,  2.77it/s] 21%|██        | 664/3145 [04:32<18:36,  2.22it/s] 21%|██▏       | 669/3145 [04:32<15:56,  2.59it/s] 21%|██        | 666/3145 [04:32<15:51,  2.60it/s] 22%|██▏       | 680/3143 [04:38<16:18,  2.52it/s] 21%|██▏       | 670/3145 [04:33<15:47,  2.61it/s] 21%|██        | 665/3145 [04:33<19:11,  2.15it/s] 21%|██        | 667/3145 [04:33<16:42,  2.47it/s] 22%|██▏       | 681/3143 [04:39<17:32,  2.34it/s] 21%|██        | 666/3145 [04:33<18:01,  2.29it/s] 21%|██▏       | 671/3145 [04:33<15:45,  2.62it/s] 21%|██        | 668/3145 [04:33<16:07,  2.56it/s] 21%|██▏       | 672/3145 [04:34<16:08,  2.55it/s] 22%|██▏       | 682/3143 [04:39<19:24,  2.11it/s] 21%|██        | 667/3145 [04:34<19:01,  2.17it/s] 21%|██▏       | 669/3145 [04:34<16:40,  2.48it/s] 21%|██▏       | 673/3145 [04:34<16:42,  2.47it/s] 21%|██        | 668/3145 [04:34<18:06,  2.28it/s] 21%|██▏       | 670/3145 [04:34<16:09,  2.55it/s] 22%|██▏       | 683/3143 [04:40<20:39,  1.99it/s] 21%|██▏       | 674/3145 [04:34<15:00,  2.74it/s] 21%|██▏       | 671/3145 [04:34<15:51,  2.60it/s] 21%|██▏       | 669/3145 [04:34<17:58,  2.30it/s] 22%|██▏       | 684/3143 [04:40<19:05,  2.15it/s] 21%|██▏       | 675/3145 [04:35<15:56,  2.58it/s] 21%|██▏       | 672/3145 [04:35<15:59,  2.58it/s] 21%|██▏       | 670/3145 [04:35<17:31,  2.35it/s] 22%|██▏       | 685/3143 [04:41<18:28,  2.22it/s] 21%|██▏       | 676/3145 [04:35<15:57,  2.58it/s] 21%|██▏       | 671/3145 [04:35<16:48,  2.45it/s] 21%|██▏       | 673/3145 [04:35<16:56,  2.43it/s] 22%|██▏       | 686/3143 [04:41<17:53,  2.29it/s] 22%|██▏       | 677/3145 [04:36<16:31,  2.49it/s] 21%|██▏       | 672/3145 [04:36<16:33,  2.49it/s] 21%|██▏       | 674/3145 [04:36<16:35,  2.48it/s] 22%|██▏       | 687/3143 [04:42<17:26,  2.35it/s] 22%|██▏       | 678/3145 [04:36<15:56,  2.58it/s] 22%|██▏       | 688/3143 [04:42<14:58,  2.73it/s] 21%|██▏       | 673/3145 [04:36<16:07,  2.55it/s] 21%|██▏       | 675/3145 [04:36<16:53,  2.44it/s] 22%|██▏       | 679/3145 [04:36<16:04,  2.56it/s] 22%|██▏       | 689/3143 [04:42<14:57,  2.74it/s] 21%|██▏       | 676/3145 [04:37<17:01,  2.42it/s] 21%|██▏       | 674/3145 [04:37<18:11,  2.26it/s] 22%|██▏       | 690/3143 [04:43<14:52,  2.75it/s] 22%|██▏       | 680/3145 [04:37<16:32,  2.48it/s] 22%|██▏       | 677/3145 [04:37<17:20,  2.37it/s] 22%|██▏       | 691/3143 [04:43<14:50,  2.75it/s] 22%|██▏       | 681/3145 [04:37<16:03,  2.56it/s] 21%|██▏       | 675/3145 [04:37<19:36,  2.10it/s] 22%|██▏       | 692/3143 [04:43<15:18,  2.67it/s] 22%|██▏       | 682/3145 [04:37<16:31,  2.48it/s] 21%|██▏       | 676/3145 [04:37<18:37,  2.21it/s] 22%|██▏       | 678/3145 [04:38<19:15,  2.13it/s] 22%|██▏       | 683/3145 [04:38<16:04,  2.55it/s] 22%|██▏       | 693/3143 [04:44<15:49,  2.58it/s] 22%|██▏       | 677/3145 [04:38<18:05,  2.27it/s] 22%|██▏       | 679/3145 [04:38<18:19,  2.24it/s] 22%|██▏       | 678/3145 [04:38<17:07,  2.40it/s] 22%|██▏       | 684/3145 [04:38<16:51,  2.43it/s] 22%|██▏       | 694/3143 [04:44<16:50,  2.42it/s] 22%|██▏       | 680/3145 [04:38<17:53,  2.30it/s] 22%|██▏       | 679/3145 [04:38<15:05,  2.72it/s] 22%|██▏       | 681/3145 [04:39<16:50,  2.44it/s] 22%|██▏       | 685/3145 [04:39<16:30,  2.48it/s] 22%|██▏       | 695/3143 [04:45<16:46,  2.43it/s] 22%|██▏       | 680/3145 [04:39<16:09,  2.54it/s] 22%|██▏       | 686/3145 [04:39<15:30,  2.64it/s] 22%|██▏       | 682/3145 [04:39<16:38,  2.47it/s] 22%|██▏       | 696/3143 [04:45<18:44,  2.18it/s] 22%|██▏       | 681/3145 [04:39<16:16,  2.52it/s] 22%|██▏       | 687/3145 [04:39<15:24,  2.66it/s] 22%|██▏       | 683/3145 [04:39<16:17,  2.52it/s] 22%|██▏       | 682/3145 [04:40<16:16,  2.52it/s] 22%|██▏       | 688/3145 [04:40<16:08,  2.54it/s] 22%|██▏       | 684/3145 [04:40<16:06,  2.55it/s] 22%|██▏       | 697/3143 [04:46<19:59,  2.04it/s] 22%|██▏       | 683/3145 [04:40<15:43,  2.61it/s] 22%|██▏       | 685/3145 [04:40<15:58,  2.57it/s] 22%|██▏       | 698/3143 [04:46<18:49,  2.16it/s] 22%|██▏       | 689/3145 [04:40<16:59,  2.41it/s] 22%|██▏       | 684/3145 [04:40<15:18,  2.68it/s] 22%|██▏       | 686/3145 [04:41<15:34,  2.63it/s] 22%|██▏       | 699/3143 [04:47<17:58,  2.27it/s] 22%|██▏       | 690/3145 [04:41<17:15,  2.37it/s] 22%|██▏       | 685/3145 [04:41<15:44,  2.61it/s] 22%|██▏       | 687/3145 [04:41<15:47,  2.59it/s] 22%|██▏       | 700/3143 [04:47<17:12,  2.37it/s] 22%|██▏       | 691/3145 [04:41<17:13,  2.37it/s] 22%|██▏       | 688/3145 [04:41<13:59,  2.93it/s] 22%|██▏       | 686/3145 [04:41<16:35,  2.47it/s] 22%|██▏       | 701/3143 [04:47<17:05,  2.38it/s] 22%|██▏       | 692/3145 [04:41<16:22,  2.50it/s] 22%|██▏       | 689/3145 [04:42<15:22,  2.66it/s] 22%|██▏       | 687/3145 [04:42<17:55,  2.28it/s] 22%|██▏       | 702/3143 [04:48<16:49,  2.42it/s] 22%|██▏       | 693/3145 [04:42<16:36,  2.46it/s] 22%|██▏       | 690/3145 [04:42<16:02,  2.55it/s] 22%|██▏       | 688/3145 [04:42<17:32,  2.33it/s] 22%|██▏       | 703/3143 [04:48<17:15,  2.36it/s] 22%|██▏       | 694/3145 [04:42<18:36,  2.20it/s] 22%|██▏       | 691/3145 [04:43<16:32,  2.47it/s] 22%|██▏       | 704/3143 [04:48<15:31,  2.62it/s] 22%|██▏       | 689/3145 [04:43<17:18,  2.36it/s] 22%|██▏       | 695/3145 [04:43<17:40,  2.31it/s] 22%|██▏       | 692/3145 [04:43<17:23,  2.35it/s] 22%|██▏       | 690/3145 [04:43<16:34,  2.47it/s] 22%|██▏       | 705/3143 [04:49<16:28,  2.47it/s] 22%|██▏       | 696/3145 [04:43<17:14,  2.37it/s] 22%|██▏       | 693/3145 [04:43<17:27,  2.34it/s] 22%|██▏       | 706/3143 [04:49<16:35,  2.45it/s] 22%|██▏       | 691/3145 [04:43<17:02,  2.40it/s] 22%|██▏       | 697/3145 [04:44<16:21,  2.49it/s] 22%|██▏       | 692/3145 [04:44<16:08,  2.53it/s] 22%|██▏       | 694/3145 [04:44<16:59,  2.41it/s] 22%|██▏       | 707/3143 [04:50<16:31,  2.46it/s] 22%|██▏       | 698/3145 [04:44<16:17,  2.50it/s] 22%|██▏       | 695/3145 [04:44<16:07,  2.53it/s] 22%|██▏       | 693/3145 [04:44<15:51,  2.58it/s] 23%|██▎       | 708/3143 [04:50<16:24,  2.47it/s] 22%|██▏       | 699/3145 [04:44<15:50,  2.57it/s] 22%|██▏       | 694/3145 [04:45<16:22,  2.50it/s] 22%|██▏       | 696/3145 [04:45<16:57,  2.41it/s] 23%|██▎       | 709/3143 [04:51<16:14,  2.50it/s] 22%|██▏       | 700/3145 [04:45<15:17,  2.67it/s] 23%|██▎       | 710/3143 [04:51<15:39,  2.59it/s] 22%|██▏       | 695/3145 [04:45<16:14,  2.51it/s] 22%|██▏       | 701/3145 [04:45<15:08,  2.69it/s] 22%|██▏       | 697/3145 [04:45<17:38,  2.31it/s] 22%|██▏       | 702/3145 [04:45<14:59,  2.72it/s] 22%|██▏       | 696/3145 [04:45<16:42,  2.44it/s] 23%|██▎       | 711/3143 [04:51<16:53,  2.40it/s] 22%|██▏       | 698/3145 [04:46<17:22,  2.35it/s] 22%|██▏       | 703/3145 [04:46<15:25,  2.64it/s] 23%|██▎       | 712/3143 [04:52<16:42,  2.42it/s] 22%|██▏       | 697/3145 [04:46<17:28,  2.33it/s] 22%|██▏       | 699/3145 [04:46<17:47,  2.29it/s] 22%|██▏       | 704/3145 [04:46<16:04,  2.53it/s] 23%|██▎       | 713/3143 [04:52<16:47,  2.41it/s] 22%|██▏       | 698/3145 [04:46<17:16,  2.36it/s] 22%|██▏       | 700/3145 [04:46<18:18,  2.23it/s] 22%|██▏       | 705/3145 [04:47<15:22,  2.64it/s] 23%|██▎       | 714/3143 [04:53<16:27,  2.46it/s] 22%|██▏       | 699/3145 [04:47<17:15,  2.36it/s] 22%|██▏       | 701/3145 [04:47<17:22,  2.34it/s] 22%|██▏       | 706/3145 [04:47<16:18,  2.49it/s] 23%|██▎       | 715/3143 [04:53<16:30,  2.45it/s] 22%|██▏       | 700/3145 [04:47<17:12,  2.37it/s] 22%|██▏       | 702/3145 [04:47<17:09,  2.37it/s] 22%|██▏       | 707/3145 [04:48<16:40,  2.44it/s] 23%|██▎       | 716/3143 [04:53<16:48,  2.41it/s] 22%|██▏       | 701/3145 [04:48<17:12,  2.37it/s] 22%|██▏       | 703/3145 [04:48<17:17,  2.35it/s] 23%|██▎       | 708/3145 [04:48<16:00,  2.54it/s] 23%|██▎       | 717/3143 [04:54<16:37,  2.43it/s] 22%|██▏       | 702/3145 [04:48<16:28,  2.47it/s] 22%|██▏       | 704/3145 [04:48<17:23,  2.34it/s] 23%|██▎       | 709/3145 [04:48<16:15,  2.50it/s] 22%|██▏       | 703/3145 [04:48<16:07,  2.52it/s] 23%|██▎       | 718/3143 [04:54<16:53,  2.39it/s] 22%|██▏       | 705/3145 [04:49<17:43,  2.29it/s] 23%|██▎       | 710/3145 [04:49<16:06,  2.52it/s] 23%|██▎       | 719/3143 [04:55<16:14,  2.49it/s] 22%|██▏       | 704/3145 [04:49<16:43,  2.43it/s] 22%|██▏       | 706/3145 [04:49<17:06,  2.38it/s] 23%|██▎       | 720/3143 [04:55<15:49,  2.55it/s] 23%|██▎       | 711/3145 [04:49<17:25,  2.33it/s] 22%|██▏       | 705/3145 [04:49<16:46,  2.42it/s] 22%|██▏       | 707/3145 [04:49<16:50,  2.41it/s] 23%|██▎       | 721/3143 [04:55<15:58,  2.53it/s] 22%|██▏       | 706/3145 [04:50<16:29,  2.46it/s] 23%|██▎       | 712/3145 [04:50<17:28,  2.32it/s] 23%|██▎       | 708/3145 [04:50<16:31,  2.46it/s] 23%|██▎       | 722/3143 [04:56<16:33,  2.44it/s] 22%|██▏       | 707/3145 [04:50<16:55,  2.40it/s] 23%|██▎       | 713/3145 [04:50<17:55,  2.26it/s] 23%|██▎       | 709/3145 [04:50<16:02,  2.53it/s] 23%|██▎       | 723/3143 [04:56<16:21,  2.47it/s] 23%|██▎       | 708/3145 [04:50<17:08,  2.37it/s] 23%|██▎       | 714/3145 [04:51<17:44,  2.28it/s] 23%|██▎       | 710/3145 [04:51<16:03,  2.53it/s] 23%|██▎       | 724/3143 [04:57<15:59,  2.52it/s] 23%|██▎       | 709/3145 [04:51<17:06,  2.37it/s] 23%|██▎       | 711/3145 [04:51<16:24,  2.47it/s] 23%|██▎       | 715/3145 [04:51<19:40,  2.06it/s] 23%|██▎       | 725/3143 [04:57<15:41,  2.57it/s] 23%|██▎       | 710/3145 [04:51<17:35,  2.31it/s] 23%|██▎       | 712/3145 [04:51<16:36,  2.44it/s] 23%|██▎       | 726/3143 [04:57<15:25,  2.61it/s] 23%|██▎       | 716/3145 [04:52<18:43,  2.16it/s] 23%|██▎       | 711/3145 [04:52<16:49,  2.41it/s] 23%|██▎       | 713/3145 [04:52<16:51,  2.40it/s] 23%|██▎       | 727/3143 [04:58<15:36,  2.58it/s] 23%|██▎       | 717/3145 [04:52<17:40,  2.29it/s] 23%|██▎       | 712/3145 [04:52<17:07,  2.37it/s] 23%|██▎       | 714/3145 [04:52<16:18,  2.49it/s] 23%|██▎       | 728/3143 [04:58<15:20,  2.62it/s] 23%|██▎       | 718/3145 [04:52<17:16,  2.34it/s] 23%|██▎       | 713/3145 [04:53<17:31,  2.31it/s] 23%|██▎       | 715/3145 [04:53<17:01,  2.38it/s] 23%|██▎       | 729/3143 [04:59<15:59,  2.52it/s] 23%|██▎       | 719/3145 [04:53<17:21,  2.33it/s] 23%|██▎       | 720/3145 [04:53<15:11,  2.66it/s] 23%|██▎       | 716/3145 [04:53<16:14,  2.49it/s] 23%|██▎       | 714/3145 [04:53<17:10,  2.36it/s] 23%|██▎       | 730/3143 [04:59<16:29,  2.44it/s] 23%|██▎       | 721/3145 [04:53<15:19,  2.64it/s] 23%|██▎       | 715/3145 [04:53<17:20,  2.33it/s] 23%|██▎       | 717/3145 [04:53<17:26,  2.32it/s] 23%|██▎       | 731/3143 [04:59<17:05,  2.35it/s] 23%|██▎       | 722/3145 [04:54<15:06,  2.67it/s] 23%|██▎       | 716/3145 [04:54<16:47,  2.41it/s] 23%|██▎       | 718/3145 [04:54<16:59,  2.38it/s] 23%|██▎       | 732/3143 [05:00<17:17,  2.32it/s] 23%|██▎       | 723/3145 [04:54<15:50,  2.55it/s] 23%|██▎       | 717/3145 [04:54<17:14,  2.35it/s] 23%|██▎       | 719/3145 [04:54<17:50,  2.27it/s] 23%|██▎       | 724/3145 [04:54<13:50,  2.91it/s] 23%|██▎       | 733/3143 [05:00<17:11,  2.34it/s] 23%|██▎       | 718/3145 [04:55<16:50,  2.40it/s] 23%|██▎       | 720/3145 [04:55<17:08,  2.36it/s] 23%|██▎       | 725/3145 [04:55<14:29,  2.78it/s] 23%|██▎       | 734/3143 [05:01<17:05,  2.35it/s] 23%|██▎       | 719/3145 [04:55<17:09,  2.36it/s] 23%|██▎       | 726/3145 [04:55<14:54,  2.70it/s] 23%|██▎       | 735/3143 [05:01<17:00,  2.36it/s] 23%|██▎       | 721/3145 [04:55<18:27,  2.19it/s] 23%|██▎       | 720/3145 [04:56<16:54,  2.39it/s] 23%|██▎       | 727/3145 [04:56<16:04,  2.51it/s] 23%|██▎       | 736/3143 [05:02<16:37,  2.41it/s] 23%|██▎       | 722/3145 [04:56<17:44,  2.28it/s] 23%|██▎       | 721/3145 [04:56<16:20,  2.47it/s] 23%|██▎       | 737/3143 [05:02<16:09,  2.48it/s] 23%|██▎       | 728/3145 [04:56<16:30,  2.44it/s] 23%|██▎       | 723/3145 [04:56<18:57,  2.13it/s] 23%|██▎       | 722/3145 [04:56<15:52,  2.54it/s] 23%|██▎       | 738/3143 [05:02<15:33,  2.58it/s] 23%|██▎       | 729/3145 [04:56<16:13,  2.48it/s] 23%|██▎       | 724/3145 [04:57<18:58,  2.13it/s] 23%|██▎       | 723/3145 [04:57<16:20,  2.47it/s] 24%|██▎       | 739/3143 [05:03<15:20,  2.61it/s] 23%|██▎       | 730/3145 [04:57<15:51,  2.54it/s] 23%|██▎       | 724/3145 [04:57<16:47,  2.40it/s] 24%|██▎       | 740/3143 [05:03<15:31,  2.58it/s] 23%|██▎       | 731/3145 [04:57<15:49,  2.54it/s] 23%|██▎       | 725/3145 [04:57<20:11,  2.00it/s] 23%|██▎       | 732/3145 [04:57<14:10,  2.84it/s] 24%|██▎       | 741/3143 [05:03<15:38,  2.56it/s] 23%|██▎       | 725/3145 [04:58<16:52,  2.39it/s] 23%|██▎       | 726/3145 [04:58<18:32,  2.17it/s] 23%|██▎       | 733/3145 [04:58<15:16,  2.63it/s] 24%|██▎       | 742/3143 [05:04<16:11,  2.47it/s] 23%|██▎       | 727/3145 [04:58<17:40,  2.28it/s] 23%|██▎       | 726/3145 [04:58<17:23,  2.32it/s] 23%|██▎       | 734/3145 [04:58<15:44,  2.55it/s] 24%|██▎       | 743/3143 [05:04<15:31,  2.58it/s] 23%|██▎       | 728/3145 [04:58<17:09,  2.35it/s] 23%|██▎       | 727/3145 [04:58<17:52,  2.25it/s] 24%|██▎       | 744/3143 [05:05<15:28,  2.58it/s] 23%|██▎       | 735/3145 [04:59<15:46,  2.55it/s] 23%|██▎       | 729/3145 [04:59<16:41,  2.41it/s] 23%|██▎       | 728/3145 [04:59<17:36,  2.29it/s] 23%|██▎       | 736/3145 [04:59<14:10,  2.83it/s] 23%|██▎       | 730/3145 [04:59<14:35,  2.76it/s] 24%|██▎       | 745/3143 [05:05<16:09,  2.47it/s] 23%|██▎       | 729/3145 [04:59<17:50,  2.26it/s] 23%|██▎       | 737/3145 [04:59<14:50,  2.71it/s] 23%|██▎       | 731/3145 [04:59<15:12,  2.65it/s] 24%|██▎       | 746/3143 [05:05<15:45,  2.54it/s] 23%|██▎       | 738/3145 [05:00<13:03,  3.07it/s] 23%|██▎       | 730/3145 [05:00<16:54,  2.38it/s] 23%|██▎       | 732/3145 [05:00<15:26,  2.61it/s] 24%|██▍       | 747/3143 [05:06<16:24,  2.43it/s] 23%|██▎       | 739/3145 [05:00<13:48,  2.90it/s] 23%|██▎       | 731/3145 [05:00<16:04,  2.50it/s] 23%|██▎       | 733/3145 [05:00<15:10,  2.65it/s] 24%|██▍       | 748/3143 [05:06<16:10,  2.47it/s] 24%|██▎       | 740/3145 [05:00<14:27,  2.77it/s] 23%|██▎       | 732/3145 [05:00<16:02,  2.51it/s] 23%|██▎       | 734/3145 [05:01<15:19,  2.62it/s] 24%|██▍       | 749/3143 [05:07<16:06,  2.48it/s] 24%|██▎       | 741/3145 [05:01<15:11,  2.64it/s] 23%|██▎       | 733/3145 [05:01<16:01,  2.51it/s] 23%|██▎       | 735/3145 [05:01<15:28,  2.60it/s] 24%|██▍       | 750/3143 [05:07<15:42,  2.54it/s] 23%|██▎       | 734/3145 [05:01<15:38,  2.57it/s] 24%|██▎       | 742/3145 [05:01<15:42,  2.55it/s] 23%|██▎       | 736/3145 [05:01<15:45,  2.55it/s] 24%|██▍       | 751/3143 [05:07<15:46,  2.53it/s] 24%|██▎       | 743/3145 [05:02<15:48,  2.53it/s] 23%|██▎       | 735/3145 [05:02<15:52,  2.53it/s] 23%|██▎       | 737/3145 [05:02<15:04,  2.66it/s] 24%|██▍       | 752/3143 [05:08<15:20,  2.60it/s] 23%|██▎       | 736/3145 [05:02<16:02,  2.50it/s] 24%|██▎       | 744/3145 [05:02<16:19,  2.45it/s] 23%|██▎       | 738/3145 [05:02<15:47,  2.54it/s] 24%|██▍       | 753/3143 [05:08<15:49,  2.52it/s] 24%|██▎       | 745/3145 [05:03<16:40,  2.40it/s] 23%|██▎       | 737/3145 [05:03<16:56,  2.37it/s] 23%|██▎       | 739/3145 [05:03<16:50,  2.38it/s] 24%|██▍       | 754/3143 [05:09<15:42,  2.54it/s] 23%|██▎       | 738/3145 [05:03<16:17,  2.46it/s] 24%|██▎       | 746/3145 [05:03<16:19,  2.45it/s] 24%|██▍       | 755/3143 [05:09<15:44,  2.53it/s] 24%|██▎       | 740/3145 [05:03<17:50,  2.25it/s] 24%|██▍       | 747/3145 [05:03<16:20,  2.45it/s] 23%|██▎       | 739/3145 [05:03<16:47,  2.39it/s] 24%|██▍       | 756/3143 [05:09<15:36,  2.55it/s] 24%|██▎       | 741/3145 [05:04<17:52,  2.24it/s] 24%|██▎       | 740/3145 [05:04<16:35,  2.42it/s] 24%|██▍       | 748/3145 [05:04<16:44,  2.39it/s] 24%|██▍       | 757/3143 [05:10<16:13,  2.45it/s] 24%|██▎       | 742/3145 [05:04<17:05,  2.34it/s] 24%|██▎       | 741/3145 [05:04<16:05,  2.49it/s] 24%|██▍       | 749/3145 [05:04<16:31,  2.42it/s] 24%|██▎       | 743/3145 [05:04<16:05,  2.49it/s] 24%|██▍       | 758/3143 [05:10<16:05,  2.47it/s] 24%|██▎       | 742/3145 [05:05<16:19,  2.45it/s] 24%|██▍       | 750/3145 [05:05<16:18,  2.45it/s] 24%|██▍       | 759/3143 [05:11<16:01,  2.48it/s] 24%|██▎       | 744/3145 [05:05<16:26,  2.43it/s] 24%|██▎       | 743/3145 [05:05<17:12,  2.33it/s] 24%|██▍       | 751/3145 [05:05<18:28,  2.16it/s] 24%|██▎       | 745/3145 [05:05<16:34,  2.41it/s] 24%|██▍       | 760/3143 [05:11<16:37,  2.39it/s] 24%|██▎       | 744/3145 [05:05<16:22,  2.44it/s] 24%|██▎       | 746/3145 [05:06<16:20,  2.45it/s] 24%|██▍       | 761/3143 [05:12<16:27,  2.41it/s] 24%|██▍       | 752/3145 [05:06<18:43,  2.13it/s] 24%|██▎       | 745/3145 [05:06<17:00,  2.35it/s] 24%|██▍       | 747/3145 [05:06<16:28,  2.43it/s] 24%|██▍       | 753/3145 [05:06<17:51,  2.23it/s] 24%|██▍       | 762/3143 [05:12<16:46,  2.37it/s] 24%|██▎       | 746/3145 [05:06<16:06,  2.48it/s] 24%|██▍       | 754/3145 [05:06<17:27,  2.28it/s] 24%|██▍       | 763/3143 [05:12<17:02,  2.33it/s] 24%|██▍       | 747/3145 [05:07<15:29,  2.58it/s] 24%|██▍       | 748/3145 [05:07<18:20,  2.18it/s] 24%|██▍       | 755/3145 [05:07<17:15,  2.31it/s] 24%|██▍       | 764/3143 [05:13<17:25,  2.28it/s] 24%|██▍       | 749/3145 [05:07<17:58,  2.22it/s] 24%|██▍       | 748/3145 [05:07<16:03,  2.49it/s] 24%|██▍       | 756/3145 [05:07<16:46,  2.37it/s] 24%|██▍       | 750/3145 [05:07<17:14,  2.31it/s] 24%|██▍       | 765/3143 [05:13<17:24,  2.28it/s] 24%|██▍       | 749/3145 [05:07<16:35,  2.41it/s] 24%|██▍       | 757/3145 [05:08<16:29,  2.41it/s] 24%|██▍       | 751/3145 [05:08<16:48,  2.37it/s] 24%|██▍       | 766/3143 [05:14<17:24,  2.28it/s] 24%|██▍       | 750/3145 [05:08<16:35,  2.41it/s] 24%|██▍       | 758/3145 [05:08<17:25,  2.28it/s] 24%|██▍       | 752/3145 [05:08<17:27,  2.28it/s] 24%|██▍       | 751/3145 [05:08<16:52,  2.37it/s] 24%|██▍       | 767/3143 [05:14<18:01,  2.20it/s] 24%|██▍       | 759/3145 [05:09<17:17,  2.30it/s] 24%|██▍       | 753/3145 [05:09<16:56,  2.35it/s] 24%|██▍       | 752/3145 [05:09<16:32,  2.41it/s] 24%|██▍       | 768/3143 [05:15<17:48,  2.22it/s] 24%|██▍       | 754/3145 [05:09<14:32,  2.74it/s] 24%|██▍       | 760/3145 [05:09<16:45,  2.37it/s] 24%|██▍       | 753/3145 [05:09<16:50,  2.37it/s] 24%|██▍       | 769/3143 [05:15<17:15,  2.29it/s] 24%|██▍       | 755/3145 [05:09<15:16,  2.61it/s] 24%|██▍       | 761/3145 [05:09<16:54,  2.35it/s] 24%|██▍       | 770/3143 [05:15<16:43,  2.37it/s] 24%|██▍       | 754/3145 [05:10<17:01,  2.34it/s] 24%|██▍       | 756/3145 [05:10<15:21,  2.59it/s] 24%|██▍       | 762/3145 [05:10<16:06,  2.47it/s] 25%|██▍       | 771/3143 [05:16<16:04,  2.46it/s] 24%|██▍       | 755/3145 [05:10<16:41,  2.39it/s] 24%|██▍       | 757/3145 [05:10<15:12,  2.62it/s] 24%|██▍       | 763/3145 [05:10<16:33,  2.40it/s] 24%|██▍       | 756/3145 [05:10<17:04,  2.33it/s] 24%|██▍       | 758/3145 [05:10<15:26,  2.58it/s] 25%|██▍       | 772/3143 [05:16<17:45,  2.22it/s] 24%|██▍       | 764/3145 [05:11<17:01,  2.33it/s] 25%|██▍       | 773/3143 [05:17<16:12,  2.44it/s] 24%|██▍       | 757/3145 [05:11<16:26,  2.42it/s] 24%|██▍       | 759/3145 [05:11<15:41,  2.53it/s] 24%|██▍       | 765/3145 [05:11<16:37,  2.39it/s] 24%|██▍       | 758/3145 [05:11<16:44,  2.38it/s] 25%|██▍       | 774/3143 [05:17<16:37,  2.37it/s] 24%|██▍       | 760/3145 [05:11<15:37,  2.54it/s] 24%|██▍       | 766/3145 [05:11<16:01,  2.48it/s] 24%|██▍       | 761/3145 [05:12<13:44,  2.89it/s] 25%|██▍       | 775/3143 [05:18<16:30,  2.39it/s] 24%|██▍       | 759/3145 [05:12<16:49,  2.36it/s] 24%|██▍       | 767/3145 [05:12<15:30,  2.56it/s] 24%|██▍       | 762/3145 [05:12<13:56,  2.85it/s] 25%|██▍       | 776/3143 [05:18<16:12,  2.43it/s] 24%|██▍       | 760/3145 [05:12<16:48,  2.37it/s] 24%|██▍       | 768/3145 [05:12<15:08,  2.62it/s] 24%|██▍       | 763/3145 [05:12<14:29,  2.74it/s] 24%|██▍       | 769/3145 [05:12<13:32,  2.92it/s] 25%|██▍       | 777/3143 [05:18<15:39,  2.52it/s] 24%|██▍       | 761/3145 [05:13<16:59,  2.34it/s] 24%|██▍       | 764/3145 [05:13<14:48,  2.68it/s] 24%|██▍       | 770/3145 [05:13<12:35,  3.14it/s] 25%|██▍       | 778/3143 [05:19<15:55,  2.48it/s] 24%|██▍       | 762/3145 [05:13<17:08,  2.32it/s] 25%|██▍       | 771/3145 [05:13<12:30,  3.16it/s] 24%|██▍       | 765/3145 [05:13<15:40,  2.53it/s] 25%|██▍       | 779/3143 [05:19<15:14,  2.59it/s] 25%|██▍       | 772/3145 [05:13<13:06,  3.02it/s] 24%|██▍       | 763/3145 [05:13<16:50,  2.36it/s] 25%|██▍       | 780/3143 [05:19<14:50,  2.65it/s] 24%|██▍       | 766/3145 [05:14<16:07,  2.46it/s] 25%|██▍       | 773/3145 [05:14<12:19,  3.21it/s] 24%|██▍       | 764/3145 [05:14<16:46,  2.36it/s] 25%|██▍       | 781/3143 [05:20<15:28,  2.54it/s] 25%|██▍       | 774/3145 [05:14<13:18,  2.97it/s] 24%|██▍       | 767/3145 [05:14<17:13,  2.30it/s] 24%|██▍       | 765/3145 [05:14<16:17,  2.44it/s] 25%|██▍       | 775/3145 [05:14<13:25,  2.94it/s] 25%|██▍       | 782/3143 [05:20<15:58,  2.46it/s] 24%|██▍       | 768/3145 [05:14<16:58,  2.33it/s] 24%|██▍       | 766/3145 [05:15<16:13,  2.44it/s] 25%|██▍       | 783/3143 [05:21<15:30,  2.54it/s] 25%|██▍       | 776/3145 [05:15<14:46,  2.67it/s] 24%|██▍       | 769/3145 [05:15<17:00,  2.33it/s] 24%|██▍       | 767/3145 [05:15<16:03,  2.47it/s] 25%|██▍       | 784/3143 [05:21<15:32,  2.53it/s] 24%|██▍       | 768/3145 [05:15<13:55,  2.85it/s] 25%|██▍       | 777/3145 [05:15<15:03,  2.62it/s] 24%|██▍       | 770/3145 [05:15<17:15,  2.29it/s] 25%|██▍       | 785/3143 [05:21<14:53,  2.64it/s] 25%|██▍       | 778/3145 [05:16<14:02,  2.81it/s] 24%|██▍       | 769/3145 [05:16<14:55,  2.65it/s] 25%|██▍       | 771/3145 [05:16<16:55,  2.34it/s] 25%|██▍       | 779/3145 [05:16<14:33,  2.71it/s] 25%|██▌       | 786/3143 [05:22<15:17,  2.57it/s] 24%|██▍       | 770/3145 [05:16<15:02,  2.63it/s] 25%|██▍       | 772/3145 [05:16<15:54,  2.49it/s] 25%|██▌       | 787/3143 [05:22<15:33,  2.52it/s] 25%|██▍       | 780/3145 [05:16<15:19,  2.57it/s] 25%|██▍       | 773/3145 [05:17<15:48,  2.50it/s] 25%|██▍       | 771/3145 [05:16<16:01,  2.47it/s] 25%|██▍       | 781/3145 [05:17<13:57,  2.82it/s] 25%|██▌       | 788/3143 [05:23<15:33,  2.52it/s] 25%|██▍       | 772/3145 [05:17<15:56,  2.48it/s] 25%|██▍       | 774/3145 [05:17<16:33,  2.39it/s] 25%|██▍       | 782/3145 [05:17<14:21,  2.74it/s] 25%|██▌       | 789/3143 [05:23<15:59,  2.45it/s] 25%|██▍       | 773/3145 [05:17<16:13,  2.44it/s] 25%|██▍       | 775/3145 [05:17<16:35,  2.38it/s] 25%|██▍       | 783/3145 [05:17<15:22,  2.56it/s] 25%|██▌       | 790/3143 [05:24<16:36,  2.36it/s] 25%|██▍       | 776/3145 [05:18<15:49,  2.49it/s] 25%|██▍       | 774/3145 [05:18<16:48,  2.35it/s] 25%|██▍       | 784/3145 [05:18<15:47,  2.49it/s] 25%|██▌       | 791/3143 [05:24<16:26,  2.38it/s] 25%|██▍       | 777/3145 [05:18<15:51,  2.49it/s] 25%|██▍       | 775/3145 [05:18<15:53,  2.48it/s] 25%|██▍       | 785/3145 [05:18<16:08,  2.44it/s] 25%|██▌       | 792/3143 [05:24<16:42,  2.35it/s] 25%|██▍       | 778/3145 [05:19<15:23,  2.56it/s] 25%|██▍       | 776/3145 [05:19<15:41,  2.52it/s] 25%|██▍       | 786/3145 [05:19<15:57,  2.46it/s] 25%|██▍       | 779/3145 [05:19<15:20,  2.57it/s] 25%|██▍       | 777/3145 [05:19<16:09,  2.44it/s] 25%|██▌       | 793/3143 [05:25<18:10,  2.16it/s] 25%|██▌       | 787/3145 [05:19<16:09,  2.43it/s] 25%|██▍       | 780/3145 [05:19<15:45,  2.50it/s] 25%|██▍       | 778/3145 [05:19<16:40,  2.37it/s] 25%|██▌       | 794/3143 [05:25<17:48,  2.20it/s] 25%|██▌       | 788/3145 [05:20<16:34,  2.37it/s] 25%|██▍       | 781/3145 [05:20<15:56,  2.47it/s] 25%|██▍       | 779/3145 [05:20<16:26,  2.40it/s] 25%|██▌       | 795/3143 [05:26<17:14,  2.27it/s] 25%|██▌       | 789/3145 [05:20<16:43,  2.35it/s] 25%|██▍       | 782/3145 [05:20<15:56,  2.47it/s] 25%|██▍       | 780/3145 [05:20<16:13,  2.43it/s] 25%|██▌       | 796/3143 [05:26<17:48,  2.20it/s] 25%|██▌       | 790/3145 [05:20<16:47,  2.34it/s] 25%|██▍       | 783/3145 [05:21<16:15,  2.42it/s] 25%|██▍       | 781/3145 [05:21<16:30,  2.39it/s] 25%|██▌       | 797/3143 [05:27<17:36,  2.22it/s] 25%|██▌       | 791/3145 [05:21<16:02,  2.45it/s] 25%|██▍       | 784/3145 [05:21<15:35,  2.52it/s] 25%|██▍       | 782/3145 [05:21<16:09,  2.44it/s] 25%|██▌       | 792/3145 [05:21<14:51,  2.64it/s] 25%|██▌       | 798/3143 [05:27<17:03,  2.29it/s] 25%|██▍       | 785/3145 [05:21<16:00,  2.46it/s] 25%|██▍       | 783/3145 [05:21<16:02,  2.45it/s] 25%|██▌       | 793/3145 [05:22<14:38,  2.68it/s] 25%|██▌       | 799/3143 [05:27<16:31,  2.37it/s] 25%|██▍       | 786/3145 [05:22<15:42,  2.50it/s] 25%|██▌       | 794/3145 [05:22<14:02,  2.79it/s] 25%|██▍       | 784/3145 [05:22<16:36,  2.37it/s] 25%|██▌       | 800/3143 [05:28<16:41,  2.34it/s] 25%|██▌       | 787/3145 [05:22<15:57,  2.46it/s] 25%|██▌       | 795/3145 [05:22<14:07,  2.77it/s] 25%|██▍       | 785/3145 [05:22<16:16,  2.42it/s] 25%|██▌       | 801/3143 [05:28<16:17,  2.40it/s] 25%|██▌       | 788/3145 [05:23<15:11,  2.58it/s] 25%|██▌       | 796/3145 [05:23<14:34,  2.69it/s] 25%|██▍       | 786/3145 [05:23<16:05,  2.44it/s] 26%|██▌       | 802/3143 [05:29<16:28,  2.37it/s] 25%|██▌       | 797/3145 [05:23<14:50,  2.64it/s] 25%|██▌       | 789/3145 [05:23<17:20,  2.26it/s] 25%|██▌       | 787/3145 [05:23<16:00,  2.45it/s] 26%|██▌       | 803/3143 [05:29<16:42,  2.33it/s] 25%|██▌       | 788/3145 [05:23<14:05,  2.79it/s] 25%|██▌       | 798/3145 [05:23<15:48,  2.48it/s] 25%|██▌       | 790/3145 [05:23<16:40,  2.35it/s] 25%|██▌       | 789/3145 [05:24<14:18,  2.75it/s] 26%|██▌       | 804/3143 [05:30<16:53,  2.31it/s] 25%|██▌       | 791/3145 [05:24<16:20,  2.40it/s] 25%|██▌       | 799/3145 [05:24<16:48,  2.33it/s] 25%|██▌       | 790/3145 [05:24<14:58,  2.62it/s] 26%|██▌       | 805/3143 [05:30<16:59,  2.29it/s] 25%|██▌       | 792/3145 [05:24<16:08,  2.43it/s] 25%|██▌       | 800/3145 [05:24<17:43,  2.21it/s] 25%|██▌       | 791/3145 [05:25<15:11,  2.58it/s] 26%|██▌       | 806/3143 [05:30<16:36,  2.35it/s] 25%|██▌       | 801/3145 [05:25<15:16,  2.56it/s] 25%|██▌       | 793/3145 [05:25<17:14,  2.27it/s] 25%|██▌       | 792/3145 [05:25<16:16,  2.41it/s] 26%|██▌       | 807/3143 [05:31<16:42,  2.33it/s] 26%|██▌       | 802/3145 [05:25<15:43,  2.48it/s] 25%|██▌       | 794/3145 [05:25<16:43,  2.34it/s] 26%|██▌       | 808/3143 [05:31<15:57,  2.44it/s] 25%|██▌       | 793/3145 [05:25<16:19,  2.40it/s] 26%|██▌       | 803/3145 [05:26<16:19,  2.39it/s] 25%|██▌       | 795/3145 [05:26<17:43,  2.21it/s] 26%|██▌       | 809/3143 [05:32<15:41,  2.48it/s] 25%|██▌       | 794/3145 [05:26<16:35,  2.36it/s] 26%|██▌       | 804/3145 [05:26<16:43,  2.33it/s] 25%|██▌       | 796/3145 [05:26<16:46,  2.33it/s] 26%|██▌       | 810/3143 [05:32<16:08,  2.41it/s] 25%|██▌       | 795/3145 [05:26<16:59,  2.30it/s] 25%|██▌       | 797/3145 [05:26<15:47,  2.48it/s] 26%|██▌       | 805/3145 [05:26<16:14,  2.40it/s] 26%|██▌       | 811/3143 [05:33<16:13,  2.40it/s] 25%|██▌       | 796/3145 [05:27<16:40,  2.35it/s] 25%|██▌       | 798/3145 [05:27<15:54,  2.46it/s] 26%|██▌       | 806/3145 [05:27<16:28,  2.37it/s] 26%|██▌       | 812/3143 [05:33<15:40,  2.48it/s] 25%|██▌       | 797/3145 [05:27<15:58,  2.45it/s] 25%|██▌       | 799/3145 [05:27<15:41,  2.49it/s] 26%|██▌       | 807/3145 [05:27<16:36,  2.35it/s] 26%|██▌       | 813/3143 [05:33<15:51,  2.45it/s] 25%|██▌       | 798/3145 [05:28<16:15,  2.41it/s] 26%|██▌       | 808/3145 [05:28<16:42,  2.33it/s] 25%|██▌       | 800/3145 [05:28<17:45,  2.20it/s] 26%|██▌       | 814/3143 [05:34<15:25,  2.52it/s] 25%|██▌       | 799/3145 [05:28<16:38,  2.35it/s] 26%|██▌       | 809/3145 [05:28<16:44,  2.33it/s] 26%|██▌       | 815/3143 [05:34<15:03,  2.58it/s] 25%|██▌       | 801/3145 [05:28<17:46,  2.20it/s] 25%|██▌       | 800/3145 [05:28<16:18,  2.40it/s] 26%|██▌       | 802/3145 [05:29<16:27,  2.37it/s] 26%|██▌       | 816/3143 [05:34<14:57,  2.59it/s] 26%|██▌       | 810/3145 [05:29<16:46,  2.32it/s] 25%|██▌       | 801/3145 [05:29<16:36,  2.35it/s] 26%|██▌       | 811/3145 [05:29<16:27,  2.36it/s] 26%|██▌       | 803/3145 [05:29<16:59,  2.30it/s] 26%|██▌       | 817/3143 [05:35<17:25,  2.22it/s] 26%|██▌       | 802/3145 [05:29<16:23,  2.38it/s] 26%|██▌       | 812/3145 [05:29<15:42,  2.47it/s] 26%|██▌       | 804/3145 [05:30<17:11,  2.27it/s] 26%|██▌       | 818/3143 [05:35<17:13,  2.25it/s] 26%|██▌       | 803/3145 [05:30<16:26,  2.37it/s] 26%|██▌       | 813/3145 [05:30<16:34,  2.34it/s] 26%|██▌       | 805/3145 [05:30<16:49,  2.32it/s] 26%|██▌       | 804/3145 [05:30<16:09,  2.42it/s] 26%|██▌       | 819/3143 [05:36<19:18,  2.01it/s] 26%|██▌       | 814/3145 [05:30<16:52,  2.30it/s] 26%|██▌       | 806/3145 [05:30<17:08,  2.27it/s] 26%|██▌       | 805/3145 [05:30<16:24,  2.38it/s] 26%|██▌       | 807/3145 [05:31<15:45,  2.47it/s] 26%|██▌       | 815/3145 [05:31<16:41,  2.33it/s] 26%|██▌       | 820/3143 [05:37<20:44,  1.87it/s] 26%|██▌       | 806/3145 [05:31<17:05,  2.28it/s] 26%|██▌       | 816/3145 [05:31<16:21,  2.37it/s] 26%|██▌       | 808/3145 [05:31<16:07,  2.42it/s] 26%|██▌       | 821/3143 [05:37<19:20,  2.00it/s] 26%|██▌       | 807/3145 [05:31<16:36,  2.35it/s] 26%|██▌       | 817/3145 [05:31<15:59,  2.43it/s] 26%|██▌       | 809/3145 [05:32<16:21,  2.38it/s] 26%|██▌       | 822/3143 [05:38<17:32,  2.20it/s] 26%|██▌       | 808/3145 [05:32<16:11,  2.41it/s] 26%|██▌       | 818/3145 [05:32<16:34,  2.34it/s] 26%|██▌       | 810/3145 [05:32<16:28,  2.36it/s] 26%|██▌       | 823/3143 [05:38<16:57,  2.28it/s] 26%|██▌       | 819/3145 [05:32<15:05,  2.57it/s] 26%|██▌       | 809/3145 [05:32<17:20,  2.25it/s] 26%|██▌       | 824/3143 [05:38<16:42,  2.31it/s] 26%|██▌       | 811/3145 [05:32<17:06,  2.27it/s] 26%|██▌       | 820/3145 [05:33<15:22,  2.52it/s] 26%|██▌       | 810/3145 [05:33<16:34,  2.35it/s] 26%|██▌       | 825/3143 [05:39<16:32,  2.34it/s] 26%|██▌       | 812/3145 [05:33<17:31,  2.22it/s] 26%|██▌       | 811/3145 [05:33<16:13,  2.40it/s] 26%|██▌       | 821/3145 [05:33<16:27,  2.35it/s] 26%|██▋       | 826/3143 [05:39<16:20,  2.36it/s] 26%|██▌       | 812/3145 [05:33<14:32,  2.67it/s] 26%|██▌       | 813/3145 [05:33<17:19,  2.24it/s] 26%|██▌       | 822/3145 [05:34<15:48,  2.45it/s] 26%|██▋       | 827/3143 [05:40<16:36,  2.32it/s] 26%|██▌       | 814/3145 [05:34<16:45,  2.32it/s] 26%|██▌       | 813/3145 [05:34<16:36,  2.34it/s] 26%|██▌       | 823/3145 [05:34<15:52,  2.44it/s] 26%|██▋       | 828/3143 [05:40<16:37,  2.32it/s] 26%|██▌       | 815/3145 [05:34<17:23,  2.23it/s] 26%|██▌       | 824/3145 [05:34<15:17,  2.53it/s] 26%|██▌       | 814/3145 [05:34<18:01,  2.15it/s] 26%|██▋       | 829/3143 [05:40<16:42,  2.31it/s] 26%|██▌       | 816/3145 [05:35<16:44,  2.32it/s] 26%|██▌       | 825/3145 [05:35<15:07,  2.56it/s] 26%|██▌       | 815/3145 [05:35<17:14,  2.25it/s] 26%|██▋       | 830/3143 [05:41<16:42,  2.31it/s] 26%|██▋       | 826/3145 [05:35<15:09,  2.55it/s] 26%|██▌       | 817/3145 [05:35<16:47,  2.31it/s] 26%|██▌       | 816/3145 [05:35<15:16,  2.54it/s] 26%|██▋       | 831/3143 [05:41<16:21,  2.36it/s] 26%|██▌       | 818/3145 [05:36<16:28,  2.35it/s] 26%|██▋       | 827/3145 [05:36<15:25,  2.50it/s] 26%|██▌       | 817/3145 [05:36<15:45,  2.46it/s] 26%|██▋       | 832/3143 [05:42<16:24,  2.35it/s] 26%|██▌       | 819/3145 [05:36<15:57,  2.43it/s] 26%|██▋       | 828/3145 [05:36<15:52,  2.43it/s] 26%|██▌       | 818/3145 [05:36<16:42,  2.32it/s] 26%|██▌       | 820/3145 [05:36<13:52,  2.79it/s] 27%|██▋       | 833/3143 [05:42<16:21,  2.35it/s] 26%|██▋       | 829/3145 [05:36<15:38,  2.47it/s] 26%|██▌       | 819/3145 [05:36<16:36,  2.33it/s] 26%|██▌       | 821/3145 [05:36<14:01,  2.76it/s] 27%|██▋       | 834/3143 [05:43<16:20,  2.36it/s] 26%|██▌       | 820/3145 [05:37<14:28,  2.68it/s] 26%|██▋       | 830/3145 [05:37<16:02,  2.41it/s] 26%|██▌       | 822/3145 [05:37<13:53,  2.79it/s] 26%|██▌       | 821/3145 [05:37<14:37,  2.65it/s] 27%|██▋       | 835/3143 [05:43<16:17,  2.36it/s] 26%|██▌       | 823/3145 [05:37<14:58,  2.58it/s] 26%|██▋       | 831/3145 [05:37<18:13,  2.12it/s] 26%|██▌       | 822/3145 [05:37<14:31,  2.67it/s] 27%|██▋       | 836/3143 [05:43<15:37,  2.46it/s] 26%|██▌       | 824/3145 [05:38<15:04,  2.57it/s] 26%|██▋       | 832/3145 [05:38<17:50,  2.16it/s] 26%|██▌       | 823/3145 [05:38<14:47,  2.62it/s] 27%|██▋       | 837/3143 [05:44<15:44,  2.44it/s] 26%|██▌       | 825/3145 [05:38<15:41,  2.46it/s] 26%|██▋       | 833/3145 [05:38<17:03,  2.26it/s] 26%|██▌       | 824/3145 [05:38<14:59,  2.58it/s] 27%|██▋       | 838/3143 [05:44<15:40,  2.45it/s] 26%|██▋       | 826/3145 [05:39<15:27,  2.50it/s] 27%|██▋       | 834/3145 [05:39<16:37,  2.32it/s] 26%|██▌       | 825/3145 [05:39<14:43,  2.63it/s] 27%|██▋       | 839/3143 [05:45<15:32,  2.47it/s] 26%|██▋       | 827/3145 [05:39<15:27,  2.50it/s] 27%|██▋       | 840/3143 [05:45<14:02,  2.73it/s] 26%|██▋       | 826/3145 [05:39<14:33,  2.66it/s] 27%|██▋       | 835/3145 [05:39<16:16,  2.37it/s] 26%|██▋       | 828/3145 [05:39<15:36,  2.47it/s] 27%|██▋       | 836/3145 [05:39<14:54,  2.58it/s] 27%|██▋       | 841/3143 [05:45<14:24,  2.66it/s] 26%|██▋       | 827/3145 [05:39<15:22,  2.51it/s] 26%|██▋       | 829/3145 [05:40<15:11,  2.54it/s] 27%|██▋       | 842/3143 [05:46<14:32,  2.64it/s] 27%|██▋       | 837/3145 [05:40<16:07,  2.38it/s] 26%|██▋       | 828/3145 [05:40<15:50,  2.44it/s] 26%|██▋       | 830/3145 [05:40<15:06,  2.55it/s] 27%|██▋       | 843/3143 [05:46<15:10,  2.53it/s] 27%|██▋       | 838/3145 [05:40<16:37,  2.31it/s] 26%|██▋       | 829/3145 [05:40<16:10,  2.39it/s] 27%|██▋       | 844/3143 [05:46<15:21,  2.49it/s] 26%|██▋       | 831/3145 [05:41<17:26,  2.21it/s] 27%|██▋       | 839/3145 [05:41<16:38,  2.31it/s] 26%|██▋       | 830/3145 [05:41<15:59,  2.41it/s] 27%|██▋       | 840/3145 [05:41<14:55,  2.58it/s] 26%|██▋       | 832/3145 [05:41<16:11,  2.38it/s] 27%|██▋       | 845/3143 [05:47<15:47,  2.43it/s] 26%|██▋       | 831/3145 [05:41<16:17,  2.37it/s] 27%|██▋       | 841/3145 [05:41<14:59,  2.56it/s] 26%|██▋       | 833/3145 [05:41<15:51,  2.43it/s] 27%|██▋       | 846/3143 [05:47<15:59,  2.39it/s] 26%|██▋       | 832/3145 [05:42<16:27,  2.34it/s] 27%|██▋       | 842/3145 [05:42<15:43,  2.44it/s] 27%|██▋       | 834/3145 [05:42<16:25,  2.35it/s] 27%|██▋       | 847/3143 [05:48<15:43,  2.43it/s] 26%|██▋       | 833/3145 [05:42<15:45,  2.44it/s] 27%|██▋       | 843/3145 [05:42<15:29,  2.48it/s] 27%|██▋       | 835/3145 [05:42<16:29,  2.33it/s] 27%|██▋       | 848/3143 [05:48<16:26,  2.33it/s] 27%|██▋       | 834/3145 [05:42<15:23,  2.50it/s] 27%|██▋       | 844/3145 [05:43<15:24,  2.49it/s] 27%|██▋       | 836/3145 [05:43<16:00,  2.40it/s] 27%|██▋       | 835/3145 [05:43<15:04,  2.55it/s] 27%|██▋       | 849/3143 [05:49<16:13,  2.36it/s] 27%|██▋       | 845/3145 [05:43<15:26,  2.48it/s] 27%|██▋       | 850/3143 [05:49<15:31,  2.46it/s] 27%|██▋       | 837/3145 [05:43<16:30,  2.33it/s] 27%|██▋       | 836/3145 [05:43<15:39,  2.46it/s] 27%|██▋       | 846/3145 [05:43<15:47,  2.43it/s] 27%|██▋       | 838/3145 [05:44<15:59,  2.41it/s] 27%|██▋       | 851/3143 [05:49<15:53,  2.40it/s] 27%|██▋       | 837/3145 [05:44<16:03,  2.40it/s] 27%|██▋       | 847/3145 [05:44<15:33,  2.46it/s] 27%|██▋       | 839/3145 [05:44<15:24,  2.49it/s] 27%|██▋       | 852/3143 [05:50<15:40,  2.44it/s] 27%|██▋       | 838/3145 [05:44<16:03,  2.40it/s] 27%|██▋       | 840/3145 [05:44<15:02,  2.55it/s] 27%|██▋       | 848/3145 [05:44<15:55,  2.40it/s] 27%|██▋       | 853/3143 [05:50<15:27,  2.47it/s] 27%|██▋       | 839/3145 [05:44<16:32,  2.32it/s] 27%|██▋       | 849/3145 [05:45<14:16,  2.68it/s] 27%|██▋       | 841/3145 [05:45<14:59,  2.56it/s] 27%|██▋       | 854/3143 [05:51<15:16,  2.50it/s] 27%|██▋       | 840/3145 [05:45<16:28,  2.33it/s] 27%|██▋       | 850/3145 [05:45<14:18,  2.67it/s] 27%|██▋       | 842/3145 [05:45<15:01,  2.55it/s] 27%|██▋       | 855/3143 [05:51<15:15,  2.50it/s] 27%|██▋       | 841/3145 [05:45<15:45,  2.44it/s] 27%|██▋       | 851/3145 [05:45<14:19,  2.67it/s] 27%|██▋       | 843/3145 [05:45<15:03,  2.55it/s] 27%|██▋       | 856/3143 [05:51<15:28,  2.46it/s] 27%|██▋       | 852/3145 [05:46<14:29,  2.64it/s] 27%|██▋       | 844/3145 [05:46<14:57,  2.56it/s] 27%|██▋       | 853/3145 [05:46<12:49,  2.98it/s] 27%|██▋       | 842/3145 [05:46<18:47,  2.04it/s] 27%|██▋       | 857/3143 [05:52<15:31,  2.45it/s] 27%|██▋       | 845/3145 [05:46<14:28,  2.65it/s] 27%|██▋       | 843/3145 [05:46<16:18,  2.35it/s] 27%|██▋       | 854/3145 [05:46<14:00,  2.73it/s] 27%|██▋       | 858/3143 [05:52<16:02,  2.37it/s] 27%|██▋       | 846/3145 [05:47<14:38,  2.62it/s] 27%|██▋       | 855/3145 [05:47<14:22,  2.65it/s] 27%|██▋       | 859/3143 [05:53<15:36,  2.44it/s] 27%|██▋       | 844/3145 [05:47<18:06,  2.12it/s] 27%|██▋       | 856/3145 [05:47<12:50,  2.97it/s] 27%|██▋       | 847/3145 [05:47<16:19,  2.35it/s] 27%|██▋       | 845/3145 [05:47<16:07,  2.38it/s] 27%|██▋       | 860/3143 [05:53<15:27,  2.46it/s] 27%|██▋       | 848/3145 [05:48<16:16,  2.35it/s] 27%|██▋       | 857/3145 [05:48<14:50,  2.57it/s] 27%|██▋       | 846/3145 [05:48<17:29,  2.19it/s] 27%|██▋       | 849/3145 [05:48<13:57,  2.74it/s] 27%|██▋       | 861/3143 [05:54<17:56,  2.12it/s] 27%|██▋       | 858/3145 [05:48<14:47,  2.58it/s] 27%|██▋       | 847/3145 [05:48<16:46,  2.28it/s] 27%|██▋       | 850/3145 [05:48<15:12,  2.51it/s] 27%|██▋       | 862/3143 [05:54<17:25,  2.18it/s] 27%|██▋       | 859/3145 [05:48<14:55,  2.55it/s] 27%|██▋       | 848/3145 [05:48<15:53,  2.41it/s] 27%|██▋       | 851/3145 [05:48<13:13,  2.89it/s] 27%|██▋       | 863/3143 [05:54<15:23,  2.47it/s] 27%|██▋       | 860/3145 [05:49<15:23,  2.48it/s] 27%|██▋       | 852/3145 [05:49<13:51,  2.76it/s] 27%|██▋       | 849/3145 [05:49<16:14,  2.36it/s] 27%|██▋       | 864/3143 [05:55<15:43,  2.42it/s] 27%|██▋       | 861/3145 [05:49<14:03,  2.71it/s] 27%|██▋       | 850/3145 [05:49<14:54,  2.57it/s] 27%|██▋       | 853/3145 [05:49<14:26,  2.64it/s] 28%|██▊       | 865/3143 [05:55<15:30,  2.45it/s] 27%|██▋       | 862/3145 [05:49<14:14,  2.67it/s] 27%|██▋       | 851/3145 [05:50<14:57,  2.56it/s] 27%|██▋       | 854/3145 [05:50<14:33,  2.62it/s] 28%|██▊       | 866/3143 [05:56<15:17,  2.48it/s] 27%|██▋       | 852/3145 [05:50<13:15,  2.88it/s] 27%|██▋       | 863/3145 [05:50<14:32,  2.62it/s] 28%|██▊       | 867/3143 [05:56<13:30,  2.81it/s] 27%|██▋       | 855/3145 [05:50<14:53,  2.56it/s] 27%|██▋       | 864/3145 [05:50<14:20,  2.65it/s] 27%|██▋       | 853/3145 [05:50<15:14,  2.51it/s] 28%|██▊       | 868/3143 [05:56<13:39,  2.78it/s] 27%|██▋       | 856/3145 [05:50<14:37,  2.61it/s] 28%|██▊       | 865/3145 [05:51<14:39,  2.59it/s] 27%|██▋       | 854/3145 [05:51<15:20,  2.49it/s] 28%|██▊       | 869/3143 [05:57<14:29,  2.62it/s] 27%|██▋       | 857/3145 [05:51<14:45,  2.58it/s] 28%|██▊       | 866/3145 [05:51<15:21,  2.47it/s] 27%|██▋       | 855/3145 [05:51<15:44,  2.42it/s] 27%|██▋       | 858/3145 [05:51<15:00,  2.54it/s] 28%|██▊       | 870/3143 [05:57<15:28,  2.45it/s] 28%|██▊       | 867/3145 [05:51<15:17,  2.48it/s] 27%|██▋       | 856/3145 [05:52<15:30,  2.46it/s] 27%|██▋       | 859/3145 [05:52<15:25,  2.47it/s] 28%|██▊       | 871/3143 [05:58<15:24,  2.46it/s] 28%|██▊       | 868/3145 [05:52<14:34,  2.60it/s] 28%|██▊       | 872/3143 [05:58<13:24,  2.82it/s] 27%|██▋       | 857/3145 [05:52<15:16,  2.50it/s] 27%|██▋       | 860/3145 [05:52<16:14,  2.34it/s] 28%|██▊       | 869/3145 [05:52<15:09,  2.50it/s] 27%|██▋       | 858/3145 [05:52<14:51,  2.56it/s] 28%|██▊       | 873/3143 [05:58<14:53,  2.54it/s] 27%|██▋       | 861/3145 [05:53<16:08,  2.36it/s] 27%|██▋       | 859/3145 [05:53<14:42,  2.59it/s] 28%|██▊       | 870/3145 [05:53<16:08,  2.35it/s] 28%|██▊       | 874/3143 [05:59<14:36,  2.59it/s] 27%|██▋       | 862/3145 [05:53<16:33,  2.30it/s] 27%|██▋       | 860/3145 [05:53<14:51,  2.56it/s] 28%|██▊       | 875/3143 [05:59<15:06,  2.50it/s] 28%|██▊       | 871/3145 [05:53<16:47,  2.26it/s] 27%|██▋       | 863/3145 [05:53<16:30,  2.30it/s] 27%|██▋       | 861/3145 [05:54<15:38,  2.43it/s] 28%|██▊       | 876/3143 [06:00<15:14,  2.48it/s] 28%|██▊       | 872/3145 [05:54<16:47,  2.26it/s] 27%|██▋       | 864/3145 [05:54<16:31,  2.30it/s] 28%|██▊       | 877/3143 [06:00<14:54,  2.53it/s] 27%|██▋       | 862/3145 [05:54<15:54,  2.39it/s] 28%|██▊       | 873/3145 [05:54<15:53,  2.38it/s] 28%|██▊       | 865/3145 [05:54<15:30,  2.45it/s] 27%|██▋       | 863/3145 [05:54<15:26,  2.46it/s] 28%|██▊       | 878/3143 [06:00<15:28,  2.44it/s] 28%|██▊       | 874/3145 [05:54<16:20,  2.32it/s] 28%|██▊       | 866/3145 [05:55<15:27,  2.46it/s] 27%|██▋       | 864/3145 [05:55<15:46,  2.41it/s] 28%|██▊       | 875/3145 [05:55<15:52,  2.38it/s] 28%|██▊       | 879/3143 [06:01<16:06,  2.34it/s] 28%|██▊       | 867/3145 [05:55<14:58,  2.53it/s] 28%|██▊       | 865/3145 [05:55<13:52,  2.74it/s] 28%|██▊       | 876/3145 [05:55<15:39,  2.42it/s] 28%|██▊       | 880/3143 [06:01<16:10,  2.33it/s] 28%|██▊       | 868/3145 [05:55<15:01,  2.53it/s] 28%|██▊       | 866/3145 [05:55<14:18,  2.65it/s] 28%|██▊       | 877/3145 [05:56<15:56,  2.37it/s] 28%|██▊       | 869/3145 [05:56<14:45,  2.57it/s] 28%|██▊       | 881/3143 [06:02<16:34,  2.27it/s] 28%|██▊       | 867/3145 [05:56<15:20,  2.47it/s] 28%|██▊       | 878/3145 [05:56<15:36,  2.42it/s] 28%|██▊       | 882/3143 [06:02<15:47,  2.39it/s] 28%|██▊       | 870/3145 [05:56<14:48,  2.56it/s] 28%|██▊       | 868/3145 [05:56<15:47,  2.40it/s] 28%|██▊       | 879/3145 [05:57<15:49,  2.39it/s] 28%|██▊       | 871/3145 [05:57<14:53,  2.54it/s] 28%|██▊       | 883/3143 [06:02<15:40,  2.40it/s] 28%|██▊       | 869/3145 [05:57<15:39,  2.42it/s] 28%|██▊       | 880/3145 [05:57<15:56,  2.37it/s] 28%|██▊       | 872/3145 [05:57<15:01,  2.52it/s] 28%|██▊       | 884/3143 [06:03<16:09,  2.33it/s] 28%|██▊       | 870/3145 [05:57<15:14,  2.49it/s] 28%|██▊       | 873/3145 [05:57<13:10,  2.87it/s] 28%|██▊       | 881/3145 [05:57<15:34,  2.42it/s] 28%|██▊       | 885/3143 [06:03<15:49,  2.38it/s] 28%|██▊       | 874/3145 [05:58<13:14,  2.86it/s] 28%|██▊       | 871/3145 [05:58<15:31,  2.44it/s] 28%|██▊       | 886/3143 [06:04<13:50,  2.72it/s] 28%|██▊       | 882/3145 [05:58<15:32,  2.43it/s] 28%|██▊       | 872/3145 [05:58<14:56,  2.53it/s] 28%|██▊       | 875/3145 [05:58<14:26,  2.62it/s] 28%|██▊       | 887/3143 [06:04<13:40,  2.75it/s] 28%|██▊       | 883/3145 [05:58<15:22,  2.45it/s] 28%|██▊       | 873/3145 [05:58<15:11,  2.49it/s] 28%|██▊       | 876/3145 [05:58<14:35,  2.59it/s] 28%|██▊       | 888/3143 [06:04<14:28,  2.60it/s] 28%|██▊       | 884/3145 [05:59<15:26,  2.44it/s] 28%|██▊       | 874/3145 [05:59<15:35,  2.43it/s] 28%|██▊       | 877/3145 [05:59<14:28,  2.61it/s] 28%|██▊       | 889/3143 [06:05<14:29,  2.59it/s] 28%|██▊       | 885/3145 [05:59<16:01,  2.35it/s] 28%|██▊       | 875/3145 [05:59<15:11,  2.49it/s] 28%|██▊       | 878/3145 [05:59<14:46,  2.56it/s] 28%|██▊       | 890/3143 [06:05<15:05,  2.49it/s] 28%|██▊       | 886/3145 [05:59<15:33,  2.42it/s] 28%|██▊       | 876/3145 [06:00<15:02,  2.51it/s] 28%|██▊       | 879/3145 [06:00<14:25,  2.62it/s] 28%|██▊       | 891/3143 [06:06<14:49,  2.53it/s] 28%|██▊       | 887/3145 [06:00<15:33,  2.42it/s] 28%|██▊       | 880/3145 [06:00<14:45,  2.56it/s] 28%|██▊       | 877/3145 [06:00<15:34,  2.43it/s] 28%|██▊       | 892/3143 [06:06<14:45,  2.54it/s] 28%|██▊       | 888/3145 [06:00<15:56,  2.36it/s] 28%|██▊       | 881/3145 [06:00<14:05,  2.68it/s] 28%|██▊       | 878/3145 [06:00<15:26,  2.45it/s] 28%|██▊       | 893/3143 [06:06<14:50,  2.53it/s] 28%|██▊       | 889/3145 [06:01<15:51,  2.37it/s] 28%|██▊       | 882/3145 [06:01<14:45,  2.56it/s] 28%|██▊       | 879/3145 [06:01<15:38,  2.41it/s] 28%|██▊       | 894/3143 [06:07<14:26,  2.60it/s] 28%|██▊       | 890/3145 [06:01<15:13,  2.47it/s] 28%|██▊       | 883/3145 [06:01<14:49,  2.54it/s] 28%|██▊       | 895/3143 [06:07<14:45,  2.54it/s] 28%|██▊       | 880/3145 [06:01<15:48,  2.39it/s] 28%|██▊       | 891/3145 [06:02<15:33,  2.41it/s] 28%|██▊       | 881/3145 [06:02<15:18,  2.47it/s] 29%|██▊       | 896/3143 [06:08<14:57,  2.50it/s] 28%|██▊       | 884/3145 [06:02<16:48,  2.24it/s] 28%|██▊       | 892/3145 [06:02<14:26,  2.60it/s] 28%|██▊       | 882/3145 [06:02<14:49,  2.54it/s] 29%|██▊       | 897/3143 [06:08<15:06,  2.48it/s] 28%|██▊       | 885/3145 [06:02<16:10,  2.33it/s] 28%|██▊       | 893/3145 [06:02<13:52,  2.71it/s] 28%|██▊       | 894/3145 [06:02<12:31,  2.99it/s] 28%|██▊       | 883/3145 [06:02<15:17,  2.47it/s] 29%|██▊       | 898/3143 [06:08<14:56,  2.50it/s] 28%|██▊       | 886/3145 [06:02<15:24,  2.44it/s] 28%|██▊       | 884/3145 [06:03<13:32,  2.78it/s] 29%|██▊       | 899/3143 [06:09<14:04,  2.66it/s] 28%|██▊       | 895/3145 [06:03<13:09,  2.85it/s] 28%|██▊       | 887/3145 [06:03<15:14,  2.47it/s] 28%|██▊       | 896/3145 [06:03<11:52,  3.16it/s] 28%|██▊       | 885/3145 [06:03<14:02,  2.68it/s] 29%|██▊       | 900/3143 [06:09<14:43,  2.54it/s] 28%|██▊       | 888/3145 [06:03<15:02,  2.50it/s] 29%|██▊       | 897/3145 [06:03<12:42,  2.95it/s] 28%|██▊       | 886/3145 [06:03<14:10,  2.66it/s] 29%|██▊       | 901/3143 [06:10<15:08,  2.47it/s] 28%|██▊       | 889/3145 [06:04<15:21,  2.45it/s] 28%|██▊       | 887/3145 [06:04<14:15,  2.64it/s] 29%|██▊       | 898/3145 [06:04<13:55,  2.69it/s] 29%|██▊       | 902/3143 [06:10<14:57,  2.50it/s] 28%|██▊       | 890/3145 [06:04<15:37,  2.40it/s] 28%|██▊       | 888/3145 [06:04<14:45,  2.55it/s] 29%|██▊       | 899/3145 [06:04<14:32,  2.57it/s] 29%|██▊       | 903/3143 [06:10<14:52,  2.51it/s] 28%|██▊       | 891/3145 [06:05<15:50,  2.37it/s] 29%|██▊       | 900/3145 [06:05<14:32,  2.57it/s] 28%|██▊       | 889/3145 [06:05<15:17,  2.46it/s] 29%|██▉       | 904/3143 [06:11<15:06,  2.47it/s] 28%|██▊       | 892/3145 [06:05<15:42,  2.39it/s] 28%|██▊       | 890/3145 [06:05<14:48,  2.54it/s] 29%|██▊       | 901/3145 [06:05<14:51,  2.52it/s] 29%|██▉       | 905/3143 [06:11<15:40,  2.38it/s] 28%|██▊       | 893/3145 [06:05<15:54,  2.36it/s] 29%|██▊       | 902/3145 [06:05<14:39,  2.55it/s] 28%|██▊       | 891/3145 [06:06<15:52,  2.37it/s] 29%|██▉       | 906/3143 [06:12<15:15,  2.44it/s] 28%|██▊       | 894/3145 [06:06<15:28,  2.42it/s] 29%|██▊       | 903/3145 [06:06<14:32,  2.57it/s] 28%|██▊       | 892/3145 [06:06<16:15,  2.31it/s] 29%|██▉       | 907/3143 [06:12<15:21,  2.43it/s] 28%|██▊       | 895/3145 [06:06<15:56,  2.35it/s] 29%|██▊       | 904/3145 [06:06<14:32,  2.57it/s] 28%|██▊       | 893/3145 [06:06<15:29,  2.42it/s] 29%|██▉       | 908/3143 [06:12<15:12,  2.45it/s] 28%|██▊       | 896/3145 [06:07<15:11,  2.47it/s] 29%|██▉       | 905/3145 [06:07<14:45,  2.53it/s] 28%|██▊       | 894/3145 [06:07<15:13,  2.46it/s] 29%|██▉       | 909/3143 [06:13<15:29,  2.40it/s] 29%|██▉       | 906/3145 [06:07<13:29,  2.77it/s] 29%|██▊       | 897/3145 [06:07<15:03,  2.49it/s] 28%|██▊       | 895/3145 [06:07<15:26,  2.43it/s] 29%|██▉       | 910/3143 [06:13<15:05,  2.47it/s] 29%|██▉       | 907/3145 [06:07<13:41,  2.72it/s] 29%|██▊       | 898/3145 [06:07<14:48,  2.53it/s] 28%|██▊       | 896/3145 [06:08<15:14,  2.46it/s] 29%|██▊       | 899/3145 [06:08<14:42,  2.55it/s] 29%|██▉       | 911/3143 [06:14<15:34,  2.39it/s] 29%|██▉       | 908/3145 [06:08<15:05,  2.47it/s] 29%|██▊       | 897/3145 [06:08<14:48,  2.53it/s] 29%|██▉       | 909/3145 [06:08<13:10,  2.83it/s] 29%|██▉       | 912/3143 [06:14<14:46,  2.52it/s] 29%|██▊       | 900/3145 [06:08<15:21,  2.44it/s] 29%|██▉       | 910/3145 [06:08<13:35,  2.74it/s] 29%|██▉       | 913/3143 [06:14<14:43,  2.52it/s] 29%|██▊       | 898/3145 [06:09<16:53,  2.22it/s] 29%|██▊       | 901/3145 [06:09<15:41,  2.38it/s] 29%|██▉       | 911/3145 [06:09<12:14,  3.04it/s] 29%|██▉       | 914/3143 [06:15<14:39,  2.54it/s] 29%|██▊       | 899/3145 [06:09<16:25,  2.28it/s] 29%|██▊       | 902/3145 [06:09<16:02,  2.33it/s] 29%|██▉       | 912/3145 [06:09<13:27,  2.76it/s] 29%|██▉       | 915/3143 [06:15<15:01,  2.47it/s] 29%|██▊       | 900/3145 [06:09<16:29,  2.27it/s] 29%|██▉       | 913/3145 [06:10<13:37,  2.73it/s] 29%|██▊       | 903/3145 [06:10<16:06,  2.32it/s] 29%|██▉       | 916/3143 [06:16<15:17,  2.43it/s] 29%|██▊       | 901/3145 [06:10<15:32,  2.41it/s] 29%|██▉       | 914/3145 [06:10<14:33,  2.56it/s] 29%|██▊       | 904/3145 [06:10<16:43,  2.23it/s] 29%|██▊       | 902/3145 [06:10<16:14,  2.30it/s] 29%|██▉       | 917/3143 [06:16<16:31,  2.24it/s] 29%|██▉       | 915/3145 [06:10<15:00,  2.48it/s] 29%|██▉       | 905/3145 [06:10<16:13,  2.30it/s] 29%|██▉       | 918/3143 [06:17<15:55,  2.33it/s] 29%|██▉       | 916/3145 [06:11<14:10,  2.62it/s] 29%|██▊       | 903/3145 [06:11<16:51,  2.22it/s] 29%|██▉       | 906/3145 [06:11<15:53,  2.35it/s] 29%|██▉       | 917/3145 [06:11<14:11,  2.62it/s] 29%|██▊       | 904/3145 [06:11<16:06,  2.32it/s] 29%|██▉       | 919/3143 [06:17<16:19,  2.27it/s] 29%|██▉       | 907/3145 [06:11<15:00,  2.48it/s] 29%|██▉       | 920/3143 [06:17<15:27,  2.40it/s] 29%|██▉       | 918/3145 [06:12<14:31,  2.55it/s] 29%|██▉       | 908/3145 [06:12<14:53,  2.50it/s] 29%|██▉       | 905/3145 [06:12<16:16,  2.29it/s] 29%|██▉       | 921/3143 [06:18<15:03,  2.46it/s] 29%|██▉       | 919/3145 [06:12<14:42,  2.52it/s] 29%|██▉       | 906/3145 [06:12<15:42,  2.38it/s] 29%|██▉       | 909/3145 [06:12<17:07,  2.18it/s] 29%|██▉       | 922/3143 [06:18<14:58,  2.47it/s] 29%|██▉       | 920/3145 [06:12<14:41,  2.52it/s] 29%|██▉       | 907/3145 [06:12<15:34,  2.39it/s] 29%|██▉       | 910/3145 [06:13<16:38,  2.24it/s] 29%|██▉       | 908/3145 [06:13<14:05,  2.64it/s] 29%|██▉       | 921/3145 [06:13<14:34,  2.54it/s] 29%|██▉       | 923/3143 [06:19<15:39,  2.36it/s] 29%|██▉       | 911/3145 [06:13<15:52,  2.34it/s] 29%|██▉       | 909/3145 [06:13<14:17,  2.61it/s] 29%|██▉       | 922/3145 [06:13<14:21,  2.58it/s] 29%|██▉       | 924/3143 [06:19<15:07,  2.44it/s] 29%|██▉       | 912/3145 [06:13<15:23,  2.42it/s] 29%|██▉       | 910/3145 [06:13<14:05,  2.64it/s] 29%|██▉       | 923/3145 [06:13<14:30,  2.55it/s] 29%|██▉       | 925/3143 [06:19<15:10,  2.44it/s] 29%|██▉       | 913/3145 [06:14<15:39,  2.37it/s] 29%|██▉       | 911/3145 [06:14<14:17,  2.61it/s] 29%|██▉       | 924/3145 [06:14<14:13,  2.60it/s] 29%|██▉       | 926/3143 [06:20<14:58,  2.47it/s] 29%|██▉       | 914/3145 [06:14<15:16,  2.43it/s] 29%|██▉       | 912/3145 [06:14<14:03,  2.65it/s] 29%|██▉       | 925/3145 [06:14<14:20,  2.58it/s] 29%|██▉       | 927/3143 [06:20<14:41,  2.51it/s] 29%|██▉       | 915/3145 [06:15<14:45,  2.52it/s] 29%|██▉       | 913/3145 [06:15<14:45,  2.52it/s] 29%|██▉       | 926/3145 [06:15<14:28,  2.56it/s] 30%|██▉       | 928/3143 [06:21<14:09,  2.61it/s] 29%|██▉       | 916/3145 [06:15<14:56,  2.49it/s] 29%|██▉       | 927/3145 [06:15<14:35,  2.53it/s] 30%|██▉       | 929/3143 [06:21<14:21,  2.57it/s] 29%|██▉       | 914/3145 [06:15<15:44,  2.36it/s] 29%|██▉       | 917/3145 [06:15<14:45,  2.52it/s] 30%|██▉       | 928/3145 [06:15<14:02,  2.63it/s] 29%|██▉       | 915/3145 [06:15<15:10,  2.45it/s] 30%|██▉       | 930/3143 [06:21<14:30,  2.54it/s] 29%|██▉       | 918/3145 [06:16<14:43,  2.52it/s] 30%|██▉       | 929/3145 [06:16<14:17,  2.58it/s] 29%|██▉       | 916/3145 [06:16<15:03,  2.47it/s] 30%|██▉       | 931/3143 [06:22<14:47,  2.49it/s] 29%|██▉       | 919/3145 [06:16<14:53,  2.49it/s] 30%|██▉       | 930/3145 [06:16<14:00,  2.63it/s] 29%|██▉       | 917/3145 [06:16<14:34,  2.55it/s] 30%|██▉       | 932/3143 [06:22<14:21,  2.57it/s] 29%|██▉       | 920/3145 [06:17<14:20,  2.58it/s] 29%|██▉       | 918/3145 [06:17<14:07,  2.63it/s] 30%|██▉       | 931/3145 [06:17<15:15,  2.42it/s] 30%|██▉       | 933/3143 [06:23<14:28,  2.54it/s] 29%|██▉       | 921/3145 [06:17<14:51,  2.49it/s] 29%|██▉       | 919/3145 [06:17<14:52,  2.49it/s] 30%|██▉       | 934/3143 [06:23<14:39,  2.51it/s] 30%|██▉       | 932/3145 [06:17<15:51,  2.33it/s] 29%|██▉       | 922/3145 [06:17<14:42,  2.52it/s] 29%|██▉       | 920/3145 [06:17<14:43,  2.52it/s] 30%|██▉       | 935/3143 [06:23<14:15,  2.58it/s] 30%|██▉       | 933/3145 [06:18<16:06,  2.29it/s] 29%|██▉       | 923/3145 [06:18<14:42,  2.52it/s] 29%|██▉       | 921/3145 [06:18<14:55,  2.48it/s] 30%|██▉       | 936/3143 [06:24<14:50,  2.48it/s] 29%|██▉       | 924/3145 [06:18<14:49,  2.50it/s] 30%|██▉       | 934/3145 [06:18<17:35,  2.09it/s] 29%|██▉       | 922/3145 [06:18<14:52,  2.49it/s] 30%|██▉       | 937/3143 [06:24<14:49,  2.48it/s] 29%|██▉       | 925/3145 [06:18<14:12,  2.60it/s] 30%|██▉       | 935/3145 [06:19<17:23,  2.12it/s] 29%|██▉       | 923/3145 [06:19<15:10,  2.44it/s] 30%|██▉       | 938/3143 [06:25<15:29,  2.37it/s] 29%|██▉       | 926/3145 [06:19<13:56,  2.65it/s] 29%|██▉       | 924/3145 [06:19<14:38,  2.53it/s] 30%|██▉       | 936/3145 [06:19<17:19,  2.12it/s] 30%|██▉       | 939/3143 [06:25<14:54,  2.46it/s] 29%|██▉       | 927/3145 [06:19<14:11,  2.60it/s] 29%|██▉       | 925/3145 [06:19<14:41,  2.52it/s] 30%|██▉       | 940/3143 [06:25<14:47,  2.48it/s] 30%|██▉       | 937/3145 [06:20<17:12,  2.14it/s] 30%|██▉       | 928/3145 [06:20<14:41,  2.52it/s] 29%|██▉       | 926/3145 [06:20<15:10,  2.44it/s] 30%|██▉       | 941/3143 [06:26<14:59,  2.45it/s] 30%|██▉       | 938/3145 [06:20<16:21,  2.25it/s] 30%|██▉       | 929/3145 [06:20<14:25,  2.56it/s] 29%|██▉       | 927/3145 [06:20<15:30,  2.38it/s] 30%|██▉       | 939/3145 [06:20<16:25,  2.24it/s] 30%|██▉       | 942/3143 [06:26<15:42,  2.34it/s] 30%|██▉       | 930/3145 [06:20<14:17,  2.58it/s] 30%|██▉       | 928/3145 [06:21<15:32,  2.38it/s] 30%|██▉       | 931/3145 [06:21<14:26,  2.56it/s] 30%|██▉       | 940/3145 [06:21<16:24,  2.24it/s] 30%|███       | 943/3143 [06:27<16:11,  2.26it/s] 30%|██▉       | 929/3145 [06:21<14:57,  2.47it/s] 30%|██▉       | 932/3145 [06:21<14:23,  2.56it/s] 30%|██▉       | 941/3145 [06:21<16:05,  2.28it/s] 30%|███       | 944/3143 [06:27<16:04,  2.28it/s] 30%|██▉       | 930/3145 [06:22<15:04,  2.45it/s] 30%|██▉       | 933/3145 [06:22<14:00,  2.63it/s] 30%|██▉       | 942/3145 [06:22<15:35,  2.35it/s] 30%|███       | 945/3143 [06:28<16:05,  2.28it/s] 30%|██▉       | 931/3145 [06:22<14:59,  2.46it/s] 30%|██▉       | 943/3145 [06:22<16:00,  2.29it/s] 30%|███       | 946/3143 [06:28<15:38,  2.34it/s] 30%|██▉       | 934/3145 [06:22<16:24,  2.25it/s] 30%|██▉       | 932/3145 [06:22<15:14,  2.42it/s] 30%|███       | 944/3145 [06:22<15:22,  2.39it/s] 30%|███       | 947/3143 [06:28<15:07,  2.42it/s] 30%|██▉       | 935/3145 [06:23<17:54,  2.06it/s] 30%|██▉       | 933/3145 [06:23<15:08,  2.44it/s] 30%|███       | 948/3143 [06:29<14:58,  2.44it/s] 30%|███       | 945/3145 [06:23<15:46,  2.32it/s] 30%|██▉       | 936/3145 [06:23<16:40,  2.21it/s] 30%|██▉       | 934/3145 [06:23<15:13,  2.42it/s] 30%|███       | 946/3145 [06:23<15:52,  2.31it/s] 30%|███       | 949/3143 [06:29<15:48,  2.31it/s] 30%|██▉       | 937/3145 [06:24<16:05,  2.29it/s] 30%|██▉       | 935/3145 [06:24<15:10,  2.43it/s] 30%|███       | 947/3145 [06:24<15:13,  2.41it/s] 30%|███       | 950/3143 [06:30<15:43,  2.32it/s] 30%|██▉       | 938/3145 [06:24<15:39,  2.35it/s] 30%|██▉       | 936/3145 [06:24<14:38,  2.52it/s] 30%|███       | 948/3145 [06:24<15:22,  2.38it/s] 30%|███       | 951/3143 [06:30<15:37,  2.34it/s] 30%|██▉       | 939/3145 [06:24<15:15,  2.41it/s] 30%|██▉       | 937/3145 [06:24<14:45,  2.49it/s] 30%|███       | 949/3145 [06:25<15:30,  2.36it/s] 30%|███       | 952/3143 [06:31<15:07,  2.41it/s] 30%|██▉       | 940/3145 [06:25<14:57,  2.46it/s] 30%|██▉       | 938/3145 [06:25<14:57,  2.46it/s] 30%|███       | 950/3145 [06:25<15:11,  2.41it/s] 30%|███       | 953/3143 [06:31<14:42,  2.48it/s] 30%|██▉       | 941/3145 [06:25<15:15,  2.41it/s] 30%|██▉       | 939/3145 [06:25<15:16,  2.41it/s] 30%|███       | 954/3143 [06:31<14:51,  2.45it/s] 30%|███       | 951/3145 [06:25<15:37,  2.34it/s] 30%|██▉       | 942/3145 [06:26<15:01,  2.44it/s] 30%|██▉       | 940/3145 [06:26<15:04,  2.44it/s] 30%|███       | 955/3143 [06:32<14:33,  2.50it/s] 30%|██▉       | 943/3145 [06:26<14:26,  2.54it/s] 30%|███       | 952/3145 [06:26<15:54,  2.30it/s] 30%|██▉       | 941/3145 [06:26<15:23,  2.39it/s] 30%|███       | 956/3143 [06:32<14:27,  2.52it/s] 30%|███       | 944/3145 [06:26<14:55,  2.46it/s] 30%|███       | 953/3145 [06:26<15:29,  2.36it/s] 30%|██▉       | 942/3145 [06:26<15:09,  2.42it/s] 30%|███       | 957/3143 [06:33<14:34,  2.50it/s] 30%|███       | 954/3145 [06:27<14:54,  2.45it/s] 30%|███       | 945/3145 [06:27<15:01,  2.44it/s] 30%|██▉       | 943/3145 [06:27<15:36,  2.35it/s] 30%|███       | 958/3143 [06:33<14:33,  2.50it/s] 30%|███       | 955/3145 [06:27<15:00,  2.43it/s] 30%|███       | 946/3145 [06:27<15:16,  2.40it/s] 30%|███       | 944/3145 [06:27<15:58,  2.30it/s] 31%|███       | 959/3143 [06:33<14:24,  2.53it/s] 30%|███       | 956/3145 [06:27<14:48,  2.46it/s] 30%|███       | 947/3145 [06:28<14:58,  2.45it/s] 31%|███       | 960/3143 [06:34<13:53,  2.62it/s] 30%|███       | 945/3145 [06:28<15:44,  2.33it/s] 30%|███       | 948/3145 [06:28<13:48,  2.65it/s] 30%|███       | 957/3145 [06:28<14:55,  2.44it/s] 31%|███       | 961/3143 [06:34<13:49,  2.63it/s] 30%|███       | 946/3145 [06:28<15:23,  2.38it/s] 30%|███       | 949/3145 [06:28<14:15,  2.57it/s] 30%|███       | 958/3145 [06:28<14:45,  2.47it/s] 31%|███       | 962/3143 [06:34<13:44,  2.64it/s] 30%|███       | 947/3145 [06:29<15:16,  2.40it/s] 30%|███       | 950/3145 [06:29<14:32,  2.52it/s] 30%|███       | 959/3145 [06:29<14:38,  2.49it/s] 31%|███       | 963/3143 [06:35<14:32,  2.50it/s] 30%|███       | 948/3145 [06:29<15:18,  2.39it/s] 30%|███       | 951/3145 [06:29<14:10,  2.58it/s] 31%|███       | 960/3145 [06:29<15:03,  2.42it/s] 30%|███       | 949/3145 [06:29<14:42,  2.49it/s] 31%|███       | 964/3143 [06:35<15:25,  2.36it/s] 30%|███       | 952/3145 [06:29<14:39,  2.49it/s] 31%|███       | 961/3145 [06:30<15:16,  2.38it/s] 31%|███       | 965/3143 [06:36<14:02,  2.58it/s] 30%|███       | 950/3145 [06:30<14:37,  2.50it/s] 30%|███       | 953/3145 [06:30<14:31,  2.51it/s] 31%|███       | 962/3145 [06:30<15:38,  2.33it/s] 31%|███       | 966/3143 [06:36<13:42,  2.65it/s] 30%|███       | 951/3145 [06:30<14:40,  2.49it/s] 30%|███       | 954/3145 [06:30<14:52,  2.45it/s] 31%|███       | 963/3145 [06:30<15:23,  2.36it/s] 31%|███       | 967/3143 [06:36<13:32,  2.68it/s] 30%|███       | 952/3145 [06:31<14:48,  2.47it/s] 30%|███       | 955/3145 [06:31<15:08,  2.41it/s] 31%|███       | 964/3145 [06:31<14:41,  2.48it/s] 31%|███       | 968/3143 [06:37<14:00,  2.59it/s] 30%|███       | 953/3145 [06:31<14:46,  2.47it/s] 30%|███       | 956/3145 [06:31<14:49,  2.46it/s] 31%|███       | 965/3145 [06:31<14:13,  2.56it/s] 31%|███       | 969/3143 [06:37<13:46,  2.63it/s] 30%|███       | 954/3145 [06:31<14:27,  2.53it/s] 31%|███       | 970/3143 [06:37<12:06,  2.99it/s] 30%|███       | 957/3145 [06:32<14:30,  2.51it/s] 31%|███       | 966/3145 [06:32<13:42,  2.65it/s] 30%|███       | 955/3145 [06:32<14:06,  2.59it/s] 31%|███       | 971/3143 [06:38<12:49,  2.82it/s] 30%|███       | 958/3145 [06:32<14:57,  2.44it/s] 31%|███       | 967/3145 [06:32<14:25,  2.52it/s] 31%|███       | 972/3143 [06:38<11:36,  3.12it/s] 30%|███       | 956/3145 [06:32<14:41,  2.48it/s] 30%|███       | 959/3145 [06:32<14:31,  2.51it/s] 31%|███       | 968/3145 [06:32<14:16,  2.54it/s] 31%|███       | 973/3143 [06:38<10:43,  3.37it/s] 31%|███       | 974/3143 [06:38<10:01,  3.61it/s] 30%|███       | 957/3145 [06:33<15:20,  2.38it/s] 31%|███       | 960/3145 [06:33<14:03,  2.59it/s] 31%|███       | 969/3145 [06:33<13:50,  2.62it/s] 31%|███       | 975/3143 [06:39<10:57,  3.30it/s] 30%|███       | 958/3145 [06:33<15:15,  2.39it/s] 31%|███       | 970/3145 [06:33<13:34,  2.67it/s] 31%|███       | 961/3145 [06:33<14:11,  2.56it/s] 31%|███       | 976/3143 [06:39<12:26,  2.90it/s] 30%|███       | 959/3145 [06:33<15:03,  2.42it/s] 31%|███       | 962/3145 [06:33<14:16,  2.55it/s] 31%|███       | 971/3145 [06:34<14:26,  2.51it/s] 31%|███       | 977/3143 [06:40<12:41,  2.85it/s] 31%|███       | 960/3145 [06:34<14:42,  2.48it/s] 31%|███       | 972/3145 [06:34<14:09,  2.56it/s] 31%|███       | 963/3145 [06:34<14:48,  2.46it/s] 31%|███       | 978/3143 [06:40<12:48,  2.82it/s] 31%|███       | 961/3145 [06:34<14:49,  2.45it/s] 31%|███       | 973/3145 [06:34<14:10,  2.56it/s] 31%|███       | 964/3145 [06:34<15:01,  2.42it/s] 31%|███       | 979/3143 [06:40<11:39,  3.09it/s] 31%|███       | 974/3145 [06:34<12:22,  2.92it/s] 31%|███       | 965/3145 [06:35<13:14,  2.74it/s] 31%|███       | 962/3145 [06:35<15:09,  2.40it/s] 31%|███       | 980/3143 [06:41<12:14,  2.95it/s] 31%|███       | 975/3145 [06:35<12:33,  2.88it/s] 31%|███       | 966/3145 [06:35<13:11,  2.75it/s] 31%|███       | 963/3145 [06:35<14:59,  2.42it/s] 31%|███       | 981/3143 [06:41<12:32,  2.87it/s] 31%|███       | 976/3145 [06:35<13:03,  2.77it/s] 31%|███       | 967/3145 [06:35<13:18,  2.73it/s] 31%|███       | 964/3145 [06:35<15:13,  2.39it/s] 31%|███       | 982/3143 [06:41<13:28,  2.67it/s] 31%|███       | 977/3145 [06:36<13:27,  2.68it/s] 31%|███       | 968/3145 [06:36<13:13,  2.74it/s] 31%|███▏      | 983/3143 [06:42<13:40,  2.63it/s] 31%|███       | 965/3145 [06:36<15:41,  2.32it/s] 31%|███       | 978/3145 [06:36<13:37,  2.65it/s] 31%|███       | 969/3145 [06:36<13:57,  2.60it/s] 31%|███▏      | 984/3143 [06:42<13:50,  2.60it/s] 31%|███       | 979/3145 [06:36<13:36,  2.65it/s] 31%|███       | 966/3145 [06:36<15:42,  2.31it/s] 31%|███       | 970/3145 [06:37<13:58,  2.59it/s] 31%|███       | 971/3145 [06:37<12:14,  2.96it/s] 31%|███▏      | 985/3143 [06:43<14:25,  2.49it/s] 31%|███       | 967/3145 [06:37<14:58,  2.42it/s] 31%|███       | 980/3145 [06:37<13:51,  2.60it/s] 31%|███▏      | 986/3143 [06:43<14:04,  2.55it/s] 31%|███       | 972/3145 [06:37<13:01,  2.78it/s] 31%|███       | 968/3145 [06:37<14:57,  2.42it/s] 31%|███       | 981/3145 [06:37<13:55,  2.59it/s] 31%|███▏      | 987/3143 [06:43<12:39,  2.84it/s] 31%|███       | 973/3145 [06:38<13:39,  2.65it/s] 31%|███       | 982/3145 [06:38<14:07,  2.55it/s] 31%|███       | 969/3145 [06:38<15:01,  2.41it/s] 31%|███       | 970/3145 [06:38<13:59,  2.59it/s] 31%|███▏      | 988/3143 [06:44<14:52,  2.41it/s] 31%|███       | 974/3145 [06:38<13:58,  2.59it/s] 31%|███▏      | 983/3145 [06:38<14:12,  2.54it/s] 31%|███▏      | 989/3143 [06:44<14:12,  2.53it/s] 31%|███▏      | 984/3145 [06:38<13:48,  2.61it/s] 31%|███       | 971/3145 [06:38<14:45,  2.45it/s] 31%|███       | 975/3145 [06:38<14:41,  2.46it/s] 31%|███▏      | 990/3143 [06:45<14:16,  2.51it/s] 31%|███▏      | 985/3145 [06:39<14:21,  2.51it/s] 31%|███       | 972/3145 [06:39<15:10,  2.39it/s] 31%|███       | 976/3145 [06:39<15:20,  2.36it/s] 32%|███▏      | 991/3143 [06:45<14:42,  2.44it/s] 31%|███▏      | 986/3145 [06:39<13:55,  2.58it/s] 31%|███       | 973/3145 [06:39<15:24,  2.35it/s] 31%|███       | 977/3145 [06:39<15:03,  2.40it/s] 31%|███▏      | 987/3145 [06:40<14:00,  2.57it/s] 32%|███▏      | 992/3143 [06:45<14:58,  2.39it/s] 31%|███       | 974/3145 [06:40<14:49,  2.44it/s] 31%|███       | 978/3145 [06:40<15:54,  2.27it/s] 31%|███▏      | 988/3145 [06:40<13:34,  2.65it/s] 32%|███▏      | 993/3143 [06:46<14:41,  2.44it/s] 31%|███       | 975/3145 [06:40<15:10,  2.38it/s] 31%|███       | 979/3145 [06:40<15:00,  2.40it/s] 31%|███▏      | 989/3145 [06:40<13:38,  2.63it/s] 32%|███▏      | 994/3143 [06:46<14:14,  2.52it/s] 31%|███       | 980/3145 [06:40<13:07,  2.75it/s] 31%|███       | 976/3145 [06:40<14:56,  2.42it/s] 31%|███▏      | 990/3145 [06:41<14:17,  2.51it/s] 31%|███       | 981/3145 [06:41<13:23,  2.69it/s] 31%|███       | 977/3145 [06:41<14:15,  2.53it/s] 32%|███▏      | 995/3143 [06:47<15:52,  2.25it/s] 32%|███▏      | 991/3145 [06:41<12:31,  2.87it/s] 31%|███       | 978/3145 [06:41<14:16,  2.53it/s] 32%|███▏      | 996/3143 [06:47<15:33,  2.30it/s] 31%|███       | 982/3145 [06:41<15:37,  2.31it/s] 32%|███▏      | 992/3145 [06:41<13:55,  2.58it/s] 31%|███       | 979/3145 [06:42<14:14,  2.54it/s] 32%|███▏      | 997/3143 [06:48<15:37,  2.29it/s] 32%|███▏      | 993/3145 [06:42<13:50,  2.59it/s] 31%|███▏      | 983/3145 [06:42<16:49,  2.14it/s] 32%|███▏      | 994/3145 [06:42<12:00,  2.99it/s] 31%|███       | 980/3145 [06:42<14:42,  2.45it/s] 32%|███▏      | 998/3143 [06:48<15:13,  2.35it/s] 31%|███▏      | 984/3145 [06:42<15:55,  2.26it/s] 32%|███▏      | 995/3145 [06:42<12:04,  2.97it/s] 32%|███▏      | 999/3143 [06:48<13:23,  2.67it/s] 31%|███       | 981/3145 [06:42<15:00,  2.40it/s] 32%|███▏      | 1000/3143 [06:49<12:13,  2.92it/s] 31%|███▏      | 985/3145 [06:43<15:49,  2.27it/s] 32%|███▏      | 996/3145 [06:43<12:07,  2.95it/s] 31%|███       | 982/3145 [06:43<14:34,  2.47it/s] 32%|███▏      | 1001/3143 [06:49<13:03,  2.73it/s] 31%|███▏      | 986/3145 [06:43<15:43,  2.29it/s] 32%|███▏      | 997/3145 [06:43<13:28,  2.66it/s] 31%|███▏      | 983/3145 [06:43<14:26,  2.49it/s] 32%|███▏      | 1002/3143 [06:49<12:57,  2.75it/s] 31%|███▏      | 987/3145 [06:44<15:44,  2.28it/s] 32%|███▏      | 998/3145 [06:44<14:03,  2.55it/s] 31%|███▏      | 984/3145 [06:44<14:55,  2.41it/s] 32%|███▏      | 1003/3143 [06:50<13:21,  2.67it/s] 31%|███▏      | 988/3145 [06:44<15:54,  2.26it/s] 32%|███▏      | 999/3145 [06:44<14:30,  2.46it/s] 31%|███▏      | 985/3145 [06:44<15:30,  2.32it/s] 32%|███▏      | 1004/3143 [06:50<13:42,  2.60it/s] 32%|███▏      | 1000/3145 [06:44<13:27,  2.66it/s] 31%|███▏      | 989/3145 [06:44<15:35,  2.30it/s] 31%|███▏      | 986/3145 [06:45<14:45,  2.44it/s] 32%|███▏      | 1005/3143 [06:51<14:15,  2.50it/s] 32%|███▏      | 1001/3145 [06:45<13:36,  2.63it/s] 31%|███▏      | 990/3145 [06:45<15:24,  2.33it/s] 31%|███▏      | 987/3145 [06:45<14:24,  2.50it/s] 32%|███▏      | 1006/3143 [06:51<13:06,  2.72it/s] 32%|███▏      | 1002/3145 [06:45<13:45,  2.59it/s] 31%|███▏      | 988/3145 [06:45<13:59,  2.57it/s] 32%|███▏      | 1007/3143 [06:51<12:27,  2.86it/s] 32%|███▏      | 991/3145 [06:45<15:46,  2.28it/s] 32%|███▏      | 1003/3145 [06:46<13:44,  2.60it/s] 32%|███▏      | 1008/3143 [06:52<12:59,  2.74it/s] 31%|███▏      | 989/3145 [06:46<14:21,  2.50it/s] 32%|███▏      | 992/3145 [06:46<15:56,  2.25it/s] 32%|███▏      | 1004/3145 [06:46<13:49,  2.58it/s] 32%|███▏      | 1009/3143 [06:52<13:18,  2.67it/s] 31%|███▏      | 990/3145 [06:46<14:12,  2.53it/s] 32%|███▏      | 993/3145 [06:46<16:04,  2.23it/s] 32%|███▏      | 1005/3145 [06:46<13:38,  2.62it/s] 32%|███▏      | 1010/3143 [06:52<13:29,  2.63it/s] 32%|███▏      | 991/3145 [06:47<14:47,  2.43it/s] 32%|███▏      | 994/3145 [06:47<15:33,  2.30it/s] 32%|███▏      | 1006/3145 [06:47<13:50,  2.58it/s] 32%|███▏      | 1011/3143 [06:53<14:11,  2.50it/s] 32%|███▏      | 992/3145 [06:47<15:35,  2.30it/s] 32%|███▏      | 1007/3145 [06:47<13:56,  2.56it/s] 32%|███▏      | 995/3145 [06:47<16:03,  2.23it/s] 32%|███▏      | 993/3145 [06:47<13:32,  2.65it/s] 32%|███▏      | 1012/3143 [06:53<14:37,  2.43it/s] 32%|███▏      | 1008/3145 [06:47<13:50,  2.57it/s] 32%|███▏      | 996/3145 [06:48<15:40,  2.28it/s] 32%|███▏      | 994/3145 [06:48<14:19,  2.50it/s] 32%|███▏      | 1013/3143 [06:54<14:32,  2.44it/s] 32%|███▏      | 1009/3145 [06:48<14:33,  2.45it/s] 32%|███▏      | 997/3145 [06:48<15:19,  2.34it/s] 32%|███▏      | 995/3145 [06:48<13:51,  2.59it/s] 32%|███▏      | 1014/3143 [06:54<14:21,  2.47it/s] 32%|███▏      | 998/3145 [06:48<14:38,  2.44it/s] 32%|███▏      | 1010/3145 [06:48<14:55,  2.38it/s] 32%|███▏      | 1015/3143 [06:54<12:46,  2.78it/s] 32%|███▏      | 996/3145 [06:48<14:09,  2.53it/s] 32%|███▏      | 999/3145 [06:49<14:41,  2.43it/s] 32%|███▏      | 1011/3145 [06:49<14:41,  2.42it/s] 32%|███▏      | 1016/3143 [06:55<12:48,  2.77it/s] 32%|███▏      | 997/3145 [06:49<13:59,  2.56it/s] 32%|███▏      | 1000/3145 [06:49<14:26,  2.47it/s] 32%|███▏      | 1012/3145 [06:49<14:24,  2.47it/s] 32%|███▏      | 1017/3143 [06:55<13:21,  2.65it/s] 32%|███▏      | 998/3145 [06:49<13:51,  2.58it/s] 32%|███▏      | 1001/3145 [06:50<14:22,  2.49it/s] 32%|███▏      | 1018/3143 [06:55<13:03,  2.71it/s] 32%|███▏      | 1013/3145 [06:50<14:42,  2.42it/s] 32%|███▏      | 999/3145 [06:50<14:53,  2.40it/s] 32%|███▏      | 1002/3145 [06:50<14:09,  2.52it/s] 32%|███▏      | 1019/3143 [06:56<13:21,  2.65it/s] 32%|███▏      | 1014/3145 [06:50<14:29,  2.45it/s] 32%|███▏      | 1000/3145 [06:50<14:51,  2.41it/s] 32%|███▏      | 1003/3145 [06:50<14:25,  2.48it/s] 32%|███▏      | 1020/3143 [06:56<14:11,  2.49it/s] 32%|███▏      | 1015/3145 [06:50<14:44,  2.41it/s] 32%|███▏      | 1001/3145 [06:51<15:05,  2.37it/s] 32%|███▏      | 1004/3145 [06:51<14:29,  2.46it/s] 32%|███▏      | 1021/3143 [06:57<13:35,  2.60it/s] 32%|███▏      | 1016/3145 [06:51<14:17,  2.48it/s] 32%|███▏      | 1002/3145 [06:51<15:57,  2.24it/s] 32%|███▏      | 1005/3145 [06:51<14:44,  2.42it/s] 33%|███▎      | 1022/3143 [06:57<13:50,  2.55it/s] 32%|███▏      | 1017/3145 [06:51<14:39,  2.42it/s] 32%|███▏      | 1006/3145 [06:52<14:00,  2.54it/s] 33%|███▎      | 1023/3143 [06:57<14:23,  2.46it/s] 32%|███▏      | 1003/3145 [06:52<17:09,  2.08it/s] 32%|███▏      | 1018/3145 [06:52<14:54,  2.38it/s] 33%|███▎      | 1024/3143 [06:58<12:52,  2.74it/s] 32%|███▏      | 1007/3145 [06:52<14:07,  2.52it/s] 32%|███▏      | 1004/3145 [06:52<14:36,  2.44it/s] 32%|███▏      | 1019/3145 [06:52<14:35,  2.43it/s] 32%|███▏      | 1005/3145 [06:52<13:36,  2.62it/s] 32%|███▏      | 1008/3145 [06:52<14:06,  2.53it/s] 33%|███▎      | 1025/3143 [06:58<13:54,  2.54it/s] 32%|███▏      | 1020/3145 [06:53<15:05,  2.35it/s] 32%|███▏      | 1009/3145 [06:53<12:46,  2.79it/s] 32%|███▏      | 1006/3145 [06:53<14:13,  2.50it/s] 33%|███▎      | 1026/3143 [06:59<14:21,  2.46it/s] 32%|███▏      | 1010/3145 [06:53<13:15,  2.68it/s] 32%|███▏      | 1021/3145 [06:53<15:28,  2.29it/s] 32%|███▏      | 1007/3145 [06:53<14:23,  2.48it/s] 33%|███▎      | 1027/3143 [06:59<15:09,  2.33it/s] 32%|███▏      | 1011/3145 [06:53<14:12,  2.50it/s] 32%|███▏      | 1008/3145 [06:53<14:19,  2.49it/s] 32%|███▏      | 1022/3145 [06:54<16:32,  2.14it/s] 33%|███▎      | 1028/3143 [07:00<14:33,  2.42it/s] 32%|███▏      | 1012/3145 [06:54<14:08,  2.51it/s] 32%|███▏      | 1009/3145 [06:54<14:46,  2.41it/s] 33%|███▎      | 1023/3145 [06:54<16:10,  2.19it/s] 33%|███▎      | 1029/3143 [07:00<14:48,  2.38it/s] 32%|███▏      | 1013/3145 [06:54<14:03,  2.53it/s] 33%|███▎      | 1024/3145 [06:54<14:34,  2.43it/s] 32%|███▏      | 1010/3145 [06:54<14:50,  2.40it/s] 33%|███▎      | 1030/3143 [07:00<14:25,  2.44it/s] 32%|███▏      | 1011/3145 [06:55<13:01,  2.73it/s] 32%|███▏      | 1014/3145 [06:55<14:16,  2.49it/s] 33%|███▎      | 1025/3145 [06:55<14:56,  2.36it/s] 33%|███▎      | 1031/3143 [07:01<14:13,  2.47it/s] 32%|███▏      | 1012/3145 [06:55<13:38,  2.61it/s] 32%|███▏      | 1015/3145 [06:55<14:35,  2.43it/s] 33%|███▎      | 1026/3145 [06:55<14:39,  2.41it/s] 33%|███▎      | 1032/3143 [07:01<14:32,  2.42it/s] 32%|███▏      | 1013/3145 [06:55<13:26,  2.64it/s] 33%|███▎      | 1033/3143 [07:01<12:29,  2.82it/s] 33%|███▎      | 1027/3145 [06:56<14:29,  2.44it/s] 32%|███▏      | 1016/3145 [06:56<15:28,  2.29it/s] 32%|███▏      | 1014/3145 [06:56<14:02,  2.53it/s] 33%|███▎      | 1028/3145 [06:56<14:21,  2.46it/s] 33%|███▎      | 1034/3143 [07:02<13:39,  2.57it/s] 32%|███▏      | 1017/3145 [06:56<16:54,  2.10it/s] 32%|███▏      | 1015/3145 [06:56<14:35,  2.43it/s] 33%|███▎      | 1029/3145 [06:56<13:54,  2.54it/s] 33%|███▎      | 1035/3143 [07:02<13:18,  2.64it/s] 32%|███▏      | 1018/3145 [06:57<16:12,  2.19it/s] 33%|███▎      | 1030/3145 [06:57<13:47,  2.56it/s] 32%|███▏      | 1016/3145 [06:57<15:07,  2.35it/s] 33%|███▎      | 1036/3143 [07:03<13:44,  2.55it/s] 32%|███▏      | 1019/3145 [06:57<15:19,  2.31it/s] 33%|███▎      | 1031/3145 [06:57<13:26,  2.62it/s] 33%|███▎      | 1037/3143 [07:03<14:04,  2.49it/s] 32%|███▏      | 1017/3145 [06:57<17:02,  2.08it/s] 32%|███▏      | 1020/3145 [06:57<15:19,  2.31it/s] 33%|███▎      | 1032/3145 [06:57<13:16,  2.65it/s] 33%|███▎      | 1038/3143 [07:03<13:56,  2.52it/s] 32%|███▏      | 1021/3145 [06:58<14:49,  2.39it/s] 32%|███▏      | 1018/3145 [06:58<16:38,  2.13it/s] 33%|███▎      | 1033/3145 [06:58<14:07,  2.49it/s] 33%|███▎      | 1039/3143 [07:04<14:23,  2.44it/s] 32%|███▏      | 1019/3145 [06:58<15:26,  2.29it/s] 32%|███▏      | 1022/3145 [06:58<15:08,  2.34it/s] 33%|███▎      | 1034/3145 [06:58<14:40,  2.40it/s] 33%|███▎      | 1040/3143 [07:04<14:16,  2.46it/s] 32%|███▏      | 1020/3145 [06:58<14:53,  2.38it/s] 33%|███▎      | 1023/3145 [06:59<15:11,  2.33it/s] 33%|███▎      | 1035/3145 [06:59<14:20,  2.45it/s] 33%|███▎      | 1041/3143 [07:05<14:03,  2.49it/s] 32%|███▏      | 1021/3145 [06:59<14:28,  2.45it/s] 33%|███▎      | 1024/3145 [06:59<15:21,  2.30it/s] 33%|███▎      | 1036/3145 [06:59<14:27,  2.43it/s] 33%|███▎      | 1042/3143 [07:05<14:52,  2.35it/s] 32%|███▏      | 1022/3145 [06:59<14:15,  2.48it/s] 33%|███▎      | 1037/3145 [06:59<14:11,  2.48it/s] 33%|███▎      | 1025/3145 [07:00<15:35,  2.27it/s] 33%|███▎      | 1023/3145 [07:00<13:57,  2.53it/s] 33%|███▎      | 1043/3143 [07:06<14:42,  2.38it/s] 33%|███▎      | 1038/3145 [07:00<14:16,  2.46it/s] 33%|███▎      | 1026/3145 [07:00<15:33,  2.27it/s] 33%|███▎      | 1024/3145 [07:00<14:08,  2.50it/s] 33%|███▎      | 1044/3143 [07:06<14:49,  2.36it/s] 33%|███▎      | 1039/3145 [07:00<14:10,  2.48it/s] 33%|███▎      | 1027/3145 [07:00<15:07,  2.33it/s] 33%|███▎      | 1025/3145 [07:00<14:12,  2.49it/s] 33%|███▎      | 1045/3143 [07:06<15:00,  2.33it/s] 33%|███▎      | 1040/3145 [07:01<13:41,  2.56it/s] 33%|███▎      | 1026/3145 [07:01<13:45,  2.57it/s] 33%|███▎      | 1028/3145 [07:01<15:38,  2.26it/s] 33%|███▎      | 1046/3143 [07:07<14:32,  2.40it/s] 33%|███▎      | 1041/3145 [07:01<13:44,  2.55it/s] 33%|███▎      | 1027/3145 [07:01<13:53,  2.54it/s] 33%|███▎      | 1047/3143 [07:07<14:49,  2.36it/s] 33%|███▎      | 1029/3145 [07:01<17:03,  2.07it/s] 33%|███▎      | 1042/3145 [07:02<15:03,  2.33it/s] 33%|███▎      | 1028/3145 [07:02<13:57,  2.53it/s] 33%|███▎      | 1048/3143 [07:08<14:15,  2.45it/s] 33%|███▎      | 1030/3145 [07:02<16:42,  2.11it/s] 33%|███▎      | 1043/3145 [07:02<14:19,  2.45it/s] 33%|███▎      | 1029/3145 [07:02<14:13,  2.48it/s] 33%|███▎      | 1049/3143 [07:08<14:19,  2.44it/s] 33%|███▎      | 1031/3145 [07:02<16:16,  2.17it/s] 33%|███▎      | 1030/3145 [07:02<14:03,  2.51it/s] 33%|███▎      | 1044/3145 [07:03<16:00,  2.19it/s] 33%|███▎      | 1050/3143 [07:08<14:14,  2.45it/s] 33%|███▎      | 1032/3145 [07:03<15:56,  2.21it/s] 33%|███▎      | 1045/3145 [07:03<14:59,  2.34it/s] 33%|███▎      | 1031/3145 [07:03<14:48,  2.38it/s] 33%|███▎      | 1051/3143 [07:09<14:22,  2.42it/s] 33%|███▎      | 1033/3145 [07:03<15:44,  2.24it/s] 33%|███▎      | 1032/3145 [07:03<14:28,  2.43it/s] 33%|███▎      | 1052/3143 [07:09<14:19,  2.43it/s] 33%|███▎      | 1046/3145 [07:03<16:24,  2.13it/s] 33%|███▎      | 1034/3145 [07:03<13:49,  2.55it/s] 33%|███▎      | 1033/3145 [07:04<14:29,  2.43it/s] 34%|███▎      | 1053/3143 [07:10<13:55,  2.50it/s] 33%|███▎      | 1035/3145 [07:04<14:18,  2.46it/s] 33%|███▎      | 1047/3145 [07:04<17:29,  2.00it/s] 33%|███▎      | 1034/3145 [07:04<14:24,  2.44it/s] 34%|███▎      | 1054/3143 [07:10<14:16,  2.44it/s] 33%|███▎      | 1036/3145 [07:04<14:20,  2.45it/s] 33%|███▎      | 1048/3145 [07:04<17:05,  2.04it/s] 33%|███▎      | 1035/3145 [07:05<14:18,  2.46it/s] 34%|███▎      | 1055/3143 [07:11<14:22,  2.42it/s] 33%|███▎      | 1037/3145 [07:05<13:48,  2.54it/s] 33%|███▎      | 1049/3145 [07:05<16:20,  2.14it/s] 33%|███▎      | 1036/3145 [07:05<14:29,  2.43it/s] 34%|███▎      | 1056/3143 [07:11<14:10,  2.45it/s] 33%|███▎      | 1038/3145 [07:05<13:43,  2.56it/s] 33%|███▎      | 1050/3145 [07:05<15:59,  2.18it/s] 33%|███▎      | 1037/3145 [07:05<14:33,  2.41it/s] 34%|███▎      | 1057/3143 [07:11<14:08,  2.46it/s] 33%|███▎      | 1039/3145 [07:05<13:46,  2.55it/s] 33%|███▎      | 1051/3145 [07:06<13:47,  2.53it/s] 33%|███▎      | 1040/3145 [07:06<12:24,  2.83it/s] 33%|███▎      | 1038/3145 [07:06<14:16,  2.46it/s] 34%|███▎      | 1058/3143 [07:12<14:31,  2.39it/s] 33%|███▎      | 1052/3145 [07:06<13:48,  2.53it/s] 33%|███▎      | 1041/3145 [07:06<12:54,  2.72it/s] 33%|███▎      | 1039/3145 [07:06<14:36,  2.40it/s] 34%|███▎      | 1059/3143 [07:12<14:31,  2.39it/s] 33%|███▎      | 1053/3145 [07:06<13:27,  2.59it/s] 33%|███▎      | 1042/3145 [07:07<13:28,  2.60it/s] 33%|███▎      | 1040/3145 [07:07<14:19,  2.45it/s] 34%|███▎      | 1060/3143 [07:13<13:59,  2.48it/s] 34%|███▎      | 1054/3145 [07:07<13:59,  2.49it/s] 33%|███▎      | 1043/3145 [07:07<12:21,  2.83it/s] 33%|███▎      | 1041/3145 [07:07<13:44,  2.55it/s] 34%|███▍      | 1061/3143 [07:13<14:09,  2.45it/s] 34%|███▎      | 1055/3145 [07:07<13:34,  2.57it/s] 33%|███▎      | 1044/3145 [07:07<12:32,  2.79it/s] 33%|███▎      | 1042/3145 [07:07<14:58,  2.34it/s] 34%|███▍      | 1062/3143 [07:13<14:02,  2.47it/s] 34%|███▎      | 1056/3145 [07:07<13:07,  2.65it/s] 33%|███▎      | 1045/3145 [07:08<13:06,  2.67it/s] 33%|███▎      | 1043/3145 [07:08<14:53,  2.35it/s] 34%|███▎      | 1057/3145 [07:08<13:30,  2.58it/s] 34%|███▍      | 1063/3143 [07:14<14:30,  2.39it/s] 33%|███▎      | 1046/3145 [07:08<13:39,  2.56it/s] 34%|███▎      | 1058/3145 [07:08<13:20,  2.61it/s] 33%|███▎      | 1044/3145 [07:08<15:03,  2.32it/s] 34%|███▍      | 1064/3143 [07:14<14:53,  2.33it/s] 33%|███▎      | 1047/3145 [07:09<15:32,  2.25it/s] 33%|███▎      | 1045/3145 [07:09<14:14,  2.46it/s] 34%|███▎      | 1059/3145 [07:09<13:51,  2.51it/s] 34%|███▍      | 1065/3143 [07:15<14:30,  2.39it/s] 33%|███▎      | 1048/3145 [07:09<15:36,  2.24it/s] 34%|███▎      | 1060/3145 [07:09<13:30,  2.57it/s] 33%|███▎      | 1046/3145 [07:09<14:45,  2.37it/s] 34%|███▍      | 1066/3143 [07:15<14:31,  2.38it/s] 33%|███▎      | 1049/3145 [07:09<14:57,  2.33it/s] 34%|███▎      | 1061/3145 [07:09<13:48,  2.51it/s] 34%|███▍      | 1067/3143 [07:15<13:28,  2.57it/s] 33%|███▎      | 1047/3145 [07:10<14:27,  2.42it/s] 33%|███▎      | 1050/3145 [07:10<14:23,  2.43it/s] 34%|███▍      | 1062/3145 [07:10<13:45,  2.52it/s] 34%|███▍      | 1068/3143 [07:16<13:13,  2.61it/s] 33%|███▎      | 1048/3145 [07:10<14:40,  2.38it/s] 33%|███▎      | 1051/3145 [07:10<14:08,  2.47it/s] 34%|███▍      | 1063/3145 [07:10<13:24,  2.59it/s] 34%|███▍      | 1069/3143 [07:16<13:28,  2.56it/s] 33%|███▎      | 1049/3145 [07:10<14:41,  2.38it/s] 33%|███▎      | 1052/3145 [07:10<12:30,  2.79it/s] 34%|███▍      | 1064/3145 [07:11<13:07,  2.64it/s] 34%|███▍      | 1070/3143 [07:17<13:27,  2.57it/s] 33%|███▎      | 1053/3145 [07:11<12:20,  2.83it/s] 33%|███▎      | 1050/3145 [07:11<14:35,  2.39it/s] 34%|███▍      | 1065/3145 [07:11<13:43,  2.53it/s] 34%|███▍      | 1071/3143 [07:17<13:10,  2.62it/s] 34%|███▎      | 1054/3145 [07:11<13:00,  2.68it/s] 33%|███▎      | 1051/3145 [07:11<14:50,  2.35it/s] 34%|███▍      | 1066/3145 [07:11<12:00,  2.89it/s] 34%|███▍      | 1072/3143 [07:17<13:19,  2.59it/s] 34%|███▍      | 1067/3145 [07:12<12:10,  2.84it/s] 33%|███▎      | 1052/3145 [07:12<14:44,  2.37it/s] 34%|███▎      | 1055/3145 [07:12<14:11,  2.46it/s] 34%|███▍      | 1073/3143 [07:18<13:49,  2.49it/s] 34%|███▎      | 1056/3145 [07:12<12:51,  2.71it/s] 33%|███▎      | 1053/3145 [07:12<14:08,  2.47it/s] 34%|███▍      | 1068/3145 [07:12<12:53,  2.68it/s] 34%|███▍      | 1074/3143 [07:18<14:24,  2.39it/s] 34%|███▎      | 1057/3145 [07:12<13:09,  2.65it/s] 34%|███▎      | 1054/3145 [07:12<13:41,  2.55it/s] 34%|███▍      | 1069/3145 [07:12<13:09,  2.63it/s] 34%|███▍      | 1075/3143 [07:19<14:12,  2.42it/s] 34%|███▎      | 1058/3145 [07:13<13:15,  2.62it/s] 34%|███▎      | 1055/3145 [07:13<13:41,  2.54it/s] 34%|███▍      | 1070/3145 [07:13<13:42,  2.52it/s] 34%|███▎      | 1059/3145 [07:13<13:14,  2.63it/s] 34%|███▍      | 1076/3143 [07:19<14:26,  2.39it/s] 34%|███▎      | 1056/3145 [07:13<13:41,  2.54it/s] 34%|███▍      | 1071/3145 [07:13<13:28,  2.56it/s] 34%|███▎      | 1060/3145 [07:14<13:15,  2.62it/s] 34%|███▍      | 1077/3143 [07:19<14:08,  2.44it/s] 34%|███▎      | 1057/3145 [07:14<13:44,  2.53it/s] 34%|███▍      | 1072/3145 [07:14<13:27,  2.57it/s] 34%|███▎      | 1061/3145 [07:14<13:19,  2.61it/s] 34%|███▎      | 1058/3145 [07:14<13:40,  2.54it/s] 34%|███▍      | 1078/3143 [07:20<14:14,  2.42it/s] 34%|███▍      | 1073/3145 [07:14<14:10,  2.44it/s] 34%|███▎      | 1059/3145 [07:14<13:32,  2.57it/s] 34%|███▍      | 1062/3145 [07:14<14:13,  2.44it/s] 34%|███▍      | 1079/3143 [07:20<14:24,  2.39it/s] 34%|███▍      | 1074/3145 [07:14<13:56,  2.48it/s] 34%|███▍      | 1080/3143 [07:21<12:31,  2.74it/s] 34%|███▍      | 1075/3145 [07:15<12:17,  2.81it/s] 34%|███▍      | 1063/3145 [07:15<14:02,  2.47it/s] 34%|███▎      | 1060/3145 [07:15<14:04,  2.47it/s] 34%|███▍      | 1081/3143 [07:21<12:55,  2.66it/s] 34%|███▍      | 1076/3145 [07:15<12:28,  2.77it/s] 34%|███▍      | 1064/3145 [07:15<13:54,  2.49it/s] 34%|███▎      | 1061/3145 [07:15<14:33,  2.39it/s] 34%|███▍      | 1082/3143 [07:21<13:04,  2.63it/s] 34%|███▍      | 1077/3145 [07:16<12:44,  2.70it/s] 34%|███▍      | 1065/3145 [07:16<13:53,  2.50it/s] 34%|███▍      | 1062/3145 [07:16<14:16,  2.43it/s] 34%|███▍      | 1083/3143 [07:22<13:11,  2.60it/s] 34%|███▍      | 1078/3145 [07:16<12:53,  2.67it/s] 34%|███▍      | 1066/3145 [07:16<13:22,  2.59it/s] 34%|███▍      | 1063/3145 [07:16<13:46,  2.52it/s] 34%|███▍      | 1084/3143 [07:22<13:19,  2.58it/s] 34%|███▍      | 1079/3145 [07:16<12:37,  2.73it/s] 34%|███▍      | 1067/3145 [07:16<14:05,  2.46it/s] 34%|███▍      | 1064/3145 [07:17<15:35,  2.22it/s] 35%|███▍      | 1085/3143 [07:22<13:13,  2.60it/s] 34%|███▍      | 1080/3145 [07:17<12:42,  2.71it/s] 34%|███▍      | 1068/3145 [07:17<13:56,  2.48it/s] 34%|███▍      | 1065/3145 [07:17<15:03,  2.30it/s] 35%|███▍      | 1086/3143 [07:23<12:59,  2.64it/s] 34%|███▍      | 1081/3145 [07:17<13:05,  2.63it/s] 34%|███▍      | 1069/3145 [07:17<13:52,  2.49it/s] 34%|███▍      | 1082/3145 [07:17<12:44,  2.70it/s] 35%|███▍      | 1087/3143 [07:23<13:27,  2.54it/s] 34%|███▍      | 1066/3145 [07:18<16:32,  2.10it/s] 34%|███▍      | 1070/3145 [07:18<14:11,  2.44it/s] 35%|███▍      | 1088/3143 [07:24<11:49,  2.90it/s] 34%|███▍      | 1083/3145 [07:18<12:42,  2.70it/s] 34%|███▍      | 1071/3145 [07:18<14:01,  2.46it/s] 35%|███▍      | 1089/3143 [07:24<12:15,  2.79it/s] 34%|███▍      | 1084/3145 [07:18<12:42,  2.70it/s] 34%|███▍      | 1067/3145 [07:18<17:37,  1.96it/s] 34%|███▍      | 1072/3145 [07:18<13:48,  2.50it/s] 35%|███▍      | 1090/3143 [07:24<12:38,  2.71it/s] 34%|███▍      | 1085/3145 [07:18<12:53,  2.66it/s] 34%|███▍      | 1068/3145 [07:19<17:06,  2.02it/s] 34%|███▍      | 1073/3145 [07:19<13:14,  2.61it/s] 35%|███▍      | 1086/3145 [07:19<12:45,  2.69it/s] 35%|███▍      | 1091/3143 [07:25<13:27,  2.54it/s] 34%|███▍      | 1074/3145 [07:19<11:32,  2.99it/s] 34%|███▍      | 1069/3145 [07:19<16:44,  2.07it/s] 35%|███▍      | 1087/3145 [07:19<12:41,  2.70it/s] 35%|███▍      | 1092/3143 [07:25<14:06,  2.42it/s] 34%|███▍      | 1075/3145 [07:19<12:32,  2.75it/s] 34%|███▍      | 1070/3145 [07:19<15:27,  2.24it/s] 35%|███▍      | 1088/3145 [07:20<12:49,  2.67it/s] 35%|███▍      | 1093/3143 [07:26<13:35,  2.51it/s] 34%|███▍      | 1076/3145 [07:20<13:28,  2.56it/s] 34%|███▍      | 1071/3145 [07:20<15:38,  2.21it/s] 35%|███▍      | 1094/3143 [07:26<12:07,  2.82it/s] 35%|███▍      | 1089/3145 [07:20<13:03,  2.62it/s] 34%|███▍      | 1072/3145 [07:20<13:32,  2.55it/s] 34%|███▍      | 1077/3145 [07:20<14:10,  2.43it/s] 35%|███▍      | 1095/3143 [07:26<12:23,  2.75it/s] 35%|███▍      | 1090/3145 [07:20<13:07,  2.61it/s] 34%|███▍      | 1073/3145 [07:21<14:19,  2.41it/s] 34%|███▍      | 1078/3145 [07:21<13:59,  2.46it/s] 35%|███▍      | 1096/3143 [07:27<12:46,  2.67it/s] 35%|███▍      | 1091/3145 [07:21<13:41,  2.50it/s] 34%|███▍      | 1079/3145 [07:21<13:29,  2.55it/s] 35%|███▍      | 1097/3143 [07:27<12:47,  2.67it/s] 34%|███▍      | 1074/3145 [07:21<15:34,  2.22it/s] 35%|███▍      | 1092/3145 [07:21<13:39,  2.50it/s] 34%|███▍      | 1080/3145 [07:21<13:07,  2.62it/s] 35%|███▍      | 1098/3143 [07:27<13:18,  2.56it/s] 34%|███▍      | 1075/3145 [07:22<15:44,  2.19it/s] 35%|███▍      | 1093/3145 [07:22<13:37,  2.51it/s] 34%|███▍      | 1081/3145 [07:22<13:38,  2.52it/s] 35%|███▍      | 1099/3143 [07:28<13:22,  2.55it/s] 34%|███▍      | 1076/3145 [07:22<14:57,  2.31it/s] 35%|███▍      | 1094/3145 [07:22<13:39,  2.50it/s] 35%|███▍      | 1100/3143 [07:28<12:33,  2.71it/s] 34%|███▍      | 1082/3145 [07:22<15:23,  2.23it/s] 34%|███▍      | 1077/3145 [07:22<15:01,  2.29it/s] 35%|███▍      | 1095/3145 [07:22<14:00,  2.44it/s] 35%|███▌      | 1101/3143 [07:28<11:51,  2.87it/s] 34%|███▍      | 1083/3145 [07:23<14:50,  2.32it/s] 34%|███▍      | 1078/3145 [07:23<14:41,  2.34it/s] 35%|███▍      | 1096/3145 [07:23<14:18,  2.39it/s] 35%|███▌      | 1102/3143 [07:29<12:49,  2.65it/s] 34%|███▍      | 1084/3145 [07:23<14:25,  2.38it/s] 35%|███▍      | 1097/3145 [07:23<13:03,  2.61it/s] 34%|███▍      | 1079/3145 [07:23<14:39,  2.35it/s] 35%|███▌      | 1103/3143 [07:29<13:10,  2.58it/s] 34%|███▍      | 1085/3145 [07:24<13:48,  2.49it/s] 35%|███▍      | 1098/3145 [07:24<14:07,  2.42it/s] 34%|███▍      | 1080/3145 [07:24<15:20,  2.24it/s] 35%|███▌      | 1104/3143 [07:30<13:36,  2.50it/s] 35%|███▍      | 1099/3145 [07:24<13:52,  2.46it/s] 34%|███▍      | 1081/3145 [07:24<14:34,  2.36it/s] 35%|███▍      | 1086/3145 [07:24<15:53,  2.16it/s] 35%|███▌      | 1105/3143 [07:30<14:10,  2.40it/s] 34%|███▍      | 1082/3145 [07:24<14:12,  2.42it/s] 35%|███▍      | 1100/3145 [07:25<14:24,  2.37it/s] 35%|███▌      | 1106/3143 [07:30<13:04,  2.60it/s] 35%|███▍      | 1087/3145 [07:25<15:49,  2.17it/s] 35%|███▍      | 1088/3145 [07:25<14:06,  2.43it/s] 35%|███▌      | 1101/3145 [07:25<13:52,  2.46it/s] 34%|███▍      | 1083/3145 [07:25<14:16,  2.41it/s] 35%|███▌      | 1107/3143 [07:31<13:21,  2.54it/s] 34%|███▍      | 1084/3145 [07:25<14:04,  2.44it/s] 35%|███▌      | 1102/3145 [07:25<13:55,  2.44it/s] 35%|███▍      | 1089/3145 [07:25<14:17,  2.40it/s] 35%|███▌      | 1108/3143 [07:31<13:43,  2.47it/s] 35%|███▍      | 1090/3145 [07:26<14:31,  2.36it/s] 35%|███▌      | 1103/3145 [07:26<14:28,  2.35it/s] 34%|███▍      | 1085/3145 [07:26<14:47,  2.32it/s] 35%|███▌      | 1109/3143 [07:32<13:23,  2.53it/s] 35%|███▍      | 1091/3145 [07:26<14:29,  2.36it/s] 35%|███▍      | 1086/3145 [07:26<14:28,  2.37it/s] 35%|███▌      | 1104/3145 [07:26<14:36,  2.33it/s] 35%|███▌      | 1110/3143 [07:32<13:38,  2.49it/s] 35%|███▍      | 1092/3145 [07:27<14:05,  2.43it/s] 35%|███▌      | 1105/3145 [07:27<14:13,  2.39it/s] 35%|███▍      | 1087/3145 [07:27<14:42,  2.33it/s] 35%|███▌      | 1111/3143 [07:33<14:06,  2.40it/s] 35%|███▍      | 1088/3145 [07:27<12:30,  2.74it/s] 35%|███▍      | 1093/3145 [07:27<14:19,  2.39it/s] 35%|███▌      | 1106/3145 [07:27<13:58,  2.43it/s] 35%|███▌      | 1112/3143 [07:33<13:55,  2.43it/s] 35%|███▍      | 1089/3145 [07:27<12:51,  2.67it/s] 35%|███▌      | 1107/3145 [07:27<14:24,  2.36it/s] 35%|███▌      | 1113/3143 [07:33<14:01,  2.41it/s] 35%|███▍      | 1094/3145 [07:28<16:20,  2.09it/s] 35%|███▍      | 1090/3145 [07:28<14:40,  2.33it/s] 35%|███▌      | 1108/3145 [07:28<14:28,  2.35it/s] 35%|███▌      | 1114/3143 [07:34<14:13,  2.38it/s] 35%|███▍      | 1095/3145 [07:28<15:42,  2.17it/s] 35%|███▍      | 1091/3145 [07:28<14:15,  2.40it/s] 35%|███▌      | 1109/3145 [07:28<13:27,  2.52it/s] 35%|███▌      | 1115/3143 [07:34<14:20,  2.36it/s] 35%|███▍      | 1096/3145 [07:28<15:24,  2.22it/s] 36%|███▌      | 1116/3143 [07:34<12:17,  2.75it/s] 35%|███▍      | 1092/3145 [07:29<14:24,  2.37it/s] 35%|███▌      | 1110/3145 [07:29<13:39,  2.48it/s] 35%|███▍      | 1097/3145 [07:29<15:04,  2.26it/s] 36%|███▌      | 1117/3143 [07:35<12:27,  2.71it/s] 35%|███▍      | 1093/3145 [07:29<14:13,  2.40it/s] 35%|███▌      | 1111/3145 [07:29<14:02,  2.41it/s] 35%|███▍      | 1098/3145 [07:29<14:36,  2.34it/s] 36%|███▌      | 1118/3143 [07:35<12:47,  2.64it/s] 35%|███▍      | 1094/3145 [07:29<14:03,  2.43it/s] 35%|███▌      | 1112/3145 [07:29<13:49,  2.45it/s] 35%|███▍      | 1095/3145 [07:30<12:08,  2.81it/s] 35%|███▍      | 1099/3145 [07:30<14:28,  2.35it/s] 36%|███▌      | 1119/3143 [07:36<13:13,  2.55it/s] 35%|███▌      | 1113/3145 [07:30<13:55,  2.43it/s] 35%|███▍      | 1096/3145 [07:30<13:23,  2.55it/s] 35%|███▍      | 1100/3145 [07:30<14:47,  2.31it/s] 35%|███▌      | 1114/3145 [07:30<13:28,  2.51it/s] 36%|███▌      | 1120/3143 [07:36<14:06,  2.39it/s] 35%|███▍      | 1097/3145 [07:30<13:09,  2.60it/s] 35%|███▌      | 1101/3145 [07:31<14:23,  2.37it/s] 35%|███▌      | 1115/3145 [07:31<13:06,  2.58it/s] 36%|███▌      | 1121/3143 [07:37<14:12,  2.37it/s] 35%|███▌      | 1102/3145 [07:31<13:12,  2.58it/s] 35%|███▍      | 1098/3145 [07:31<13:16,  2.57it/s] 35%|███▌      | 1116/3145 [07:31<12:54,  2.62it/s] 36%|███▌      | 1122/3143 [07:37<14:21,  2.35it/s] 35%|███▌      | 1103/3145 [07:31<13:21,  2.55it/s] 35%|███▍      | 1099/3145 [07:31<13:39,  2.50it/s] 36%|███▌      | 1123/3143 [07:37<12:36,  2.67it/s] 36%|███▌      | 1117/3145 [07:31<13:10,  2.57it/s] 35%|███▌      | 1104/3145 [07:32<13:24,  2.54it/s] 36%|███▌      | 1124/3143 [07:38<12:44,  2.64it/s] 35%|███▍      | 1100/3145 [07:32<14:18,  2.38it/s] 36%|███▌      | 1118/3145 [07:32<13:18,  2.54it/s] 35%|███▌      | 1105/3145 [07:32<13:39,  2.49it/s] 36%|███▌      | 1125/3143 [07:38<12:49,  2.62it/s] 35%|███▌      | 1101/3145 [07:32<13:55,  2.45it/s] 36%|███▌      | 1119/3145 [07:32<13:25,  2.52it/s] 35%|███▌      | 1106/3145 [07:32<13:10,  2.58it/s] 36%|███▌      | 1126/3143 [07:38<12:46,  2.63it/s] 35%|███▌      | 1102/3145 [07:33<13:49,  2.46it/s] 36%|███▌      | 1120/3145 [07:33<13:45,  2.45it/s] 35%|███▌      | 1107/3145 [07:33<13:15,  2.56it/s] 35%|███▌      | 1103/3145 [07:33<14:08,  2.41it/s] 36%|███▌      | 1127/3143 [07:39<13:44,  2.44it/s] 36%|███▌      | 1121/3145 [07:33<14:03,  2.40it/s] 35%|███▌      | 1108/3145 [07:33<14:24,  2.36it/s] 36%|███▌      | 1128/3143 [07:39<13:06,  2.56it/s] 35%|███▌      | 1104/3145 [07:33<13:52,  2.45it/s] 36%|███▌      | 1122/3145 [07:34<14:17,  2.36it/s] 36%|███▌      | 1129/3143 [07:40<12:36,  2.66it/s] 35%|███▌      | 1109/3145 [07:34<14:30,  2.34it/s] 36%|███▌      | 1123/3145 [07:34<13:01,  2.59it/s] 35%|███▌      | 1105/3145 [07:34<14:13,  2.39it/s] 36%|███▌      | 1130/3143 [07:40<12:39,  2.65it/s] 35%|███▌      | 1110/3145 [07:34<13:51,  2.45it/s] 36%|███▌      | 1124/3145 [07:34<12:32,  2.69it/s] 35%|███▌      | 1106/3145 [07:34<13:54,  2.44it/s] 35%|███▌      | 1111/3145 [07:34<12:40,  2.67it/s] 36%|███▌      | 1131/3143 [07:40<12:45,  2.63it/s] 36%|███▌      | 1125/3145 [07:34<12:14,  2.75it/s] 35%|███▌      | 1107/3145 [07:35<13:48,  2.46it/s] 35%|███▌      | 1112/3145 [07:35<12:40,  2.67it/s] 36%|███▌      | 1126/3145 [07:35<11:39,  2.89it/s] 36%|███▌      | 1132/3143 [07:41<13:16,  2.52it/s] 35%|███▌      | 1108/3145 [07:35<13:50,  2.45it/s] 36%|███▌      | 1127/3145 [07:35<12:14,  2.75it/s] 35%|███▌      | 1113/3145 [07:35<13:52,  2.44it/s] 36%|███▌      | 1133/3143 [07:41<13:54,  2.41it/s] 35%|███▌      | 1109/3145 [07:35<14:14,  2.38it/s] 36%|███▌      | 1128/3145 [07:36<12:32,  2.68it/s] 35%|███▌      | 1114/3145 [07:36<14:05,  2.40it/s] 36%|███▌      | 1134/3143 [07:42<14:04,  2.38it/s] 35%|███▌      | 1110/3145 [07:36<13:44,  2.47it/s] 36%|███▌      | 1129/3145 [07:36<12:18,  2.73it/s] 36%|███▌      | 1135/3143 [07:42<13:28,  2.48it/s] 35%|███▌      | 1115/3145 [07:36<14:43,  2.30it/s] 35%|███▌      | 1111/3145 [07:36<13:35,  2.49it/s] 36%|███▌      | 1130/3145 [07:36<12:37,  2.66it/s] 36%|███▌      | 1136/3143 [07:42<13:23,  2.50it/s] 35%|███▌      | 1116/3145 [07:37<14:19,  2.36it/s] 35%|███▌      | 1112/3145 [07:37<14:07,  2.40it/s] 36%|███▌      | 1131/3145 [07:37<12:50,  2.61it/s] 36%|███▌      | 1117/3145 [07:37<13:37,  2.48it/s] 36%|███▌      | 1137/3143 [07:43<13:32,  2.47it/s] 35%|███▌      | 1113/3145 [07:37<13:55,  2.43it/s] 36%|███▌      | 1132/3145 [07:37<12:53,  2.60it/s] 36%|███▌      | 1118/3145 [07:37<13:01,  2.59it/s] 36%|███▌      | 1138/3143 [07:43<13:29,  2.48it/s] 35%|███▌      | 1114/3145 [07:37<13:34,  2.49it/s] 36%|███▌      | 1133/3145 [07:37<12:30,  2.68it/s] 36%|███▌      | 1119/3145 [07:38<12:18,  2.74it/s] 36%|███▌      | 1139/3143 [07:44<13:49,  2.42it/s] 36%|███▌      | 1134/3145 [07:38<12:50,  2.61it/s] 35%|███▌      | 1115/3145 [07:38<14:00,  2.41it/s] 36%|███▌      | 1120/3145 [07:38<12:38,  2.67it/s] 36%|███▋      | 1140/3143 [07:44<13:42,  2.44it/s] 36%|███▌      | 1121/3145 [07:38<11:22,  2.96it/s] 36%|███▌      | 1135/3145 [07:38<12:54,  2.59it/s] 35%|███▌      | 1116/3145 [07:38<14:11,  2.38it/s] 36%|███▌      | 1122/3145 [07:38<10:13,  3.30it/s] 36%|███▋      | 1141/3143 [07:44<13:16,  2.51it/s] 36%|███▌      | 1136/3145 [07:39<13:08,  2.55it/s] 36%|███▌      | 1123/3145 [07:39<09:26,  3.57it/s] 36%|███▌      | 1117/3145 [07:39<14:00,  2.41it/s] 36%|███▋      | 1142/3143 [07:45<13:54,  2.40it/s] 36%|███▌      | 1124/3145 [07:39<10:36,  3.18it/s] 36%|███▌      | 1118/3145 [07:39<13:20,  2.53it/s] 36%|███▌      | 1137/3145 [07:39<13:53,  2.41it/s] 36%|███▋      | 1143/3143 [07:45<13:44,  2.43it/s] 36%|███▌      | 1119/3145 [07:39<13:03,  2.58it/s] 36%|███▌      | 1138/3145 [07:40<13:48,  2.42it/s] 36%|███▌      | 1125/3145 [07:40<12:08,  2.77it/s] 36%|███▋      | 1144/3143 [07:46<13:10,  2.53it/s] 36%|███▌      | 1139/3145 [07:40<12:07,  2.76it/s] 36%|███▌      | 1120/3145 [07:40<13:34,  2.49it/s] 36%|███▌      | 1126/3145 [07:40<12:52,  2.61it/s] 36%|███▌      | 1140/3145 [07:40<12:41,  2.63it/s] 36%|███▌      | 1127/3145 [07:40<11:17,  2.98it/s] 36%|███▋      | 1145/3143 [07:46<14:00,  2.38it/s] 36%|███▌      | 1121/3145 [07:40<13:33,  2.49it/s] 36%|███▋      | 1141/3145 [07:41<12:33,  2.66it/s] 36%|███▌      | 1128/3145 [07:41<12:01,  2.80it/s] 36%|███▋      | 1146/3143 [07:47<13:54,  2.39it/s] 36%|███▌      | 1122/3145 [07:41<13:23,  2.52it/s] 36%|███▋      | 1142/3145 [07:41<12:23,  2.70it/s] 36%|███▋      | 1147/3143 [07:47<13:42,  2.43it/s] 36%|███▌      | 1129/3145 [07:41<13:09,  2.55it/s] 36%|███▌      | 1123/3145 [07:41<13:49,  2.44it/s] 36%|███▋      | 1143/3145 [07:41<13:13,  2.52it/s] 37%|███▋      | 1148/3143 [07:47<13:54,  2.39it/s] 36%|███▌      | 1130/3145 [07:42<13:27,  2.50it/s] 36%|███▌      | 1124/3145 [07:42<14:22,  2.34it/s] 36%|███▋      | 1144/3145 [07:42<13:12,  2.53it/s] 36%|███▌      | 1131/3145 [07:42<13:02,  2.57it/s] 37%|███▋      | 1149/3143 [07:48<13:53,  2.39it/s] 36%|███▌      | 1125/3145 [07:42<14:21,  2.34it/s] 36%|███▋      | 1145/3145 [07:42<12:46,  2.61it/s] 36%|███▌      | 1132/3145 [07:42<13:40,  2.45it/s] 37%|███▋      | 1150/3143 [07:48<14:20,  2.32it/s] 36%|███▌      | 1126/3145 [07:42<14:17,  2.36it/s] 36%|███▋      | 1146/3145 [07:43<12:41,  2.62it/s] 37%|███▋      | 1151/3143 [07:49<14:03,  2.36it/s] 36%|███▌      | 1133/3145 [07:43<14:33,  2.30it/s] 36%|███▌      | 1127/3145 [07:43<14:28,  2.32it/s] 36%|███▋      | 1147/3145 [07:43<13:26,  2.48it/s] 37%|███▋      | 1152/3143 [07:49<14:19,  2.32it/s] 37%|███▋      | 1148/3145 [07:43<12:24,  2.68it/s] 36%|███▌      | 1128/3145 [07:43<14:22,  2.34it/s] 36%|███▌      | 1134/3145 [07:43<15:09,  2.21it/s] 37%|███▋      | 1153/3143 [07:49<13:26,  2.47it/s] 36%|███▌      | 1129/3145 [07:44<13:53,  2.42it/s] 36%|███▌      | 1135/3145 [07:44<14:30,  2.31it/s] 37%|███▋      | 1149/3145 [07:44<13:04,  2.55it/s] 36%|███▌      | 1130/3145 [07:44<13:22,  2.51it/s] 37%|███▋      | 1154/3143 [07:50<14:07,  2.35it/s] 37%|███▋      | 1150/3145 [07:44<12:58,  2.56it/s] 36%|███▌      | 1136/3145 [07:44<14:46,  2.27it/s] 37%|███▋      | 1155/3143 [07:50<12:29,  2.65it/s] 36%|███▌      | 1131/3145 [07:44<13:37,  2.46it/s] 36%|███▌      | 1137/3145 [07:45<14:13,  2.35it/s] 37%|███▋      | 1151/3145 [07:45<13:56,  2.38it/s] 37%|███▋      | 1156/3143 [07:51<13:06,  2.53it/s] 36%|███▌      | 1132/3145 [07:45<13:23,  2.50it/s] 36%|███▌      | 1138/3145 [07:45<13:49,  2.42it/s] 37%|███▋      | 1152/3145 [07:45<13:35,  2.45it/s] 37%|███▋      | 1157/3143 [07:51<13:10,  2.51it/s] 36%|███▌      | 1133/3145 [07:45<13:25,  2.50it/s] 36%|███▌      | 1139/3145 [07:45<13:24,  2.49it/s] 37%|███▋      | 1153/3145 [07:45<13:25,  2.47it/s] 37%|███▋      | 1158/3143 [07:51<13:10,  2.51it/s] 36%|███▌      | 1134/3145 [07:46<13:15,  2.53it/s] 36%|███▌      | 1140/3145 [07:46<13:33,  2.46it/s] 37%|███▋      | 1154/3145 [07:46<13:18,  2.49it/s] 37%|███▋      | 1159/3143 [07:52<13:32,  2.44it/s] 36%|███▌      | 1135/3145 [07:46<13:24,  2.50it/s] 36%|███▋      | 1141/3145 [07:46<13:28,  2.48it/s] 37%|███▋      | 1155/3145 [07:46<13:19,  2.49it/s] 37%|███▋      | 1160/3143 [07:52<13:19,  2.48it/s] 36%|███▌      | 1136/3145 [07:47<14:00,  2.39it/s] 36%|███▋      | 1142/3145 [07:47<13:25,  2.49it/s] 37%|███▋      | 1156/3145 [07:47<13:14,  2.50it/s] 37%|███▋      | 1161/3143 [07:53<13:02,  2.53it/s] 37%|███▋      | 1157/3145 [07:47<12:50,  2.58it/s] 36%|███▌      | 1137/3145 [07:47<13:50,  2.42it/s] 36%|███▋      | 1143/3145 [07:47<13:23,  2.49it/s] 37%|███▋      | 1162/3143 [07:53<12:39,  2.61it/s] 37%|███▋      | 1158/3145 [07:47<12:46,  2.59it/s] 36%|███▌      | 1138/3145 [07:47<13:53,  2.41it/s] 36%|███▋      | 1144/3145 [07:47<14:03,  2.37it/s] 37%|███▋      | 1163/3143 [07:53<12:35,  2.62it/s] 36%|███▋      | 1145/3145 [07:48<13:25,  2.48it/s] 36%|███▌      | 1139/3145 [07:48<14:09,  2.36it/s] 37%|███▋      | 1159/3145 [07:48<14:38,  2.26it/s] 37%|███▋      | 1164/3143 [07:54<13:04,  2.52it/s] 36%|███▌      | 1140/3145 [07:48<13:26,  2.49it/s] 36%|███▋      | 1146/3145 [07:48<13:36,  2.45it/s] 37%|███▋      | 1165/3143 [07:54<13:02,  2.53it/s] 36%|███▋      | 1147/3145 [07:48<12:00,  2.77it/s] 37%|███▋      | 1160/3145 [07:48<16:12,  2.04it/s] 36%|███▋      | 1141/3145 [07:49<13:33,  2.46it/s] 37%|███▋      | 1166/3143 [07:55<12:58,  2.54it/s] 36%|███▋      | 1142/3145 [07:49<12:59,  2.57it/s] 37%|███▋      | 1148/3145 [07:49<13:06,  2.54it/s] 37%|███▋      | 1161/3145 [07:49<16:36,  1.99it/s] 37%|███▋      | 1167/3143 [07:55<13:00,  2.53it/s] 36%|███▋      | 1143/3145 [07:49<12:45,  2.62it/s] 37%|███▋      | 1149/3145 [07:49<13:19,  2.50it/s] 37%|███▋      | 1162/3145 [07:49<16:13,  2.04it/s] 37%|███▋      | 1168/3143 [07:55<13:02,  2.52it/s] 36%|███▋      | 1144/3145 [07:50<12:36,  2.64it/s] 37%|███▋      | 1150/3145 [07:50<13:37,  2.44it/s] 37%|███▋      | 1163/3145 [07:50<15:09,  2.18it/s] 37%|███▋      | 1169/3143 [07:56<13:31,  2.43it/s] 36%|███▋      | 1145/3145 [07:50<12:32,  2.66it/s] 37%|███▋      | 1151/3145 [07:50<13:37,  2.44it/s] 37%|███▋      | 1170/3143 [07:56<13:16,  2.48it/s] 36%|███▋      | 1146/3145 [07:50<12:40,  2.63it/s] 37%|███▋      | 1164/3145 [07:50<15:57,  2.07it/s] 37%|███▋      | 1152/3145 [07:51<13:04,  2.54it/s] 37%|███▋      | 1171/3143 [07:57<12:52,  2.55it/s] 36%|███▋      | 1147/3145 [07:51<13:13,  2.52it/s] 37%|███▋      | 1165/3145 [07:51<16:29,  2.00it/s] 37%|███▋      | 1153/3145 [07:51<13:30,  2.46it/s] 37%|███▋      | 1172/3143 [07:57<12:52,  2.55it/s] 37%|███▋      | 1148/3145 [07:51<13:08,  2.53it/s] 37%|███▋      | 1166/3145 [07:51<15:28,  2.13it/s] 37%|███▋      | 1154/3145 [07:51<13:58,  2.38it/s] 37%|███▋      | 1173/3143 [07:57<12:52,  2.55it/s] 37%|███▋      | 1149/3145 [07:52<13:33,  2.45it/s] 37%|███▋      | 1167/3145 [07:52<14:46,  2.23it/s] 37%|███▋      | 1155/3145 [07:52<13:20,  2.49it/s] 37%|███▋      | 1174/3143 [07:58<12:54,  2.54it/s] 37%|███▋      | 1150/3145 [07:52<13:48,  2.41it/s] 37%|███▋      | 1156/3145 [07:52<12:53,  2.57it/s] 37%|███▋      | 1168/3145 [07:52<14:37,  2.25it/s] 37%|███▋      | 1175/3143 [07:58<12:40,  2.59it/s] 37%|███▋      | 1151/3145 [07:52<13:42,  2.43it/s] 37%|███▋      | 1157/3145 [07:53<12:55,  2.56it/s] 37%|███▋      | 1169/3145 [07:53<14:01,  2.35it/s] 37%|███▋      | 1176/3143 [07:59<13:08,  2.49it/s] 37%|███▋      | 1152/3145 [07:53<13:53,  2.39it/s] 37%|███▋      | 1170/3145 [07:53<13:38,  2.41it/s] 37%|███▋      | 1158/3145 [07:53<13:22,  2.48it/s] 37%|███▋      | 1177/3143 [07:59<13:18,  2.46it/s] 37%|███▋      | 1159/3145 [07:53<12:41,  2.61it/s] 37%|███▋      | 1171/3145 [07:53<14:00,  2.35it/s] 37%|███▋      | 1153/3145 [07:53<14:18,  2.32it/s] 37%|███▋      | 1178/3143 [07:59<12:42,  2.58it/s] 37%|███▋      | 1160/3145 [07:54<12:32,  2.64it/s] 37%|███▋      | 1172/3145 [07:54<14:10,  2.32it/s] 38%|███▊      | 1179/3143 [08:00<12:45,  2.57it/s] 37%|███▋      | 1154/3145 [07:54<14:40,  2.26it/s] 37%|███▋      | 1161/3145 [07:54<12:43,  2.60it/s] 37%|███▋      | 1173/3145 [07:54<13:37,  2.41it/s] 38%|███▊      | 1180/3143 [08:00<12:35,  2.60it/s] 37%|███▋      | 1155/3145 [07:54<14:43,  2.25it/s] 37%|███▋      | 1162/3145 [07:54<12:47,  2.58it/s] 38%|███▊      | 1181/3143 [08:00<12:24,  2.64it/s] 37%|███▋      | 1174/3145 [07:55<13:36,  2.41it/s] 37%|███▋      | 1156/3145 [07:55<14:25,  2.30it/s] 37%|███▋      | 1163/3145 [07:55<12:54,  2.56it/s] 38%|███▊      | 1182/3143 [08:01<12:21,  2.64it/s] 37%|███▋      | 1175/3145 [07:55<14:16,  2.30it/s] 37%|███▋      | 1157/3145 [07:55<13:57,  2.37it/s] 37%|███▋      | 1164/3145 [07:55<13:10,  2.51it/s] 38%|███▊      | 1183/3143 [08:01<12:15,  2.67it/s] 37%|███▋      | 1176/3145 [07:56<13:49,  2.37it/s] 37%|███▋      | 1158/3145 [07:56<13:50,  2.39it/s] 37%|███▋      | 1165/3145 [07:56<13:10,  2.51it/s] 38%|███▊      | 1184/3143 [08:02<12:27,  2.62it/s] 37%|███▋      | 1177/3145 [07:56<13:45,  2.38it/s] 37%|███▋      | 1159/3145 [07:56<13:44,  2.41it/s] 37%|███▋      | 1166/3145 [07:56<13:22,  2.46it/s] 38%|███▊      | 1185/3143 [08:02<12:36,  2.59it/s] 37%|███▋      | 1178/3145 [07:56<13:52,  2.36it/s] 37%|███▋      | 1160/3145 [07:56<13:56,  2.37it/s] 37%|███▋      | 1167/3145 [07:56<12:47,  2.58it/s] 38%|███▊      | 1186/3143 [08:02<12:38,  2.58it/s] 37%|███▋      | 1179/3145 [07:57<13:38,  2.40it/s] 37%|███▋      | 1161/3145 [07:57<13:42,  2.41it/s] 37%|███▋      | 1168/3145 [07:57<12:51,  2.56it/s] 38%|███▊      | 1187/3143 [08:03<12:45,  2.56it/s] 38%|███▊      | 1180/3145 [07:57<13:25,  2.44it/s] 37%|███▋      | 1162/3145 [07:57<14:06,  2.34it/s] 37%|███▋      | 1169/3145 [07:57<13:14,  2.49it/s] 38%|███▊      | 1188/3143 [08:03<12:47,  2.55it/s] 38%|███▊      | 1181/3145 [07:58<13:24,  2.44it/s] 37%|███▋      | 1163/3145 [07:58<13:40,  2.42it/s] 38%|███▊      | 1189/3143 [08:04<12:35,  2.59it/s] 37%|███▋      | 1170/3145 [07:58<13:31,  2.43it/s] 38%|███▊      | 1182/3145 [07:58<13:18,  2.46it/s] 37%|███▋      | 1164/3145 [07:58<13:33,  2.43it/s] 38%|███▊      | 1190/3143 [08:04<12:41,  2.56it/s] 37%|███▋      | 1171/3145 [07:58<13:51,  2.37it/s] 38%|███▊      | 1183/3145 [07:58<13:04,  2.50it/s] 37%|███▋      | 1165/3145 [07:58<13:27,  2.45it/s] 38%|███▊      | 1191/3143 [08:04<12:44,  2.55it/s] 37%|███▋      | 1172/3145 [07:59<13:15,  2.48it/s] 38%|███▊      | 1184/3145 [07:59<13:05,  2.50it/s] 38%|███▊      | 1192/3143 [08:05<12:47,  2.54it/s] 37%|███▋      | 1173/3145 [07:59<13:33,  2.42it/s] 37%|███▋      | 1166/3145 [07:59<15:08,  2.18it/s] 38%|███▊      | 1185/3145 [07:59<13:02,  2.51it/s] 38%|███▊      | 1193/3143 [08:05<12:32,  2.59it/s] 37%|███▋      | 1174/3145 [07:59<13:14,  2.48it/s] 37%|███▋      | 1167/3145 [07:59<14:56,  2.21it/s] 38%|███▊      | 1186/3145 [08:00<13:19,  2.45it/s] 38%|███▊      | 1194/3143 [08:06<12:40,  2.56it/s] 37%|███▋      | 1175/3145 [08:00<13:53,  2.36it/s] 37%|███▋      | 1168/3145 [08:00<14:24,  2.29it/s] 38%|███▊      | 1187/3145 [08:00<13:15,  2.46it/s] 37%|███▋      | 1169/3145 [08:00<12:12,  2.70it/s] 38%|███▊      | 1195/3143 [08:06<13:08,  2.47it/s] 37%|███▋      | 1176/3145 [08:00<13:58,  2.35it/s] 38%|███▊      | 1188/3145 [08:00<13:13,  2.47it/s] 37%|███▋      | 1170/3145 [08:00<12:30,  2.63it/s] 38%|███▊      | 1196/3143 [08:06<13:00,  2.49it/s] 37%|███▋      | 1177/3145 [08:01<13:49,  2.37it/s] 38%|███▊      | 1189/3145 [08:01<13:32,  2.41it/s] 37%|███▋      | 1171/3145 [08:01<12:39,  2.60it/s] 38%|███▊      | 1197/3143 [08:07<12:39,  2.56it/s] 37%|███▋      | 1178/3145 [08:01<13:23,  2.45it/s] 38%|███▊      | 1190/3145 [08:01<13:38,  2.39it/s] 38%|███▊      | 1198/3143 [08:07<12:48,  2.53it/s] 37%|███▋      | 1172/3145 [08:01<13:14,  2.48it/s] 37%|███▋      | 1179/3145 [08:01<13:27,  2.43it/s] 38%|███▊      | 1199/3143 [08:07<11:25,  2.84it/s] 37%|███▋      | 1173/3145 [08:02<12:57,  2.54it/s] 38%|███▊      | 1180/3145 [08:02<13:13,  2.48it/s] 38%|███▊      | 1191/3145 [08:02<15:27,  2.11it/s] 38%|███▊      | 1200/3143 [08:08<12:30,  2.59it/s] 37%|███▋      | 1174/3145 [08:02<13:38,  2.41it/s] 38%|███▊      | 1181/3145 [08:02<13:04,  2.50it/s] 38%|███▊      | 1192/3145 [08:02<15:08,  2.15it/s] 38%|███▊      | 1201/3143 [08:08<13:02,  2.48it/s] 37%|███▋      | 1175/3145 [08:03<13:26,  2.44it/s] 38%|███▊      | 1182/3145 [08:03<13:08,  2.49it/s] 38%|███▊      | 1193/3145 [08:03<14:32,  2.24it/s] 38%|███▊      | 1202/3143 [08:09<13:06,  2.47it/s] 37%|███▋      | 1176/3145 [08:03<13:31,  2.43it/s] 38%|███▊      | 1183/3145 [08:03<13:26,  2.43it/s] 38%|███▊      | 1194/3145 [08:03<13:42,  2.37it/s] 38%|███▊      | 1203/3143 [08:09<13:25,  2.41it/s] 37%|███▋      | 1177/3145 [08:03<14:00,  2.34it/s] 38%|███▊      | 1184/3145 [08:03<13:03,  2.50it/s] 38%|███▊      | 1195/3145 [08:04<14:09,  2.29it/s] 38%|███▊      | 1204/3143 [08:10<12:59,  2.49it/s] 37%|███▋      | 1178/3145 [08:04<13:38,  2.40it/s] 38%|███▊      | 1185/3145 [08:04<13:05,  2.49it/s] 38%|███▊      | 1196/3145 [08:04<13:28,  2.41it/s] 38%|███▊      | 1205/3143 [08:10<12:36,  2.56it/s] 38%|███▊      | 1197/3145 [08:04<11:37,  2.79it/s] 38%|███▊      | 1186/3145 [08:04<12:23,  2.63it/s] 37%|███▋      | 1179/3145 [08:04<13:22,  2.45it/s] 38%|███▊      | 1206/3143 [08:10<12:59,  2.48it/s] 38%|███▊      | 1187/3145 [08:05<12:12,  2.67it/s] 38%|███▊      | 1198/3145 [08:05<12:22,  2.62it/s] 38%|███▊      | 1180/3145 [08:05<13:41,  2.39it/s] 38%|███▊      | 1207/3143 [08:11<13:30,  2.39it/s] 38%|███▊      | 1188/3145 [08:05<12:34,  2.59it/s] 38%|███▊      | 1199/3145 [08:05<12:32,  2.59it/s] 38%|███▊      | 1181/3145 [08:05<14:21,  2.28it/s] 38%|███▊      | 1189/3145 [08:05<12:36,  2.59it/s] 38%|███▊      | 1208/3143 [08:11<13:36,  2.37it/s] 38%|███▊      | 1200/3145 [08:05<12:59,  2.49it/s] 38%|███▊      | 1182/3145 [08:06<14:19,  2.28it/s] 38%|███▊      | 1190/3145 [08:06<12:31,  2.60it/s] 38%|███▊      | 1209/3143 [08:12<13:24,  2.40it/s] 38%|███▊      | 1201/3145 [08:06<12:57,  2.50it/s] 38%|███▊      | 1210/3143 [08:12<11:36,  2.78it/s] 38%|███▊      | 1183/3145 [08:06<13:58,  2.34it/s] 38%|███▊      | 1191/3145 [08:06<12:37,  2.58it/s] 38%|███▊      | 1202/3145 [08:06<12:48,  2.53it/s] 38%|███▊      | 1184/3145 [08:06<13:53,  2.35it/s] 38%|███▊      | 1192/3145 [08:06<12:39,  2.57it/s] 38%|███▊      | 1203/3145 [08:07<12:11,  2.65it/s] 39%|███▊      | 1211/3143 [08:12<13:42,  2.35it/s] 38%|███▊      | 1185/3145 [08:07<13:18,  2.45it/s] 38%|███▊      | 1193/3145 [08:07<13:06,  2.48it/s] 38%|███▊      | 1204/3145 [08:07<12:51,  2.52it/s] 39%|███▊      | 1212/3143 [08:13<14:29,  2.22it/s] 38%|███▊      | 1186/3145 [08:07<13:08,  2.49it/s] 38%|███▊      | 1194/3145 [08:07<12:57,  2.51it/s] 38%|███▊      | 1205/3145 [08:07<13:25,  2.41it/s] 39%|███▊      | 1213/3143 [08:13<14:01,  2.29it/s] 38%|███▊      | 1187/3145 [08:07<12:50,  2.54it/s] 38%|███▊      | 1206/3145 [08:08<12:35,  2.57it/s] 38%|███▊      | 1195/3145 [08:08<13:22,  2.43it/s] 38%|███▊      | 1188/3145 [08:08<12:54,  2.53it/s] 39%|███▊      | 1214/3143 [08:14<15:20,  2.10it/s] 38%|███▊      | 1196/3145 [08:08<13:33,  2.40it/s] 38%|███▊      | 1207/3145 [08:08<13:44,  2.35it/s] 38%|███▊      | 1189/3145 [08:08<13:38,  2.39it/s] 39%|███▊      | 1215/3143 [08:14<14:40,  2.19it/s] 38%|███▊      | 1197/3145 [08:09<13:25,  2.42it/s] 38%|███▊      | 1190/3145 [08:09<13:01,  2.50it/s] 38%|███▊      | 1208/3145 [08:09<14:31,  2.22it/s] 39%|███▊      | 1216/3143 [08:15<14:02,  2.29it/s] 38%|███▊      | 1198/3145 [08:09<13:35,  2.39it/s] 39%|███▊      | 1217/3143 [08:15<12:18,  2.61it/s] 38%|███▊      | 1191/3145 [08:09<13:23,  2.43it/s] 38%|███▊      | 1209/3145 [08:09<14:27,  2.23it/s] 39%|███▉      | 1218/3143 [08:15<11:11,  2.87it/s] 38%|███▊      | 1199/3145 [08:09<13:08,  2.47it/s] 39%|███▉      | 1219/3143 [08:15<10:22,  3.09it/s] 38%|███▊      | 1192/3145 [08:10<14:13,  2.29it/s] 38%|███▊      | 1210/3145 [08:10<14:46,  2.18it/s] 38%|███▊      | 1200/3145 [08:10<12:51,  2.52it/s] 39%|███▉      | 1220/3143 [08:16<10:50,  2.96it/s] 39%|███▊      | 1211/3145 [08:10<14:06,  2.28it/s] 38%|███▊      | 1193/3145 [08:10<13:54,  2.34it/s] 38%|███▊      | 1201/3145 [08:10<13:03,  2.48it/s] 39%|███▉      | 1221/3143 [08:16<11:34,  2.77it/s] 38%|███▊      | 1202/3145 [08:10<11:19,  2.86it/s] 38%|███▊      | 1194/3145 [08:10<13:36,  2.39it/s] 39%|███▊      | 1212/3145 [08:11<14:14,  2.26it/s] 38%|███▊      | 1203/3145 [08:11<11:24,  2.84it/s] 39%|███▉      | 1222/3143 [08:17<12:08,  2.64it/s] 39%|███▊      | 1213/3145 [08:11<13:27,  2.39it/s] 38%|███▊      | 1195/3145 [08:11<13:49,  2.35it/s] 38%|███▊      | 1204/3145 [08:11<12:01,  2.69it/s] 39%|███▉      | 1223/3143 [08:17<12:19,  2.60it/s] 39%|███▊      | 1214/3145 [08:11<13:14,  2.43it/s] 38%|███▊      | 1196/3145 [08:11<13:24,  2.42it/s] 38%|███▊      | 1205/3145 [08:12<12:10,  2.66it/s] 39%|███▉      | 1224/3143 [08:18<12:26,  2.57it/s] 38%|███▊      | 1197/3145 [08:12<13:14,  2.45it/s] 39%|███▊      | 1215/3145 [08:12<13:42,  2.35it/s] 38%|███▊      | 1206/3145 [08:12<12:21,  2.62it/s] 39%|███▉      | 1225/3143 [08:18<12:35,  2.54it/s] 38%|███▊      | 1198/3145 [08:12<12:43,  2.55it/s] 39%|███▊      | 1216/3145 [08:12<13:23,  2.40it/s] 38%|███▊      | 1207/3145 [08:12<12:28,  2.59it/s] 38%|███▊      | 1199/3145 [08:12<12:50,  2.53it/s] 39%|███▉      | 1226/3143 [08:18<13:19,  2.40it/s] 39%|███▊      | 1217/3145 [08:12<12:52,  2.50it/s] 38%|███▊      | 1208/3145 [08:13<12:13,  2.64it/s] 39%|███▊      | 1218/3145 [08:13<12:33,  2.56it/s] 39%|███▉      | 1227/3143 [08:19<13:12,  2.42it/s] 38%|███▊      | 1200/3145 [08:13<13:17,  2.44it/s] 38%|███▊      | 1209/3145 [08:13<12:24,  2.60it/s] 39%|███▉      | 1219/3145 [08:13<12:18,  2.61it/s] 38%|███▊      | 1201/3145 [08:13<13:13,  2.45it/s] 39%|███▉      | 1228/3143 [08:19<13:10,  2.42it/s] 38%|███▊      | 1210/3145 [08:14<12:41,  2.54it/s] 39%|███▉      | 1220/3145 [08:14<12:23,  2.59it/s] 38%|███▊      | 1202/3145 [08:14<13:39,  2.37it/s] 39%|███▉      | 1229/3143 [08:20<13:44,  2.32it/s] 39%|███▉      | 1221/3145 [08:14<11:14,  2.85it/s] 39%|███▊      | 1211/3145 [08:14<13:27,  2.40it/s] 38%|███▊      | 1203/3145 [08:14<14:03,  2.30it/s] 39%|███▉      | 1230/3143 [08:20<13:55,  2.29it/s] 39%|███▉      | 1222/3145 [08:14<12:25,  2.58it/s] 39%|███▊      | 1212/3145 [08:14<13:14,  2.43it/s] 38%|███▊      | 1204/3145 [08:15<13:49,  2.34it/s] 39%|███▉      | 1231/3143 [08:21<14:20,  2.22it/s] 39%|███▊      | 1213/3145 [08:15<12:43,  2.53it/s] 39%|███▉      | 1223/3145 [08:15<14:13,  2.25it/s] 39%|███▉      | 1232/3143 [08:21<13:21,  2.38it/s] 38%|███▊      | 1205/3145 [08:15<14:17,  2.26it/s] 39%|███▊      | 1214/3145 [08:15<12:38,  2.54it/s] 39%|███▉      | 1224/3145 [08:15<13:41,  2.34it/s] 39%|███▉      | 1233/3143 [08:21<13:17,  2.40it/s] 38%|███▊      | 1206/3145 [08:15<13:53,  2.33it/s] 39%|███▊      | 1215/3145 [08:16<13:01,  2.47it/s] 39%|███▉      | 1225/3145 [08:16<13:09,  2.43it/s] 39%|███▉      | 1234/3143 [08:22<12:49,  2.48it/s] 38%|███▊      | 1207/3145 [08:16<13:55,  2.32it/s] 39%|███▊      | 1216/3145 [08:16<12:53,  2.49it/s] 39%|███▉      | 1226/3145 [08:16<13:20,  2.40it/s] 39%|███▉      | 1235/3143 [08:22<11:47,  2.70it/s] 38%|███▊      | 1208/3145 [08:16<13:21,  2.42it/s] 39%|███▉      | 1227/3145 [08:17<12:59,  2.46it/s] 39%|███▊      | 1217/3145 [08:17<14:11,  2.26it/s] 39%|███▉      | 1236/3143 [08:22<12:09,  2.61it/s] 38%|███▊      | 1209/3145 [08:17<11:39,  2.77it/s] 39%|███▊      | 1218/3145 [08:17<13:47,  2.33it/s] 38%|███▊      | 1210/3145 [08:17<12:01,  2.68it/s] 39%|███▉      | 1228/3145 [08:17<13:23,  2.39it/s] 39%|███▉      | 1237/3143 [08:23<12:57,  2.45it/s] 39%|███▉      | 1219/3145 [08:17<13:50,  2.32it/s] 39%|███▉      | 1229/3145 [08:17<13:41,  2.33it/s] 39%|███▊      | 1211/3145 [08:17<13:22,  2.41it/s] 39%|███▉      | 1238/3143 [08:23<13:26,  2.36it/s] 39%|███▊      | 1212/3145 [08:18<11:47,  2.73it/s] 39%|███▉      | 1220/3145 [08:18<13:30,  2.38it/s] 39%|███▉      | 1230/3145 [08:18<13:21,  2.39it/s] 39%|███▉      | 1239/3143 [08:24<12:59,  2.44it/s] 39%|███▊      | 1213/3145 [08:18<12:22,  2.60it/s] 39%|███▉      | 1221/3145 [08:18<13:24,  2.39it/s] 39%|███▉      | 1231/3145 [08:18<13:28,  2.37it/s] 39%|███▉      | 1240/3143 [08:24<13:02,  2.43it/s] 39%|███▊      | 1214/3145 [08:18<11:55,  2.70it/s] 39%|███▉      | 1222/3145 [08:19<13:19,  2.41it/s] 39%|███▉      | 1232/3145 [08:19<13:12,  2.41it/s] 39%|███▊      | 1215/3145 [08:19<10:35,  3.04it/s] 39%|███▉      | 1241/3143 [08:25<13:29,  2.35it/s] 39%|███▉      | 1223/3145 [08:19<13:19,  2.40it/s] 39%|███▉      | 1233/3145 [08:19<12:44,  2.50it/s] 39%|███▊      | 1216/3145 [08:19<10:31,  3.05it/s] 40%|███▉      | 1242/3143 [08:25<13:17,  2.38it/s] 39%|███▉      | 1234/3145 [08:19<12:26,  2.56it/s] 39%|███▉      | 1224/3145 [08:19<13:14,  2.42it/s] 39%|███▊      | 1217/3145 [08:19<11:34,  2.78it/s] 40%|███▉      | 1243/3143 [08:25<13:29,  2.35it/s] 39%|███▉      | 1235/3145 [08:20<12:36,  2.53it/s] 39%|███▊      | 1218/3145 [08:20<11:27,  2.80it/s] 39%|███▉      | 1225/3145 [08:20<13:24,  2.39it/s] 40%|███▉      | 1244/3143 [08:26<13:11,  2.40it/s] 39%|███▉      | 1236/3145 [08:20<12:14,  2.60it/s] 39%|███▉      | 1219/3145 [08:20<12:23,  2.59it/s] 39%|███▉      | 1226/3145 [08:20<13:43,  2.33it/s] 40%|███▉      | 1245/3143 [08:26<12:40,  2.50it/s] 39%|███▉      | 1237/3145 [08:20<12:00,  2.65it/s] 39%|███▉      | 1227/3145 [08:21<12:06,  2.64it/s] 39%|███▉      | 1220/3145 [08:21<12:26,  2.58it/s] 40%|███▉      | 1246/3143 [08:27<12:39,  2.50it/s] 39%|███▉      | 1238/3145 [08:21<11:42,  2.71it/s] 39%|███▉      | 1228/3145 [08:21<12:14,  2.61it/s] 39%|███▉      | 1221/3145 [08:21<12:55,  2.48it/s] 40%|███▉      | 1247/3143 [08:27<13:13,  2.39it/s] 39%|███▉      | 1239/3145 [08:21<11:39,  2.73it/s] 39%|███▉      | 1229/3145 [08:21<12:45,  2.50it/s] 39%|███▉      | 1222/3145 [08:22<13:14,  2.42it/s] 40%|███▉      | 1248/3143 [08:27<12:39,  2.49it/s] 39%|███▉      | 1240/3145 [08:22<11:58,  2.65it/s] 39%|███▉      | 1230/3145 [08:22<11:49,  2.70it/s] 39%|███▉      | 1223/3145 [08:22<13:17,  2.41it/s] 39%|███▉      | 1241/3145 [08:22<11:58,  2.65it/s] 40%|███▉      | 1249/3143 [08:28<13:00,  2.43it/s] 39%|███▉      | 1231/3145 [08:22<11:51,  2.69it/s] 39%|███▉      | 1224/3145 [08:22<13:16,  2.41it/s] 39%|███▉      | 1242/3145 [08:22<12:20,  2.57it/s] 40%|███▉      | 1250/3143 [08:28<13:14,  2.38it/s] 39%|███▉      | 1232/3145 [08:22<12:23,  2.57it/s] 39%|███▉      | 1233/3145 [08:23<11:21,  2.81it/s] 39%|███▉      | 1225/3145 [08:23<13:07,  2.44it/s] 40%|███▉      | 1251/3143 [08:29<13:04,  2.41it/s] 40%|███▉      | 1243/3145 [08:23<12:56,  2.45it/s] 39%|███▉      | 1234/3145 [08:23<11:46,  2.70it/s] 39%|███▉      | 1226/3145 [08:23<13:03,  2.45it/s] 40%|███▉      | 1252/3143 [08:29<12:54,  2.44it/s] 40%|███▉      | 1244/3145 [08:23<12:37,  2.51it/s] 39%|███▉      | 1235/3145 [08:24<12:03,  2.64it/s] 39%|███▉      | 1227/3145 [08:24<12:48,  2.50it/s] 40%|███▉      | 1253/3143 [08:30<12:50,  2.45it/s] 40%|███▉      | 1245/3145 [08:24<13:03,  2.42it/s] 40%|███▉      | 1254/3143 [08:30<12:16,  2.56it/s] 39%|███▉      | 1228/3145 [08:24<13:12,  2.42it/s] 39%|███▉      | 1236/3145 [08:24<13:04,  2.43it/s] 40%|███▉      | 1246/3145 [08:24<13:14,  2.39it/s] 39%|███▉      | 1229/3145 [08:24<13:06,  2.43it/s] 39%|███▉      | 1237/3145 [08:24<13:00,  2.45it/s] 40%|███▉      | 1255/3143 [08:30<13:39,  2.30it/s] 40%|███▉      | 1247/3145 [08:25<13:37,  2.32it/s] 39%|███▉      | 1230/3145 [08:25<12:52,  2.48it/s] 39%|███▉      | 1238/3145 [08:25<12:47,  2.48it/s] 40%|███▉      | 1256/3143 [08:31<13:44,  2.29it/s] 40%|███▉      | 1248/3145 [08:25<13:36,  2.32it/s] 39%|███▉      | 1239/3145 [08:25<13:04,  2.43it/s] 40%|███▉      | 1249/3145 [08:25<12:56,  2.44it/s] 39%|███▉      | 1231/3145 [08:25<14:36,  2.18it/s] 40%|███▉      | 1257/3143 [08:31<14:09,  2.22it/s] 39%|███▉      | 1240/3145 [08:26<12:35,  2.52it/s] 40%|███▉      | 1250/3145 [08:26<13:22,  2.36it/s] 39%|███▉      | 1232/3145 [08:26<14:32,  2.19it/s] 40%|████      | 1258/3143 [08:32<14:45,  2.13it/s] 39%|███▉      | 1241/3145 [08:26<13:10,  2.41it/s] 40%|███▉      | 1251/3145 [08:26<13:02,  2.42it/s] 39%|███▉      | 1233/3145 [08:26<14:00,  2.28it/s] 40%|████      | 1259/3143 [08:32<15:06,  2.08it/s] 39%|███▉      | 1242/3145 [08:27<13:18,  2.38it/s] 40%|███▉      | 1252/3145 [08:27<12:30,  2.52it/s] 39%|███▉      | 1234/3145 [08:27<15:17,  2.08it/s] 40%|████      | 1260/3143 [08:33<13:53,  2.26it/s] 40%|███▉      | 1243/3145 [08:27<13:07,  2.42it/s] 40%|███▉      | 1253/3145 [08:27<12:22,  2.55it/s] 40%|████      | 1261/3143 [08:33<13:35,  2.31it/s] 39%|███▉      | 1235/3145 [08:27<14:40,  2.17it/s] 40%|███▉      | 1244/3145 [08:27<12:51,  2.47it/s] 40%|███▉      | 1254/3145 [08:27<12:29,  2.52it/s] 40%|████      | 1262/3143 [08:33<12:56,  2.42it/s] 39%|███▉      | 1236/3145 [08:28<14:06,  2.25it/s] 40%|███▉      | 1245/3145 [08:28<12:28,  2.54it/s] 40%|███▉      | 1255/3145 [08:28<12:53,  2.44it/s] 40%|███▉      | 1256/3145 [08:28<11:19,  2.78it/s] 39%|███▉      | 1237/3145 [08:28<14:03,  2.26it/s] 40%|████      | 1263/3143 [08:34<13:42,  2.29it/s] 40%|███▉      | 1246/3145 [08:28<12:57,  2.44it/s] 40%|███▉      | 1257/3145 [08:28<11:18,  2.78it/s] 39%|███▉      | 1238/3145 [08:28<13:26,  2.37it/s] 40%|████      | 1264/3143 [08:34<13:24,  2.34it/s] 40%|███▉      | 1247/3145 [08:29<13:09,  2.40it/s] 40%|████      | 1258/3145 [08:29<11:38,  2.70it/s] 39%|███▉      | 1239/3145 [08:29<12:58,  2.45it/s] 40%|████      | 1265/3143 [08:35<12:50,  2.44it/s] 40%|███▉      | 1248/3145 [08:29<12:55,  2.45it/s] 40%|████      | 1259/3145 [08:29<12:00,  2.62it/s] 40%|████      | 1266/3143 [08:35<12:15,  2.55it/s] 39%|███▉      | 1240/3145 [08:29<12:56,  2.45it/s] 40%|███▉      | 1249/3145 [08:29<12:48,  2.47it/s] 40%|████      | 1260/3145 [08:30<12:14,  2.57it/s] 40%|████      | 1267/3143 [08:35<12:16,  2.55it/s] 39%|███▉      | 1241/3145 [08:30<13:13,  2.40it/s] 40%|███▉      | 1250/3145 [08:30<12:53,  2.45it/s] 40%|████      | 1261/3145 [08:30<12:43,  2.47it/s] 39%|███▉      | 1242/3145 [08:30<13:01,  2.44it/s] 40%|████      | 1268/3143 [08:36<13:10,  2.37it/s] 40%|███▉      | 1251/3145 [08:30<12:59,  2.43it/s] 40%|███▉      | 1243/3145 [08:30<12:52,  2.46it/s] 40%|████      | 1262/3145 [08:30<12:53,  2.43it/s] 40%|████      | 1269/3143 [08:36<13:00,  2.40it/s] 40%|███▉      | 1252/3145 [08:31<13:09,  2.40it/s] 40%|███▉      | 1244/3145 [08:31<11:32,  2.75it/s] 40%|████      | 1263/3145 [08:31<12:58,  2.42it/s] 40%|███▉      | 1253/3145 [08:31<12:45,  2.47it/s] 40%|████      | 1270/3143 [08:37<13:37,  2.29it/s] 40%|███▉      | 1245/3145 [08:31<12:15,  2.58it/s] 40%|████      | 1264/3145 [08:31<13:10,  2.38it/s] 40%|███▉      | 1254/3145 [08:31<13:00,  2.42it/s] 40%|████      | 1271/3143 [08:37<14:22,  2.17it/s] 40%|███▉      | 1246/3145 [08:31<11:53,  2.66it/s] 40%|████      | 1265/3145 [08:32<12:58,  2.42it/s] 40%|███▉      | 1255/3145 [08:32<13:39,  2.31it/s] 40%|███▉      | 1247/3145 [08:32<12:29,  2.53it/s] 40%|████      | 1266/3145 [08:32<11:22,  2.75it/s] 40%|████      | 1272/3143 [08:38<15:09,  2.06it/s] 40%|███▉      | 1256/3145 [08:32<13:12,  2.38it/s] 40%|███▉      | 1248/3145 [08:32<12:18,  2.57it/s] 40%|████      | 1267/3145 [08:32<11:48,  2.65it/s] 41%|████      | 1273/3143 [08:38<14:59,  2.08it/s] 40%|███▉      | 1257/3145 [08:33<13:22,  2.35it/s] 40%|████      | 1268/3145 [08:33<11:54,  2.63it/s] 40%|███▉      | 1249/3145 [08:33<12:44,  2.48it/s] 41%|████      | 1274/3143 [08:39<14:16,  2.18it/s] 40%|███▉      | 1250/3145 [08:33<11:45,  2.68it/s] 40%|████      | 1269/3145 [08:33<12:30,  2.50it/s] 40%|████      | 1258/3145 [08:33<13:59,  2.25it/s] 41%|████      | 1275/3143 [08:39<13:29,  2.31it/s] 40%|███▉      | 1251/3145 [08:33<10:43,  2.94it/s] 40%|████      | 1259/3145 [08:34<13:35,  2.31it/s] 40%|████      | 1270/3145 [08:34<12:41,  2.46it/s] 40%|███▉      | 1252/3145 [08:34<10:49,  2.92it/s] 41%|████      | 1276/3143 [08:40<13:24,  2.32it/s] 40%|███▉      | 1253/3145 [08:34<10:12,  3.09it/s] 40%|████      | 1271/3145 [08:34<12:10,  2.57it/s] 41%|████      | 1277/3143 [08:40<11:57,  2.60it/s] 40%|████      | 1260/3145 [08:34<13:32,  2.32it/s] 40%|███▉      | 1254/3145 [08:34<10:54,  2.89it/s] 40%|████      | 1272/3145 [08:34<12:07,  2.57it/s] 40%|████      | 1261/3145 [08:34<12:45,  2.46it/s] 41%|████      | 1278/3143 [08:40<12:25,  2.50it/s] 40%|███▉      | 1255/3145 [08:35<11:08,  2.83it/s] 40%|████      | 1262/3145 [08:35<12:38,  2.48it/s] 40%|████      | 1273/3145 [08:35<12:24,  2.51it/s] 41%|████      | 1279/3143 [08:41<12:06,  2.57it/s] 40%|███▉      | 1256/3145 [08:35<11:38,  2.71it/s] 41%|████      | 1280/3143 [08:41<12:14,  2.53it/s] 41%|████      | 1274/3145 [08:35<13:09,  2.37it/s] 40%|████      | 1263/3145 [08:35<14:01,  2.24it/s] 41%|████      | 1281/3143 [08:41<11:05,  2.80it/s] 40%|███▉      | 1257/3145 [08:36<12:16,  2.56it/s] 41%|████      | 1275/3145 [08:36<12:45,  2.44it/s] 40%|████      | 1264/3145 [08:36<13:50,  2.26it/s] 41%|████      | 1282/3143 [08:42<11:50,  2.62it/s] 40%|████      | 1258/3145 [08:36<13:38,  2.31it/s] 41%|████      | 1276/3145 [08:36<13:46,  2.26it/s] 40%|████      | 1265/3145 [08:36<13:45,  2.28it/s] 41%|████      | 1283/3143 [08:42<13:02,  2.38it/s] 40%|████      | 1259/3145 [08:36<13:14,  2.37it/s] 40%|████      | 1266/3145 [08:37<13:01,  2.41it/s] 41%|████      | 1277/3145 [08:37<13:56,  2.23it/s] 40%|████      | 1260/3145 [08:37<11:57,  2.63it/s] 41%|████      | 1284/3143 [08:43<12:51,  2.41it/s] 41%|████      | 1278/3145 [08:37<13:34,  2.29it/s] 40%|████      | 1267/3145 [08:37<13:35,  2.30it/s] 41%|████      | 1285/3143 [08:43<13:04,  2.37it/s] 41%|████      | 1279/3145 [08:37<11:49,  2.63it/s] 40%|████      | 1261/3145 [08:37<13:16,  2.37it/s] 40%|████      | 1268/3145 [08:37<13:41,  2.29it/s] 41%|████      | 1286/3143 [08:44<12:45,  2.43it/s] 41%|████      | 1280/3145 [08:38<12:08,  2.56it/s] 40%|████      | 1269/3145 [08:38<12:49,  2.44it/s] 40%|████      | 1262/3145 [08:38<14:54,  2.11it/s] 41%|████      | 1281/3145 [08:38<10:45,  2.89it/s] 41%|████      | 1287/3143 [08:44<12:10,  2.54it/s] 41%|████      | 1282/3145 [08:38<09:38,  3.22it/s] 40%|████      | 1270/3145 [08:38<12:41,  2.46it/s] 41%|████      | 1288/3143 [08:44<12:19,  2.51it/s] 40%|████      | 1263/3145 [08:38<15:17,  2.05it/s] 41%|████      | 1283/3145 [08:39<11:00,  2.82it/s] 40%|████      | 1271/3145 [08:39<13:09,  2.37it/s] 40%|████      | 1264/3145 [08:39<13:26,  2.33it/s] 41%|████      | 1289/3143 [08:45<12:02,  2.57it/s] 41%|████      | 1284/3145 [08:39<11:36,  2.67it/s] 40%|████      | 1272/3145 [08:39<12:56,  2.41it/s] 40%|████      | 1265/3145 [08:39<13:02,  2.40it/s] 41%|████      | 1290/3143 [08:45<12:23,  2.49it/s] 40%|████      | 1273/3145 [08:39<12:18,  2.54it/s] 41%|████      | 1285/3145 [08:39<11:58,  2.59it/s] 40%|████      | 1266/3145 [08:40<13:15,  2.36it/s] 41%|████      | 1291/3143 [08:45<12:03,  2.56it/s] 41%|████      | 1274/3145 [08:40<12:18,  2.53it/s] 41%|████      | 1286/3145 [08:40<12:23,  2.50it/s] 40%|████      | 1267/3145 [08:40<13:03,  2.40it/s] 41%|████      | 1292/3143 [08:46<12:09,  2.54it/s] 41%|████      | 1275/3145 [08:40<12:13,  2.55it/s] 41%|████      | 1287/3145 [08:40<12:38,  2.45it/s] 40%|████      | 1268/3145 [08:40<13:13,  2.37it/s] 41%|████      | 1293/3143 [08:46<12:37,  2.44it/s] 41%|████      | 1276/3145 [08:41<12:14,  2.54it/s] 41%|████      | 1288/3145 [08:41<12:31,  2.47it/s] 40%|████      | 1269/3145 [08:41<12:41,  2.46it/s] 41%|████      | 1294/3143 [08:47<12:29,  2.47it/s] 41%|████      | 1277/3145 [08:41<12:12,  2.55it/s] 40%|████      | 1270/3145 [08:41<12:37,  2.48it/s] 41%|████      | 1289/3145 [08:41<12:53,  2.40it/s] 41%|████      | 1295/3143 [08:47<12:29,  2.47it/s] 41%|████      | 1278/3145 [08:41<12:09,  2.56it/s] 40%|████      | 1271/3145 [08:41<12:16,  2.54it/s] 41%|████      | 1290/3145 [08:42<12:53,  2.40it/s] 41%|████      | 1296/3143 [08:48<12:21,  2.49it/s] 41%|████      | 1279/3145 [08:42<11:00,  2.83it/s] 40%|████      | 1272/3145 [08:42<12:12,  2.56it/s] 41%|████      | 1291/3145 [08:42<12:54,  2.39it/s] 41%|████▏     | 1297/3143 [08:48<12:26,  2.47it/s] 41%|████      | 1280/3145 [08:42<11:55,  2.61it/s] 40%|████      | 1273/3145 [08:42<12:53,  2.42it/s] 41%|████      | 1292/3145 [08:42<12:56,  2.39it/s] 41%|████▏     | 1298/3143 [08:48<12:21,  2.49it/s] 41%|████      | 1281/3145 [08:43<12:02,  2.58it/s] 41%|████      | 1274/3145 [08:43<13:05,  2.38it/s] 41%|████      | 1293/3145 [08:43<12:59,  2.38it/s] 41%|████▏     | 1299/3143 [08:49<12:47,  2.40it/s] 41%|████      | 1282/3145 [08:43<12:26,  2.50it/s] 41%|████      | 1283/3145 [08:43<11:14,  2.76it/s] 41%|████      | 1294/3145 [08:43<12:47,  2.41it/s] 41%|████      | 1275/3145 [08:43<13:36,  2.29it/s] 41%|████▏     | 1300/3143 [08:49<12:45,  2.41it/s] 41%|████      | 1295/3145 [08:44<12:25,  2.48it/s] 41%|████      | 1284/3145 [08:44<11:56,  2.60it/s] 41%|████      | 1276/3145 [08:44<13:04,  2.38it/s] 41%|████▏     | 1301/3143 [08:50<14:17,  2.15it/s] 41%|████      | 1296/3145 [08:44<12:15,  2.51it/s] 41%|████      | 1285/3145 [08:44<12:04,  2.57it/s] 41%|████      | 1277/3145 [08:44<13:11,  2.36it/s] 41%|████▏     | 1302/3143 [08:50<13:27,  2.28it/s] 41%|████      | 1286/3145 [08:44<11:51,  2.61it/s] 41%|████      | 1297/3145 [08:44<12:38,  2.43it/s] 41%|████      | 1278/3145 [08:45<13:26,  2.31it/s] 41%|████▏     | 1303/3143 [08:51<13:04,  2.35it/s] 41%|████▏     | 1298/3145 [08:45<12:33,  2.45it/s] 41%|████      | 1287/3145 [08:45<12:25,  2.49it/s] 41%|████▏     | 1304/3143 [08:51<11:30,  2.66it/s] 41%|████      | 1279/3145 [08:45<12:50,  2.42it/s] 41%|████      | 1288/3145 [08:45<12:08,  2.55it/s] 42%|████▏     | 1305/3143 [08:51<11:22,  2.69it/s] 41%|████▏     | 1299/3145 [08:45<13:00,  2.36it/s] 41%|████      | 1280/3145 [08:45<12:45,  2.44it/s] 41%|████      | 1289/3145 [08:46<12:02,  2.57it/s] 42%|████▏     | 1306/3143 [08:52<11:56,  2.56it/s] 41%|████▏     | 1300/3145 [08:46<12:45,  2.41it/s] 41%|████      | 1281/3145 [08:46<13:08,  2.36it/s] 41%|████▏     | 1301/3145 [08:46<12:16,  2.50it/s] 41%|████      | 1290/3145 [08:46<13:01,  2.37it/s] 42%|████▏     | 1307/3143 [08:52<12:14,  2.50it/s] 41%|████      | 1282/3145 [08:46<13:28,  2.30it/s] 41%|████▏     | 1302/3145 [08:46<10:52,  2.83it/s] 42%|████▏     | 1308/3143 [08:52<11:01,  2.77it/s] 41%|████      | 1291/3145 [08:47<12:50,  2.41it/s] 41%|████      | 1283/3145 [08:47<13:30,  2.30it/s] 41%|████▏     | 1303/3145 [08:47<10:58,  2.80it/s] 42%|████▏     | 1309/3143 [08:53<11:24,  2.68it/s] 41%|████      | 1292/3145 [08:47<13:00,  2.38it/s] 41%|████      | 1284/3145 [08:47<13:09,  2.36it/s] 41%|████▏     | 1304/3145 [08:47<11:38,  2.64it/s] 42%|████▏     | 1310/3143 [08:53<11:43,  2.61it/s] 41%|████      | 1293/3145 [08:47<12:27,  2.48it/s] 41%|████      | 1285/3145 [08:47<13:06,  2.36it/s] 41%|████▏     | 1305/3145 [08:48<12:49,  2.39it/s] 42%|████▏     | 1311/3143 [08:54<12:07,  2.52it/s] 41%|████      | 1294/3145 [08:48<12:30,  2.46it/s] 41%|████      | 1286/3145 [08:48<12:40,  2.45it/s] 42%|████▏     | 1306/3145 [08:48<12:16,  2.50it/s] 42%|████▏     | 1312/3143 [08:54<11:53,  2.57it/s] 41%|████      | 1295/3145 [08:48<12:07,  2.54it/s] 42%|████▏     | 1313/3143 [08:54<10:34,  2.88it/s] 41%|████      | 1287/3145 [08:48<12:39,  2.45it/s] 42%|████▏     | 1307/3145 [08:48<11:55,  2.57it/s] 41%|████      | 1296/3145 [08:49<12:40,  2.43it/s] 42%|████▏     | 1314/3143 [08:55<11:01,  2.77it/s] 41%|████      | 1288/3145 [08:49<12:53,  2.40it/s] 42%|████▏     | 1308/3145 [08:49<12:27,  2.46it/s] 42%|████▏     | 1315/3143 [08:55<11:32,  2.64it/s] 41%|████      | 1297/3145 [08:49<14:21,  2.14it/s] 41%|████      | 1289/3145 [08:49<13:07,  2.36it/s] 42%|████▏     | 1309/3145 [08:49<12:21,  2.48it/s] 41%|████▏     | 1298/3145 [08:49<13:11,  2.33it/s] 42%|████▏     | 1316/3143 [08:55<11:56,  2.55it/s] 41%|████      | 1290/3145 [08:49<12:28,  2.48it/s] 42%|████▏     | 1310/3145 [08:50<13:16,  2.30it/s] 41%|████▏     | 1299/3145 [08:50<13:00,  2.36it/s] 41%|████      | 1291/3145 [08:50<12:37,  2.45it/s] 42%|████▏     | 1317/3143 [08:56<12:23,  2.46it/s] 42%|████▏     | 1311/3145 [08:50<13:17,  2.30it/s] 42%|████▏     | 1318/3143 [08:56<12:05,  2.52it/s] 41%|████▏     | 1300/3145 [08:50<13:04,  2.35it/s] 41%|████      | 1292/3145 [08:50<12:36,  2.45it/s] 42%|████▏     | 1312/3145 [08:51<12:55,  2.36it/s] 41%|████▏     | 1301/3145 [08:51<12:31,  2.45it/s] 42%|████▏     | 1319/3143 [08:57<12:06,  2.51it/s] 41%|████      | 1293/3145 [08:51<12:55,  2.39it/s] 42%|████▏     | 1313/3145 [08:51<12:26,  2.45it/s] 41%|████▏     | 1302/3145 [08:51<12:25,  2.47it/s] 41%|████      | 1294/3145 [08:51<12:17,  2.51it/s] 42%|████▏     | 1320/3143 [08:57<12:26,  2.44it/s] 42%|████▏     | 1314/3145 [08:51<12:18,  2.48it/s] 41%|████▏     | 1303/3145 [08:51<12:10,  2.52it/s] 41%|████      | 1295/3145 [08:52<12:37,  2.44it/s] 42%|████▏     | 1321/3143 [08:58<13:08,  2.31it/s] 42%|████▏     | 1315/3145 [08:52<12:34,  2.42it/s] 41%|████▏     | 1304/3145 [08:52<11:49,  2.59it/s] 41%|████      | 1296/3145 [08:52<12:33,  2.46it/s] 42%|████▏     | 1322/3143 [08:58<13:00,  2.33it/s] 42%|████▏     | 1316/3145 [08:52<12:00,  2.54it/s] 41%|████▏     | 1305/3145 [08:52<11:52,  2.58it/s] 42%|████▏     | 1317/3145 [08:52<10:24,  2.93it/s] 42%|████▏     | 1323/3143 [08:58<12:37,  2.40it/s] 41%|████      | 1297/3145 [08:52<13:44,  2.24it/s] 42%|████▏     | 1306/3145 [08:53<11:55,  2.57it/s] 42%|████▏     | 1318/3145 [08:53<10:46,  2.83it/s] 42%|████▏     | 1324/3143 [08:59<12:45,  2.38it/s] 42%|████▏     | 1307/3145 [08:53<11:38,  2.63it/s] 41%|████▏     | 1298/3145 [08:53<14:56,  2.06it/s] 42%|████▏     | 1319/3145 [08:53<11:33,  2.63it/s] 42%|████▏     | 1325/3143 [08:59<12:36,  2.40it/s] 42%|████▏     | 1308/3145 [08:53<11:35,  2.64it/s] 42%|████▏     | 1320/3145 [08:54<11:52,  2.56it/s] 42%|████▏     | 1326/3143 [09:00<12:07,  2.50it/s] 41%|████▏     | 1299/3145 [08:54<16:01,  1.92it/s] 42%|████▏     | 1309/3145 [08:54<12:05,  2.53it/s] 42%|████▏     | 1321/3145 [08:54<11:56,  2.55it/s] 42%|████▏     | 1327/3143 [09:00<12:07,  2.50it/s] 41%|████▏     | 1300/3145 [08:54<15:07,  2.03it/s] 42%|████▏     | 1310/3145 [08:54<12:12,  2.50it/s] 42%|████▏     | 1322/3145 [08:54<12:11,  2.49it/s] 42%|████▏     | 1328/3143 [09:00<11:45,  2.57it/s] 41%|████▏     | 1301/3145 [08:54<14:18,  2.15it/s] 42%|████▏     | 1311/3145 [08:55<12:32,  2.44it/s] 42%|████▏     | 1329/3143 [09:01<11:45,  2.57it/s] 42%|████▏     | 1323/3145 [08:55<12:57,  2.34it/s] 41%|████▏     | 1302/3145 [08:55<13:27,  2.28it/s] 42%|████▏     | 1312/3145 [08:55<12:43,  2.40it/s] 42%|████▏     | 1330/3143 [09:01<11:27,  2.64it/s] 42%|████▏     | 1324/3145 [08:55<12:54,  2.35it/s] 41%|████▏     | 1303/3145 [08:55<13:26,  2.28it/s] 42%|████▏     | 1313/3145 [08:55<12:53,  2.37it/s] 42%|████▏     | 1331/3143 [09:01<11:58,  2.52it/s] 42%|████▏     | 1325/3145 [08:56<12:34,  2.41it/s] 41%|████▏     | 1304/3145 [08:56<14:20,  2.14it/s] 42%|████▏     | 1314/3145 [08:56<12:57,  2.35it/s] 42%|████▏     | 1332/3143 [09:02<12:21,  2.44it/s] 42%|████▏     | 1326/3145 [08:56<12:19,  2.46it/s] 41%|████▏     | 1305/3145 [08:56<13:35,  2.26it/s] 42%|████▏     | 1315/3145 [08:56<12:35,  2.42it/s] 42%|████▏     | 1327/3145 [08:56<11:55,  2.54it/s] 42%|████▏     | 1333/3143 [09:02<12:17,  2.45it/s] 42%|████▏     | 1316/3145 [08:57<11:55,  2.56it/s] 42%|████▏     | 1306/3145 [08:57<13:04,  2.34it/s] 42%|████▏     | 1328/3145 [08:57<11:55,  2.54it/s] 42%|████▏     | 1334/3143 [09:03<12:35,  2.40it/s] 42%|████▏     | 1317/3145 [08:57<12:02,  2.53it/s] 42%|████▏     | 1307/3145 [08:57<13:13,  2.32it/s] 42%|████▏     | 1329/3145 [08:57<12:26,  2.43it/s] 42%|████▏     | 1318/3145 [08:57<10:28,  2.91it/s] 42%|████▏     | 1335/3143 [09:03<12:28,  2.42it/s] 42%|████▏     | 1308/3145 [08:57<12:49,  2.39it/s] 42%|████▏     | 1330/3145 [08:57<10:50,  2.79it/s] 42%|████▏     | 1319/3145 [08:58<11:03,  2.75it/s] 43%|████▎     | 1336/3143 [09:04<12:44,  2.36it/s] 42%|████▏     | 1309/3145 [08:58<12:28,  2.45it/s] 42%|████▏     | 1331/3145 [08:58<11:10,  2.71it/s] 42%|████▏     | 1320/3145 [08:58<11:51,  2.57it/s] 43%|████▎     | 1337/3143 [09:04<12:56,  2.33it/s] 42%|████▏     | 1332/3145 [08:58<10:49,  2.79it/s] 42%|████▏     | 1310/3145 [08:58<12:22,  2.47it/s] 43%|████▎     | 1338/3143 [09:04<12:49,  2.35it/s] 42%|████▏     | 1321/3145 [08:59<12:31,  2.43it/s] 42%|████▏     | 1333/3145 [08:59<11:12,  2.69it/s] 42%|████▏     | 1311/3145 [08:59<12:33,  2.43it/s] 42%|████▏     | 1334/3145 [08:59<11:24,  2.65it/s] 42%|████▏     | 1322/3145 [08:59<12:42,  2.39it/s] 43%|████▎     | 1339/3143 [09:05<13:11,  2.28it/s] 42%|████▏     | 1312/3145 [08:59<12:34,  2.43it/s] 42%|████▏     | 1335/3145 [08:59<11:42,  2.58it/s] 43%|████▎     | 1340/3143 [09:05<12:50,  2.34it/s] 42%|████▏     | 1313/3145 [08:59<12:30,  2.44it/s] 42%|████▏     | 1323/3145 [09:00<13:25,  2.26it/s] 42%|████▏     | 1336/3145 [09:00<11:34,  2.60it/s] 43%|████▎     | 1341/3143 [09:06<12:30,  2.40it/s] 42%|████▏     | 1314/3145 [09:00<12:06,  2.52it/s] 42%|████▏     | 1324/3145 [09:00<13:03,  2.32it/s] 43%|████▎     | 1337/3145 [09:00<11:21,  2.65it/s] 43%|████▎     | 1342/3143 [09:06<12:00,  2.50it/s] 42%|████▏     | 1315/3145 [09:00<12:37,  2.42it/s] 42%|████▏     | 1325/3145 [09:00<13:04,  2.32it/s] 43%|████▎     | 1338/3145 [09:01<11:51,  2.54it/s] 43%|████▎     | 1343/3143 [09:07<12:15,  2.45it/s] 42%|████▏     | 1326/3145 [09:01<12:30,  2.42it/s] 42%|████▏     | 1316/3145 [09:01<12:58,  2.35it/s] 42%|████▏     | 1317/3145 [09:01<11:27,  2.66it/s] 43%|████▎     | 1339/3145 [09:01<12:15,  2.46it/s] 43%|████▎     | 1344/3143 [09:07<12:12,  2.46it/s] 42%|████▏     | 1327/3145 [09:01<12:16,  2.47it/s] 42%|████▏     | 1318/3145 [09:01<11:19,  2.69it/s] 42%|████▏     | 1328/3145 [09:01<11:56,  2.53it/s] 43%|████▎     | 1340/3145 [09:01<12:45,  2.36it/s] 43%|████▎     | 1345/3143 [09:07<12:52,  2.33it/s] 42%|████▏     | 1319/3145 [09:02<11:16,  2.70it/s] 42%|████▏     | 1329/3145 [09:02<11:56,  2.53it/s] 43%|████▎     | 1346/3143 [09:08<12:33,  2.39it/s] 43%|████▎     | 1341/3145 [09:02<12:54,  2.33it/s] 42%|████▏     | 1320/3145 [09:02<12:04,  2.52it/s] 42%|████▏     | 1330/3145 [09:02<11:36,  2.61it/s] 43%|████▎     | 1347/3143 [09:08<12:44,  2.35it/s] 43%|████▎     | 1342/3145 [09:02<12:58,  2.32it/s] 42%|████▏     | 1331/3145 [09:03<11:21,  2.66it/s] 42%|████▏     | 1321/3145 [09:03<11:54,  2.55it/s] 43%|████▎     | 1348/3143 [09:09<12:55,  2.31it/s] 43%|████▎     | 1343/3145 [09:03<12:58,  2.32it/s] 42%|████▏     | 1332/3145 [09:03<11:16,  2.68it/s] 42%|████▏     | 1322/3145 [09:03<12:00,  2.53it/s] 43%|████▎     | 1349/3143 [09:09<12:39,  2.36it/s] 43%|████▎     | 1344/3145 [09:03<13:03,  2.30it/s] 42%|████▏     | 1333/3145 [09:03<11:34,  2.61it/s] 42%|████▏     | 1323/3145 [09:03<12:04,  2.51it/s] 43%|████▎     | 1350/3143 [09:09<12:13,  2.45it/s] 43%|████▎     | 1345/3145 [09:04<12:56,  2.32it/s] 42%|████▏     | 1334/3145 [09:04<11:46,  2.56it/s] 42%|████▏     | 1324/3145 [09:04<12:37,  2.40it/s] 43%|████▎     | 1351/3143 [09:10<12:09,  2.46it/s] 43%|████▎     | 1346/3145 [09:04<12:28,  2.41it/s] 42%|████▏     | 1335/3145 [09:04<12:33,  2.40it/s] 42%|████▏     | 1325/3145 [09:04<12:32,  2.42it/s] 43%|████▎     | 1347/3145 [09:04<11:02,  2.71it/s] 43%|████▎     | 1352/3143 [09:10<11:46,  2.53it/s] 43%|████▎     | 1348/3145 [09:05<09:48,  3.05it/s] 42%|████▏     | 1336/3145 [09:05<12:21,  2.44it/s] 42%|████▏     | 1326/3145 [09:05<12:58,  2.34it/s] 43%|████▎     | 1353/3143 [09:11<11:54,  2.51it/s] 43%|████▎     | 1349/3145 [09:05<10:22,  2.88it/s] 43%|████▎     | 1337/3145 [09:05<12:24,  2.43it/s] 43%|████▎     | 1354/3143 [09:11<11:31,  2.59it/s] 42%|████▏     | 1327/3145 [09:05<12:38,  2.40it/s] 43%|████▎     | 1350/3145 [09:05<10:55,  2.74it/s] 43%|████▎     | 1338/3145 [09:05<12:13,  2.46it/s] 42%|████▏     | 1328/3145 [09:06<12:53,  2.35it/s] 43%|████▎     | 1355/3143 [09:11<12:07,  2.46it/s] 43%|████▎     | 1351/3145 [09:06<11:15,  2.66it/s] 43%|████▎     | 1339/3145 [09:06<11:41,  2.58it/s] 43%|████▎     | 1356/3143 [09:12<12:01,  2.48it/s] 42%|████▏     | 1329/3145 [09:06<13:02,  2.32it/s] 43%|████▎     | 1352/3145 [09:06<11:42,  2.55it/s] 43%|████▎     | 1340/3145 [09:06<11:39,  2.58it/s] 43%|████▎     | 1357/3143 [09:12<11:44,  2.54it/s] 42%|████▏     | 1330/3145 [09:06<12:32,  2.41it/s] 43%|████▎     | 1353/3145 [09:07<12:02,  2.48it/s] 43%|████▎     | 1341/3145 [09:07<12:05,  2.49it/s] 42%|████▏     | 1331/3145 [09:07<12:21,  2.45it/s] 43%|████▎     | 1358/3143 [09:13<12:23,  2.40it/s] 43%|████▎     | 1342/3145 [09:07<12:03,  2.49it/s] 43%|████▎     | 1354/3145 [09:07<12:19,  2.42it/s] 42%|████▏     | 1332/3145 [09:07<12:32,  2.41it/s] 43%|████▎     | 1359/3143 [09:13<12:39,  2.35it/s] 43%|████▎     | 1343/3145 [09:07<12:01,  2.50it/s] 43%|████▎     | 1355/3145 [09:07<12:40,  2.35it/s] 43%|████▎     | 1360/3143 [09:13<11:35,  2.56it/s] 42%|████▏     | 1333/3145 [09:08<12:26,  2.43it/s] 43%|████▎     | 1344/3145 [09:08<12:28,  2.40it/s] 43%|████▎     | 1356/3145 [09:08<12:36,  2.36it/s] 43%|████▎     | 1361/3143 [09:14<11:59,  2.48it/s] 42%|████▏     | 1334/3145 [09:08<12:45,  2.37it/s] 43%|████▎     | 1362/3143 [09:14<10:27,  2.84it/s] 43%|████▎     | 1345/3145 [09:08<12:13,  2.45it/s] 43%|████▎     | 1357/3145 [09:08<12:40,  2.35it/s] 42%|████▏     | 1335/3145 [09:08<12:55,  2.33it/s] 43%|████▎     | 1346/3145 [09:09<11:49,  2.54it/s] 43%|████▎     | 1363/3143 [09:15<11:12,  2.65it/s] 43%|████▎     | 1358/3145 [09:09<12:25,  2.40it/s] 42%|████▏     | 1336/3145 [09:09<12:26,  2.42it/s] 43%|████▎     | 1347/3145 [09:09<11:44,  2.55it/s] 43%|████▎     | 1364/3143 [09:15<11:34,  2.56it/s] 43%|████▎     | 1359/3145 [09:09<12:12,  2.44it/s] 43%|████▎     | 1337/3145 [09:09<12:21,  2.44it/s] 43%|████▎     | 1348/3145 [09:09<12:12,  2.45it/s] 43%|████▎     | 1360/3145 [09:09<11:39,  2.55it/s] 43%|████▎     | 1365/3143 [09:15<11:40,  2.54it/s] 43%|████▎     | 1338/3145 [09:10<12:45,  2.36it/s] 43%|████▎     | 1361/3145 [09:10<10:35,  2.81it/s] 43%|████▎     | 1349/3145 [09:10<12:21,  2.42it/s] 43%|████▎     | 1366/3143 [09:16<12:00,  2.47it/s] 43%|████▎     | 1362/3145 [09:10<11:03,  2.69it/s] 43%|████▎     | 1339/3145 [09:10<13:00,  2.31it/s] 43%|████▎     | 1350/3145 [09:10<12:42,  2.35it/s] 43%|████▎     | 1367/3143 [09:16<12:34,  2.35it/s] 43%|████▎     | 1363/3145 [09:11<11:17,  2.63it/s] 43%|████▎     | 1351/3145 [09:11<12:23,  2.41it/s] 43%|████▎     | 1340/3145 [09:11<13:54,  2.16it/s] 44%|████▎     | 1368/3143 [09:17<12:18,  2.40it/s] 43%|████▎     | 1364/3145 [09:11<11:07,  2.67it/s] 43%|████▎     | 1341/3145 [09:11<13:36,  2.21it/s] 43%|████▎     | 1352/3145 [09:11<12:34,  2.38it/s] 44%|████▎     | 1369/3143 [09:17<12:14,  2.41it/s] 43%|████▎     | 1365/3145 [09:11<11:15,  2.64it/s] 43%|████▎     | 1353/3145 [09:12<12:18,  2.43it/s] 43%|████▎     | 1342/3145 [09:12<13:09,  2.28it/s] 44%|████▎     | 1370/3143 [09:17<12:03,  2.45it/s] 43%|████▎     | 1366/3145 [09:12<11:18,  2.62it/s] 43%|████▎     | 1343/3145 [09:12<12:31,  2.40it/s] 44%|████▎     | 1371/3143 [09:18<11:34,  2.55it/s] 43%|████▎     | 1354/3145 [09:12<12:10,  2.45it/s] 43%|████▎     | 1367/3145 [09:12<11:50,  2.50it/s] 43%|████▎     | 1344/3145 [09:12<12:15,  2.45it/s] 44%|████▎     | 1372/3143 [09:18<11:42,  2.52it/s] 43%|████▎     | 1355/3145 [09:12<12:30,  2.38it/s] 43%|████▎     | 1368/3145 [09:13<12:05,  2.45it/s] 43%|████▎     | 1345/3145 [09:13<12:12,  2.46it/s] 44%|████▎     | 1373/3143 [09:19<12:16,  2.40it/s] 43%|████▎     | 1356/3145 [09:13<12:37,  2.36it/s] 44%|████▎     | 1369/3145 [09:13<11:59,  2.47it/s] 43%|████▎     | 1346/3145 [09:13<12:07,  2.47it/s] 44%|████▎     | 1374/3143 [09:19<11:39,  2.53it/s] 43%|████▎     | 1357/3145 [09:13<12:14,  2.43it/s] 44%|████▎     | 1370/3145 [09:13<12:26,  2.38it/s] 43%|████▎     | 1347/3145 [09:14<12:37,  2.37it/s] 44%|████▎     | 1375/3143 [09:20<12:14,  2.41it/s] 43%|████▎     | 1358/3145 [09:14<12:28,  2.39it/s] 44%|████▎     | 1371/3145 [09:14<12:14,  2.42it/s] 44%|████▍     | 1376/3143 [09:20<11:44,  2.51it/s] 43%|████▎     | 1348/3145 [09:14<12:45,  2.35it/s] 43%|████▎     | 1359/3145 [09:14<12:03,  2.47it/s] 44%|████▎     | 1372/3145 [09:14<12:03,  2.45it/s] 44%|████▍     | 1377/3143 [09:20<11:45,  2.50it/s] 43%|████▎     | 1360/3145 [09:14<12:24,  2.40it/s] 43%|████▎     | 1349/3145 [09:14<12:59,  2.30it/s] 44%|████▎     | 1373/3145 [09:15<12:15,  2.41it/s] 44%|████▍     | 1378/3143 [09:21<12:02,  2.44it/s] 43%|████▎     | 1361/3145 [09:15<12:15,  2.43it/s] 43%|████▎     | 1350/3145 [09:15<13:09,  2.27it/s] 44%|████▍     | 1379/3143 [09:21<10:21,  2.84it/s] 44%|████▎     | 1374/3145 [09:15<12:31,  2.36it/s] 43%|████▎     | 1362/3145 [09:15<11:44,  2.53it/s] 43%|████▎     | 1351/3145 [09:15<13:12,  2.26it/s] 44%|████▍     | 1380/3143 [09:21<10:46,  2.73it/s] 44%|████▎     | 1375/3145 [09:15<12:01,  2.45it/s] 43%|████▎     | 1363/3145 [09:16<11:39,  2.55it/s] 43%|████▎     | 1352/3145 [09:16<13:01,  2.30it/s] 44%|████▍     | 1381/3143 [09:22<11:03,  2.66it/s] 44%|████▍     | 1376/3145 [09:16<11:57,  2.46it/s] 43%|████▎     | 1364/3145 [09:16<11:32,  2.57it/s] 43%|████▎     | 1353/3145 [09:16<12:38,  2.36it/s] 44%|████▍     | 1377/3145 [09:16<11:38,  2.53it/s] 44%|████▍     | 1382/3143 [09:22<11:14,  2.61it/s] 43%|████▎     | 1365/3145 [09:16<10:26,  2.84it/s] 43%|████▎     | 1354/3145 [09:17<12:20,  2.42it/s] 43%|████▎     | 1366/3145 [09:17<10:50,  2.74it/s] 44%|████▍     | 1378/3145 [09:17<11:59,  2.46it/s] 44%|████▍     | 1383/3143 [09:23<12:02,  2.44it/s] 43%|████▎     | 1355/3145 [09:17<10:55,  2.73it/s] 43%|████▎     | 1367/3145 [09:17<11:19,  2.62it/s] 44%|████▍     | 1384/3143 [09:23<11:46,  2.49it/s] 44%|████▍     | 1379/3145 [09:17<12:17,  2.40it/s] 43%|████▎     | 1356/3145 [09:17<10:53,  2.74it/s] 44%|████▍     | 1385/3143 [09:23<11:40,  2.51it/s] 43%|████▎     | 1368/3145 [09:18<11:45,  2.52it/s] 44%|████▍     | 1380/3145 [09:18<12:07,  2.43it/s] 43%|████▎     | 1357/3145 [09:18<11:07,  2.68it/s] 44%|████▍     | 1386/3143 [09:24<11:38,  2.51it/s] 44%|████▎     | 1369/3145 [09:18<11:49,  2.50it/s] 44%|████▍     | 1381/3145 [09:18<12:04,  2.44it/s] 43%|████▎     | 1358/3145 [09:18<11:07,  2.68it/s] 44%|████▍     | 1387/3143 [09:24<11:07,  2.63it/s] 44%|████▎     | 1370/3145 [09:18<11:41,  2.53it/s] 43%|████▎     | 1359/3145 [09:18<10:47,  2.76it/s] 44%|████▍     | 1382/3145 [09:18<12:05,  2.43it/s] 44%|████▍     | 1388/3143 [09:24<11:16,  2.59it/s] 44%|████▍     | 1383/3145 [09:19<11:59,  2.45it/s] 44%|████▎     | 1371/3145 [09:19<12:02,  2.45it/s] 43%|████▎     | 1360/3145 [09:19<11:25,  2.60it/s] 44%|████▍     | 1389/3143 [09:25<11:44,  2.49it/s] 44%|████▎     | 1372/3145 [09:19<11:49,  2.50it/s] 44%|████▍     | 1384/3145 [09:19<11:50,  2.48it/s] 43%|████▎     | 1361/3145 [09:19<12:18,  2.42it/s] 44%|████▎     | 1373/3145 [09:19<11:22,  2.60it/s] 44%|████▍     | 1385/3145 [09:20<12:13,  2.40it/s] 44%|████▍     | 1390/3143 [09:25<12:53,  2.27it/s] 43%|████▎     | 1362/3145 [09:20<12:44,  2.33it/s] 44%|████▎     | 1374/3145 [09:20<11:29,  2.57it/s] 44%|████▍     | 1386/3145 [09:20<12:24,  2.36it/s] 44%|████▍     | 1391/3143 [09:26<13:04,  2.23it/s] 43%|████▎     | 1363/3145 [09:20<12:28,  2.38it/s] 44%|████▎     | 1375/3145 [09:20<11:29,  2.57it/s] 44%|████▍     | 1387/3145 [09:20<12:30,  2.34it/s] 44%|████▍     | 1392/3143 [09:26<12:38,  2.31it/s] 43%|████▎     | 1364/3145 [09:20<12:35,  2.36it/s] 44%|████▍     | 1376/3145 [09:21<11:32,  2.55it/s] 44%|████▍     | 1388/3145 [09:21<12:16,  2.39it/s] 43%|████▎     | 1365/3145 [09:21<12:24,  2.39it/s] 44%|████▍     | 1393/3143 [09:27<12:55,  2.26it/s] 44%|████▍     | 1377/3145 [09:21<11:50,  2.49it/s] 44%|████▍     | 1394/3143 [09:27<11:12,  2.60it/s] 44%|████▍     | 1389/3145 [09:21<12:24,  2.36it/s] 43%|████▎     | 1366/3145 [09:21<12:43,  2.33it/s] 44%|████▍     | 1378/3145 [09:21<11:34,  2.54it/s] 44%|████▍     | 1395/3143 [09:27<11:04,  2.63it/s] 44%|████▍     | 1390/3145 [09:22<12:13,  2.39it/s] 43%|████▎     | 1367/3145 [09:22<13:00,  2.28it/s] 44%|████▍     | 1379/3145 [09:22<11:55,  2.47it/s] 44%|████▍     | 1396/3143 [09:28<11:04,  2.63it/s] 44%|████▍     | 1391/3145 [09:22<13:34,  2.15it/s] 44%|████▍     | 1380/3145 [09:22<11:51,  2.48it/s] 43%|████▎     | 1368/3145 [09:22<13:11,  2.24it/s] 44%|████▍     | 1397/3143 [09:28<11:42,  2.49it/s] 44%|████▍     | 1381/3145 [09:23<12:28,  2.36it/s] 44%|████▍     | 1398/3143 [09:29<11:37,  2.50it/s] 44%|████▍     | 1392/3145 [09:23<14:29,  2.02it/s] 44%|████▎     | 1369/3145 [09:23<14:46,  2.00it/s] 44%|████▍     | 1382/3145 [09:23<12:09,  2.42it/s] 44%|████▍     | 1393/3145 [09:23<13:14,  2.21it/s] 45%|████▍     | 1399/3143 [09:29<12:00,  2.42it/s] 44%|████▎     | 1370/3145 [09:23<13:51,  2.13it/s] 44%|████▍     | 1394/3145 [09:24<12:25,  2.35it/s] 44%|████▍     | 1383/3145 [09:24<12:34,  2.34it/s] 45%|████▍     | 1400/3143 [09:29<11:51,  2.45it/s] 44%|████▎     | 1371/3145 [09:24<12:49,  2.31it/s] 44%|████▍     | 1395/3145 [09:24<12:10,  2.40it/s] 45%|████▍     | 1401/3143 [09:30<12:13,  2.37it/s] 44%|████▍     | 1384/3145 [09:24<12:55,  2.27it/s] 44%|████▎     | 1372/3145 [09:24<12:45,  2.32it/s] 44%|████▍     | 1396/3145 [09:24<12:16,  2.37it/s] 44%|████▎     | 1373/3145 [09:24<12:27,  2.37it/s] 45%|████▍     | 1402/3143 [09:30<12:23,  2.34it/s] 44%|████▍     | 1385/3145 [09:25<12:56,  2.27it/s] 44%|████▍     | 1397/3145 [09:25<12:21,  2.36it/s] 44%|████▎     | 1374/3145 [09:25<12:26,  2.37it/s] 45%|████▍     | 1403/3143 [09:31<12:25,  2.33it/s] 44%|████▍     | 1386/3145 [09:25<12:53,  2.27it/s] 44%|████▍     | 1398/3145 [09:25<11:38,  2.50it/s] 44%|████▍     | 1387/3145 [09:25<12:25,  2.36it/s] 44%|████▎     | 1375/3145 [09:25<12:29,  2.36it/s] 45%|████▍     | 1404/3143 [09:31<13:23,  2.17it/s] 44%|████▍     | 1399/3145 [09:26<11:21,  2.56it/s] 44%|████▍     | 1388/3145 [09:26<12:17,  2.38it/s] 44%|████▍     | 1376/3145 [09:26<12:27,  2.37it/s] 45%|████▍     | 1405/3143 [09:32<13:07,  2.21it/s] 44%|████▍     | 1389/3145 [09:26<10:33,  2.77it/s] 45%|████▍     | 1400/3145 [09:26<12:06,  2.40it/s] 44%|████▍     | 1377/3145 [09:26<12:22,  2.38it/s] 45%|████▍     | 1406/3143 [09:32<12:52,  2.25it/s] 44%|████▍     | 1390/3145 [09:26<11:08,  2.62it/s] 45%|████▍     | 1401/3145 [09:26<12:15,  2.37it/s] 44%|████▍     | 1378/3145 [09:27<12:15,  2.40it/s] 45%|████▍     | 1407/3143 [09:33<12:12,  2.37it/s] 44%|████▍     | 1391/3145 [09:27<11:37,  2.51it/s] 45%|████▍     | 1402/3145 [09:27<12:14,  2.37it/s] 45%|████▍     | 1408/3143 [09:33<12:18,  2.35it/s] 44%|████▍     | 1379/3145 [09:27<13:47,  2.13it/s] 44%|████▍     | 1392/3145 [09:27<11:18,  2.58it/s] 45%|████▍     | 1403/3145 [09:27<12:01,  2.41it/s] 45%|████▍     | 1409/3143 [09:33<12:36,  2.29it/s] 44%|████▍     | 1393/3145 [09:28<11:27,  2.55it/s] 44%|████▍     | 1380/3145 [09:28<13:35,  2.16it/s] 45%|████▍     | 1404/3145 [09:28<12:12,  2.38it/s] 45%|████▍     | 1410/3143 [09:34<12:10,  2.37it/s] 44%|████▍     | 1394/3145 [09:28<11:45,  2.48it/s] 44%|████▍     | 1381/3145 [09:28<13:12,  2.23it/s] 45%|████▍     | 1405/3145 [09:28<12:11,  2.38it/s] 45%|████▍     | 1411/3143 [09:34<11:32,  2.50it/s] 44%|████▍     | 1395/3145 [09:28<11:48,  2.47it/s] 45%|████▍     | 1406/3145 [09:28<11:51,  2.44it/s] 44%|████▍     | 1382/3145 [09:28<13:30,  2.17it/s] 45%|████▍     | 1412/3143 [09:35<12:00,  2.40it/s] 44%|████▍     | 1396/3145 [09:29<11:25,  2.55it/s] 45%|████▍     | 1407/3145 [09:29<11:37,  2.49it/s] 44%|████▍     | 1383/3145 [09:29<13:18,  2.21it/s] 44%|████▍     | 1397/3145 [09:29<10:40,  2.73it/s] 45%|████▍     | 1413/3143 [09:35<11:34,  2.49it/s] 45%|████▍     | 1408/3145 [09:29<12:03,  2.40it/s] 44%|████▍     | 1384/3145 [09:29<13:16,  2.21it/s] 45%|████▍     | 1414/3143 [09:35<11:35,  2.48it/s] 44%|████▍     | 1398/3145 [09:30<11:26,  2.54it/s] 45%|████▍     | 1409/3145 [09:30<12:24,  2.33it/s] 44%|████▍     | 1385/3145 [09:30<13:09,  2.23it/s] 45%|████▌     | 1415/3143 [09:36<11:15,  2.56it/s] 44%|████▍     | 1399/3145 [09:30<12:01,  2.42it/s] 45%|████▍     | 1410/3145 [09:30<12:26,  2.32it/s] 44%|████▍     | 1386/3145 [09:30<13:16,  2.21it/s] 45%|████▌     | 1416/3143 [09:36<11:32,  2.49it/s] 45%|████▍     | 1400/3145 [09:30<11:51,  2.45it/s] 45%|████▍     | 1411/3145 [09:31<11:50,  2.44it/s] 45%|████▌     | 1417/3143 [09:37<11:51,  2.42it/s] 45%|████▍     | 1401/3145 [09:31<11:26,  2.54it/s] 44%|████▍     | 1387/3145 [09:31<14:14,  2.06it/s] 45%|████▍     | 1412/3145 [09:31<12:02,  2.40it/s] 45%|████▌     | 1418/3143 [09:37<11:37,  2.47it/s] 45%|████▍     | 1402/3145 [09:31<12:00,  2.42it/s] 44%|████▍     | 1388/3145 [09:31<13:21,  2.19it/s] 45%|████▍     | 1413/3145 [09:31<12:11,  2.37it/s] 45%|████▌     | 1419/3143 [09:37<11:32,  2.49it/s] 45%|████▍     | 1403/3145 [09:32<11:32,  2.52it/s] 44%|████▍     | 1389/3145 [09:32<13:15,  2.21it/s] 45%|████▍     | 1414/3145 [09:32<12:20,  2.34it/s] 45%|████▍     | 1404/3145 [09:32<10:40,  2.72it/s] 45%|████▌     | 1420/3143 [09:38<11:23,  2.52it/s] 44%|████▍     | 1390/3145 [09:32<12:48,  2.28it/s] 45%|████▍     | 1405/3145 [09:32<09:42,  2.99it/s] 45%|████▌     | 1421/3143 [09:38<09:57,  2.88it/s] 45%|████▍     | 1415/3145 [09:32<12:04,  2.39it/s] 45%|████▍     | 1406/3145 [09:33<10:13,  2.83it/s] 44%|████▍     | 1391/3145 [09:33<12:50,  2.28it/s] 45%|████▌     | 1422/3143 [09:38<10:21,  2.77it/s] 45%|████▌     | 1416/3145 [09:33<12:34,  2.29it/s] 45%|████▌     | 1423/3143 [09:39<10:25,  2.75it/s] 44%|████▍     | 1392/3145 [09:33<12:29,  2.34it/s] 45%|████▍     | 1407/3145 [09:33<10:43,  2.70it/s] 45%|████▌     | 1417/3145 [09:33<12:25,  2.32it/s] 45%|████▌     | 1424/3143 [09:39<10:33,  2.71it/s] 44%|████▍     | 1393/3145 [09:33<12:24,  2.35it/s] 45%|████▍     | 1408/3145 [09:33<11:03,  2.62it/s] 45%|████▌     | 1418/3145 [09:34<11:43,  2.46it/s] 45%|████▌     | 1425/3143 [09:40<10:30,  2.72it/s] 44%|████▍     | 1394/3145 [09:34<12:33,  2.32it/s] 45%|████▍     | 1409/3145 [09:34<12:02,  2.40it/s] 45%|████▌     | 1419/3145 [09:34<12:06,  2.38it/s] 45%|████▌     | 1426/3143 [09:40<11:05,  2.58it/s] 44%|████▍     | 1395/3145 [09:34<12:34,  2.32it/s] 45%|████▍     | 1410/3145 [09:34<12:22,  2.34it/s] 45%|████▌     | 1420/3145 [09:34<12:27,  2.31it/s] 45%|████▌     | 1427/3143 [09:40<10:33,  2.71it/s] 45%|████▍     | 1411/3145 [09:35<11:03,  2.61it/s] 44%|████▍     | 1396/3145 [09:35<12:16,  2.38it/s] 45%|████▌     | 1428/3143 [09:41<10:46,  2.65it/s] 45%|████▌     | 1421/3145 [09:35<12:27,  2.31it/s] 45%|████▍     | 1412/3145 [09:35<11:13,  2.57it/s] 44%|████▍     | 1397/3145 [09:35<11:52,  2.45it/s] 45%|████▌     | 1429/3143 [09:41<10:26,  2.74it/s] 45%|████▌     | 1422/3145 [09:35<12:07,  2.37it/s] 44%|████▍     | 1398/3145 [09:35<11:37,  2.51it/s] 45%|████▍     | 1413/3145 [09:35<11:39,  2.48it/s] 45%|████▌     | 1430/3143 [09:41<09:56,  2.87it/s] 45%|████▌     | 1423/3145 [09:36<11:48,  2.43it/s] 44%|████▍     | 1399/3145 [09:36<11:14,  2.59it/s] 46%|████▌     | 1431/3143 [09:42<09:39,  2.95it/s] 45%|████▍     | 1414/3145 [09:36<11:27,  2.52it/s] 45%|████▌     | 1424/3145 [09:36<11:38,  2.46it/s] 45%|████▍     | 1400/3145 [09:36<11:37,  2.50it/s] 45%|████▍     | 1415/3145 [09:36<11:11,  2.58it/s] 46%|████▌     | 1432/3143 [09:42<10:12,  2.79it/s] 45%|████▌     | 1425/3145 [09:36<11:53,  2.41it/s] 45%|████▍     | 1401/3145 [09:37<11:45,  2.47it/s] 46%|████▌     | 1433/3143 [09:43<10:49,  2.63it/s] 45%|████▌     | 1416/3145 [09:37<11:47,  2.44it/s] 45%|████▌     | 1426/3145 [09:37<11:38,  2.46it/s] 45%|████▌     | 1417/3145 [09:37<11:23,  2.53it/s] 46%|████▌     | 1434/3143 [09:43<11:02,  2.58it/s] 45%|████▍     | 1402/3145 [09:37<12:04,  2.41it/s] 45%|████▌     | 1427/3145 [09:37<11:32,  2.48it/s] 45%|████▍     | 1403/3145 [09:37<11:42,  2.48it/s] 45%|████▌     | 1418/3145 [09:37<11:29,  2.50it/s] 46%|████▌     | 1435/3143 [09:43<12:09,  2.34it/s] 45%|████▌     | 1428/3145 [09:38<11:38,  2.46it/s] 45%|████▍     | 1404/3145 [09:38<11:29,  2.53it/s] 45%|████▌     | 1419/3145 [09:38<11:25,  2.52it/s] 46%|████▌     | 1436/3143 [09:44<12:17,  2.32it/s] 45%|████▌     | 1429/3145 [09:38<11:24,  2.51it/s] 45%|████▌     | 1420/3145 [09:38<10:50,  2.65it/s] 45%|████▍     | 1405/3145 [09:38<11:31,  2.52it/s] 46%|████▌     | 1437/3143 [09:44<11:59,  2.37it/s] 45%|████▌     | 1430/3145 [09:38<11:43,  2.44it/s] 45%|████▌     | 1421/3145 [09:39<10:52,  2.64it/s] 45%|████▍     | 1406/3145 [09:39<11:31,  2.52it/s] 46%|████▌     | 1431/3145 [09:39<11:39,  2.45it/s] 46%|████▌     | 1438/3143 [09:45<12:40,  2.24it/s] 45%|████▌     | 1422/3145 [09:39<10:54,  2.63it/s] 45%|████▍     | 1407/3145 [09:39<11:54,  2.43it/s] 46%|████▌     | 1432/3145 [09:39<11:06,  2.57it/s] 45%|████▌     | 1423/3145 [09:39<10:50,  2.65it/s] 46%|████▌     | 1439/3143 [09:45<12:55,  2.20it/s] 45%|████▍     | 1408/3145 [09:39<12:09,  2.38it/s] 46%|████▌     | 1433/3145 [09:39<09:55,  2.87it/s] 45%|████▌     | 1424/3145 [09:40<11:18,  2.54it/s] 46%|████▌     | 1440/3143 [09:46<12:21,  2.30it/s] 45%|████▍     | 1409/3145 [09:40<11:42,  2.47it/s] 46%|████▌     | 1434/3145 [09:40<10:08,  2.81it/s] 45%|████▌     | 1425/3145 [09:40<11:04,  2.59it/s] 46%|████▌     | 1441/3143 [09:46<12:21,  2.29it/s] 46%|████▌     | 1435/3145 [09:40<10:31,  2.71it/s] 45%|████▍     | 1410/3145 [09:40<12:16,  2.35it/s] 45%|████▌     | 1426/3145 [09:41<11:19,  2.53it/s] 46%|████▌     | 1436/3145 [09:41<11:06,  2.57it/s] 45%|████▍     | 1411/3145 [09:41<12:31,  2.31it/s] 46%|████▌     | 1442/3143 [09:47<13:31,  2.10it/s] 45%|████▌     | 1427/3145 [09:41<10:57,  2.61it/s] 46%|████▌     | 1443/3143 [09:47<11:37,  2.44it/s] 46%|████▌     | 1437/3145 [09:41<11:28,  2.48it/s] 45%|████▌     | 1428/3145 [09:41<10:40,  2.68it/s] 45%|████▍     | 1412/3145 [09:41<13:21,  2.16it/s] 46%|████▌     | 1444/3143 [09:47<11:49,  2.39it/s] 46%|████▌     | 1438/3145 [09:42<11:11,  2.54it/s] 45%|████▌     | 1429/3145 [09:42<10:51,  2.63it/s] 45%|████▍     | 1413/3145 [09:42<13:13,  2.18it/s] 46%|████▌     | 1439/3145 [09:42<11:30,  2.47it/s] 46%|████▌     | 1445/3143 [09:48<12:38,  2.24it/s] 45%|████▌     | 1430/3145 [09:42<11:18,  2.53it/s] 45%|████▍     | 1414/3145 [09:42<13:03,  2.21it/s] 46%|████▌     | 1440/3145 [09:42<11:22,  2.50it/s] 46%|████▌     | 1446/3143 [09:48<11:59,  2.36it/s] 46%|████▌     | 1431/3145 [09:42<11:51,  2.41it/s] 46%|████▌     | 1447/3143 [09:48<10:30,  2.69it/s] 45%|████▍     | 1415/3145 [09:43<12:50,  2.24it/s] 46%|████▌     | 1441/3145 [09:43<10:59,  2.58it/s] 46%|████▌     | 1432/3145 [09:43<12:02,  2.37it/s] 46%|████▌     | 1448/3143 [09:49<10:26,  2.71it/s] 45%|████▌     | 1416/3145 [09:43<12:26,  2.31it/s] 46%|████▌     | 1442/3145 [09:43<11:21,  2.50it/s] 46%|████▌     | 1433/3145 [09:43<12:04,  2.36it/s] 46%|████▌     | 1449/3143 [09:49<10:56,  2.58it/s] 45%|████▌     | 1417/3145 [09:43<12:08,  2.37it/s] 46%|████▌     | 1443/3145 [09:43<10:59,  2.58it/s] 46%|████▌     | 1450/3143 [09:50<09:46,  2.89it/s] 46%|████▌     | 1434/3145 [09:44<11:42,  2.44it/s] 45%|████▌     | 1418/3145 [09:44<12:38,  2.28it/s] 46%|████▌     | 1444/3145 [09:44<11:22,  2.49it/s] 46%|████▌     | 1451/3143 [09:50<10:31,  2.68it/s] 46%|████▌     | 1435/3145 [09:44<11:52,  2.40it/s] 45%|████▌     | 1419/3145 [09:44<12:19,  2.33it/s] 46%|████▌     | 1445/3145 [09:44<11:59,  2.36it/s] 46%|████▌     | 1452/3143 [09:50<10:39,  2.65it/s] 46%|████▌     | 1436/3145 [09:45<11:33,  2.46it/s] 45%|████▌     | 1420/3145 [09:45<11:35,  2.48it/s] 46%|████▌     | 1446/3145 [09:45<12:02,  2.35it/s] 46%|████▌     | 1453/3143 [09:51<10:51,  2.59it/s] 46%|████▌     | 1437/3145 [09:45<11:26,  2.49it/s] 45%|████▌     | 1421/3145 [09:45<11:29,  2.50it/s] 46%|████▌     | 1447/3145 [09:45<11:49,  2.39it/s] 46%|████▋     | 1454/3143 [09:51<10:48,  2.60it/s] 46%|████▌     | 1438/3145 [09:45<11:53,  2.39it/s] 45%|████▌     | 1422/3145 [09:45<11:38,  2.47it/s] 46%|████▌     | 1448/3145 [09:46<11:21,  2.49it/s] 46%|████▋     | 1455/3143 [09:52<11:04,  2.54it/s] 46%|████▌     | 1439/3145 [09:46<11:44,  2.42it/s] 45%|████▌     | 1423/3145 [09:46<11:49,  2.43it/s] 46%|████▌     | 1449/3145 [09:46<11:11,  2.53it/s] 46%|████▋     | 1456/3143 [09:52<11:02,  2.55it/s] 46%|████▌     | 1440/3145 [09:46<12:10,  2.33it/s] 46%|████▌     | 1450/3145 [09:46<10:56,  2.58it/s] 45%|████▌     | 1424/3145 [09:46<12:17,  2.33it/s] 46%|████▋     | 1457/3143 [09:52<11:06,  2.53it/s] 46%|████▌     | 1441/3145 [09:47<11:45,  2.42it/s] 46%|████▌     | 1451/3145 [09:47<10:35,  2.66it/s] 45%|████▌     | 1425/3145 [09:47<12:13,  2.34it/s] 46%|████▋     | 1458/3143 [09:53<11:52,  2.36it/s] 46%|████▌     | 1442/3145 [09:47<11:40,  2.43it/s] 46%|████▌     | 1452/3145 [09:47<10:40,  2.64it/s] 45%|████▌     | 1426/3145 [09:47<12:09,  2.36it/s] 46%|████▋     | 1459/3143 [09:53<11:41,  2.40it/s] 46%|████▌     | 1453/3145 [09:47<10:39,  2.65it/s] 46%|████▌     | 1443/3145 [09:48<12:06,  2.34it/s] 45%|████▌     | 1427/3145 [09:48<11:40,  2.45it/s] 46%|████▋     | 1460/3143 [09:54<11:15,  2.49it/s] 45%|████▌     | 1428/3145 [09:48<10:04,  2.84it/s] 46%|████▌     | 1454/3145 [09:48<10:44,  2.62it/s] 46%|████▌     | 1444/3145 [09:48<12:07,  2.34it/s] 46%|████▋     | 1461/3143 [09:54<10:50,  2.59it/s] 45%|████▌     | 1429/3145 [09:48<10:15,  2.79it/s] 46%|████▋     | 1455/3145 [09:48<10:35,  2.66it/s] 46%|████▌     | 1445/3145 [09:48<12:24,  2.28it/s] 47%|████▋     | 1462/3143 [09:54<10:56,  2.56it/s] 46%|████▋     | 1456/3145 [09:49<10:25,  2.70it/s] 45%|████▌     | 1430/3145 [09:49<11:17,  2.53it/s] 46%|████▌     | 1446/3145 [09:49<12:08,  2.33it/s] 47%|████▋     | 1463/3143 [09:55<11:19,  2.47it/s] 46%|████▋     | 1457/3145 [09:49<10:20,  2.72it/s] 46%|████▌     | 1431/3145 [09:49<11:40,  2.45it/s] 46%|████▌     | 1447/3145 [09:49<11:55,  2.37it/s] 47%|████▋     | 1464/3143 [09:55<11:17,  2.48it/s] 46%|████▋     | 1458/3145 [09:49<10:52,  2.58it/s] 46%|████▌     | 1432/3145 [09:49<11:38,  2.45it/s] 46%|████▌     | 1448/3145 [09:50<11:59,  2.36it/s] 47%|████▋     | 1465/3143 [09:56<11:22,  2.46it/s] 46%|████▋     | 1459/3145 [09:50<11:04,  2.54it/s] 46%|████▌     | 1433/3145 [09:50<12:00,  2.38it/s] 46%|████▌     | 1449/3145 [09:50<11:51,  2.38it/s] 47%|████▋     | 1466/3143 [09:56<11:16,  2.48it/s] 46%|████▋     | 1460/3145 [09:50<11:37,  2.42it/s] 46%|████▌     | 1434/3145 [09:50<11:59,  2.38it/s] 46%|████▌     | 1450/3145 [09:50<11:56,  2.37it/s] 47%|████▋     | 1467/3143 [09:56<11:19,  2.47it/s] 46%|████▋     | 1461/3145 [09:51<11:08,  2.52it/s] 46%|████▌     | 1435/3145 [09:51<11:54,  2.39it/s] 46%|████▌     | 1451/3145 [09:51<12:02,  2.35it/s] 47%|████▋     | 1468/3143 [09:57<11:08,  2.51it/s] 46%|████▋     | 1462/3145 [09:51<11:07,  2.52it/s] 46%|████▌     | 1436/3145 [09:51<11:30,  2.48it/s] 47%|████▋     | 1469/3143 [09:57<11:13,  2.49it/s] 46%|████▌     | 1452/3145 [09:51<12:00,  2.35it/s] 47%|████▋     | 1463/3145 [09:51<11:02,  2.54it/s] 46%|████▌     | 1437/3145 [09:52<12:14,  2.33it/s] 46%|████▌     | 1453/3145 [09:52<11:34,  2.44it/s] 47%|████▋     | 1470/3143 [09:58<11:22,  2.45it/s] 47%|████▋     | 1464/3145 [09:52<11:20,  2.47it/s] 46%|████▌     | 1438/3145 [09:52<12:33,  2.27it/s] 46%|████▌     | 1454/3145 [09:52<11:23,  2.47it/s] 47%|████▋     | 1471/3143 [09:58<11:30,  2.42it/s] 47%|████▋     | 1465/3145 [09:52<11:13,  2.49it/s] 46%|████▋     | 1455/3145 [09:53<11:26,  2.46it/s] 47%|████▋     | 1472/3143 [09:58<11:25,  2.44it/s] 47%|████▋     | 1466/3145 [09:53<11:11,  2.50it/s] 46%|████▌     | 1439/3145 [09:53<13:28,  2.11it/s] 46%|████▋     | 1456/3145 [09:53<11:51,  2.37it/s] 47%|████▋     | 1467/3145 [09:53<11:25,  2.45it/s] 47%|████▋     | 1473/3143 [09:59<11:47,  2.36it/s] 46%|████▌     | 1440/3145 [09:53<13:46,  2.06it/s] 46%|████▋     | 1457/3145 [09:53<11:38,  2.42it/s] 47%|████▋     | 1468/3145 [09:53<11:51,  2.36it/s] 47%|████▋     | 1474/3143 [09:59<11:55,  2.33it/s] 46%|████▌     | 1441/3145 [09:54<13:41,  2.07it/s] 46%|████▋     | 1458/3145 [09:54<11:24,  2.47it/s] 47%|████▋     | 1469/3145 [09:54<11:46,  2.37it/s] 47%|████▋     | 1475/3143 [10:00<11:45,  2.36it/s] 46%|████▌     | 1442/3145 [09:54<13:17,  2.14it/s] 46%|████▋     | 1459/3145 [09:54<12:00,  2.34it/s] 47%|████▋     | 1476/3143 [10:00<11:31,  2.41it/s] 47%|████▋     | 1470/3145 [09:54<11:54,  2.34it/s] 46%|████▌     | 1443/3145 [09:55<13:38,  2.08it/s] 46%|████▋     | 1460/3145 [09:55<12:00,  2.34it/s] 47%|████▋     | 1477/3143 [10:01<11:23,  2.44it/s] 47%|████▋     | 1471/3145 [09:55<11:48,  2.36it/s] 47%|████▋     | 1478/3143 [10:01<10:58,  2.53it/s] 46%|████▋     | 1461/3145 [09:55<12:02,  2.33it/s] 46%|████▌     | 1444/3145 [09:55<13:55,  2.03it/s] 47%|████▋     | 1472/3145 [09:55<11:40,  2.39it/s] 46%|████▋     | 1462/3145 [09:55<11:35,  2.42it/s] 47%|████▋     | 1479/3143 [10:01<11:42,  2.37it/s] 46%|████▌     | 1445/3145 [09:56<13:45,  2.06it/s] 47%|████▋     | 1473/3145 [09:56<11:52,  2.35it/s] 47%|████▋     | 1463/3145 [09:56<11:38,  2.41it/s] 46%|████▌     | 1446/3145 [09:56<12:58,  2.18it/s] 47%|████▋     | 1474/3145 [09:56<11:41,  2.38it/s] 47%|████▋     | 1480/3143 [10:02<12:05,  2.29it/s] 47%|████▋     | 1464/3145 [09:56<11:49,  2.37it/s] 47%|████▋     | 1475/3145 [09:56<11:37,  2.39it/s] 47%|████▋     | 1481/3143 [10:02<11:55,  2.32it/s] 46%|████▌     | 1447/3145 [09:57<14:14,  1.99it/s] 47%|████▋     | 1465/3145 [09:57<11:27,  2.44it/s] 47%|████▋     | 1476/3145 [09:57<11:10,  2.49it/s] 47%|████▋     | 1482/3143 [10:03<11:51,  2.33it/s] 46%|████▌     | 1448/3145 [09:57<13:49,  2.05it/s] 47%|████▋     | 1477/3145 [09:57<11:06,  2.50it/s] 47%|████▋     | 1483/3143 [10:03<11:36,  2.38it/s] 47%|████▋     | 1466/3145 [09:57<12:31,  2.23it/s] 46%|████▌     | 1449/3145 [09:57<13:04,  2.16it/s] 47%|████▋     | 1478/3145 [09:58<11:25,  2.43it/s] 47%|████▋     | 1484/3143 [10:04<11:25,  2.42it/s] 47%|████▋     | 1467/3145 [09:58<13:14,  2.11it/s] 46%|████▌     | 1450/3145 [09:58<12:30,  2.26it/s] 47%|████▋     | 1485/3143 [10:04<10:05,  2.74it/s] 47%|████▋     | 1479/3145 [09:58<11:18,  2.46it/s] 46%|████▌     | 1451/3145 [09:58<12:29,  2.26it/s] 47%|████▋     | 1486/3143 [10:04<10:20,  2.67it/s] 47%|████▋     | 1480/3145 [09:58<10:53,  2.55it/s] 47%|████▋     | 1468/3145 [09:58<14:23,  1.94it/s] 47%|████▋     | 1487/3143 [10:05<10:20,  2.67it/s] 46%|████▌     | 1452/3145 [09:59<12:08,  2.33it/s] 47%|████▋     | 1481/3145 [09:59<10:48,  2.57it/s] 47%|████▋     | 1488/3143 [10:05<09:08,  3.02it/s] 47%|████▋     | 1469/3145 [09:59<15:04,  1.85it/s] 46%|████▌     | 1453/3145 [09:59<11:57,  2.36it/s] 47%|████▋     | 1482/3145 [09:59<11:16,  2.46it/s] 47%|████▋     | 1489/3143 [10:05<09:43,  2.84it/s] 47%|████▋     | 1470/3145 [09:59<14:43,  1.90it/s] 46%|████▌     | 1454/3145 [10:00<12:17,  2.29it/s] 47%|████▋     | 1483/3145 [10:00<11:00,  2.52it/s] 47%|████▋     | 1490/3143 [10:06<10:03,  2.74it/s] 46%|████▋     | 1455/3145 [10:00<11:43,  2.40it/s] 47%|████▋     | 1471/3145 [10:00<13:57,  2.00it/s] 47%|████▋     | 1484/3145 [10:00<10:56,  2.53it/s] 47%|████▋     | 1491/3143 [10:06<10:23,  2.65it/s] 46%|████▋     | 1456/3145 [10:00<11:34,  2.43it/s] 47%|████▋     | 1485/3145 [10:00<11:13,  2.46it/s] 47%|████▋     | 1472/3145 [10:00<13:37,  2.05it/s] 47%|████▋     | 1492/3143 [10:06<10:42,  2.57it/s] 46%|████▋     | 1457/3145 [10:01<11:47,  2.39it/s] 47%|████▋     | 1486/3145 [10:01<10:57,  2.52it/s] 47%|████▋     | 1473/3145 [10:01<12:51,  2.17it/s] 48%|████▊     | 1493/3143 [10:07<10:56,  2.52it/s] 47%|████▋     | 1487/3145 [10:01<11:05,  2.49it/s] 46%|████▋     | 1458/3145 [10:01<12:00,  2.34it/s] 47%|████▋     | 1474/3145 [10:01<12:36,  2.21it/s] 48%|████▊     | 1494/3143 [10:07<11:03,  2.49it/s] 47%|████▋     | 1475/3145 [10:02<11:52,  2.34it/s] 47%|████▋     | 1488/3145 [10:02<11:19,  2.44it/s] 46%|████▋     | 1459/3145 [10:02<12:10,  2.31it/s] 48%|████▊     | 1495/3143 [10:08<11:18,  2.43it/s] 47%|████▋     | 1476/3145 [10:02<11:24,  2.44it/s] 47%|████▋     | 1489/3145 [10:02<11:13,  2.46it/s] 46%|████▋     | 1460/3145 [10:02<11:56,  2.35it/s] 48%|████▊     | 1496/3143 [10:08<10:42,  2.56it/s] 47%|████▋     | 1477/3145 [10:02<11:06,  2.50it/s] 47%|████▋     | 1490/3145 [10:02<11:31,  2.39it/s] 46%|████▋     | 1461/3145 [10:02<11:49,  2.37it/s] 48%|████▊     | 1497/3143 [10:08<11:03,  2.48it/s] 46%|████▋     | 1462/3145 [10:03<10:10,  2.76it/s] 47%|████▋     | 1478/3145 [10:03<11:13,  2.48it/s] 47%|████▋     | 1491/3145 [10:03<11:21,  2.43it/s] 48%|████▊     | 1498/3143 [10:09<11:17,  2.43it/s] 47%|████▋     | 1479/3145 [10:03<10:02,  2.76it/s] 47%|████▋     | 1463/3145 [10:03<10:50,  2.58it/s] 47%|████▋     | 1492/3145 [10:03<11:10,  2.47it/s] 48%|████▊     | 1499/3143 [10:09<11:25,  2.40it/s] 47%|████▋     | 1480/3145 [10:03<10:34,  2.63it/s] 47%|████▋     | 1464/3145 [10:04<11:36,  2.41it/s] 47%|████▋     | 1493/3145 [10:04<11:02,  2.50it/s] 48%|████▊     | 1500/3143 [10:10<11:32,  2.37it/s] 47%|████▋     | 1481/3145 [10:04<11:16,  2.46it/s] 47%|████▋     | 1465/3145 [10:04<11:12,  2.50it/s] 48%|████▊     | 1494/3145 [10:04<10:58,  2.51it/s] 47%|████▋     | 1482/3145 [10:04<10:44,  2.58it/s] 48%|████▊     | 1501/3143 [10:10<11:40,  2.34it/s] 47%|████▋     | 1466/3145 [10:04<11:32,  2.42it/s] 48%|████▊     | 1495/3145 [10:04<11:16,  2.44it/s] 47%|████▋     | 1483/3145 [10:05<10:48,  2.56it/s] 48%|████▊     | 1502/3143 [10:11<11:19,  2.42it/s] 47%|████▋     | 1467/3145 [10:05<11:28,  2.44it/s] 48%|████▊     | 1496/3145 [10:05<11:31,  2.39it/s] 47%|████▋     | 1484/3145 [10:05<10:51,  2.55it/s] 47%|████▋     | 1468/3145 [10:05<11:20,  2.46it/s] 48%|████▊     | 1503/3143 [10:11<12:32,  2.18it/s] 48%|████▊     | 1497/3145 [10:05<11:04,  2.48it/s] 47%|████▋     | 1485/3145 [10:05<10:57,  2.53it/s] 47%|████▋     | 1469/3145 [10:06<11:28,  2.43it/s] 48%|████▊     | 1498/3145 [10:06<11:14,  2.44it/s] 48%|████▊     | 1504/3143 [10:12<13:10,  2.07it/s] 47%|████▋     | 1486/3145 [10:06<11:40,  2.37it/s] 47%|████▋     | 1470/3145 [10:06<11:46,  2.37it/s] 48%|████▊     | 1499/3145 [10:06<11:13,  2.45it/s] 48%|████▊     | 1505/3143 [10:12<14:09,  1.93it/s] 47%|████▋     | 1487/3145 [10:06<11:59,  2.30it/s] 48%|████▊     | 1500/3145 [10:06<11:09,  2.46it/s] 47%|████▋     | 1471/3145 [10:07<12:01,  2.32it/s] 48%|████▊     | 1506/3143 [10:13<12:49,  2.13it/s] 47%|████▋     | 1488/3145 [10:07<11:34,  2.38it/s] 48%|████▊     | 1501/3145 [10:07<10:46,  2.54it/s] 47%|████▋     | 1472/3145 [10:07<11:56,  2.33it/s] 48%|████▊     | 1507/3143 [10:13<12:25,  2.19it/s] 47%|████▋     | 1489/3145 [10:07<11:37,  2.38it/s] 48%|████▊     | 1502/3145 [10:07<10:48,  2.53it/s] 47%|████▋     | 1473/3145 [10:07<11:19,  2.46it/s] 48%|████▊     | 1508/3143 [10:13<11:51,  2.30it/s] 47%|████▋     | 1490/3145 [10:08<12:03,  2.29it/s] 48%|████▊     | 1503/3145 [10:08<11:08,  2.45it/s] 47%|████▋     | 1474/3145 [10:08<11:22,  2.45it/s] 48%|████▊     | 1509/3143 [10:14<11:14,  2.42it/s] 48%|████▊     | 1504/3145 [10:08<11:05,  2.47it/s] 47%|████▋     | 1491/3145 [10:08<12:00,  2.29it/s] 47%|████▋     | 1475/3145 [10:08<11:22,  2.45it/s] 48%|████▊     | 1505/3145 [10:08<09:36,  2.84it/s] 48%|████▊     | 1510/3143 [10:14<11:36,  2.34it/s] 47%|████▋     | 1476/3145 [10:09<11:22,  2.45it/s] 47%|████▋     | 1492/3145 [10:09<12:09,  2.27it/s] 48%|████▊     | 1506/3145 [10:09<09:57,  2.74it/s] 48%|████▊     | 1511/3143 [10:15<12:20,  2.20it/s] 47%|████▋     | 1493/3145 [10:09<12:08,  2.27it/s] 47%|████▋     | 1477/3145 [10:09<11:43,  2.37it/s] 48%|████▊     | 1507/3145 [10:09<10:24,  2.62it/s] 48%|████▊     | 1512/3143 [10:15<11:35,  2.34it/s] 47%|████▋     | 1478/3145 [10:09<11:21,  2.45it/s] 48%|████▊     | 1494/3145 [10:09<12:19,  2.23it/s] 48%|████▊     | 1508/3145 [10:10<11:00,  2.48it/s] 48%|████▊     | 1513/3143 [10:16<11:26,  2.37it/s] 47%|████▋     | 1479/3145 [10:10<11:24,  2.43it/s] 48%|████▊     | 1495/3145 [10:10<12:12,  2.25it/s] 48%|████▊     | 1509/3145 [10:10<10:43,  2.54it/s] 47%|████▋     | 1480/3145 [10:10<10:13,  2.71it/s] 48%|████▊     | 1514/3143 [10:16<11:32,  2.35it/s] 48%|████▊     | 1496/3145 [10:10<11:03,  2.48it/s] 48%|████▊     | 1510/3145 [10:10<11:06,  2.45it/s] 47%|████▋     | 1481/3145 [10:10<10:30,  2.64it/s] 48%|████▊     | 1515/3143 [10:16<11:42,  2.32it/s] 48%|████▊     | 1497/3145 [10:11<11:00,  2.50it/s] 48%|████▊     | 1511/3145 [10:11<10:33,  2.58it/s] 47%|████▋     | 1482/3145 [10:11<10:36,  2.61it/s] 48%|████▊     | 1498/3145 [10:11<10:36,  2.59it/s] 48%|████▊     | 1516/3143 [10:17<11:43,  2.31it/s] 48%|████▊     | 1512/3145 [10:11<11:33,  2.36it/s] 47%|████▋     | 1483/3145 [10:11<10:39,  2.60it/s] 48%|████▊     | 1499/3145 [10:11<10:51,  2.53it/s] 48%|████▊     | 1517/3143 [10:17<11:39,  2.32it/s] 47%|████▋     | 1484/3145 [10:12<10:33,  2.62it/s] 48%|████▊     | 1513/3145 [10:12<11:12,  2.43it/s] 48%|████▊     | 1518/3143 [10:18<11:46,  2.30it/s] 48%|████▊     | 1500/3145 [10:12<11:23,  2.41it/s] 47%|████▋     | 1485/3145 [10:12<09:48,  2.82it/s] 48%|████▊     | 1514/3145 [10:12<11:22,  2.39it/s] 48%|████▊     | 1519/3143 [10:18<11:09,  2.42it/s] 48%|████▊     | 1501/3145 [10:12<11:05,  2.47it/s] 47%|████▋     | 1486/3145 [10:12<10:09,  2.72it/s] 48%|████▊     | 1515/3145 [10:12<11:28,  2.37it/s] 48%|████▊     | 1520/3143 [10:18<10:58,  2.46it/s] 48%|████▊     | 1502/3145 [10:13<11:10,  2.45it/s] 47%|████▋     | 1487/3145 [10:13<10:29,  2.63it/s] 48%|████▊     | 1516/3145 [10:13<11:33,  2.35it/s] 48%|████▊     | 1521/3143 [10:19<10:48,  2.50it/s] 48%|████▊     | 1503/3145 [10:13<10:54,  2.51it/s] 47%|████▋     | 1488/3145 [10:13<10:35,  2.61it/s] 48%|████▊     | 1517/3145 [10:13<11:26,  2.37it/s] 48%|████▊     | 1504/3145 [10:13<10:42,  2.55it/s] 48%|████▊     | 1522/3143 [10:19<11:11,  2.41it/s] 47%|████▋     | 1489/3145 [10:14<11:23,  2.42it/s] 48%|████▊     | 1505/3145 [10:14<10:18,  2.65it/s] 48%|████▊     | 1518/3145 [10:14<11:30,  2.36it/s] 48%|████▊     | 1523/3143 [10:20<11:47,  2.29it/s] 47%|████▋     | 1490/3145 [10:14<11:16,  2.45it/s] 48%|████▊     | 1506/3145 [10:14<10:28,  2.61it/s] 48%|████▊     | 1519/3145 [10:14<11:00,  2.46it/s] 48%|████▊     | 1524/3143 [10:20<11:56,  2.26it/s] 47%|████▋     | 1491/3145 [10:14<11:18,  2.44it/s] 48%|████▊     | 1520/3145 [10:14<10:39,  2.54it/s] 48%|████▊     | 1507/3145 [10:15<10:51,  2.51it/s] 49%|████▊     | 1525/3143 [10:21<11:29,  2.35it/s] 47%|████▋     | 1492/3145 [10:15<11:44,  2.35it/s] 48%|████▊     | 1508/3145 [10:15<10:46,  2.53it/s] 48%|████▊     | 1521/3145 [10:15<11:10,  2.42it/s] 49%|████▊     | 1526/3143 [10:21<11:16,  2.39it/s] 47%|████▋     | 1493/3145 [10:15<11:07,  2.48it/s] 48%|████▊     | 1522/3145 [10:15<11:01,  2.45it/s] 48%|████▊     | 1509/3145 [10:15<11:50,  2.30it/s] 48%|████▊     | 1494/3145 [10:16<10:39,  2.58it/s] 49%|████▊     | 1527/3143 [10:21<11:28,  2.35it/s] 48%|████▊     | 1523/3145 [10:16<10:56,  2.47it/s] 48%|████▊     | 1510/3145 [10:16<11:02,  2.47it/s] 49%|████▊     | 1528/3143 [10:22<10:06,  2.66it/s] 48%|████▊     | 1495/3145 [10:16<10:37,  2.59it/s] 48%|████▊     | 1524/3145 [10:16<10:34,  2.56it/s] 48%|████▊     | 1511/3145 [10:16<11:08,  2.44it/s] 49%|████▊     | 1529/3143 [10:22<11:22,  2.36it/s] 48%|████▊     | 1496/3145 [10:16<11:08,  2.47it/s] 48%|████▊     | 1512/3145 [10:17<11:00,  2.47it/s] 48%|████▊     | 1525/3145 [10:17<11:27,  2.36it/s] 48%|████▊     | 1497/3145 [10:17<10:40,  2.57it/s] 49%|████▊     | 1530/3143 [10:23<11:31,  2.33it/s] 49%|████▊     | 1526/3145 [10:17<11:22,  2.37it/s] 48%|████▊     | 1513/3145 [10:17<11:14,  2.42it/s] 49%|████▊     | 1531/3143 [10:23<10:16,  2.62it/s] 48%|████▊     | 1498/3145 [10:17<10:18,  2.66it/s] 48%|████▊     | 1514/3145 [10:17<09:50,  2.76it/s] 49%|████▊     | 1527/3145 [10:17<10:56,  2.47it/s] 49%|████▊     | 1532/3143 [10:23<10:22,  2.59it/s] 48%|████▊     | 1499/3145 [10:18<11:18,  2.43it/s] 49%|████▊     | 1528/3145 [10:18<09:48,  2.75it/s] 48%|████▊     | 1515/3145 [10:18<10:47,  2.52it/s] 49%|████▉     | 1533/3143 [10:24<10:23,  2.58it/s] 48%|████▊     | 1500/3145 [10:18<11:04,  2.47it/s] 49%|████▊     | 1529/3145 [10:18<10:04,  2.67it/s] 48%|████▊     | 1516/3145 [10:18<10:26,  2.60it/s] 49%|████▉     | 1534/3143 [10:24<10:29,  2.56it/s] 48%|████▊     | 1501/3145 [10:18<10:50,  2.53it/s] 49%|████▊     | 1530/3145 [10:18<10:30,  2.56it/s] 48%|████▊     | 1517/3145 [10:19<10:33,  2.57it/s] 49%|████▉     | 1535/3143 [10:25<10:36,  2.53it/s] 48%|████▊     | 1502/3145 [10:19<10:57,  2.50it/s] 49%|████▊     | 1531/3145 [10:19<10:35,  2.54it/s] 48%|████▊     | 1518/3145 [10:19<10:44,  2.52it/s] 49%|████▉     | 1536/3143 [10:25<10:26,  2.57it/s] 48%|████▊     | 1503/3145 [10:19<11:08,  2.45it/s] 49%|████▊     | 1532/3145 [10:19<10:30,  2.56it/s] 48%|████▊     | 1519/3145 [10:19<10:33,  2.57it/s] 49%|████▉     | 1537/3143 [10:25<09:30,  2.81it/s] 49%|████▊     | 1533/3145 [10:20<10:18,  2.61it/s] 48%|████▊     | 1504/3145 [10:20<11:49,  2.31it/s] 48%|████▊     | 1520/3145 [10:20<10:58,  2.47it/s] 49%|████▉     | 1538/3143 [10:26<10:11,  2.62it/s] 49%|████▉     | 1534/3145 [10:20<10:28,  2.57it/s] 48%|████▊     | 1505/3145 [10:20<11:51,  2.31it/s] 49%|████▉     | 1539/3143 [10:26<10:06,  2.65it/s] 48%|████▊     | 1521/3145 [10:20<11:11,  2.42it/s] 49%|████▉     | 1535/3145 [10:20<10:20,  2.60it/s] 48%|████▊     | 1506/3145 [10:20<11:34,  2.36it/s] 49%|████▉     | 1540/3143 [10:27<10:58,  2.43it/s] 48%|████▊     | 1522/3145 [10:21<11:30,  2.35it/s] 49%|████▉     | 1536/3145 [10:21<10:30,  2.55it/s] 48%|████▊     | 1507/3145 [10:21<11:27,  2.38it/s] 48%|████▊     | 1523/3145 [10:21<11:22,  2.38it/s] 49%|████▉     | 1541/3143 [10:27<11:18,  2.36it/s] 49%|████▉     | 1537/3145 [10:21<10:14,  2.62it/s] 48%|████▊     | 1508/3145 [10:21<11:01,  2.48it/s] 48%|████▊     | 1524/3145 [10:21<11:29,  2.35it/s] 49%|████▉     | 1542/3143 [10:27<11:35,  2.30it/s] 49%|████▉     | 1538/3145 [10:22<10:14,  2.61it/s] 48%|████▊     | 1509/3145 [10:22<11:01,  2.47it/s] 48%|████▊     | 1525/3145 [10:22<11:22,  2.37it/s] 49%|████▉     | 1543/3143 [10:28<11:12,  2.38it/s] 49%|████▉     | 1539/3145 [10:22<10:36,  2.52it/s] 48%|████▊     | 1510/3145 [10:22<10:58,  2.48it/s] 49%|████▊     | 1526/3145 [10:22<11:08,  2.42it/s] 49%|████▉     | 1540/3145 [10:22<10:12,  2.62it/s] 49%|████▉     | 1544/3143 [10:28<11:23,  2.34it/s] 48%|████▊     | 1511/3145 [10:22<11:07,  2.45it/s] 49%|████▊     | 1527/3145 [10:23<10:49,  2.49it/s] 49%|████▉     | 1541/3145 [10:23<10:34,  2.53it/s] 49%|████▉     | 1545/3143 [10:29<11:09,  2.39it/s] 48%|████▊     | 1512/3145 [10:23<11:32,  2.36it/s] 49%|████▊     | 1528/3145 [10:23<10:45,  2.50it/s] 49%|████▉     | 1546/3143 [10:29<11:07,  2.39it/s] 49%|████▉     | 1542/3145 [10:23<10:56,  2.44it/s] 48%|████▊     | 1513/3145 [10:23<11:12,  2.43it/s] 49%|████▊     | 1529/3145 [10:23<10:43,  2.51it/s] 49%|████▉     | 1547/3143 [10:29<11:09,  2.38it/s] 49%|████▉     | 1543/3145 [10:24<10:50,  2.46it/s] 49%|████▊     | 1530/3145 [10:24<09:50,  2.74it/s] 48%|████▊     | 1514/3145 [10:24<11:04,  2.45it/s] 49%|████▉     | 1548/3143 [10:30<11:04,  2.40it/s] 49%|████▉     | 1544/3145 [10:24<11:07,  2.40it/s] 49%|████▊     | 1531/3145 [10:24<10:14,  2.63it/s] 48%|████▊     | 1515/3145 [10:24<11:04,  2.45it/s] 49%|████▉     | 1549/3143 [10:30<10:46,  2.47it/s] 49%|████▉     | 1545/3145 [10:24<11:16,  2.37it/s] 49%|████▊     | 1532/3145 [10:25<10:05,  2.66it/s] 48%|████▊     | 1516/3145 [10:25<10:42,  2.54it/s] 49%|████▉     | 1550/3143 [10:31<10:58,  2.42it/s] 49%|████▉     | 1546/3145 [10:25<11:06,  2.40it/s] 49%|████▊     | 1533/3145 [10:25<10:20,  2.60it/s] 48%|████▊     | 1517/3145 [10:25<11:53,  2.28it/s] 49%|████▉     | 1551/3143 [10:31<10:47,  2.46it/s] 49%|████▉     | 1547/3145 [10:25<10:44,  2.48it/s] 49%|████▉     | 1534/3145 [10:25<10:28,  2.57it/s] 49%|████▉     | 1552/3143 [10:32<10:51,  2.44it/s] 49%|████▉     | 1548/3145 [10:26<10:44,  2.48it/s] 48%|████▊     | 1518/3145 [10:26<13:13,  2.05it/s] 49%|████▉     | 1535/3145 [10:26<10:30,  2.55it/s] 49%|████▉     | 1553/3143 [10:32<10:49,  2.45it/s] 49%|████▉     | 1549/3145 [10:26<10:44,  2.48it/s] 48%|████▊     | 1519/3145 [10:26<12:40,  2.14it/s] 49%|████▉     | 1536/3145 [10:26<10:44,  2.50it/s] 49%|████▉     | 1554/3143 [10:32<10:46,  2.46it/s] 49%|████▉     | 1550/3145 [10:26<10:51,  2.45it/s] 48%|████▊     | 1520/3145 [10:27<13:00,  2.08it/s] 49%|████▉     | 1537/3145 [10:27<11:34,  2.32it/s] 49%|████▉     | 1555/3143 [10:33<09:25,  2.81it/s] 49%|████▉     | 1551/3145 [10:27<11:16,  2.36it/s] 49%|████▉     | 1538/3145 [10:27<11:16,  2.38it/s] 50%|████▉     | 1556/3143 [10:33<09:42,  2.72it/s] 48%|████▊     | 1521/3145 [10:27<13:15,  2.04it/s] 49%|████▉     | 1552/3145 [10:27<10:57,  2.42it/s] 49%|████▉     | 1539/3145 [10:27<11:05,  2.41it/s] 50%|████▉     | 1557/3143 [10:33<10:22,  2.55it/s] 48%|████▊     | 1522/3145 [10:28<13:57,  1.94it/s] 49%|████▉     | 1553/3145 [10:28<10:49,  2.45it/s] 49%|████▉     | 1540/3145 [10:28<11:13,  2.38it/s] 50%|████▉     | 1558/3143 [10:34<10:11,  2.59it/s] 48%|████▊     | 1523/3145 [10:28<13:01,  2.07it/s] 49%|████▉     | 1554/3145 [10:28<10:49,  2.45it/s] 49%|████▉     | 1541/3145 [10:28<11:02,  2.42it/s] 50%|████▉     | 1559/3143 [10:34<10:22,  2.54it/s] 48%|████▊     | 1524/3145 [10:28<11:22,  2.37it/s] 49%|████▉     | 1555/3145 [10:29<10:45,  2.46it/s] 49%|████▉     | 1542/3145 [10:29<10:46,  2.48it/s] 50%|████▉     | 1560/3143 [10:35<09:57,  2.65it/s] 48%|████▊     | 1525/3145 [10:29<10:48,  2.50it/s] 49%|████▉     | 1556/3145 [10:29<10:23,  2.55it/s] 49%|████▉     | 1543/3145 [10:29<10:57,  2.44it/s] 50%|████▉     | 1561/3143 [10:35<10:33,  2.50it/s] 49%|████▊     | 1526/3145 [10:29<11:16,  2.39it/s] 50%|████▉     | 1557/3145 [10:29<10:25,  2.54it/s] 49%|████▉     | 1544/3145 [10:29<10:50,  2.46it/s] 50%|████▉     | 1562/3143 [10:35<10:29,  2.51it/s] 49%|████▊     | 1527/3145 [10:30<11:21,  2.38it/s] 50%|████▉     | 1558/3145 [10:30<10:18,  2.57it/s] 50%|████▉     | 1563/3143 [10:36<10:26,  2.52it/s] 49%|████▉     | 1545/3145 [10:30<11:03,  2.41it/s] 49%|████▊     | 1528/3145 [10:30<10:51,  2.48it/s] 50%|████▉     | 1559/3145 [10:30<09:57,  2.65it/s] 50%|████▉     | 1564/3143 [10:36<10:21,  2.54it/s] 49%|████▉     | 1546/3145 [10:30<11:17,  2.36it/s] 49%|████▊     | 1529/3145 [10:30<10:40,  2.52it/s] 50%|████▉     | 1560/3145 [10:30<09:52,  2.67it/s] 50%|████▉     | 1565/3143 [10:37<10:24,  2.53it/s] 49%|████▊     | 1530/3145 [10:31<10:47,  2.49it/s] 50%|████▉     | 1561/3145 [10:31<10:02,  2.63it/s] 49%|████▉     | 1547/3145 [10:31<11:44,  2.27it/s] 50%|████▉     | 1566/3143 [10:37<10:16,  2.56it/s] 49%|████▊     | 1531/3145 [10:31<10:54,  2.47it/s] 50%|████▉     | 1562/3145 [10:31<10:28,  2.52it/s] 49%|████▉     | 1548/3145 [10:31<11:42,  2.27it/s] 50%|████▉     | 1567/3143 [10:37<10:35,  2.48it/s] 49%|████▊     | 1532/3145 [10:32<10:49,  2.48it/s] 50%|████▉     | 1563/3145 [10:32<10:11,  2.59it/s] 49%|████▉     | 1549/3145 [10:32<11:17,  2.36it/s] 50%|████▉     | 1568/3143 [10:38<10:38,  2.47it/s] 50%|████▉     | 1564/3145 [10:32<10:16,  2.57it/s] 49%|████▉     | 1550/3145 [10:32<11:08,  2.38it/s] 49%|████▊     | 1533/3145 [10:32<12:05,  2.22it/s] 50%|████▉     | 1569/3143 [10:38<10:37,  2.47it/s] 50%|████▉     | 1565/3145 [10:32<10:20,  2.55it/s] 49%|████▉     | 1551/3145 [10:32<10:46,  2.47it/s] 49%|████▉     | 1534/3145 [10:33<12:17,  2.19it/s] 50%|████▉     | 1570/3143 [10:39<10:10,  2.58it/s] 50%|████▉     | 1566/3145 [10:33<10:21,  2.54it/s] 49%|████▉     | 1552/3145 [10:33<10:57,  2.42it/s] 49%|████▉     | 1535/3145 [10:33<11:22,  2.36it/s] 50%|████▉     | 1571/3143 [10:39<10:16,  2.55it/s] 50%|████▉     | 1567/3145 [10:33<10:41,  2.46it/s] 49%|████▉     | 1536/3145 [10:33<10:37,  2.52it/s] 49%|████▉     | 1553/3145 [10:33<12:01,  2.21it/s] 50%|█████     | 1572/3143 [10:39<10:23,  2.52it/s] 49%|████▉     | 1537/3145 [10:34<09:17,  2.88it/s] 50%|████▉     | 1568/3145 [10:34<10:35,  2.48it/s] 49%|████▉     | 1554/3145 [10:34<11:23,  2.33it/s] 49%|████▉     | 1538/3145 [10:34<09:30,  2.82it/s] 50%|█████     | 1573/3143 [10:40<10:54,  2.40it/s] 50%|████▉     | 1569/3145 [10:34<10:35,  2.48it/s] 49%|████▉     | 1555/3145 [10:34<10:52,  2.44it/s] 49%|████▉     | 1539/3145 [10:34<10:02,  2.66it/s] 50%|█████     | 1574/3143 [10:40<10:55,  2.39it/s] 50%|████▉     | 1570/3145 [10:34<10:30,  2.50it/s] 49%|████▉     | 1540/3145 [10:35<08:59,  2.98it/s] 49%|████▉     | 1556/3145 [10:35<11:06,  2.38it/s] 50%|█████     | 1575/3143 [10:41<10:39,  2.45it/s] 50%|████▉     | 1571/3145 [10:35<10:10,  2.58it/s] 50%|████▉     | 1557/3145 [10:35<11:04,  2.39it/s] 49%|████▉     | 1541/3145 [10:35<10:04,  2.65it/s] 50%|█████     | 1576/3143 [10:41<10:33,  2.47it/s] 50%|████▉     | 1572/3145 [10:35<10:17,  2.55it/s] 50%|█████     | 1573/3145 [10:35<09:06,  2.88it/s] 50%|████▉     | 1558/3145 [10:35<11:00,  2.40it/s] 49%|████▉     | 1542/3145 [10:35<10:34,  2.53it/s] 50%|█████     | 1577/3143 [10:41<10:45,  2.43it/s] 50%|█████     | 1574/3145 [10:36<08:08,  3.22it/s] 50%|████▉     | 1559/3145 [10:36<11:10,  2.36it/s] 49%|████▉     | 1543/3145 [10:36<10:46,  2.48it/s] 50%|█████     | 1578/3143 [10:42<10:48,  2.41it/s] 50%|█████     | 1575/3145 [10:36<08:51,  2.96it/s] 49%|████▉     | 1544/3145 [10:36<10:43,  2.49it/s] 50%|████▉     | 1560/3145 [10:36<11:51,  2.23it/s] 50%|█████     | 1579/3143 [10:42<10:40,  2.44it/s] 50%|█████     | 1576/3145 [10:36<09:14,  2.83it/s] 50%|█████     | 1577/3145 [10:37<08:22,  3.12it/s] 49%|████▉     | 1545/3145 [10:37<10:45,  2.48it/s] 50%|████▉     | 1561/3145 [10:37<11:14,  2.35it/s] 50%|█████     | 1580/3143 [10:43<11:08,  2.34it/s] 50%|█████     | 1578/3145 [10:37<08:53,  2.94it/s] 50%|████▉     | 1562/3145 [10:37<10:50,  2.44it/s] 49%|████▉     | 1546/3145 [10:37<11:00,  2.42it/s] 50%|█████     | 1581/3143 [10:43<10:38,  2.44it/s] 50%|█████     | 1579/3145 [10:37<08:10,  3.19it/s] 50%|████▉     | 1563/3145 [10:38<10:42,  2.46it/s] 49%|████▉     | 1547/3145 [10:38<10:53,  2.45it/s] 50%|█████     | 1582/3143 [10:44<10:52,  2.39it/s] 50%|█████     | 1580/3145 [10:38<08:36,  3.03it/s] 50%|████▉     | 1564/3145 [10:38<10:56,  2.41it/s] 49%|████▉     | 1548/3145 [10:38<11:18,  2.35it/s] 50%|█████     | 1583/3143 [10:44<10:42,  2.43it/s] 50%|█████     | 1581/3145 [10:38<09:16,  2.81it/s] 50%|████▉     | 1565/3145 [10:38<10:52,  2.42it/s] 49%|████▉     | 1549/3145 [10:38<11:02,  2.41it/s] 50%|█████     | 1582/3145 [10:38<08:59,  2.90it/s] 50%|█████     | 1584/3143 [10:44<10:40,  2.43it/s] 50%|████▉     | 1566/3145 [10:39<11:08,  2.36it/s] 49%|████▉     | 1550/3145 [10:39<11:13,  2.37it/s] 50%|█████     | 1583/3145 [10:39<09:51,  2.64it/s] 50%|█████     | 1585/3143 [10:45<10:56,  2.37it/s] 50%|████▉     | 1567/3145 [10:39<11:10,  2.35it/s] 49%|████▉     | 1551/3145 [10:39<11:14,  2.36it/s] 50%|█████     | 1584/3145 [10:39<10:00,  2.60it/s] 50%|█████     | 1586/3143 [10:45<10:49,  2.40it/s] 50%|████▉     | 1568/3145 [10:40<10:59,  2.39it/s] 49%|████▉     | 1552/3145 [10:40<11:04,  2.40it/s] 50%|█████     | 1585/3145 [10:40<10:04,  2.58it/s] 50%|█████     | 1587/3143 [10:46<11:00,  2.36it/s] 49%|████▉     | 1553/3145 [10:40<10:40,  2.49it/s] 50%|████▉     | 1569/3145 [10:40<11:05,  2.37it/s] 50%|█████     | 1586/3145 [10:40<10:28,  2.48it/s] 51%|█████     | 1588/3143 [10:46<11:19,  2.29it/s] 50%|████▉     | 1570/3145 [10:40<10:45,  2.44it/s] 49%|████▉     | 1554/3145 [10:40<10:55,  2.43it/s] 50%|█████     | 1587/3145 [10:41<11:27,  2.27it/s] 51%|█████     | 1589/3143 [10:47<11:15,  2.30it/s] 49%|████▉     | 1555/3145 [10:41<11:11,  2.37it/s] 50%|████▉     | 1571/3145 [10:41<11:38,  2.25it/s] 50%|█████     | 1588/3145 [10:41<11:25,  2.27it/s] 51%|█████     | 1590/3143 [10:47<12:27,  2.08it/s] 49%|████▉     | 1556/3145 [10:41<10:46,  2.46it/s] 50%|████▉     | 1572/3145 [10:41<11:16,  2.32it/s] 51%|█████     | 1589/3145 [10:42<11:22,  2.28it/s] 51%|█████     | 1591/3143 [10:48<11:49,  2.19it/s] 50%|████▉     | 1557/3145 [10:42<10:36,  2.49it/s] 50%|█████     | 1573/3145 [10:42<10:59,  2.38it/s] 51%|█████     | 1590/3145 [10:42<11:22,  2.28it/s] 51%|█████     | 1592/3143 [10:48<11:37,  2.22it/s] 50%|████▉     | 1558/3145 [10:42<11:12,  2.36it/s] 50%|█████     | 1574/3145 [10:42<11:04,  2.36it/s] 51%|█████     | 1591/3145 [10:42<11:24,  2.27it/s] 51%|█████     | 1593/3143 [10:48<11:03,  2.34it/s] 50%|████▉     | 1559/3145 [10:43<11:19,  2.34it/s] 50%|█████     | 1575/3145 [10:43<11:08,  2.35it/s] 51%|█████     | 1592/3145 [10:43<11:14,  2.30it/s] 50%|████▉     | 1560/3145 [10:43<09:51,  2.68it/s] 51%|█████     | 1594/3143 [10:49<10:46,  2.40it/s] 50%|█████     | 1576/3145 [10:43<11:13,  2.33it/s] 51%|█████     | 1593/3145 [10:43<10:36,  2.44it/s] 51%|█████     | 1595/3143 [10:49<10:19,  2.50it/s] 50%|████▉     | 1561/3145 [10:43<10:44,  2.46it/s] 50%|█████     | 1577/3145 [10:43<10:56,  2.39it/s] 50%|████▉     | 1562/3145 [10:44<09:30,  2.77it/s] 51%|█████     | 1594/3145 [10:44<10:49,  2.39it/s] 51%|█████     | 1596/3143 [10:50<10:29,  2.46it/s] 50%|█████     | 1578/3145 [10:44<10:31,  2.48it/s] 50%|████▉     | 1563/3145 [10:44<09:13,  2.86it/s] 51%|█████     | 1595/3145 [10:44<10:23,  2.49it/s] 51%|█████     | 1597/3143 [10:50<10:25,  2.47it/s] 50%|█████     | 1579/3145 [10:44<09:21,  2.79it/s] 50%|████▉     | 1564/3145 [10:44<09:53,  2.67it/s] 51%|█████     | 1596/3145 [10:44<10:18,  2.50it/s] 51%|█████     | 1598/3143 [10:50<10:20,  2.49it/s] 50%|█████     | 1580/3145 [10:45<09:49,  2.66it/s] 50%|████▉     | 1565/3145 [10:45<10:02,  2.62it/s] 51%|█████     | 1599/3143 [10:51<10:01,  2.57it/s] 51%|█████     | 1597/3145 [10:45<10:36,  2.43it/s] 50%|█████     | 1581/3145 [10:45<10:19,  2.52it/s] 50%|████▉     | 1566/3145 [10:45<10:08,  2.59it/s] 51%|█████     | 1600/3143 [10:51<10:03,  2.56it/s] 51%|█████     | 1598/3145 [10:45<10:25,  2.47it/s] 50%|█████     | 1582/3145 [10:45<10:07,  2.57it/s] 50%|████▉     | 1567/3145 [10:45<10:14,  2.57it/s] 51%|█████     | 1601/3143 [10:51<09:46,  2.63it/s] 51%|█████     | 1599/3145 [10:46<09:56,  2.59it/s] 50%|█████     | 1583/3145 [10:46<10:11,  2.55it/s] 51%|█████     | 1600/3145 [10:46<08:48,  2.93it/s] 51%|█████     | 1602/3143 [10:52<09:52,  2.60it/s] 50%|████▉     | 1568/3145 [10:46<11:26,  2.30it/s] 50%|█████     | 1584/3145 [10:46<09:53,  2.63it/s] 51%|█████     | 1601/3145 [10:46<09:34,  2.69it/s] 51%|█████     | 1603/3143 [10:52<10:15,  2.50it/s] 50%|█████     | 1585/3145 [10:46<09:52,  2.64it/s] 51%|█████     | 1602/3145 [10:47<09:04,  2.83it/s] 50%|████▉     | 1569/3145 [10:47<11:44,  2.24it/s] 51%|█████     | 1604/3143 [10:53<10:16,  2.50it/s] 51%|█████     | 1603/3145 [10:47<08:16,  3.10it/s] 50%|█████     | 1586/3145 [10:47<09:56,  2.62it/s] 50%|████▉     | 1570/3145 [10:47<11:41,  2.25it/s] 51%|█████     | 1604/3145 [10:47<08:45,  2.93it/s] 51%|█████     | 1605/3143 [10:53<10:26,  2.46it/s] 50%|█████     | 1587/3145 [10:47<10:09,  2.56it/s] 50%|████▉     | 1571/3145 [10:47<11:06,  2.36it/s] 51%|█████     | 1606/3143 [10:53<09:57,  2.57it/s] 51%|█████     | 1605/3145 [10:48<09:39,  2.66it/s] 50%|█████     | 1588/3145 [10:48<10:26,  2.49it/s] 50%|████▉     | 1572/3145 [10:48<10:40,  2.46it/s] 51%|█████     | 1607/3143 [10:54<09:47,  2.62it/s] 51%|█████     | 1606/3145 [10:48<09:35,  2.67it/s] 51%|█████     | 1589/3145 [10:48<10:22,  2.50it/s] 50%|█████     | 1573/3145 [10:48<11:08,  2.35it/s] 51%|█████     | 1608/3143 [10:54<10:14,  2.50it/s] 51%|█████     | 1607/3145 [10:48<09:42,  2.64it/s] 51%|█████     | 1590/3145 [10:48<10:15,  2.53it/s] 50%|█████     | 1574/3145 [10:49<11:12,  2.33it/s] 51%|█████     | 1609/3143 [10:55<10:17,  2.49it/s] 51%|█████     | 1591/3145 [10:49<09:58,  2.60it/s] 51%|█████     | 1608/3145 [10:49<10:18,  2.49it/s] 50%|█████     | 1575/3145 [10:49<11:02,  2.37it/s] 51%|█████     | 1610/3143 [10:55<10:23,  2.46it/s] 51%|█████     | 1592/3145 [10:49<09:46,  2.65it/s] 51%|█████     | 1609/3145 [10:49<10:16,  2.49it/s] 50%|█████     | 1576/3145 [10:49<10:50,  2.41it/s] 51%|█████▏    | 1611/3143 [10:55<10:04,  2.53it/s] 51%|█████     | 1593/3145 [10:50<10:06,  2.56it/s] 51%|█████     | 1610/3145 [10:50<10:25,  2.46it/s] 50%|█████     | 1577/3145 [10:50<10:40,  2.45it/s] 51%|█████▏    | 1612/3143 [10:56<10:03,  2.54it/s] 51%|█████     | 1594/3145 [10:50<10:01,  2.58it/s] 51%|█████     | 1611/3145 [10:50<10:30,  2.43it/s] 51%|█████     | 1595/3145 [10:50<08:52,  2.91it/s] 50%|█████     | 1578/3145 [10:50<10:54,  2.39it/s] 51%|█████▏    | 1613/3143 [10:56<10:25,  2.45it/s] 51%|█████▏    | 1612/3145 [10:50<10:14,  2.49it/s] 50%|█████     | 1579/3145 [10:50<09:34,  2.72it/s] 51%|█████     | 1596/3145 [10:51<09:52,  2.61it/s] 51%|█████▏    | 1614/3143 [10:57<10:35,  2.41it/s] 51%|█████▏    | 1613/3145 [10:51<10:28,  2.44it/s] 50%|█████     | 1580/3145 [10:51<10:05,  2.58it/s] 51%|█████▏    | 1615/3143 [10:57<10:09,  2.51it/s] 51%|█████     | 1597/3145 [10:51<10:43,  2.40it/s] 51%|█████▏    | 1614/3145 [10:51<10:38,  2.40it/s] 50%|█████     | 1581/3145 [10:51<10:07,  2.58it/s] 51%|█████     | 1598/3145 [10:52<10:49,  2.38it/s] 50%|█████     | 1582/3145 [10:52<09:57,  2.61it/s] 51%|█████▏    | 1615/3145 [10:52<10:21,  2.46it/s] 51%|█████▏    | 1616/3143 [10:58<11:31,  2.21it/s] 51%|█████     | 1599/3145 [10:52<11:04,  2.33it/s] 51%|█████▏    | 1616/3145 [10:52<10:10,  2.51it/s] 50%|█████     | 1583/3145 [10:52<10:05,  2.58it/s] 51%|█████▏    | 1617/3143 [10:58<11:07,  2.29it/s] 50%|█████     | 1584/3145 [10:52<09:55,  2.62it/s] 51%|█████▏    | 1617/3145 [10:52<10:15,  2.48it/s] 51%|█████     | 1600/3145 [10:53<11:40,  2.21it/s] 51%|█████▏    | 1618/3143 [10:59<11:26,  2.22it/s] 50%|█████     | 1585/3145 [10:53<10:03,  2.58it/s] 51%|█████▏    | 1618/3145 [10:53<10:12,  2.49it/s] 52%|█████▏    | 1619/3143 [10:59<11:01,  2.30it/s] 51%|█████     | 1601/3145 [10:53<12:14,  2.10it/s] 51%|█████▏    | 1619/3145 [10:53<08:57,  2.84it/s] 50%|█████     | 1586/3145 [10:53<09:59,  2.60it/s] 52%|█████▏    | 1620/3143 [10:59<10:50,  2.34it/s] 52%|█████▏    | 1620/3145 [10:53<09:04,  2.80it/s] 51%|█████     | 1602/3145 [10:54<11:39,  2.21it/s] 50%|█████     | 1587/3145 [10:54<10:06,  2.57it/s] 52%|█████▏    | 1621/3143 [11:00<10:34,  2.40it/s] 52%|█████▏    | 1621/3145 [10:54<09:39,  2.63it/s] 51%|█████     | 1603/3145 [10:54<11:35,  2.22it/s] 50%|█████     | 1588/3145 [10:54<10:06,  2.57it/s] 52%|█████▏    | 1622/3143 [11:00<10:44,  2.36it/s] 52%|█████▏    | 1622/3145 [10:54<10:01,  2.53it/s] 51%|█████     | 1589/3145 [10:54<09:56,  2.61it/s] 51%|█████     | 1604/3145 [10:54<11:49,  2.17it/s] 51%|█████     | 1590/3145 [10:55<09:01,  2.87it/s] 52%|█████▏    | 1623/3143 [11:01<10:52,  2.33it/s] 52%|█████▏    | 1623/3145 [10:55<10:01,  2.53it/s] 51%|█████     | 1605/3145 [10:55<11:21,  2.26it/s] 51%|█████     | 1606/3145 [10:55<09:48,  2.62it/s] 52%|█████▏    | 1624/3145 [10:55<09:48,  2.59it/s] 52%|█████▏    | 1624/3143 [11:01<10:50,  2.34it/s] 51%|█████     | 1591/3145 [10:55<10:07,  2.56it/s] 51%|█████     | 1607/3145 [10:55<09:36,  2.67it/s] 52%|█████▏    | 1625/3143 [11:01<10:28,  2.41it/s] 52%|█████▏    | 1625/3145 [10:56<10:12,  2.48it/s] 51%|█████     | 1592/3145 [10:56<10:27,  2.48it/s] 51%|█████     | 1608/3145 [10:56<09:54,  2.58it/s] 51%|█████     | 1593/3145 [10:56<09:27,  2.73it/s] 52%|█████▏    | 1626/3143 [11:02<10:22,  2.44it/s] 52%|█████▏    | 1626/3145 [10:56<09:49,  2.57it/s] 51%|█████     | 1594/3145 [10:56<09:25,  2.74it/s] 52%|█████▏    | 1627/3143 [11:02<10:00,  2.52it/s] 51%|█████     | 1609/3145 [10:56<10:11,  2.51it/s] 52%|█████▏    | 1627/3145 [10:56<10:23,  2.44it/s] 51%|█████     | 1595/3145 [10:57<09:25,  2.74it/s] 52%|█████▏    | 1628/3143 [11:03<09:46,  2.58it/s] 52%|█████▏    | 1628/3145 [10:57<09:53,  2.56it/s] 51%|█████     | 1610/3145 [10:57<10:53,  2.35it/s] 51%|█████     | 1596/3145 [10:57<08:41,  2.97it/s] 52%|█████▏    | 1629/3143 [11:03<09:44,  2.59it/s] 52%|█████▏    | 1629/3145 [10:57<09:55,  2.55it/s] 51%|█████     | 1597/3145 [10:57<08:21,  3.09it/s] 51%|█████     | 1611/3145 [10:57<10:53,  2.35it/s] 52%|█████▏    | 1630/3143 [11:03<09:50,  2.56it/s] 52%|█████▏    | 1630/3145 [10:58<09:52,  2.56it/s] 51%|█████     | 1598/3145 [10:58<08:55,  2.89it/s] 51%|█████▏    | 1612/3145 [10:58<10:54,  2.34it/s] 52%|█████▏    | 1631/3143 [11:04<09:56,  2.54it/s] 52%|█████▏    | 1631/3145 [10:58<09:45,  2.59it/s] 51%|█████     | 1599/3145 [10:58<09:38,  2.67it/s] 51%|█████▏    | 1613/3145 [10:58<10:41,  2.39it/s] 52%|█████▏    | 1632/3143 [11:04<09:40,  2.60it/s] 52%|█████▏    | 1632/3145 [10:58<10:05,  2.50it/s] 51%|█████     | 1600/3145 [10:58<09:58,  2.58it/s] 51%|█████▏    | 1614/3145 [10:58<10:57,  2.33it/s] 52%|█████▏    | 1633/3145 [10:59<10:21,  2.43it/s] 51%|█████     | 1601/3145 [10:59<09:49,  2.62it/s] 52%|█████▏    | 1633/3143 [11:05<11:30,  2.19it/s] 51%|█████▏    | 1615/3145 [10:59<10:40,  2.39it/s] 52%|█████▏    | 1634/3145 [10:59<08:52,  2.84it/s] 51%|█████▏    | 1616/3145 [10:59<09:19,  2.73it/s] 51%|█████     | 1602/3145 [10:59<10:24,  2.47it/s] 52%|█████▏    | 1634/3143 [11:05<11:30,  2.18it/s] 51%|█████▏    | 1617/3145 [10:59<08:22,  3.04it/s] 52%|█████▏    | 1635/3145 [10:59<09:39,  2.61it/s] 51%|█████     | 1603/3145 [11:00<10:04,  2.55it/s] 52%|█████▏    | 1635/3143 [11:06<11:19,  2.22it/s] 51%|█████▏    | 1618/3145 [11:00<08:52,  2.87it/s] 52%|█████▏    | 1636/3145 [11:00<10:00,  2.51it/s] 51%|█████     | 1604/3145 [11:00<10:11,  2.52it/s] 51%|█████▏    | 1619/3145 [11:00<09:04,  2.80it/s] 52%|█████▏    | 1636/3143 [11:06<11:22,  2.21it/s] 52%|█████▏    | 1637/3145 [11:00<09:58,  2.52it/s] 51%|█████     | 1605/3145 [11:00<10:08,  2.53it/s] 52%|█████▏    | 1637/3143 [11:06<10:54,  2.30it/s] 52%|█████▏    | 1620/3145 [11:01<09:41,  2.62it/s] 52%|█████▏    | 1638/3145 [11:01<09:58,  2.52it/s] 51%|█████     | 1606/3145 [11:01<10:20,  2.48it/s] 52%|█████▏    | 1621/3145 [11:01<09:36,  2.64it/s] 52%|█████▏    | 1638/3143 [11:07<10:43,  2.34it/s] 52%|█████▏    | 1639/3145 [11:01<10:14,  2.45it/s] 51%|█████     | 1607/3145 [11:01<10:17,  2.49it/s] 52%|█████▏    | 1622/3145 [11:01<09:45,  2.60it/s] 52%|█████▏    | 1639/3143 [11:07<10:48,  2.32it/s] 51%|█████     | 1608/3145 [11:02<09:57,  2.57it/s] 52%|█████▏    | 1640/3145 [11:02<11:38,  2.16it/s] 52%|█████▏    | 1623/3145 [11:02<09:50,  2.58it/s] 52%|█████▏    | 1640/3143 [11:08<10:23,  2.41it/s] 51%|█████     | 1609/3145 [11:02<08:56,  2.86it/s] 52%|█████▏    | 1641/3145 [11:02<11:03,  2.27it/s] 52%|█████▏    | 1624/3145 [11:02<09:42,  2.61it/s] 52%|█████▏    | 1641/3143 [11:08<10:22,  2.41it/s] 51%|█████     | 1610/3145 [11:02<09:46,  2.62it/s] 52%|█████▏    | 1642/3145 [11:02<10:41,  2.34it/s] 52%|█████▏    | 1625/3145 [11:02<09:37,  2.63it/s] 52%|█████▏    | 1642/3143 [11:08<10:11,  2.45it/s] 51%|█████     | 1611/3145 [11:03<09:41,  2.64it/s] 52%|█████▏    | 1643/3145 [11:03<10:17,  2.43it/s] 52%|█████▏    | 1626/3145 [11:03<09:31,  2.66it/s] 52%|█████▏    | 1643/3143 [11:09<10:28,  2.39it/s] 51%|█████▏    | 1612/3145 [11:03<09:35,  2.66it/s] 52%|█████▏    | 1644/3145 [11:03<09:43,  2.57it/s] 52%|█████▏    | 1627/3145 [11:03<10:01,  2.52it/s] 52%|█████▏    | 1644/3143 [11:09<10:19,  2.42it/s] 51%|█████▏    | 1613/3145 [11:03<10:13,  2.50it/s] 52%|█████▏    | 1645/3145 [11:04<09:50,  2.54it/s] 52%|█████▏    | 1628/3145 [11:04<09:49,  2.57it/s] 52%|█████▏    | 1645/3143 [11:10<10:17,  2.43it/s] 51%|█████▏    | 1614/3145 [11:04<10:12,  2.50it/s] 52%|█████▏    | 1629/3145 [11:04<09:38,  2.62it/s] 52%|█████▏    | 1646/3145 [11:04<10:17,  2.43it/s] 52%|█████▏    | 1646/3143 [11:10<10:03,  2.48it/s] 51%|█████▏    | 1615/3145 [11:04<10:05,  2.53it/s] 52%|█████▏    | 1630/3145 [11:04<09:32,  2.65it/s] 52%|█████▏    | 1647/3145 [11:04<10:21,  2.41it/s] 52%|█████▏    | 1648/3145 [11:05<08:54,  2.80it/s] 51%|█████▏    | 1616/3145 [11:05<10:24,  2.45it/s] 52%|█████▏    | 1647/3143 [11:11<10:52,  2.29it/s] 52%|█████▏    | 1631/3145 [11:05<09:40,  2.61it/s] 52%|█████▏    | 1649/3145 [11:05<08:54,  2.80it/s] 52%|█████▏    | 1648/3143 [11:11<10:26,  2.39it/s] 52%|█████▏    | 1632/3145 [11:05<09:28,  2.66it/s] 51%|█████▏    | 1617/3145 [11:05<10:36,  2.40it/s] 52%|█████▏    | 1650/3145 [11:05<09:14,  2.70it/s] 52%|█████▏    | 1649/3143 [11:11<10:00,  2.49it/s] 51%|█████▏    | 1618/3145 [11:06<10:26,  2.44it/s] 52%|█████▏    | 1633/3145 [11:06<09:49,  2.56it/s] 52%|█████▏    | 1651/3145 [11:06<09:44,  2.56it/s] 52%|█████▏    | 1650/3143 [11:12<10:04,  2.47it/s] 52%|█████▏    | 1634/3145 [11:06<09:40,  2.60it/s] 51%|█████▏    | 1619/3145 [11:06<10:17,  2.47it/s] 53%|█████▎    | 1652/3145 [11:06<09:53,  2.52it/s] 52%|█████▏    | 1620/3145 [11:06<09:52,  2.58it/s] 52%|█████▏    | 1635/3145 [11:06<09:48,  2.56it/s] 53%|█████▎    | 1651/3143 [11:12<10:41,  2.33it/s] 53%|█████▎    | 1653/3145 [11:07<10:08,  2.45it/s] 52%|█████▏    | 1636/3145 [11:07<10:04,  2.50it/s] 52%|█████▏    | 1621/3145 [11:07<10:40,  2.38it/s] 53%|█████▎    | 1652/3143 [11:13<10:35,  2.35it/s] 53%|█████▎    | 1654/3145 [11:07<08:49,  2.81it/s] 52%|█████▏    | 1622/3145 [11:07<10:13,  2.48it/s] 52%|█████▏    | 1637/3145 [11:07<10:23,  2.42it/s] 53%|█████▎    | 1653/3143 [11:13<10:47,  2.30it/s] 53%|█████▎    | 1655/3145 [11:07<08:29,  2.92it/s] 52%|█████▏    | 1623/3145 [11:08<10:18,  2.46it/s] 53%|█████▎    | 1654/3143 [11:14<10:18,  2.41it/s] 52%|█████▏    | 1638/3145 [11:08<10:30,  2.39it/s] 53%|█████▎    | 1656/3145 [11:08<08:58,  2.76it/s] 53%|█████▎    | 1655/3143 [11:14<08:57,  2.77it/s] 52%|█████▏    | 1624/3145 [11:08<09:59,  2.54it/s] 53%|█████▎    | 1657/3145 [11:08<08:34,  2.89it/s] 52%|█████▏    | 1639/3145 [11:08<10:37,  2.36it/s] 53%|█████▎    | 1656/3143 [11:14<09:03,  2.74it/s] 53%|█████▎    | 1658/3145 [11:08<08:52,  2.79it/s] 52%|█████▏    | 1625/3145 [11:08<10:23,  2.44it/s] 52%|█████▏    | 1640/3145 [11:09<10:48,  2.32it/s] 53%|█████▎    | 1657/3143 [11:15<09:21,  2.65it/s] 52%|█████▏    | 1626/3145 [11:09<10:04,  2.51it/s] 53%|█████▎    | 1659/3145 [11:09<09:09,  2.70it/s] 53%|█████▎    | 1658/3143 [11:15<08:29,  2.92it/s] 52%|█████▏    | 1641/3145 [11:09<10:47,  2.32it/s] 53%|█████▎    | 1660/3145 [11:09<09:23,  2.63it/s] 52%|█████▏    | 1627/3145 [11:09<10:29,  2.41it/s] 53%|█████▎    | 1659/3143 [11:15<08:52,  2.79it/s] 52%|█████▏    | 1642/3145 [11:09<10:46,  2.33it/s] 53%|█████▎    | 1661/3145 [11:10<09:48,  2.52it/s] 52%|█████▏    | 1628/3145 [11:10<10:44,  2.35it/s] 53%|█████▎    | 1660/3143 [11:16<09:07,  2.71it/s] 52%|█████▏    | 1643/3145 [11:10<10:40,  2.35it/s] 53%|█████▎    | 1662/3145 [11:10<09:31,  2.59it/s] 52%|█████▏    | 1629/3145 [11:10<10:43,  2.35it/s] 53%|█████▎    | 1661/3143 [11:16<09:32,  2.59it/s] 52%|█████▏    | 1644/3145 [11:10<10:10,  2.46it/s] 53%|█████▎    | 1663/3145 [11:10<09:39,  2.56it/s] 52%|█████▏    | 1630/3145 [11:11<11:01,  2.29it/s] 53%|█████▎    | 1662/3143 [11:16<09:55,  2.49it/s] 52%|█████▏    | 1645/3145 [11:11<10:08,  2.47it/s] 53%|█████▎    | 1664/3145 [11:11<09:44,  2.53it/s] 52%|█████▏    | 1631/3145 [11:11<10:46,  2.34it/s] 52%|█████▏    | 1646/3145 [11:11<10:14,  2.44it/s] 53%|█████▎    | 1663/3143 [11:17<10:06,  2.44it/s] 53%|█████▎    | 1665/3145 [11:11<09:48,  2.51it/s] 53%|█████▎    | 1664/3143 [11:17<10:08,  2.43it/s] 52%|█████▏    | 1632/3145 [11:11<11:00,  2.29it/s] 52%|█████▏    | 1647/3145 [11:11<10:21,  2.41it/s] 53%|█████▎    | 1666/3145 [11:12<10:03,  2.45it/s] 52%|█████▏    | 1633/3145 [11:12<10:53,  2.31it/s] 53%|█████▎    | 1667/3145 [11:12<08:47,  2.80it/s] 52%|█████▏    | 1648/3145 [11:12<10:57,  2.28it/s] 53%|█████▎    | 1665/3143 [11:18<10:58,  2.25it/s] 52%|█████▏    | 1634/3145 [11:12<10:48,  2.33it/s] 53%|█████▎    | 1666/3143 [11:18<10:24,  2.36it/s] 53%|█████▎    | 1668/3145 [11:12<09:37,  2.56it/s] 52%|█████▏    | 1649/3145 [11:12<11:00,  2.27it/s] 53%|█████▎    | 1669/3145 [11:13<08:31,  2.89it/s] 52%|█████▏    | 1635/3145 [11:13<10:32,  2.39it/s] 53%|█████▎    | 1667/3143 [11:19<10:13,  2.40it/s] 52%|█████▏    | 1650/3145 [11:13<10:23,  2.40it/s] 53%|█████▎    | 1670/3145 [11:13<09:09,  2.69it/s] 52%|█████▏    | 1651/3145 [11:13<09:51,  2.52it/s] 52%|█████▏    | 1636/3145 [11:13<10:43,  2.34it/s] 53%|█████▎    | 1668/3143 [11:19<10:34,  2.32it/s] 53%|█████▎    | 1671/3145 [11:13<09:35,  2.56it/s] 53%|█████▎    | 1652/3145 [11:13<09:51,  2.52it/s] 53%|█████▎    | 1669/3143 [11:19<10:03,  2.44it/s] 52%|█████▏    | 1637/3145 [11:14<10:53,  2.31it/s] 53%|█████▎    | 1672/3145 [11:14<09:52,  2.48it/s] 53%|█████▎    | 1653/3145 [11:14<10:24,  2.39it/s] 53%|█████▎    | 1670/3143 [11:20<10:14,  2.40it/s] 52%|█████▏    | 1638/3145 [11:14<10:51,  2.31it/s] 53%|█████▎    | 1673/3145 [11:14<09:34,  2.56it/s] 52%|█████▏    | 1639/3145 [11:14<09:34,  2.62it/s] 53%|█████▎    | 1654/3145 [11:14<10:29,  2.37it/s] 53%|█████▎    | 1671/3143 [11:20<10:22,  2.36it/s] 52%|█████▏    | 1640/3145 [11:15<09:43,  2.58it/s] 53%|█████▎    | 1674/3145 [11:15<10:13,  2.40it/s] 53%|█████▎    | 1655/3145 [11:15<10:32,  2.35it/s] 53%|█████▎    | 1672/3143 [11:21<10:13,  2.40it/s] 52%|█████▏    | 1641/3145 [11:15<10:07,  2.48it/s] 53%|█████▎    | 1675/3145 [11:15<10:01,  2.44it/s] 53%|█████▎    | 1656/3145 [11:15<10:36,  2.34it/s] 53%|█████▎    | 1673/3143 [11:21<10:36,  2.31it/s] 52%|█████▏    | 1642/3145 [11:15<10:04,  2.49it/s] 53%|█████▎    | 1676/3145 [11:15<09:54,  2.47it/s] 53%|█████▎    | 1674/3143 [11:22<10:04,  2.43it/s] 53%|█████▎    | 1657/3145 [11:16<11:02,  2.25it/s] 52%|█████▏    | 1643/3145 [11:16<10:11,  2.46it/s] 53%|█████▎    | 1677/3145 [11:16<10:23,  2.35it/s] 53%|█████▎    | 1675/3143 [11:22<10:14,  2.39it/s] 53%|█████▎    | 1658/3145 [11:16<10:53,  2.28it/s] 52%|█████▏    | 1644/3145 [11:16<10:26,  2.39it/s] 53%|█████▎    | 1678/3145 [11:16<10:59,  2.22it/s] 53%|█████▎    | 1676/3143 [11:22<10:21,  2.36it/s] 53%|█████▎    | 1659/3145 [11:17<10:29,  2.36it/s] 52%|█████▏    | 1645/3145 [11:17<10:07,  2.47it/s] 53%|█████▎    | 1679/3145 [11:17<10:51,  2.25it/s] 53%|█████▎    | 1677/3143 [11:23<10:11,  2.40it/s] 53%|█████▎    | 1660/3145 [11:17<10:32,  2.35it/s] 52%|█████▏    | 1646/3145 [11:17<09:58,  2.51it/s] 53%|█████▎    | 1678/3143 [11:23<10:20,  2.36it/s] 53%|█████▎    | 1661/3145 [11:17<10:14,  2.41it/s] 53%|█████▎    | 1680/3145 [11:17<11:11,  2.18it/s] 52%|█████▏    | 1647/3145 [11:18<10:18,  2.42it/s] 53%|█████▎    | 1662/3145 [11:18<10:17,  2.40it/s] 53%|█████▎    | 1681/3145 [11:18<10:41,  2.28it/s] 53%|█████▎    | 1679/3143 [11:24<10:31,  2.32it/s] 52%|█████▏    | 1648/3145 [11:18<10:35,  2.36it/s] 53%|█████▎    | 1682/3145 [11:18<10:29,  2.32it/s] 53%|█████▎    | 1680/3143 [11:24<10:34,  2.31it/s] 53%|█████▎    | 1663/3145 [11:18<10:51,  2.28it/s] 52%|█████▏    | 1649/3145 [11:18<10:42,  2.33it/s] 54%|█████▎    | 1683/3145 [11:19<10:22,  2.35it/s] 53%|█████▎    | 1681/3143 [11:25<10:23,  2.34it/s] 53%|█████▎    | 1664/3145 [11:19<10:54,  2.26it/s] 52%|█████▏    | 1650/3145 [11:19<10:46,  2.31it/s] 54%|█████▎    | 1682/3143 [11:25<10:25,  2.33it/s] 54%|█████▎    | 1684/3145 [11:19<10:44,  2.27it/s] 53%|█████▎    | 1665/3145 [11:19<10:50,  2.27it/s] 52%|█████▏    | 1651/3145 [11:19<10:23,  2.40it/s] 54%|█████▎    | 1683/3143 [11:25<10:27,  2.33it/s] 53%|█████▎    | 1666/3145 [11:20<10:42,  2.30it/s] 54%|█████▎    | 1685/3145 [11:20<11:15,  2.16it/s] 53%|█████▎    | 1652/3145 [11:20<10:35,  2.35it/s] 54%|█████▎    | 1684/3143 [11:26<09:56,  2.44it/s] 53%|█████▎    | 1667/3145 [11:20<10:04,  2.44it/s] 54%|█████▎    | 1686/3145 [11:20<11:33,  2.10it/s] 53%|█████▎    | 1653/3145 [11:20<10:22,  2.40it/s] 54%|█████▎    | 1685/3143 [11:26<09:52,  2.46it/s] 53%|█████▎    | 1668/3145 [11:20<09:50,  2.50it/s] 53%|█████▎    | 1654/3145 [11:20<10:17,  2.41it/s] 54%|█████▎    | 1687/3145 [11:21<11:21,  2.14it/s] 54%|█████▎    | 1686/3143 [11:27<09:35,  2.53it/s] 53%|█████▎    | 1669/3145 [11:21<10:42,  2.30it/s] 53%|█████▎    | 1655/3145 [11:21<09:57,  2.49it/s] 54%|█████▎    | 1688/3145 [11:21<11:14,  2.16it/s] 54%|█████▎    | 1687/3143 [11:27<09:41,  2.50it/s] 53%|█████▎    | 1670/3145 [11:21<10:59,  2.23it/s] 53%|█████▎    | 1656/3145 [11:21<10:25,  2.38it/s] 54%|█████▎    | 1689/3145 [11:21<11:13,  2.16it/s] 54%|█████▎    | 1688/3143 [11:27<09:53,  2.45it/s] 53%|█████▎    | 1671/3145 [11:22<10:22,  2.37it/s] 53%|█████▎    | 1657/3145 [11:22<10:37,  2.33it/s] 54%|█████▎    | 1690/3145 [11:22<11:07,  2.18it/s] 54%|█████▎    | 1689/3143 [11:28<10:06,  2.40it/s] 53%|█████▎    | 1672/3145 [11:22<10:38,  2.31it/s] 53%|█████▎    | 1658/3145 [11:22<10:45,  2.30it/s] 54%|█████▍    | 1691/3145 [11:22<10:30,  2.31it/s] 54%|█████▍    | 1690/3143 [11:28<09:59,  2.42it/s] 53%|█████▎    | 1673/3145 [11:23<10:18,  2.38it/s] 53%|█████▎    | 1659/3145 [11:23<10:17,  2.41it/s] 54%|█████▍    | 1692/3145 [11:23<10:14,  2.37it/s] 54%|█████▍    | 1691/3143 [11:29<09:52,  2.45it/s] 53%|█████▎    | 1674/3145 [11:23<10:34,  2.32it/s] 54%|█████▍    | 1693/3145 [11:23<09:41,  2.50it/s] 53%|█████▎    | 1660/3145 [11:23<10:39,  2.32it/s] 54%|█████▍    | 1692/3143 [11:29<09:42,  2.49it/s] 54%|█████▍    | 1694/3145 [11:23<09:22,  2.58it/s] 53%|█████▎    | 1661/3145 [11:23<10:25,  2.37it/s] 54%|█████▍    | 1693/3143 [11:29<09:59,  2.42it/s] 53%|█████▎    | 1675/3145 [11:24<11:46,  2.08it/s] 54%|█████▍    | 1695/3145 [11:24<09:12,  2.63it/s] 54%|█████▍    | 1694/3143 [11:30<09:38,  2.51it/s] 53%|█████▎    | 1662/3145 [11:24<10:44,  2.30it/s] 53%|█████▎    | 1676/3145 [11:24<12:38,  1.94it/s] 54%|█████▍    | 1696/3145 [11:24<09:36,  2.51it/s] 54%|█████▍    | 1695/3143 [11:30<09:33,  2.52it/s] 53%|█████▎    | 1663/3145 [11:24<10:08,  2.44it/s] 53%|█████▎    | 1677/3145 [11:25<11:30,  2.13it/s] 54%|█████▍    | 1697/3145 [11:25<09:34,  2.52it/s] 53%|█████▎    | 1664/3145 [11:25<09:55,  2.49it/s] 54%|█████▍    | 1696/3143 [11:31<09:53,  2.44it/s] 53%|█████▎    | 1678/3145 [11:25<11:11,  2.19it/s] 54%|█████▍    | 1698/3145 [11:25<09:48,  2.46it/s] 54%|█████▍    | 1697/3143 [11:31<09:44,  2.48it/s] 53%|█████▎    | 1665/3145 [11:25<10:15,  2.40it/s] 53%|█████▎    | 1679/3145 [11:25<10:54,  2.24it/s] 54%|█████▍    | 1699/3145 [11:25<09:29,  2.54it/s] 54%|█████▍    | 1698/3143 [11:31<10:02,  2.40it/s] 53%|█████▎    | 1666/3145 [11:26<11:37,  2.12it/s] 54%|█████▍    | 1700/3145 [11:26<09:34,  2.52it/s] 53%|█████▎    | 1680/3145 [11:26<10:48,  2.26it/s] 53%|█████▎    | 1667/3145 [11:26<10:06,  2.44it/s] 54%|█████▍    | 1699/3143 [11:32<10:48,  2.23it/s] 53%|█████▎    | 1681/3145 [11:26<10:15,  2.38it/s] 54%|█████▍    | 1701/3145 [11:26<09:47,  2.46it/s] 53%|█████▎    | 1668/3145 [11:26<09:23,  2.62it/s] 54%|█████▍    | 1700/3143 [11:32<10:45,  2.24it/s] 53%|█████▎    | 1682/3145 [11:27<09:59,  2.44it/s] 54%|█████▍    | 1702/3145 [11:27<10:32,  2.28it/s] 53%|█████▎    | 1669/3145 [11:27<09:55,  2.48it/s] 54%|█████▎    | 1683/3145 [11:27<09:58,  2.44it/s] 54%|█████▍    | 1701/3143 [11:33<10:45,  2.23it/s] 53%|█████▎    | 1670/3145 [11:27<09:17,  2.65it/s] 54%|█████▍    | 1703/3145 [11:27<10:21,  2.32it/s] 54%|█████▎    | 1684/3145 [11:27<09:47,  2.49it/s] 54%|█████▍    | 1702/3143 [11:33<10:35,  2.27it/s] 53%|█████▎    | 1671/3145 [11:27<09:29,  2.59it/s] 54%|█████▍    | 1704/3145 [11:28<10:20,  2.32it/s] 54%|█████▎    | 1685/3145 [11:28<09:21,  2.60it/s] 54%|█████▍    | 1703/3143 [11:34<10:15,  2.34it/s] 53%|█████▎    | 1672/3145 [11:28<09:31,  2.58it/s] 54%|█████▍    | 1705/3145 [11:28<09:49,  2.44it/s] 54%|█████▎    | 1686/3145 [11:28<09:09,  2.65it/s] 54%|█████▍    | 1704/3143 [11:34<10:12,  2.35it/s] 53%|█████▎    | 1673/3145 [11:28<09:49,  2.50it/s] 54%|█████▍    | 1706/3145 [11:28<09:59,  2.40it/s] 54%|█████▎    | 1687/3145 [11:28<09:07,  2.66it/s] 54%|█████▍    | 1705/3143 [11:35<10:18,  2.32it/s] 53%|█████▎    | 1674/3145 [11:29<10:19,  2.37it/s] 54%|█████▍    | 1707/3145 [11:29<10:07,  2.37it/s] 54%|█████▎    | 1688/3145 [11:29<09:55,  2.45it/s] 54%|█████▍    | 1708/3145 [11:29<10:04,  2.38it/s] 53%|█████▎    | 1675/3145 [11:29<10:27,  2.34it/s] 54%|█████▍    | 1706/3143 [11:35<11:41,  2.05it/s] 54%|█████▎    | 1689/3145 [11:29<10:25,  2.33it/s] 54%|█████▍    | 1709/3145 [11:30<09:41,  2.47it/s] 53%|█████▎    | 1676/3145 [11:30<10:45,  2.27it/s] 54%|█████▎    | 1690/3145 [11:30<10:25,  2.33it/s] 54%|█████▍    | 1707/3143 [11:36<12:13,  1.96it/s] 54%|█████▍    | 1710/3145 [11:30<09:31,  2.51it/s] 53%|█████▎    | 1677/3145 [11:30<10:33,  2.32it/s] 54%|█████▍    | 1691/3145 [11:30<10:02,  2.42it/s] 54%|█████▍    | 1708/3143 [11:36<11:35,  2.06it/s] 54%|█████▍    | 1711/3145 [11:30<09:42,  2.46it/s] 53%|█████▎    | 1678/3145 [11:31<10:45,  2.27it/s] 54%|█████▍    | 1692/3145 [11:31<09:54,  2.44it/s] 54%|█████▍    | 1709/3143 [11:37<10:59,  2.18it/s] 54%|█████▍    | 1712/3145 [11:31<09:38,  2.48it/s] 53%|█████▎    | 1679/3145 [11:31<10:30,  2.33it/s] 54%|█████▍    | 1693/3145 [11:31<10:12,  2.37it/s] 54%|█████▍    | 1713/3145 [11:31<09:45,  2.45it/s] 54%|█████▍    | 1710/3143 [11:37<12:11,  1.96it/s] 53%|█████▎    | 1680/3145 [11:31<10:38,  2.29it/s] 54%|█████▍    | 1694/3145 [11:31<10:04,  2.40it/s] 54%|█████▍    | 1714/3145 [11:32<09:53,  2.41it/s] 54%|█████▍    | 1711/3143 [11:38<11:03,  2.16it/s] 53%|█████▎    | 1681/3145 [11:32<10:39,  2.29it/s] 54%|█████▍    | 1695/3145 [11:32<10:25,  2.32it/s] 54%|█████▍    | 1712/3143 [11:38<10:49,  2.20it/s] 55%|█████▍    | 1715/3145 [11:32<10:28,  2.28it/s] 53%|█████▎    | 1682/3145 [11:32<10:24,  2.34it/s] 54%|█████▍    | 1696/3145 [11:32<10:06,  2.39it/s] 55%|█████▍    | 1713/3143 [11:38<10:28,  2.28it/s] 55%|█████▍    | 1716/3145 [11:33<10:04,  2.36it/s] 54%|█████▎    | 1683/3145 [11:33<10:14,  2.38it/s] 54%|█████▍    | 1697/3145 [11:33<10:12,  2.36it/s] 55%|█████▍    | 1717/3145 [11:33<09:52,  2.41it/s] 55%|█████▍    | 1714/3143 [11:39<10:48,  2.20it/s] 54%|█████▎    | 1684/3145 [11:33<10:37,  2.29it/s] 54%|█████▍    | 1698/3145 [11:33<10:09,  2.37it/s] 55%|█████▍    | 1718/3145 [11:33<10:33,  2.25it/s] 55%|█████▍    | 1715/3143 [11:39<10:42,  2.22it/s] 54%|█████▎    | 1685/3145 [11:34<10:32,  2.31it/s] 54%|█████▍    | 1699/3145 [11:34<10:21,  2.33it/s] 55%|█████▍    | 1716/3143 [11:40<10:29,  2.27it/s] 55%|█████▍    | 1719/3145 [11:34<10:30,  2.26it/s] 54%|█████▎    | 1686/3145 [11:34<10:11,  2.39it/s] 54%|█████▍    | 1700/3145 [11:34<10:16,  2.34it/s] 55%|█████▍    | 1720/3145 [11:34<10:01,  2.37it/s] 55%|█████▍    | 1717/3143 [11:40<10:30,  2.26it/s] 54%|█████▎    | 1687/3145 [11:34<10:04,  2.41it/s] 54%|█████▍    | 1701/3145 [11:34<10:03,  2.39it/s] 54%|█████▎    | 1688/3145 [11:35<08:48,  2.76it/s] 55%|█████▍    | 1721/3145 [11:35<09:36,  2.47it/s] 55%|█████▍    | 1718/3143 [11:41<09:54,  2.40it/s] 54%|█████▍    | 1702/3145 [11:35<09:50,  2.44it/s] 55%|█████▍    | 1722/3145 [11:35<08:23,  2.83it/s] 54%|█████▎    | 1689/3145 [11:35<08:56,  2.71it/s] 55%|█████▍    | 1719/3143 [11:41<10:13,  2.32it/s] 54%|█████▍    | 1703/3145 [11:35<09:41,  2.48it/s] 55%|█████▍    | 1723/3145 [11:35<08:47,  2.70it/s] 54%|█████▎    | 1690/3145 [11:35<09:14,  2.63it/s] 55%|█████▍    | 1720/3143 [11:41<09:46,  2.43it/s] 55%|█████▍    | 1724/3145 [11:36<09:05,  2.60it/s] 54%|█████▍    | 1704/3145 [11:36<10:13,  2.35it/s] 54%|█████▍    | 1691/3145 [11:36<09:40,  2.51it/s] 55%|█████▍    | 1721/3143 [11:42<10:01,  2.36it/s] 55%|█████▍    | 1725/3145 [11:36<08:17,  2.85it/s] 54%|█████▍    | 1705/3145 [11:36<10:42,  2.24it/s] 54%|█████▍    | 1692/3145 [11:36<09:40,  2.50it/s] 55%|█████▍    | 1722/3143 [11:42<09:33,  2.48it/s] 55%|█████▍    | 1726/3145 [11:36<08:34,  2.76it/s] 54%|█████▍    | 1706/3145 [11:37<10:16,  2.33it/s] 55%|█████▍    | 1727/3145 [11:37<08:27,  2.79it/s] 54%|█████▍    | 1693/3145 [11:37<10:12,  2.37it/s] 55%|█████▍    | 1723/3143 [11:43<09:39,  2.45it/s] 54%|█████▍    | 1707/3145 [11:37<10:05,  2.37it/s] 55%|█████▍    | 1728/3145 [11:37<08:59,  2.63it/s] 55%|█████▍    | 1724/3143 [11:43<09:37,  2.46it/s] 54%|█████▍    | 1694/3145 [11:37<10:25,  2.32it/s] 54%|█████▍    | 1708/3145 [11:37<10:06,  2.37it/s] 55%|█████▍    | 1729/3145 [11:37<09:01,  2.62it/s] 55%|█████▍    | 1725/3143 [11:43<09:48,  2.41it/s] 54%|█████▍    | 1695/3145 [11:38<10:13,  2.36it/s] 54%|█████▍    | 1709/3145 [11:38<10:14,  2.34it/s] 55%|█████▌    | 1730/3145 [11:38<09:12,  2.56it/s] 55%|█████▍    | 1726/3143 [11:44<09:34,  2.47it/s] 54%|█████▍    | 1696/3145 [11:38<10:40,  2.26it/s] 54%|█████▍    | 1710/3145 [11:38<10:30,  2.28it/s] 55%|█████▌    | 1731/3145 [11:38<09:16,  2.54it/s] 55%|█████▍    | 1727/3143 [11:44<09:57,  2.37it/s] 54%|█████▍    | 1697/3145 [11:38<10:36,  2.28it/s] 54%|█████▍    | 1711/3145 [11:39<10:26,  2.29it/s] 55%|█████▍    | 1728/3143 [11:45<09:20,  2.52it/s] 55%|█████▌    | 1732/3145 [11:39<09:43,  2.42it/s] 54%|█████▍    | 1698/3145 [11:39<10:19,  2.34it/s] 55%|█████▌    | 1733/3145 [11:39<09:28,  2.48it/s] 55%|█████▌    | 1729/3143 [11:45<09:29,  2.48it/s] 54%|█████▍    | 1712/3145 [11:39<10:35,  2.26it/s] 54%|█████▍    | 1699/3145 [11:39<10:31,  2.29it/s] 55%|█████▌    | 1734/3145 [11:40<09:15,  2.54it/s] 55%|█████▌    | 1730/3143 [11:45<09:21,  2.52it/s] 54%|█████▍    | 1713/3145 [11:40<10:16,  2.32it/s] 54%|█████▍    | 1700/3145 [11:40<10:07,  2.38it/s] 55%|█████▌    | 1735/3145 [11:40<09:16,  2.53it/s] 55%|█████▌    | 1731/3143 [11:46<09:24,  2.50it/s] 54%|█████▍    | 1714/3145 [11:40<10:03,  2.37it/s] 54%|█████▍    | 1701/3145 [11:40<09:56,  2.42it/s] 55%|█████▌    | 1736/3145 [11:40<09:01,  2.60it/s] 55%|█████▍    | 1715/3145 [11:40<09:39,  2.47it/s] 55%|█████▌    | 1732/3143 [11:46<09:30,  2.47it/s] 54%|█████▍    | 1702/3145 [11:40<09:47,  2.46it/s] 55%|█████▌    | 1737/3145 [11:41<08:51,  2.65it/s] 55%|█████▍    | 1716/3145 [11:41<09:32,  2.50it/s] 55%|█████▌    | 1733/3143 [11:47<09:38,  2.44it/s] 54%|█████▍    | 1703/3145 [11:41<10:02,  2.39it/s] 55%|█████▌    | 1738/3145 [11:41<08:58,  2.61it/s] 55%|█████▍    | 1717/3145 [11:41<09:27,  2.52it/s] 55%|█████▌    | 1734/3143 [11:47<09:36,  2.44it/s] 54%|█████▍    | 1704/3145 [11:41<10:12,  2.35it/s] 55%|█████▌    | 1739/3145 [11:41<09:21,  2.50it/s] 55%|█████▍    | 1718/3145 [11:42<09:22,  2.54it/s] 55%|█████▌    | 1735/3143 [11:47<09:32,  2.46it/s] 54%|█████▍    | 1705/3145 [11:42<08:49,  2.72it/s] 55%|█████▌    | 1740/3145 [11:42<09:19,  2.51it/s] 55%|█████▍    | 1719/3145 [11:42<09:33,  2.49it/s] 54%|█████▍    | 1706/3145 [11:42<09:02,  2.65it/s] 55%|█████▌    | 1736/3143 [11:48<09:47,  2.39it/s] 55%|█████▍    | 1720/3145 [11:42<09:47,  2.43it/s] 55%|█████▌    | 1741/3145 [11:42<10:04,  2.32it/s] 54%|█████▍    | 1707/3145 [11:42<09:34,  2.50it/s] 55%|█████▌    | 1737/3143 [11:48<10:14,  2.29it/s] 55%|█████▍    | 1721/3145 [11:43<09:42,  2.44it/s] 55%|█████▌    | 1742/3145 [11:43<10:03,  2.33it/s] 55%|█████▌    | 1738/3143 [11:49<09:54,  2.36it/s] 54%|█████▍    | 1708/3145 [11:43<09:57,  2.41it/s] 55%|█████▍    | 1722/3145 [11:43<09:38,  2.46it/s] 55%|█████▌    | 1743/3145 [11:43<10:14,  2.28it/s] 54%|█████▍    | 1709/3145 [11:43<09:35,  2.50it/s] 55%|█████▌    | 1739/3143 [11:49<10:03,  2.33it/s] 55%|█████▍    | 1723/3145 [11:44<09:55,  2.39it/s] 55%|█████▌    | 1744/3145 [11:44<09:49,  2.38it/s] 55%|█████▌    | 1740/3143 [11:50<09:49,  2.38it/s] 54%|█████▍    | 1710/3145 [11:44<10:01,  2.39it/s] 55%|█████▍    | 1724/3145 [11:44<09:26,  2.51it/s] 55%|█████▌    | 1745/3145 [11:44<09:34,  2.44it/s] 55%|█████▌    | 1741/3143 [11:50<09:42,  2.41it/s] 54%|█████▍    | 1711/3145 [11:44<09:57,  2.40it/s] 55%|█████▍    | 1725/3145 [11:44<09:22,  2.52it/s] 56%|█████▌    | 1746/3145 [11:44<09:28,  2.46it/s] 55%|█████▌    | 1742/3143 [11:50<09:13,  2.53it/s] 54%|█████▍    | 1712/3145 [11:45<09:38,  2.48it/s] 56%|█████▌    | 1747/3145 [11:45<09:21,  2.49it/s] 55%|█████▍    | 1726/3145 [11:45<10:01,  2.36it/s] 55%|█████▌    | 1743/3143 [11:51<09:13,  2.53it/s] 54%|█████▍    | 1713/3145 [11:45<09:27,  2.52it/s] 56%|█████▌    | 1748/3145 [11:45<09:10,  2.54it/s] 55%|█████▍    | 1727/3145 [11:45<09:34,  2.47it/s] 55%|█████▌    | 1744/3143 [11:51<09:03,  2.58it/s] 54%|█████▍    | 1714/3145 [11:45<09:41,  2.46it/s] 56%|█████▌    | 1749/3145 [11:46<09:27,  2.46it/s] 55%|█████▍    | 1728/3145 [11:46<09:48,  2.41it/s] 56%|█████▌    | 1745/3143 [11:52<09:08,  2.55it/s] 55%|█████▍    | 1715/3145 [11:46<09:56,  2.40it/s] 55%|█████▍    | 1729/3145 [11:46<09:24,  2.51it/s] 56%|█████▌    | 1750/3145 [11:46<09:30,  2.45it/s] 56%|█████▌    | 1746/3143 [11:52<09:10,  2.54it/s] 55%|█████▍    | 1716/3145 [11:46<10:04,  2.36it/s] 56%|█████▌    | 1751/3145 [11:46<09:09,  2.54it/s] 55%|█████▌    | 1730/3145 [11:46<09:38,  2.45it/s] 56%|█████▌    | 1747/3143 [11:52<09:07,  2.55it/s] 55%|█████▍    | 1717/3145 [11:47<09:59,  2.38it/s] 56%|█████▌    | 1752/3145 [11:47<09:19,  2.49it/s] 56%|█████▌    | 1748/3143 [11:53<08:57,  2.60it/s] 55%|█████▌    | 1731/3145 [11:47<09:36,  2.45it/s] 55%|█████▍    | 1718/3145 [11:47<10:05,  2.36it/s] 55%|█████▌    | 1732/3145 [11:47<08:30,  2.77it/s] 56%|█████▌    | 1753/3145 [11:47<09:05,  2.55it/s] 56%|█████▌    | 1749/3143 [11:53<08:58,  2.59it/s] 55%|█████▍    | 1719/3145 [11:48<10:22,  2.29it/s] 55%|█████▌    | 1733/3145 [11:48<09:12,  2.55it/s] 56%|█████▌    | 1750/3143 [11:53<08:50,  2.63it/s] 56%|█████▌    | 1754/3145 [11:48<09:15,  2.50it/s] 55%|█████▍    | 1720/3145 [11:48<09:52,  2.40it/s] 55%|█████▌    | 1734/3145 [11:48<09:15,  2.54it/s] 56%|█████▌    | 1755/3145 [11:48<09:14,  2.51it/s] 56%|█████▌    | 1751/3143 [11:54<09:10,  2.53it/s] 55%|█████▍    | 1721/3145 [11:48<08:49,  2.69it/s] 55%|█████▌    | 1735/3145 [11:48<08:25,  2.79it/s] 56%|█████▌    | 1756/3145 [11:48<09:26,  2.45it/s] 56%|█████▌    | 1752/3143 [11:54<09:51,  2.35it/s] 55%|█████▌    | 1736/3145 [11:49<07:48,  3.01it/s] 55%|█████▍    | 1722/3145 [11:49<09:48,  2.42it/s] 56%|█████▌    | 1757/3145 [11:49<09:13,  2.51it/s] 56%|█████▌    | 1753/3143 [11:55<09:40,  2.39it/s] 55%|█████▌    | 1737/3145 [11:49<08:14,  2.85it/s] 55%|█████▍    | 1723/3145 [11:49<08:44,  2.71it/s] 56%|█████▌    | 1758/3145 [11:49<09:08,  2.53it/s] 56%|█████▌    | 1754/3143 [11:55<09:45,  2.37it/s] 55%|█████▌    | 1738/3145 [11:49<08:55,  2.63it/s] 55%|█████▍    | 1724/3145 [11:49<08:58,  2.64it/s] 56%|█████▌    | 1759/3145 [11:50<09:22,  2.46it/s] 56%|█████▌    | 1755/3143 [11:56<09:36,  2.41it/s] 55%|█████▌    | 1739/3145 [11:50<09:08,  2.57it/s] 55%|█████▍    | 1725/3145 [11:50<09:26,  2.51it/s] 56%|█████▌    | 1760/3145 [11:50<09:14,  2.50it/s] 56%|█████▌    | 1756/3143 [11:56<09:30,  2.43it/s] 55%|█████▌    | 1740/3145 [11:50<09:28,  2.47it/s] 55%|█████▍    | 1726/3145 [11:50<09:28,  2.50it/s] 56%|█████▌    | 1761/3145 [11:50<09:30,  2.43it/s] 56%|█████▌    | 1757/3143 [11:56<09:40,  2.39it/s] 55%|█████▌    | 1741/3145 [11:51<09:20,  2.51it/s] 55%|█████▍    | 1727/3145 [11:51<09:26,  2.50it/s] 56%|█████▌    | 1762/3145 [11:51<09:47,  2.36it/s] 56%|█████▌    | 1758/3143 [11:57<09:30,  2.43it/s] 55%|█████▌    | 1742/3145 [11:51<09:12,  2.54it/s] 55%|█████▍    | 1728/3145 [11:51<09:18,  2.54it/s] 56%|█████▌    | 1763/3145 [11:51<09:25,  2.44it/s] 56%|█████▌    | 1759/3143 [11:57<09:34,  2.41it/s] 55%|█████▌    | 1743/3145 [11:51<09:23,  2.49it/s] 55%|█████▍    | 1729/3145 [11:51<09:36,  2.46it/s] 56%|█████▌    | 1764/3145 [11:52<09:11,  2.51it/s] 55%|█████▌    | 1744/3145 [11:52<09:19,  2.50it/s] 56%|█████▌    | 1760/3143 [11:58<09:50,  2.34it/s] 55%|█████▌    | 1730/3145 [11:52<09:43,  2.43it/s] 56%|█████▌    | 1765/3145 [11:52<09:21,  2.46it/s] 55%|█████▌    | 1745/3145 [11:52<09:29,  2.46it/s] 56%|█████▌    | 1761/3143 [11:58<09:51,  2.34it/s] 55%|█████▌    | 1731/3145 [11:52<10:07,  2.33it/s] 56%|█████▌    | 1746/3145 [11:53<09:18,  2.51it/s] 56%|█████▌    | 1766/3145 [11:53<10:24,  2.21it/s] 56%|█████▌    | 1762/3143 [11:59<09:38,  2.39it/s] 55%|█████▌    | 1732/3145 [11:53<09:53,  2.38it/s] 56%|█████▌    | 1747/3145 [11:53<09:09,  2.54it/s] 56%|█████▌    | 1767/3145 [11:53<10:01,  2.29it/s] 55%|█████▌    | 1733/3145 [11:53<09:30,  2.48it/s] 56%|█████▌    | 1763/3143 [11:59<09:46,  2.35it/s] 56%|█████▌    | 1748/3145 [11:53<09:09,  2.54it/s] 56%|█████▌    | 1768/3145 [11:53<09:52,  2.32it/s] 55%|█████▌    | 1734/3145 [11:53<09:38,  2.44it/s] 56%|█████▌    | 1749/3145 [11:54<07:58,  2.92it/s] 56%|█████▌    | 1764/3143 [11:59<10:08,  2.27it/s] 56%|█████▌    | 1769/3145 [11:54<09:32,  2.40it/s] 55%|█████▌    | 1735/3145 [11:54<09:34,  2.45it/s] 56%|█████▌    | 1765/3143 [12:00<09:40,  2.37it/s] 56%|█████▌    | 1750/3145 [11:54<08:34,  2.71it/s] 56%|█████▋    | 1770/3145 [11:54<09:27,  2.42it/s] 56%|█████▌    | 1766/3143 [12:00<09:24,  2.44it/s] 55%|█████▌    | 1736/3145 [11:54<10:11,  2.30it/s] 56%|█████▌    | 1751/3145 [11:54<08:50,  2.63it/s] 56%|█████▋    | 1771/3145 [11:54<08:18,  2.76it/s] 56%|█████▋    | 1772/3145 [11:55<07:35,  3.01it/s] 56%|█████▌    | 1767/3143 [12:01<09:33,  2.40it/s] 56%|█████▌    | 1752/3145 [11:55<08:55,  2.60it/s] 55%|█████▌    | 1737/3145 [11:55<10:44,  2.18it/s] 56%|█████▋    | 1773/3145 [11:55<08:00,  2.86it/s] 56%|█████▌    | 1753/3145 [11:55<08:41,  2.67it/s] 56%|█████▋    | 1768/3143 [12:01<09:39,  2.37it/s] 55%|█████▌    | 1738/3145 [11:55<10:38,  2.20it/s] 56%|█████▋    | 1774/3145 [11:56<08:43,  2.62it/s] 56%|█████▌    | 1754/3145 [11:56<09:14,  2.51it/s] 56%|█████▋    | 1769/3143 [12:02<09:36,  2.38it/s] 55%|█████▌    | 1739/3145 [11:56<10:12,  2.30it/s] 56%|█████▋    | 1775/3145 [11:56<08:47,  2.60it/s] 56%|█████▋    | 1770/3143 [12:02<09:14,  2.48it/s] 56%|█████▌    | 1755/3145 [11:56<09:07,  2.54it/s] 55%|█████▌    | 1740/3145 [11:56<10:21,  2.26it/s] 56%|█████▋    | 1776/3145 [11:56<08:42,  2.62it/s] 56%|█████▋    | 1771/3143 [12:02<09:10,  2.49it/s] 56%|█████▌    | 1756/3145 [11:56<09:34,  2.42it/s] 55%|█████▌    | 1741/3145 [11:57<09:36,  2.44it/s] 57%|█████▋    | 1777/3145 [11:57<08:50,  2.58it/s] 56%|█████▋    | 1772/3143 [12:03<09:22,  2.44it/s] 55%|█████▌    | 1742/3145 [11:57<09:47,  2.39it/s] 56%|█████▌    | 1757/3145 [11:57<10:21,  2.23it/s] 57%|█████▋    | 1778/3145 [11:57<07:58,  2.85it/s] 56%|█████▋    | 1773/3143 [12:03<09:26,  2.42it/s] 57%|█████▋    | 1779/3145 [11:57<08:41,  2.62it/s] 55%|█████▌    | 1743/3145 [11:58<10:59,  2.13it/s] 56%|█████▌    | 1758/3145 [11:58<11:26,  2.02it/s] 56%|█████▋    | 1774/3143 [12:04<09:21,  2.44it/s] 57%|█████▋    | 1780/3145 [11:58<08:00,  2.84it/s] 55%|█████▌    | 1744/3145 [11:58<10:37,  2.20it/s] 56%|█████▌    | 1759/3145 [11:58<10:59,  2.10it/s] 56%|█████▋    | 1775/3143 [12:04<09:31,  2.39it/s] 57%|█████▋    | 1781/3145 [11:58<08:02,  2.83it/s] 55%|█████▌    | 1745/3145 [11:58<10:16,  2.27it/s] 57%|█████▋    | 1776/3143 [12:04<09:18,  2.45it/s] 56%|█████▌    | 1760/3145 [11:59<11:00,  2.10it/s] 57%|█████▋    | 1782/3145 [11:59<08:42,  2.61it/s] 56%|█████▌    | 1746/3145 [11:59<09:44,  2.40it/s] 57%|█████▋    | 1777/3143 [12:05<09:11,  2.48it/s] 56%|█████▌    | 1761/3145 [11:59<10:47,  2.14it/s] 57%|█████▋    | 1783/3145 [11:59<08:48,  2.58it/s] 56%|█████▌    | 1747/3145 [11:59<08:31,  2.73it/s] 57%|█████▋    | 1778/3143 [12:05<08:18,  2.74it/s] 57%|█████▋    | 1784/3145 [11:59<08:47,  2.58it/s] 56%|█████▌    | 1748/3145 [11:59<08:45,  2.66it/s] 56%|█████▌    | 1762/3145 [11:59<11:00,  2.09it/s] 57%|█████▋    | 1779/3143 [12:05<08:36,  2.64it/s] 57%|█████▋    | 1785/3145 [12:00<08:36,  2.63it/s] 56%|█████▌    | 1763/3145 [12:00<10:27,  2.20it/s] 56%|█████▌    | 1749/3145 [12:00<09:19,  2.50it/s] 57%|█████▋    | 1780/3143 [12:06<08:21,  2.72it/s] 57%|█████▋    | 1786/3145 [12:00<08:35,  2.64it/s] 56%|█████▌    | 1764/3145 [12:00<09:45,  2.36it/s] 57%|█████▋    | 1781/3143 [12:06<08:40,  2.61it/s] 56%|█████▌    | 1750/3145 [12:00<09:35,  2.43it/s] 57%|█████▋    | 1787/3145 [12:00<08:19,  2.72it/s] 56%|█████▌    | 1751/3145 [12:01<08:23,  2.77it/s] 56%|█████▌    | 1765/3145 [12:01<09:36,  2.39it/s] 57%|█████▋    | 1782/3143 [12:07<09:02,  2.51it/s] 57%|█████▋    | 1788/3145 [12:01<08:44,  2.59it/s] 56%|█████▌    | 1752/3145 [12:01<08:41,  2.67it/s] 56%|█████▌    | 1766/3145 [12:01<09:08,  2.51it/s] 57%|█████▋    | 1783/3143 [12:07<08:53,  2.55it/s] 57%|█████▋    | 1789/3145 [12:01<09:03,  2.50it/s] 56%|█████▌    | 1753/3145 [12:01<09:20,  2.48it/s] 56%|█████▌    | 1767/3145 [12:01<09:49,  2.34it/s] 57%|█████▋    | 1784/3143 [12:07<08:40,  2.61it/s] 57%|█████▋    | 1790/3145 [12:02<07:58,  2.83it/s] 56%|█████▌    | 1754/3145 [12:02<09:19,  2.49it/s] 57%|█████▋    | 1785/3143 [12:08<08:39,  2.62it/s] 56%|█████▌    | 1768/3145 [12:02<09:46,  2.35it/s] 57%|█████▋    | 1791/3145 [12:02<08:15,  2.73it/s] 56%|█████▌    | 1755/3145 [12:02<09:14,  2.51it/s] 56%|█████▌    | 1769/3145 [12:02<09:36,  2.39it/s] 57%|█████▋    | 1786/3143 [12:08<09:04,  2.49it/s] 57%|█████▋    | 1792/3145 [12:02<08:45,  2.58it/s] 56%|█████▋    | 1770/3145 [12:03<09:09,  2.50it/s] 56%|█████▌    | 1756/3145 [12:03<09:39,  2.40it/s] 57%|█████▋    | 1787/3143 [12:09<09:06,  2.48it/s] 57%|█████▋    | 1793/3145 [12:03<08:35,  2.62it/s] 57%|█████▋    | 1794/3145 [12:03<07:30,  3.00it/s] 56%|█████▋    | 1771/3145 [12:03<09:15,  2.47it/s] 56%|█████▌    | 1757/3145 [12:03<09:30,  2.43it/s] 57%|█████▋    | 1788/3143 [12:09<09:06,  2.48it/s] 56%|█████▋    | 1772/3145 [12:03<08:08,  2.81it/s] 56%|█████▌    | 1758/3145 [12:03<08:58,  2.58it/s] 57%|█████▋    | 1789/3143 [12:09<08:51,  2.55it/s] 57%|█████▋    | 1795/3145 [12:04<09:07,  2.47it/s] 56%|█████▋    | 1773/3145 [12:04<08:38,  2.64it/s] 56%|█████▌    | 1759/3145 [12:04<09:00,  2.56it/s] 57%|█████▋    | 1796/3145 [12:04<08:48,  2.55it/s] 57%|█████▋    | 1790/3143 [12:10<09:13,  2.45it/s] 56%|█████▌    | 1760/3145 [12:04<09:01,  2.56it/s] 56%|█████▋    | 1774/3145 [12:04<09:15,  2.47it/s] 57%|█████▋    | 1797/3145 [12:04<08:35,  2.62it/s] 57%|█████▋    | 1791/3143 [12:10<09:12,  2.45it/s] 56%|█████▌    | 1761/3145 [12:05<09:15,  2.49it/s] 57%|█████▋    | 1798/3145 [12:05<08:41,  2.58it/s] 56%|█████▋    | 1775/3145 [12:05<09:48,  2.33it/s] 57%|█████▋    | 1792/3143 [12:11<09:23,  2.40it/s] 56%|█████▌    | 1762/3145 [12:05<09:12,  2.50it/s] 56%|█████▋    | 1776/3145 [12:05<09:27,  2.41it/s] 57%|█████▋    | 1799/3145 [12:05<09:10,  2.45it/s] 57%|█████▋    | 1793/3143 [12:11<09:19,  2.41it/s] 56%|█████▌    | 1763/3145 [12:05<09:24,  2.45it/s] 57%|█████▋    | 1777/3145 [12:05<09:16,  2.46it/s] 57%|█████▋    | 1800/3145 [12:06<09:20,  2.40it/s] 57%|█████▋    | 1794/3143 [12:11<09:00,  2.49it/s] 57%|█████▋    | 1778/3145 [12:06<09:16,  2.46it/s] 56%|█████▌    | 1764/3145 [12:06<09:42,  2.37it/s] 57%|█████▋    | 1801/3145 [12:06<09:15,  2.42it/s] 57%|█████▋    | 1795/3143 [12:12<09:19,  2.41it/s] 57%|█████▋    | 1779/3145 [12:06<09:12,  2.47it/s] 56%|█████▌    | 1765/3145 [12:06<09:35,  2.40it/s] 57%|█████▋    | 1796/3143 [12:12<09:03,  2.48it/s] 57%|█████▋    | 1802/3145 [12:06<09:31,  2.35it/s] 56%|█████▌    | 1766/3145 [12:07<08:26,  2.72it/s] 57%|█████▋    | 1780/3145 [12:07<08:52,  2.56it/s] 57%|█████▋    | 1797/3143 [12:13<08:58,  2.50it/s] 57%|█████▋    | 1803/3145 [12:07<09:32,  2.35it/s] 56%|█████▌    | 1767/3145 [12:07<08:57,  2.56it/s] 57%|█████▋    | 1781/3145 [12:07<09:09,  2.48it/s] 57%|█████▋    | 1798/3143 [12:13<09:12,  2.43it/s] 57%|█████▋    | 1804/3145 [12:07<09:18,  2.40it/s] 56%|█████▌    | 1768/3145 [12:07<08:49,  2.60it/s] 57%|█████▋    | 1782/3145 [12:07<09:03,  2.51it/s] 57%|█████▋    | 1799/3143 [12:13<09:01,  2.48it/s] 57%|█████▋    | 1805/3145 [12:08<09:11,  2.43it/s] 56%|█████▌    | 1769/3145 [12:08<09:00,  2.54it/s] 57%|█████▋    | 1800/3143 [12:14<08:23,  2.67it/s] 57%|█████▋    | 1783/3145 [12:08<10:16,  2.21it/s] 57%|█████▋    | 1806/3145 [12:08<09:28,  2.35it/s] 56%|█████▋    | 1770/3145 [12:08<09:22,  2.45it/s] 57%|█████▋    | 1801/3143 [12:14<08:24,  2.66it/s] 57%|█████▋    | 1784/3145 [12:08<09:38,  2.35it/s] 57%|█████▋    | 1807/3145 [12:08<09:11,  2.43it/s] 57%|█████▋    | 1802/3143 [12:15<08:55,  2.51it/s] 56%|█████▋    | 1771/3145 [12:09<10:02,  2.28it/s] 57%|█████▋    | 1785/3145 [12:09<09:42,  2.33it/s] 57%|█████▋    | 1808/3145 [12:09<10:23,  2.15it/s] 56%|█████▋    | 1772/3145 [12:09<09:55,  2.30it/s] 57%|█████▋    | 1803/3143 [12:15<09:14,  2.42it/s] 57%|█████▋    | 1786/3145 [12:09<09:42,  2.33it/s] 58%|█████▊    | 1809/3145 [12:10<10:38,  2.09it/s] 56%|█████▋    | 1773/3145 [12:10<09:49,  2.33it/s] 57%|█████▋    | 1804/3143 [12:16<09:33,  2.33it/s] 57%|█████▋    | 1787/3145 [12:10<09:49,  2.30it/s] 57%|█████▋    | 1788/3145 [12:10<09:26,  2.39it/s] 57%|█████▋    | 1805/3143 [12:16<09:46,  2.28it/s] 58%|█████▊    | 1810/3145 [12:10<11:14,  1.98it/s] 56%|█████▋    | 1774/3145 [12:10<11:08,  2.05it/s] 57%|█████▋    | 1789/3145 [12:10<09:02,  2.50it/s] 57%|█████▋    | 1806/3143 [12:16<09:29,  2.35it/s] 58%|█████▊    | 1811/3145 [12:11<10:55,  2.03it/s] 56%|█████▋    | 1775/3145 [12:11<11:36,  1.97it/s] 57%|█████▋    | 1790/3145 [12:11<09:24,  2.40it/s] 57%|█████▋    | 1807/3143 [12:17<09:33,  2.33it/s] 58%|█████▊    | 1812/3145 [12:11<10:30,  2.11it/s] 57%|█████▋    | 1791/3145 [12:11<09:23,  2.40it/s] 58%|█████▊    | 1808/3143 [12:17<09:32,  2.33it/s] 56%|█████▋    | 1776/3145 [12:11<12:21,  1.85it/s] 58%|█████▊    | 1813/3145 [12:11<10:06,  2.20it/s] 58%|█████▊    | 1809/3143 [12:18<09:14,  2.40it/s] 57%|█████▋    | 1792/3145 [12:12<09:48,  2.30it/s] 58%|█████▊    | 1814/3145 [12:12<10:14,  2.17it/s] 57%|█████▋    | 1777/3145 [12:12<12:31,  1.82it/s] 58%|█████▊    | 1810/3143 [12:18<09:05,  2.44it/s] 57%|█████▋    | 1793/3145 [12:12<09:29,  2.37it/s] 58%|█████▊    | 1815/3145 [12:12<10:04,  2.20it/s] 57%|█████▋    | 1778/3145 [12:12<11:35,  1.97it/s] 58%|█████▊    | 1811/3143 [12:18<08:49,  2.52it/s] 57%|█████▋    | 1794/3145 [12:13<09:32,  2.36it/s] 58%|█████▊    | 1816/3145 [12:13<09:24,  2.35it/s] 57%|█████▋    | 1779/3145 [12:13<11:10,  2.04it/s] 57%|█████▋    | 1795/3145 [12:13<09:07,  2.47it/s] 58%|█████▊    | 1812/3143 [12:19<09:07,  2.43it/s] 58%|█████▊    | 1817/3145 [12:13<09:31,  2.32it/s] 57%|█████▋    | 1780/3145 [12:13<10:14,  2.22it/s] 57%|█████▋    | 1796/3145 [12:13<09:22,  2.40it/s] 58%|█████▊    | 1813/3143 [12:19<09:19,  2.38it/s] 58%|█████▊    | 1818/3145 [12:14<09:14,  2.39it/s] 57%|█████▋    | 1781/3145 [12:14<09:53,  2.30it/s] 58%|█████▊    | 1814/3143 [12:20<09:10,  2.41it/s] 57%|█████▋    | 1797/3145 [12:14<09:21,  2.40it/s] 57%|█████▋    | 1782/3145 [12:14<09:41,  2.34it/s] 58%|█████▊    | 1819/3145 [12:14<09:22,  2.36it/s] 57%|█████▋    | 1798/3145 [12:14<09:28,  2.37it/s] 58%|█████▊    | 1815/3143 [12:20<09:26,  2.35it/s] 57%|█████▋    | 1783/3145 [12:14<09:48,  2.31it/s] 58%|█████▊    | 1820/3145 [12:14<09:31,  2.32it/s] 57%|█████▋    | 1799/3145 [12:15<09:10,  2.44it/s] 58%|█████▊    | 1816/3143 [12:21<09:21,  2.36it/s] 57%|█████▋    | 1784/3145 [12:15<09:34,  2.37it/s] 58%|█████▊    | 1821/3145 [12:15<10:23,  2.12it/s] 57%|█████▋    | 1800/3145 [12:15<09:03,  2.48it/s] 58%|█████▊    | 1817/3143 [12:21<09:26,  2.34it/s] 57%|█████▋    | 1785/3145 [12:15<09:18,  2.44it/s] 57%|█████▋    | 1801/3145 [12:15<07:56,  2.82it/s] 58%|█████▊    | 1822/3145 [12:15<09:41,  2.28it/s] 57%|█████▋    | 1786/3145 [12:15<08:13,  2.75it/s] 58%|█████▊    | 1818/3143 [12:21<09:23,  2.35it/s] 57%|█████▋    | 1802/3145 [12:16<08:21,  2.68it/s] 58%|█████▊    | 1823/3145 [12:16<09:37,  2.29it/s] 57%|█████▋    | 1787/3145 [12:16<08:30,  2.66it/s] 58%|█████▊    | 1819/3143 [12:22<09:10,  2.41it/s] 57%|█████▋    | 1803/3145 [12:16<08:23,  2.66it/s] 58%|█████▊    | 1824/3145 [12:16<09:17,  2.37it/s] 58%|█████▊    | 1820/3143 [12:22<09:18,  2.37it/s] 57%|█████▋    | 1788/3145 [12:16<09:44,  2.32it/s] 57%|█████▋    | 1804/3145 [12:16<08:45,  2.55it/s] 58%|█████▊    | 1825/3145 [12:17<09:09,  2.40it/s] 57%|█████▋    | 1789/3145 [12:17<09:17,  2.43it/s] 58%|█████▊    | 1821/3143 [12:23<09:24,  2.34it/s] 57%|█████▋    | 1805/3145 [12:17<09:03,  2.46it/s] 58%|█████▊    | 1826/3145 [12:17<09:01,  2.44it/s] 58%|█████▊    | 1822/3143 [12:23<09:12,  2.39it/s] 57%|█████▋    | 1790/3145 [12:17<09:29,  2.38it/s] 57%|█████▋    | 1806/3145 [12:17<09:15,  2.41it/s] 58%|█████▊    | 1827/3145 [12:17<09:13,  2.38it/s] 58%|█████▊    | 1823/3143 [12:24<09:21,  2.35it/s] 57%|█████▋    | 1791/3145 [12:18<09:50,  2.29it/s] 57%|█████▋    | 1807/3145 [12:18<09:25,  2.37it/s] 58%|█████▊    | 1828/3145 [12:18<09:17,  2.36it/s] 58%|█████▊    | 1824/3143 [12:24<09:23,  2.34it/s] 57%|█████▋    | 1792/3145 [12:18<09:32,  2.36it/s] 57%|█████▋    | 1808/3145 [12:18<09:19,  2.39it/s] 58%|█████▊    | 1829/3145 [12:18<09:06,  2.41it/s] 58%|█████▊    | 1825/3143 [12:24<09:28,  2.32it/s] 57%|█████▋    | 1793/3145 [12:19<09:40,  2.33it/s] 58%|█████▊    | 1809/3145 [12:19<09:18,  2.39it/s] 58%|█████▊    | 1830/3145 [12:19<08:59,  2.44it/s] 58%|█████▊    | 1826/3143 [12:25<09:08,  2.40it/s] 57%|█████▋    | 1794/3145 [12:19<09:31,  2.36it/s] 58%|█████▊    | 1810/3145 [12:19<09:18,  2.39it/s] 58%|█████▊    | 1831/3145 [12:19<09:18,  2.35it/s] 58%|█████▊    | 1827/3143 [12:25<09:16,  2.36it/s] 57%|█████▋    | 1795/3145 [12:19<09:19,  2.41it/s] 58%|█████▊    | 1811/3145 [12:19<09:24,  2.36it/s] 58%|█████▊    | 1832/3145 [12:20<09:07,  2.40it/s] 58%|█████▊    | 1828/3143 [12:26<09:16,  2.36it/s] 58%|█████▊    | 1833/3145 [12:20<08:08,  2.69it/s] 57%|█████▋    | 1796/3145 [12:20<09:41,  2.32it/s] 58%|█████▊    | 1812/3145 [12:20<09:20,  2.38it/s] 58%|█████▊    | 1834/3145 [12:20<07:31,  2.90it/s] 58%|█████▊    | 1829/3143 [12:26<09:09,  2.39it/s] 57%|█████▋    | 1797/3145 [12:20<09:30,  2.36it/s] 58%|█████▊    | 1813/3145 [12:20<09:13,  2.41it/s] 58%|█████▊    | 1835/3145 [12:20<07:36,  2.87it/s] 57%|█████▋    | 1798/3145 [12:21<09:08,  2.46it/s] 58%|█████▊    | 1830/3143 [12:26<09:17,  2.35it/s] 58%|█████▊    | 1814/3145 [12:21<08:52,  2.50it/s] 58%|█████▊    | 1836/3145 [12:21<07:57,  2.74it/s] 57%|█████▋    | 1799/3145 [12:21<09:25,  2.38it/s] 58%|█████▊    | 1831/3143 [12:27<09:19,  2.34it/s] 58%|█████▊    | 1815/3145 [12:21<08:51,  2.50it/s] 58%|█████▊    | 1837/3145 [12:21<08:23,  2.60it/s] 58%|█████▊    | 1832/3143 [12:27<09:12,  2.37it/s] 57%|█████▋    | 1800/3145 [12:21<09:44,  2.30it/s] 58%|█████▊    | 1816/3145 [12:22<09:05,  2.43it/s] 58%|█████▊    | 1838/3145 [12:22<08:40,  2.51it/s] 58%|█████▊    | 1833/3143 [12:28<09:26,  2.31it/s] 58%|█████▊    | 1817/3145 [12:22<09:17,  2.38it/s] 57%|█████▋    | 1801/3145 [12:22<09:51,  2.27it/s] 58%|█████▊    | 1839/3145 [12:22<08:34,  2.54it/s] 57%|█████▋    | 1802/3145 [12:22<08:41,  2.57it/s] 58%|█████▊    | 1834/3143 [12:28<09:10,  2.38it/s] 58%|█████▊    | 1818/3145 [12:22<09:42,  2.28it/s] 59%|█████▊    | 1840/3145 [12:22<08:32,  2.55it/s] 57%|█████▋    | 1803/3145 [12:23<09:03,  2.47it/s] 58%|█████▊    | 1835/3143 [12:29<09:07,  2.39it/s] 58%|█████▊    | 1819/3145 [12:23<09:21,  2.36it/s] 59%|█████▊    | 1841/3145 [12:23<08:34,  2.53it/s] 57%|█████▋    | 1804/3145 [12:23<08:49,  2.53it/s] 58%|█████▊    | 1836/3143 [12:29<08:48,  2.47it/s] 59%|█████▊    | 1842/3145 [12:23<08:23,  2.59it/s] 58%|█████▊    | 1820/3145 [12:23<09:36,  2.30it/s] 58%|█████▊    | 1837/3143 [12:29<08:54,  2.44it/s] 57%|█████▋    | 1805/3145 [12:23<09:20,  2.39it/s] 59%|█████▊    | 1843/3145 [12:24<08:12,  2.64it/s] 58%|█████▊    | 1821/3145 [12:24<09:21,  2.36it/s] 57%|█████▋    | 1806/3145 [12:24<09:02,  2.47it/s] 58%|█████▊    | 1838/3143 [12:30<08:50,  2.46it/s] 59%|█████▊    | 1844/3145 [12:24<08:15,  2.62it/s] 58%|█████▊    | 1822/3145 [12:24<09:09,  2.41it/s] 57%|█████▋    | 1807/3145 [12:24<08:38,  2.58it/s] 59%|█████▊    | 1839/3143 [12:30<09:01,  2.41it/s] 59%|█████▊    | 1845/3145 [12:24<08:19,  2.60it/s] 58%|█████▊    | 1823/3145 [12:24<09:03,  2.43it/s] 57%|█████▋    | 1808/3145 [12:25<09:01,  2.47it/s] 59%|█████▊    | 1846/3145 [12:25<08:23,  2.58it/s] 59%|█████▊    | 1840/3143 [12:31<09:10,  2.37it/s] 58%|█████▊    | 1824/3145 [12:25<08:58,  2.45it/s] 58%|█████▊    | 1809/3145 [12:25<09:17,  2.40it/s] 59%|█████▊    | 1847/3145 [12:25<08:38,  2.50it/s] 59%|█████▊    | 1841/3143 [12:31<09:01,  2.41it/s] 58%|█████▊    | 1825/3145 [12:25<08:54,  2.47it/s] 58%|█████▊    | 1810/3145 [12:26<09:15,  2.41it/s] 59%|█████▊    | 1842/3143 [12:31<08:44,  2.48it/s] 59%|█████▉    | 1848/3145 [12:26<08:42,  2.48it/s] 58%|█████▊    | 1826/3145 [12:26<08:49,  2.49it/s] 59%|█████▊    | 1843/3143 [12:32<08:42,  2.49it/s] 59%|█████▉    | 1849/3145 [12:26<08:27,  2.55it/s] 58%|█████▊    | 1811/3145 [12:26<09:32,  2.33it/s] 58%|█████▊    | 1827/3145 [12:26<09:00,  2.44it/s] 59%|█████▊    | 1844/3143 [12:32<08:32,  2.53it/s] 58%|█████▊    | 1828/3145 [12:26<08:03,  2.72it/s] 59%|█████▉    | 1850/3145 [12:26<08:35,  2.51it/s] 58%|█████▊    | 1812/3145 [12:26<09:20,  2.38it/s] 58%|█████▊    | 1813/3145 [12:27<08:32,  2.60it/s] 59%|█████▊    | 1845/3143 [12:33<08:39,  2.50it/s] 58%|█████▊    | 1829/3145 [12:27<08:14,  2.66it/s] 59%|█████▉    | 1851/3145 [12:27<08:55,  2.42it/s] 58%|█████▊    | 1814/3145 [12:27<08:52,  2.50it/s] 59%|█████▊    | 1846/3143 [12:33<08:48,  2.45it/s] 59%|█████▉    | 1852/3145 [12:27<09:03,  2.38it/s] 58%|█████▊    | 1830/3145 [12:27<09:35,  2.29it/s] 58%|█████▊    | 1815/3145 [12:28<08:54,  2.49it/s] 59%|█████▉    | 1847/3143 [12:33<08:46,  2.46it/s] 59%|█████▉    | 1853/3145 [12:28<09:10,  2.35it/s] 58%|█████▊    | 1831/3145 [12:28<09:25,  2.32it/s] 59%|█████▉    | 1848/3143 [12:34<08:56,  2.41it/s] 59%|█████▉    | 1854/3145 [12:28<09:06,  2.36it/s] 58%|█████▊    | 1816/3145 [12:28<10:05,  2.20it/s] 58%|█████▊    | 1832/3145 [12:28<09:35,  2.28it/s] 59%|█████▉    | 1849/3143 [12:34<08:51,  2.43it/s] 59%|█████▉    | 1855/3145 [12:28<08:47,  2.45it/s] 58%|█████▊    | 1817/3145 [12:28<09:42,  2.28it/s] 58%|█████▊    | 1833/3145 [12:29<09:15,  2.36it/s] 59%|█████▉    | 1850/3143 [12:35<08:52,  2.43it/s] 59%|█████▉    | 1856/3145 [12:29<08:29,  2.53it/s] 58%|█████▊    | 1818/3145 [12:29<09:24,  2.35it/s] 58%|█████▊    | 1834/3145 [12:29<09:17,  2.35it/s] 59%|█████▉    | 1851/3143 [12:35<08:48,  2.45it/s] 58%|█████▊    | 1819/3145 [12:29<09:04,  2.43it/s] 59%|█████▉    | 1857/3145 [12:29<09:13,  2.33it/s] 58%|█████▊    | 1835/3145 [12:29<09:06,  2.40it/s] 59%|█████▉    | 1852/3143 [12:36<08:50,  2.43it/s] 58%|█████▊    | 1820/3145 [12:30<09:12,  2.40it/s] 59%|█████▉    | 1858/3145 [12:30<08:54,  2.41it/s] 58%|█████▊    | 1836/3145 [12:30<08:37,  2.53it/s] 59%|█████▉    | 1853/3143 [12:36<09:09,  2.35it/s] 58%|█████▊    | 1837/3145 [12:30<08:12,  2.65it/s] 58%|█████▊    | 1821/3145 [12:30<09:11,  2.40it/s] 59%|█████▉    | 1859/3145 [12:30<09:05,  2.36it/s] 59%|█████▉    | 1854/3143 [12:36<08:48,  2.44it/s] 58%|█████▊    | 1838/3145 [12:31<08:18,  2.62it/s] 58%|█████▊    | 1822/3145 [12:31<09:06,  2.42it/s] 59%|█████▉    | 1860/3145 [12:31<08:57,  2.39it/s] 59%|█████▉    | 1855/3143 [12:37<08:59,  2.39it/s] 58%|█████▊    | 1839/3145 [12:31<08:49,  2.47it/s] 58%|█████▊    | 1823/3145 [12:31<09:20,  2.36it/s] 59%|█████▉    | 1861/3145 [12:31<09:04,  2.36it/s] 59%|█████▊    | 1840/3145 [12:31<08:41,  2.50it/s] 59%|█████▉    | 1862/3145 [12:31<08:55,  2.39it/s] 58%|█████▊    | 1824/3145 [12:31<09:43,  2.27it/s] 59%|█████▉    | 1856/3143 [12:37<09:59,  2.15it/s] 59%|█████▊    | 1841/3145 [12:32<08:52,  2.45it/s] 59%|█████▉    | 1863/3145 [12:32<08:51,  2.41it/s] 58%|█████▊    | 1825/3145 [12:32<09:19,  2.36it/s] 59%|█████▉    | 1857/3143 [12:38<09:49,  2.18it/s] 58%|█████▊    | 1826/3145 [12:32<08:04,  2.72it/s] 59%|█████▊    | 1842/3145 [12:32<09:11,  2.36it/s] 59%|█████▉    | 1864/3145 [12:32<09:09,  2.33it/s] 59%|█████▉    | 1858/3143 [12:38<09:34,  2.23it/s] 58%|█████▊    | 1827/3145 [12:33<08:42,  2.52it/s] 59%|█████▊    | 1843/3145 [12:33<09:23,  2.31it/s] 59%|█████▉    | 1865/3145 [12:33<08:54,  2.40it/s] 59%|█████▉    | 1859/3143 [12:39<09:29,  2.26it/s] 58%|█████▊    | 1828/3145 [12:33<08:41,  2.52it/s] 59%|█████▉    | 1860/3143 [12:39<08:15,  2.59it/s] 59%|█████▊    | 1844/3145 [12:33<09:05,  2.39it/s] 58%|█████▊    | 1829/3145 [12:33<08:06,  2.71it/s] 59%|█████▉    | 1866/3145 [12:33<09:56,  2.15it/s] 59%|█████▉    | 1861/3143 [12:39<08:35,  2.49it/s] 59%|█████▊    | 1845/3145 [12:33<09:05,  2.38it/s] 58%|█████▊    | 1830/3145 [12:34<08:31,  2.57it/s] 59%|█████▉    | 1867/3145 [12:34<09:45,  2.18it/s] 59%|█████▉    | 1862/3143 [12:40<08:43,  2.45it/s] 59%|█████▊    | 1846/3145 [12:34<09:06,  2.37it/s] 59%|█████▉    | 1868/3145 [12:34<09:22,  2.27it/s] 58%|█████▊    | 1831/3145 [12:34<08:53,  2.46it/s] 59%|█████▉    | 1863/3143 [12:40<07:48,  2.73it/s] 59%|█████▊    | 1847/3145 [12:34<08:57,  2.41it/s] 59%|█████▉    | 1869/3145 [12:34<08:57,  2.38it/s] 59%|█████▉    | 1864/3143 [12:40<07:47,  2.74it/s] 58%|█████▊    | 1832/3145 [12:35<09:10,  2.39it/s] 59%|█████▉    | 1848/3145 [12:35<09:06,  2.37it/s] 59%|█████▉    | 1870/3145 [12:35<08:28,  2.51it/s] 59%|█████▉    | 1865/3143 [12:41<07:59,  2.66it/s] 58%|█████▊    | 1833/3145 [12:35<09:19,  2.35it/s] 59%|█████▉    | 1849/3145 [12:35<09:12,  2.35it/s] 59%|█████▉    | 1871/3145 [12:35<08:27,  2.51it/s] 58%|█████▊    | 1834/3145 [12:35<08:09,  2.68it/s] 59%|█████▉    | 1866/3143 [12:41<08:07,  2.62it/s] 60%|█████▉    | 1872/3145 [12:36<08:24,  2.52it/s] 59%|█████▉    | 1850/3145 [12:36<09:26,  2.29it/s] 58%|█████▊    | 1835/3145 [12:36<08:22,  2.61it/s] 59%|█████▉    | 1867/3143 [12:42<08:27,  2.51it/s] 59%|█████▉    | 1851/3145 [12:36<09:05,  2.37it/s] 60%|█████▉    | 1873/3145 [12:36<08:34,  2.47it/s] 58%|█████▊    | 1836/3145 [12:36<08:27,  2.58it/s] 59%|█████▉    | 1868/3143 [12:42<08:24,  2.53it/s] 60%|█████▉    | 1874/3145 [12:36<08:29,  2.49it/s] 59%|█████▉    | 1852/3145 [12:36<09:15,  2.33it/s] 58%|█████▊    | 1837/3145 [12:36<08:42,  2.50it/s] 59%|█████▉    | 1869/3143 [12:42<08:33,  2.48it/s] 59%|█████▉    | 1870/3143 [12:43<07:26,  2.85it/s] 59%|█████▉    | 1853/3145 [12:37<08:46,  2.45it/s] 60%|█████▉    | 1875/3145 [12:37<08:48,  2.40it/s] 58%|█████▊    | 1838/3145 [12:37<08:50,  2.46it/s] 60%|█████▉    | 1871/3143 [12:43<06:45,  3.14it/s] 58%|█████▊    | 1839/3145 [12:37<07:53,  2.76it/s] 60%|█████▉    | 1876/3145 [12:37<08:25,  2.51it/s] 59%|█████▉    | 1854/3145 [12:37<08:43,  2.46it/s] 60%|█████▉    | 1872/3143 [12:43<07:12,  2.94it/s] 59%|█████▊    | 1840/3145 [12:38<08:26,  2.58it/s] 59%|█████▉    | 1855/3145 [12:38<08:55,  2.41it/s] 60%|█████▉    | 1877/3145 [12:38<08:44,  2.42it/s] 60%|█████▉    | 1873/3143 [12:44<07:47,  2.71it/s] 59%|█████▊    | 1841/3145 [12:38<08:46,  2.48it/s] 60%|█████▉    | 1878/3145 [12:38<08:41,  2.43it/s] 59%|█████▉    | 1856/3145 [12:38<09:03,  2.37it/s] 60%|█████▉    | 1874/3143 [12:44<08:30,  2.49it/s] 59%|█████▊    | 1842/3145 [12:38<07:54,  2.74it/s] 59%|█████▉    | 1857/3145 [12:38<08:39,  2.48it/s] 60%|█████▉    | 1879/3145 [12:39<08:58,  2.35it/s] 60%|█████▉    | 1875/3143 [12:45<08:20,  2.53it/s] 59%|█████▊    | 1843/3145 [12:39<08:25,  2.57it/s] 59%|█████▉    | 1858/3145 [12:39<08:35,  2.50it/s] 60%|█████▉    | 1880/3145 [12:39<08:48,  2.39it/s] 60%|█████▉    | 1876/3143 [12:45<07:32,  2.80it/s] 59%|█████▉    | 1859/3145 [12:39<08:19,  2.58it/s] 59%|█████▊    | 1844/3145 [12:39<08:47,  2.47it/s] 60%|█████▉    | 1881/3145 [12:39<08:49,  2.39it/s] 60%|█████▉    | 1877/3143 [12:45<08:38,  2.44it/s] 59%|█████▉    | 1860/3145 [12:40<08:18,  2.58it/s] 59%|█████▊    | 1845/3145 [12:40<09:01,  2.40it/s] 60%|█████▉    | 1882/3145 [12:40<08:39,  2.43it/s] 60%|█████▉    | 1878/3143 [12:46<08:31,  2.47it/s] 59%|█████▉    | 1861/3145 [12:40<08:13,  2.60it/s] 59%|█████▊    | 1846/3145 [12:40<08:59,  2.41it/s] 60%|█████▉    | 1883/3145 [12:40<08:11,  2.57it/s] 60%|█████▉    | 1879/3143 [12:46<08:48,  2.39it/s] 59%|█████▉    | 1862/3145 [12:40<08:30,  2.51it/s] 60%|█████▉    | 1884/3145 [12:40<08:03,  2.61it/s] 59%|█████▊    | 1847/3145 [12:40<08:58,  2.41it/s] 60%|█████▉    | 1880/3143 [12:47<08:53,  2.37it/s] 60%|█████▉    | 1885/3145 [12:41<07:54,  2.66it/s] 59%|█████▉    | 1863/3145 [12:41<08:43,  2.45it/s] 59%|█████▉    | 1848/3145 [12:41<09:37,  2.24it/s] 60%|█████▉    | 1881/3143 [12:47<08:57,  2.35it/s] 59%|█████▉    | 1864/3145 [12:41<08:34,  2.49it/s] 60%|█████▉    | 1886/3145 [12:41<08:10,  2.57it/s] 59%|█████▉    | 1849/3145 [12:41<09:08,  2.36it/s] 59%|█████▉    | 1865/3145 [12:42<08:18,  2.57it/s] 60%|██████    | 1887/3145 [12:42<07:54,  2.65it/s] 60%|█████▉    | 1882/3143 [12:48<08:47,  2.39it/s] 59%|█████▉    | 1850/3145 [12:42<09:17,  2.32it/s] 60%|█████▉    | 1883/3143 [12:48<08:24,  2.50it/s] 59%|█████▉    | 1866/3145 [12:42<08:34,  2.48it/s] 60%|██████    | 1888/3145 [12:42<08:16,  2.53it/s] 59%|█████▉    | 1851/3145 [12:42<09:17,  2.32it/s] 60%|█████▉    | 1884/3143 [12:48<08:09,  2.57it/s] 59%|█████▉    | 1867/3145 [12:42<08:19,  2.56it/s] 60%|██████    | 1889/3145 [12:42<08:33,  2.45it/s] 59%|█████▉    | 1852/3145 [12:43<08:33,  2.52it/s] 59%|█████▉    | 1868/3145 [12:43<07:37,  2.79it/s] 60%|█████▉    | 1885/3143 [12:49<07:55,  2.65it/s] 60%|██████    | 1890/3145 [12:43<08:52,  2.36it/s] 59%|█████▉    | 1869/3145 [12:43<07:15,  2.93it/s] 59%|█████▉    | 1853/3145 [12:43<08:35,  2.51it/s] 60%|██████    | 1886/3143 [12:49<07:51,  2.67it/s] 59%|█████▉    | 1870/3145 [12:43<07:04,  3.00it/s] 59%|█████▉    | 1854/3145 [12:43<07:59,  2.69it/s] 60%|██████    | 1891/3145 [12:43<08:43,  2.40it/s] 60%|██████    | 1887/3143 [12:49<08:13,  2.54it/s] 60%|██████    | 1892/3145 [12:44<08:23,  2.49it/s] 59%|█████▉    | 1871/3145 [12:44<07:42,  2.75it/s] 59%|█████▉    | 1855/3145 [12:44<08:34,  2.51it/s] 60%|██████    | 1888/3143 [12:50<08:28,  2.47it/s] 60%|█████▉    | 1872/3145 [12:44<07:14,  2.93it/s] 60%|██████    | 1893/3145 [12:44<08:16,  2.52it/s] 59%|█████▉    | 1856/3145 [12:44<08:18,  2.59it/s] 60%|██████    | 1889/3143 [12:50<08:51,  2.36it/s] 60%|█████▉    | 1873/3145 [12:44<07:56,  2.67it/s] 60%|██████    | 1894/3145 [12:45<08:29,  2.46it/s] 59%|█████▉    | 1857/3145 [12:45<08:25,  2.55it/s] 60%|██████    | 1890/3143 [12:51<08:49,  2.37it/s] 60%|█████▉    | 1874/3145 [12:45<08:11,  2.59it/s] 60%|██████    | 1895/3145 [12:45<08:41,  2.40it/s] 59%|█████▉    | 1858/3145 [12:45<08:41,  2.47it/s] 60%|█████▉    | 1875/3145 [12:45<07:36,  2.78it/s] 60%|██████    | 1891/3143 [12:51<09:00,  2.32it/s] 60%|██████    | 1896/3145 [12:45<08:32,  2.44it/s] 59%|█████▉    | 1859/3145 [12:45<08:35,  2.50it/s] 60%|█████▉    | 1876/3145 [12:45<07:12,  2.93it/s] 60%|██████    | 1892/3143 [12:52<08:53,  2.34it/s] 60%|██████    | 1897/3145 [12:46<08:19,  2.50it/s] 60%|█████▉    | 1877/3145 [12:46<06:40,  3.17it/s] 59%|█████▉    | 1860/3145 [12:46<08:59,  2.38it/s] 60%|██████    | 1898/3145 [12:46<08:06,  2.57it/s] 60%|██████    | 1893/3143 [12:52<08:52,  2.35it/s] 60%|█████▉    | 1878/3145 [12:46<07:14,  2.92it/s] 59%|█████▉    | 1861/3145 [12:46<08:50,  2.42it/s] 60%|██████    | 1899/3145 [12:46<07:55,  2.62it/s] 60%|█████▉    | 1879/3145 [12:47<07:19,  2.88it/s] 60%|██████    | 1894/3143 [12:52<08:39,  2.40it/s] 59%|█████▉    | 1862/3145 [12:47<08:40,  2.46it/s] 59%|█████▉    | 1863/3145 [12:47<07:33,  2.83it/s] 60%|██████    | 1900/3145 [12:47<08:02,  2.58it/s] 60%|█████▉    | 1880/3145 [12:47<07:33,  2.79it/s] 60%|██████    | 1895/3143 [12:53<08:46,  2.37it/s] 59%|█████▉    | 1864/3145 [12:47<07:51,  2.72it/s] 60%|██████    | 1901/3145 [12:47<08:19,  2.49it/s] 60%|██████    | 1896/3143 [12:53<08:22,  2.48it/s] 60%|█████▉    | 1881/3145 [12:47<08:53,  2.37it/s] 59%|█████▉    | 1865/3145 [12:48<08:06,  2.63it/s] 60%|██████    | 1897/3143 [12:54<08:20,  2.49it/s] 60%|██████    | 1902/3145 [12:48<08:31,  2.43it/s] 60%|█████▉    | 1882/3145 [12:48<08:28,  2.48it/s] 59%|█████▉    | 1866/3145 [12:48<08:09,  2.62it/s] 61%|██████    | 1903/3145 [12:48<08:23,  2.47it/s] 60%|██████    | 1898/3143 [12:54<08:41,  2.39it/s] 60%|█████▉    | 1883/3145 [12:48<09:48,  2.14it/s] 59%|█████▉    | 1867/3145 [12:48<08:40,  2.45it/s] 61%|██████    | 1904/3145 [12:49<08:22,  2.47it/s] 60%|██████    | 1899/3143 [12:54<08:40,  2.39it/s] 61%|██████    | 1905/3145 [12:49<08:04,  2.56it/s] 59%|█████▉    | 1868/3145 [12:49<09:02,  2.35it/s] 60%|█████▉    | 1884/3145 [12:49<10:18,  2.04it/s] 60%|██████    | 1900/3143 [12:55<08:55,  2.32it/s] 61%|██████    | 1906/3145 [12:49<08:05,  2.55it/s] 60%|█████▉    | 1885/3145 [12:49<09:41,  2.17it/s] 59%|█████▉    | 1869/3145 [12:49<08:57,  2.38it/s] 60%|██████    | 1901/3143 [12:55<09:03,  2.28it/s] 61%|██████    | 1907/3145 [12:50<08:20,  2.47it/s] 59%|█████▉    | 1870/3145 [12:50<08:53,  2.39it/s] 60%|█████▉    | 1886/3145 [12:50<09:28,  2.21it/s] 61%|██████    | 1902/3143 [12:56<08:54,  2.32it/s] 61%|██████    | 1908/3145 [12:50<08:56,  2.30it/s] 59%|█████▉    | 1871/3145 [12:50<09:00,  2.36it/s] 60%|██████    | 1887/3145 [12:50<09:28,  2.21it/s] 61%|██████    | 1903/3143 [12:56<08:29,  2.43it/s] 61%|██████    | 1909/3145 [12:50<07:40,  2.68it/s] 60%|██████    | 1888/3145 [12:51<08:48,  2.38it/s] 60%|█████▉    | 1872/3145 [12:51<09:05,  2.33it/s] 61%|██████    | 1904/3143 [12:57<08:27,  2.44it/s] 61%|██████    | 1910/3145 [12:51<07:47,  2.64it/s] 61%|██████    | 1905/3143 [12:57<08:25,  2.45it/s] 60%|█████▉    | 1873/3145 [12:51<09:21,  2.26it/s] 60%|██████    | 1889/3145 [12:51<09:46,  2.14it/s] 61%|██████    | 1911/3145 [12:51<07:42,  2.67it/s] 61%|██████    | 1906/3143 [12:57<08:19,  2.48it/s] 60%|█████▉    | 1874/3145 [12:51<08:48,  2.41it/s] 61%|██████    | 1912/3145 [12:52<07:46,  2.64it/s] 60%|██████    | 1890/3145 [12:52<10:01,  2.09it/s] 60%|█████▉    | 1875/3145 [12:52<07:48,  2.71it/s] 61%|██████    | 1907/3143 [12:58<08:15,  2.49it/s] 61%|██████    | 1913/3145 [12:52<08:16,  2.48it/s] 60%|█████▉    | 1876/3145 [12:52<08:14,  2.57it/s] 60%|██████    | 1891/3145 [12:52<10:37,  1.97it/s] 61%|██████    | 1908/3143 [12:58<08:13,  2.50it/s] 61%|██████    | 1914/3145 [12:52<07:58,  2.58it/s] 60%|█████▉    | 1877/3145 [12:53<08:19,  2.54it/s] 60%|██████    | 1892/3145 [12:53<09:53,  2.11it/s] 61%|██████    | 1909/3143 [12:59<08:25,  2.44it/s] 61%|██████    | 1915/3145 [12:53<07:56,  2.58it/s] 60%|█████▉    | 1878/3145 [12:53<08:03,  2.62it/s] 61%|██████    | 1910/3143 [12:59<08:28,  2.42it/s] 60%|██████    | 1893/3145 [12:53<10:31,  1.98it/s] 61%|██████    | 1916/3145 [12:53<08:21,  2.45it/s] 60%|█████▉    | 1879/3145 [12:53<08:07,  2.60it/s] 60%|██████    | 1894/3145 [12:54<09:30,  2.19it/s] 61%|██████    | 1911/3143 [12:59<08:49,  2.33it/s] 61%|██████    | 1917/3145 [12:54<08:47,  2.33it/s] 60%|█████▉    | 1880/3145 [12:54<08:20,  2.53it/s] 60%|██████    | 1895/3145 [12:54<09:10,  2.27it/s] 61%|██████    | 1912/3143 [13:00<09:00,  2.28it/s] 61%|██████    | 1918/3145 [12:54<08:48,  2.32it/s] 60%|█████▉    | 1881/3145 [12:54<08:29,  2.48it/s] 60%|██████    | 1896/3145 [12:54<08:52,  2.34it/s] 61%|██████    | 1913/3143 [13:00<09:00,  2.28it/s] 60%|█████▉    | 1882/3145 [12:55<08:15,  2.55it/s] 61%|██████    | 1919/3145 [12:55<09:06,  2.24it/s] 60%|██████    | 1897/3145 [12:55<08:45,  2.37it/s] 61%|██████    | 1914/3143 [13:01<08:58,  2.28it/s] 60%|█████▉    | 1883/3145 [12:55<08:23,  2.51it/s] 61%|██████    | 1920/3145 [12:55<09:01,  2.26it/s] 60%|██████    | 1898/3145 [12:55<08:34,  2.42it/s] 61%|██████    | 1915/3143 [13:01<07:43,  2.65it/s] 60%|█████▉    | 1884/3145 [12:55<08:10,  2.57it/s] 61%|██████    | 1921/3145 [12:55<08:53,  2.29it/s] 61%|██████    | 1916/3143 [13:01<07:38,  2.68it/s] 60%|██████    | 1899/3145 [12:56<08:29,  2.44it/s] 60%|█████▉    | 1885/3145 [12:56<08:16,  2.54it/s] 61%|██████    | 1922/3145 [12:56<08:45,  2.33it/s] 61%|██████    | 1917/3143 [13:02<07:58,  2.56it/s] 60%|██████    | 1900/3145 [12:56<08:55,  2.32it/s] 60%|█████▉    | 1886/3145 [12:56<08:28,  2.48it/s] 61%|██████    | 1923/3145 [12:56<08:21,  2.43it/s] 61%|██████    | 1918/3143 [13:02<07:54,  2.58it/s] 60%|██████    | 1901/3145 [12:57<09:03,  2.29it/s] 60%|██████    | 1887/3145 [12:57<08:50,  2.37it/s] 61%|██████    | 1924/3145 [12:57<08:24,  2.42it/s] 61%|██████    | 1919/3143 [13:03<07:59,  2.55it/s] 60%|██████    | 1888/3145 [12:57<07:40,  2.73it/s] 60%|██████    | 1902/3145 [12:57<08:49,  2.35it/s] 61%|██████    | 1920/3143 [13:03<08:23,  2.43it/s] 61%|██████    | 1925/3145 [12:57<09:09,  2.22it/s] 60%|██████    | 1889/3145 [12:57<07:58,  2.62it/s] 61%|██████    | 1903/3145 [12:57<08:51,  2.34it/s] 61%|██████    | 1921/3143 [13:03<07:53,  2.58it/s] 61%|██████    | 1926/3145 [12:58<08:34,  2.37it/s] 60%|██████    | 1890/3145 [12:58<08:03,  2.60it/s] 61%|██████    | 1904/3145 [12:58<08:49,  2.35it/s] 61%|██████    | 1922/3143 [13:04<06:55,  2.94it/s] 61%|██████▏   | 1927/3145 [12:58<08:24,  2.41it/s] 60%|██████    | 1891/3145 [12:58<08:33,  2.44it/s] 61%|██████    | 1905/3145 [12:58<08:41,  2.38it/s] 61%|██████    | 1923/3143 [13:04<07:33,  2.69it/s] 61%|██████▏   | 1928/3145 [12:58<07:46,  2.61it/s] 60%|██████    | 1892/3145 [12:58<07:31,  2.77it/s] 61%|██████▏   | 1929/3145 [12:59<07:08,  2.83it/s] 61%|██████    | 1924/3143 [13:04<07:42,  2.63it/s] 61%|██████    | 1906/3145 [12:59<08:56,  2.31it/s] 60%|██████    | 1893/3145 [12:59<08:02,  2.59it/s] 61%|██████▏   | 1930/3145 [12:59<06:45,  2.99it/s] 61%|██████    | 1925/3143 [13:05<07:31,  2.70it/s] 61%|██████    | 1907/3145 [12:59<08:57,  2.30it/s] 61%|██████▏   | 1931/3145 [12:59<06:05,  3.32it/s] 60%|██████    | 1894/3145 [12:59<08:12,  2.54it/s] 61%|██████▏   | 1926/3143 [13:05<07:38,  2.65it/s] 61%|██████    | 1908/3145 [12:59<08:43,  2.36it/s] 61%|██████▏   | 1932/3145 [12:59<06:37,  3.05it/s] 60%|██████    | 1895/3145 [13:00<08:40,  2.40it/s] 61%|██████▏   | 1927/3143 [13:06<08:01,  2.52it/s] 61%|██████▏   | 1933/3145 [13:00<07:03,  2.86it/s] 61%|██████    | 1909/3145 [13:00<08:57,  2.30it/s] 61%|██████▏   | 1934/3145 [13:00<06:20,  3.18it/s] 60%|██████    | 1896/3145 [13:00<08:44,  2.38it/s] 61%|██████▏   | 1928/3143 [13:06<08:04,  2.51it/s] 61%|██████    | 1910/3145 [13:00<09:02,  2.28it/s] 62%|██████▏   | 1935/3145 [13:00<06:39,  3.03it/s] 60%|██████    | 1897/3145 [13:01<08:41,  2.39it/s] 61%|██████▏   | 1929/3143 [13:07<08:32,  2.37it/s] 61%|██████    | 1911/3145 [13:01<08:57,  2.30it/s] 62%|██████▏   | 1936/3145 [13:01<07:11,  2.80it/s] 60%|██████    | 1898/3145 [13:01<08:48,  2.36it/s] 61%|██████▏   | 1930/3143 [13:07<08:38,  2.34it/s] 61%|██████    | 1912/3145 [13:01<08:54,  2.30it/s] 62%|██████▏   | 1937/3145 [13:01<07:24,  2.72it/s] 60%|██████    | 1899/3145 [13:01<08:40,  2.40it/s] 61%|██████▏   | 1931/3143 [13:07<08:29,  2.38it/s] 62%|██████▏   | 1938/3145 [13:02<07:13,  2.79it/s] 61%|██████    | 1913/3145 [13:02<08:49,  2.33it/s] 60%|██████    | 1900/3145 [13:02<08:50,  2.34it/s] 61%|██████▏   | 1932/3143 [13:08<08:17,  2.44it/s] 62%|██████▏   | 1939/3145 [13:02<07:23,  2.72it/s] 61%|██████    | 1914/3145 [13:02<09:00,  2.28it/s] 62%|██████▏   | 1940/3145 [13:02<06:38,  3.02it/s] 62%|██████▏   | 1933/3143 [13:08<08:11,  2.46it/s] 60%|██████    | 1901/3145 [13:02<09:18,  2.23it/s] 61%|██████    | 1915/3145 [13:03<08:45,  2.34it/s] 62%|██████▏   | 1934/3143 [13:08<07:32,  2.67it/s] 62%|██████▏   | 1941/3145 [13:03<07:18,  2.75it/s] 60%|██████    | 1902/3145 [13:03<09:16,  2.23it/s] 61%|██████    | 1916/3145 [13:03<08:27,  2.42it/s] 62%|██████▏   | 1935/3143 [13:09<07:49,  2.57it/s] 62%|██████▏   | 1942/3145 [13:03<07:29,  2.68it/s] 61%|██████    | 1917/3145 [13:03<07:26,  2.75it/s] 61%|██████    | 1903/3145 [13:03<09:12,  2.25it/s] 62%|██████▏   | 1943/3145 [13:03<06:49,  2.94it/s] 62%|██████▏   | 1936/3143 [13:09<08:01,  2.51it/s] 61%|██████    | 1918/3145 [13:04<07:33,  2.70it/s] 61%|██████    | 1904/3145 [13:04<09:27,  2.19it/s] 62%|██████▏   | 1937/3143 [13:10<07:44,  2.60it/s] 62%|██████▏   | 1944/3145 [13:04<07:30,  2.66it/s] 61%|██████    | 1919/3145 [13:04<07:51,  2.60it/s] 62%|██████▏   | 1938/3143 [13:10<07:41,  2.61it/s] 61%|██████    | 1905/3145 [13:04<09:36,  2.15it/s] 62%|██████▏   | 1945/3145 [13:04<07:33,  2.64it/s] 61%|██████    | 1920/3145 [13:04<07:55,  2.58it/s] 62%|██████▏   | 1939/3143 [13:10<07:47,  2.57it/s] 62%|██████▏   | 1946/3145 [13:05<07:29,  2.67it/s] 61%|██████    | 1906/3145 [13:05<09:21,  2.21it/s] 61%|██████    | 1921/3145 [13:05<07:53,  2.58it/s] 62%|██████▏   | 1940/3143 [13:11<07:47,  2.57it/s] 62%|██████▏   | 1947/3145 [13:05<07:37,  2.62it/s] 61%|██████    | 1907/3145 [13:05<08:54,  2.32it/s] 61%|██████    | 1922/3145 [13:05<08:11,  2.49it/s] 62%|██████▏   | 1941/3143 [13:11<07:02,  2.84it/s] 62%|██████▏   | 1948/3145 [13:05<07:39,  2.61it/s] 61%|██████    | 1908/3145 [13:05<08:30,  2.42it/s] 61%|██████    | 1923/3145 [13:06<08:09,  2.50it/s] 62%|██████▏   | 1942/3143 [13:11<07:11,  2.79it/s] 62%|██████▏   | 1949/3145 [13:06<07:38,  2.61it/s] 61%|██████    | 1909/3145 [13:06<08:38,  2.39it/s] 61%|██████    | 1924/3145 [13:06<08:00,  2.54it/s] 62%|██████▏   | 1943/3143 [13:12<07:13,  2.77it/s] 62%|██████▏   | 1950/3145 [13:06<07:42,  2.58it/s] 61%|██████    | 1910/3145 [13:06<08:40,  2.37it/s] 61%|██████    | 1925/3145 [13:06<08:05,  2.51it/s] 62%|██████▏   | 1944/3143 [13:12<07:29,  2.67it/s] 62%|██████▏   | 1951/3145 [13:07<07:46,  2.56it/s] 61%|██████    | 1911/3145 [13:07<08:33,  2.40it/s] 61%|██████    | 1926/3145 [13:07<08:16,  2.45it/s] 62%|██████▏   | 1945/3143 [13:13<07:41,  2.60it/s] 62%|██████▏   | 1952/3145 [13:07<08:00,  2.48it/s] 61%|██████    | 1912/3145 [13:07<08:27,  2.43it/s] 61%|██████▏   | 1927/3145 [13:07<07:59,  2.54it/s] 62%|██████▏   | 1946/3143 [13:13<07:59,  2.50it/s] 61%|██████▏   | 1928/3145 [13:07<06:57,  2.91it/s] 62%|██████▏   | 1953/3145 [13:07<08:11,  2.42it/s] 61%|██████    | 1913/3145 [13:07<08:21,  2.46it/s] 61%|██████▏   | 1929/3145 [13:08<06:14,  3.25it/s] 62%|██████▏   | 1947/3143 [13:14<08:13,  2.42it/s] 62%|██████▏   | 1954/3145 [13:08<07:57,  2.49it/s] 61%|██████    | 1914/3145 [13:08<08:33,  2.40it/s] 61%|██████▏   | 1930/3145 [13:08<07:00,  2.89it/s] 62%|██████▏   | 1948/3143 [13:14<07:55,  2.51it/s] 62%|██████▏   | 1955/3145 [13:08<08:12,  2.42it/s] 61%|██████    | 1915/3145 [13:08<08:19,  2.46it/s] 62%|██████▏   | 1949/3143 [13:14<07:35,  2.62it/s] 61%|██████▏   | 1931/3145 [13:08<07:33,  2.68it/s] 62%|██████▏   | 1956/3145 [13:09<08:18,  2.39it/s] 61%|██████    | 1916/3145 [13:09<08:34,  2.39it/s] 62%|██████▏   | 1950/3143 [13:15<07:22,  2.70it/s] 61%|██████▏   | 1932/3145 [13:09<08:13,  2.46it/s] 62%|██████▏   | 1951/3143 [13:15<07:16,  2.73it/s] 62%|██████▏   | 1957/3145 [13:09<08:27,  2.34it/s] 61%|██████    | 1917/3145 [13:09<08:29,  2.41it/s] 61%|██████▏   | 1933/3145 [13:09<08:17,  2.44it/s] 62%|██████▏   | 1952/3143 [13:15<07:20,  2.70it/s] 62%|██████▏   | 1958/3145 [13:10<08:29,  2.33it/s] 61%|██████    | 1918/3145 [13:10<08:43,  2.34it/s] 61%|██████▏   | 1934/3145 [13:10<08:23,  2.41it/s] 62%|██████▏   | 1953/3143 [13:16<07:23,  2.68it/s] 61%|██████    | 1919/3145 [13:10<08:20,  2.45it/s] 62%|██████▏   | 1959/3145 [13:10<08:31,  2.32it/s] 62%|██████▏   | 1935/3145 [13:10<08:15,  2.44it/s] 62%|██████▏   | 1960/3145 [13:10<07:25,  2.66it/s] 62%|██████▏   | 1954/3143 [13:16<07:37,  2.60it/s] 61%|██████    | 1920/3145 [13:10<08:31,  2.40it/s] 62%|██████▏   | 1936/3145 [13:11<08:12,  2.45it/s] 62%|██████▏   | 1961/3145 [13:11<07:25,  2.65it/s] 62%|██████▏   | 1955/3143 [13:17<07:38,  2.59it/s] 61%|██████    | 1921/3145 [13:11<08:40,  2.35it/s] 62%|██████▏   | 1937/3145 [13:11<08:20,  2.41it/s] 62%|██████▏   | 1962/3145 [13:11<07:46,  2.54it/s] 62%|██████▏   | 1956/3143 [13:17<07:45,  2.55it/s] 61%|██████    | 1922/3145 [13:11<08:44,  2.33it/s] 62%|██████▏   | 1938/3145 [13:11<07:59,  2.52it/s] 62%|██████▏   | 1963/3145 [13:11<07:52,  2.50it/s] 62%|██████▏   | 1957/3143 [13:17<07:50,  2.52it/s] 61%|██████    | 1923/3145 [13:12<08:35,  2.37it/s] 62%|██████▏   | 1939/3145 [13:12<07:44,  2.59it/s] 62%|██████▏   | 1964/3145 [13:12<07:53,  2.49it/s] 62%|██████▏   | 1958/3143 [13:18<08:07,  2.43it/s] 62%|██████▏   | 1940/3145 [13:12<07:37,  2.64it/s] 61%|██████    | 1924/3145 [13:12<08:45,  2.32it/s] 62%|██████▏   | 1959/3143 [13:18<07:35,  2.60it/s] 62%|██████▏   | 1965/3145 [13:12<07:58,  2.47it/s] 61%|██████    | 1925/3145 [13:12<08:23,  2.42it/s] 62%|██████▏   | 1941/3145 [13:12<07:40,  2.62it/s] 62%|██████▏   | 1960/3143 [13:19<08:03,  2.45it/s] 63%|██████▎   | 1966/3145 [13:13<08:17,  2.37it/s] 61%|██████    | 1926/3145 [13:13<08:10,  2.49it/s] 62%|██████▏   | 1942/3145 [13:13<07:57,  2.52it/s] 62%|██████▏   | 1961/3143 [13:19<08:13,  2.40it/s] 63%|██████▎   | 1967/3145 [13:13<08:21,  2.35it/s] 61%|██████▏   | 1927/3145 [13:13<08:11,  2.48it/s] 62%|██████▏   | 1943/3145 [13:13<07:43,  2.59it/s] 63%|██████▎   | 1968/3145 [13:14<08:02,  2.44it/s] 62%|██████▏   | 1962/3143 [13:19<08:21,  2.36it/s] 62%|██████▏   | 1944/3145 [13:14<07:30,  2.66it/s] 61%|██████▏   | 1928/3145 [13:14<07:57,  2.55it/s] 62%|██████▏   | 1963/3143 [13:20<08:11,  2.40it/s] 63%|██████▎   | 1969/3145 [13:14<08:09,  2.40it/s] 62%|██████▏   | 1945/3145 [13:14<07:31,  2.66it/s] 61%|██████▏   | 1929/3145 [13:14<08:08,  2.49it/s] 62%|██████▏   | 1964/3143 [13:20<08:12,  2.39it/s] 62%|██████▏   | 1946/3145 [13:14<07:39,  2.61it/s] 63%|██████▎   | 1970/3145 [13:14<08:29,  2.31it/s] 61%|██████▏   | 1930/3145 [13:14<08:12,  2.47it/s] 62%|██████▏   | 1947/3145 [13:15<07:30,  2.66it/s] 63%|██████▎   | 1965/3143 [13:21<08:24,  2.33it/s] 63%|██████▎   | 1971/3145 [13:15<08:28,  2.31it/s] 61%|██████▏   | 1931/3145 [13:15<08:14,  2.46it/s] 62%|██████▏   | 1948/3145 [13:15<07:27,  2.67it/s] 63%|██████▎   | 1972/3145 [13:15<07:37,  2.57it/s] 63%|██████▎   | 1966/3143 [13:21<08:15,  2.37it/s] 61%|██████▏   | 1932/3145 [13:15<08:03,  2.51it/s] 62%|██████▏   | 1949/3145 [13:15<07:21,  2.71it/s] 63%|██████▎   | 1973/3145 [13:16<07:36,  2.57it/s] 63%|██████▎   | 1967/3143 [13:22<08:16,  2.37it/s] 61%|██████▏   | 1933/3145 [13:16<08:10,  2.47it/s] 62%|██████▏   | 1950/3145 [13:16<07:29,  2.66it/s] 63%|██████▎   | 1974/3145 [13:16<07:41,  2.54it/s] 63%|██████▎   | 1968/3143 [13:22<07:57,  2.46it/s] 61%|██████▏   | 1934/3145 [13:16<08:05,  2.49it/s] 62%|██████▏   | 1951/3145 [13:16<07:24,  2.69it/s] 63%|██████▎   | 1975/3145 [13:16<07:35,  2.57it/s] 62%|██████▏   | 1935/3145 [13:16<08:01,  2.52it/s] 63%|██████▎   | 1969/3143 [13:22<08:10,  2.39it/s] 62%|██████▏   | 1952/3145 [13:17<07:18,  2.72it/s] 63%|██████▎   | 1976/3145 [13:17<07:15,  2.69it/s] 63%|██████▎   | 1970/3143 [13:23<08:04,  2.42it/s] 62%|██████▏   | 1936/3145 [13:17<08:19,  2.42it/s] 63%|██████▎   | 1977/3145 [13:17<06:32,  2.97it/s] 62%|██████▏   | 1953/3145 [13:17<07:37,  2.61it/s] 63%|██████▎   | 1978/3145 [13:17<06:47,  2.86it/s] 62%|██████▏   | 1937/3145 [13:17<08:16,  2.43it/s] 63%|██████▎   | 1971/3143 [13:23<08:28,  2.31it/s] 62%|██████▏   | 1954/3145 [13:17<07:25,  2.67it/s] 63%|██████▎   | 1972/3143 [13:23<07:17,  2.68it/s] 62%|██████▏   | 1955/3145 [13:18<07:14,  2.74it/s] 62%|██████▏   | 1938/3145 [13:18<08:26,  2.38it/s] 63%|██████▎   | 1979/3145 [13:18<07:36,  2.55it/s] 63%|██████▎   | 1973/3143 [13:24<07:27,  2.61it/s] 62%|██████▏   | 1956/3145 [13:18<07:11,  2.76it/s] 62%|██████▏   | 1939/3145 [13:18<08:19,  2.41it/s] 63%|██████▎   | 1980/3145 [13:18<07:43,  2.51it/s] 63%|██████▎   | 1974/3143 [13:24<07:40,  2.54it/s] 62%|██████▏   | 1957/3145 [13:18<07:24,  2.67it/s] 63%|██████▎   | 1981/3145 [13:19<07:25,  2.61it/s] 62%|██████▏   | 1940/3145 [13:19<08:27,  2.37it/s] 63%|██████▎   | 1975/3143 [13:25<07:39,  2.54it/s] 62%|██████▏   | 1958/3145 [13:19<07:11,  2.75it/s] 63%|██████▎   | 1982/3145 [13:19<07:16,  2.67it/s] 62%|██████▏   | 1941/3145 [13:19<08:22,  2.40it/s] 63%|██████▎   | 1976/3143 [13:25<07:47,  2.50it/s] 62%|██████▏   | 1959/3145 [13:19<07:54,  2.50it/s] 63%|██████▎   | 1983/3145 [13:19<07:36,  2.55it/s] 62%|██████▏   | 1942/3145 [13:19<08:13,  2.44it/s] 62%|██████▏   | 1960/3145 [13:20<07:36,  2.59it/s] 63%|██████▎   | 1977/3143 [13:26<08:12,  2.37it/s] 63%|██████▎   | 1984/3145 [13:20<07:29,  2.58it/s] 62%|██████▏   | 1943/3145 [13:20<08:10,  2.45it/s] 62%|██████▏   | 1961/3145 [13:20<06:48,  2.90it/s] 63%|██████▎   | 1985/3145 [13:20<07:36,  2.54it/s] 63%|██████▎   | 1978/3143 [13:26<08:34,  2.26it/s] 62%|██████▏   | 1944/3145 [13:20<08:18,  2.41it/s] 62%|██████▏   | 1962/3145 [13:20<07:06,  2.77it/s] 63%|██████▎   | 1986/3145 [13:21<07:35,  2.54it/s] 62%|██████▏   | 1945/3145 [13:21<08:10,  2.45it/s] 63%|██████▎   | 1979/3143 [13:27<08:57,  2.17it/s] 62%|██████▏   | 1963/3145 [13:21<07:31,  2.62it/s] 63%|██████▎   | 1987/3145 [13:21<07:34,  2.55it/s] 62%|██████▏   | 1946/3145 [13:21<08:08,  2.46it/s] 62%|██████▏   | 1964/3145 [13:21<07:02,  2.80it/s] 63%|██████▎   | 1980/3143 [13:27<09:03,  2.14it/s] 63%|██████▎   | 1988/3145 [13:21<07:43,  2.50it/s] 62%|██████▏   | 1965/3145 [13:21<07:15,  2.71it/s] 62%|██████▏   | 1947/3145 [13:21<08:25,  2.37it/s] 63%|██████▎   | 1981/3143 [13:27<08:44,  2.21it/s] 63%|██████▎   | 1989/3145 [13:22<07:55,  2.43it/s] 63%|██████▎   | 1966/3145 [13:22<07:06,  2.76it/s] 63%|██████▎   | 1982/3143 [13:28<07:35,  2.55it/s] 62%|██████▏   | 1948/3145 [13:22<08:05,  2.46it/s] 63%|██████▎   | 1983/3143 [13:28<06:54,  2.80it/s] 63%|██████▎   | 1967/3145 [13:22<07:09,  2.74it/s] 63%|██████▎   | 1990/3145 [13:22<08:29,  2.26it/s] 62%|██████▏   | 1949/3145 [13:22<08:16,  2.41it/s] 63%|██████▎   | 1984/3143 [13:28<06:48,  2.84it/s] 63%|██████▎   | 1968/3145 [13:23<07:20,  2.67it/s] 63%|██████▎   | 1991/3145 [13:23<08:10,  2.35it/s] 62%|██████▏   | 1950/3145 [13:23<08:33,  2.33it/s] 63%|██████▎   | 1985/3143 [13:29<07:16,  2.65it/s] 63%|██████▎   | 1969/3145 [13:23<07:39,  2.56it/s] 63%|██████▎   | 1992/3145 [13:23<08:20,  2.30it/s] 62%|██████▏   | 1951/3145 [13:23<08:36,  2.31it/s] 63%|██████▎   | 1970/3145 [13:23<07:19,  2.68it/s] 63%|██████▎   | 1986/3143 [13:29<07:32,  2.56it/s] 63%|██████▎   | 1993/3145 [13:23<08:04,  2.38it/s] 63%|██████▎   | 1971/3145 [13:24<07:14,  2.70it/s] 62%|██████▏   | 1952/3145 [13:24<08:53,  2.24it/s] 63%|██████▎   | 1987/3143 [13:30<07:24,  2.60it/s] 63%|██████▎   | 1994/3145 [13:24<07:42,  2.49it/s] 62%|██████▏   | 1953/3145 [13:24<08:27,  2.35it/s] 63%|██████▎   | 1972/3145 [13:24<07:38,  2.56it/s] 63%|██████▎   | 1988/3143 [13:30<07:46,  2.47it/s] 63%|██████▎   | 1995/3145 [13:24<07:37,  2.51it/s] 63%|██████▎   | 1973/3145 [13:25<07:48,  2.50it/s] 63%|██████▎   | 1989/3143 [13:30<07:46,  2.47it/s] 62%|██████▏   | 1954/3145 [13:25<09:21,  2.12it/s] 63%|██████▎   | 1996/3145 [13:25<07:35,  2.52it/s] 63%|██████▎   | 1974/3145 [13:25<07:58,  2.45it/s] 63%|██████▎   | 1990/3143 [13:31<07:58,  2.41it/s] 62%|██████▏   | 1955/3145 [13:25<09:09,  2.17it/s] 63%|██████▎   | 1997/3145 [13:25<07:50,  2.44it/s] 63%|██████▎   | 1975/3145 [13:25<07:45,  2.51it/s] 62%|██████▏   | 1956/3145 [13:25<08:30,  2.33it/s] 64%|██████▎   | 1998/3145 [13:25<07:35,  2.52it/s] 63%|██████▎   | 1991/3143 [13:31<08:35,  2.24it/s] 64%|██████▎   | 1999/3145 [13:26<06:37,  2.88it/s] 63%|██████▎   | 1976/3145 [13:26<07:56,  2.45it/s] 62%|██████▏   | 1957/3145 [13:26<08:22,  2.36it/s] 63%|██████▎   | 1992/3143 [13:32<08:37,  2.22it/s] 64%|██████▎   | 2000/3145 [13:26<06:24,  2.97it/s] 63%|██████▎   | 1977/3145 [13:26<07:52,  2.47it/s] 62%|██████▏   | 1958/3145 [13:26<08:24,  2.35it/s] 64%|██████▎   | 2001/3145 [13:26<07:00,  2.72it/s] 63%|██████▎   | 1993/3143 [13:32<08:59,  2.13it/s] 63%|██████▎   | 1978/3145 [13:27<07:45,  2.51it/s] 62%|██████▏   | 1959/3145 [13:27<08:14,  2.40it/s] 64%|██████▎   | 2002/3145 [13:27<07:07,  2.67it/s] 63%|██████▎   | 1994/3143 [13:33<08:49,  2.17it/s] 63%|██████▎   | 1979/3145 [13:27<07:44,  2.51it/s] 62%|██████▏   | 1960/3145 [13:27<08:10,  2.42it/s] 64%|██████▎   | 2003/3145 [13:27<07:12,  2.64it/s] 63%|██████▎   | 1980/3145 [13:27<07:25,  2.61it/s] 63%|██████▎   | 1995/3143 [13:33<08:29,  2.25it/s] 62%|██████▏   | 1961/3145 [13:27<07:55,  2.49it/s] 63%|██████▎   | 1981/3145 [13:28<07:03,  2.75it/s] 64%|██████▎   | 2004/3145 [13:28<07:37,  2.49it/s] 64%|██████▎   | 1996/3143 [13:34<08:42,  2.20it/s] 62%|██████▏   | 1962/3145 [13:28<07:56,  2.48it/s] 63%|██████▎   | 1982/3145 [13:28<07:13,  2.68it/s] 64%|██████▍   | 2005/3145 [13:28<07:48,  2.43it/s] 64%|██████▎   | 1997/3143 [13:34<08:12,  2.33it/s] 62%|██████▏   | 1963/3145 [13:28<07:55,  2.49it/s] 63%|██████▎   | 1983/3145 [13:28<07:28,  2.59it/s] 64%|██████▍   | 2006/3145 [13:29<07:50,  2.42it/s] 64%|██████▎   | 1998/3143 [13:34<08:12,  2.32it/s] 62%|██████▏   | 1964/3145 [13:29<08:08,  2.42it/s] 63%|██████▎   | 1984/3145 [13:29<07:38,  2.53it/s] 64%|██████▍   | 2007/3145 [13:29<07:53,  2.41it/s] 64%|██████▎   | 1999/3143 [13:35<08:14,  2.32it/s] 62%|██████▏   | 1965/3145 [13:29<07:58,  2.47it/s] 63%|██████▎   | 1985/3145 [13:29<07:35,  2.55it/s] 64%|██████▎   | 2000/3143 [13:35<08:04,  2.36it/s] 63%|██████▎   | 1966/3145 [13:29<08:03,  2.44it/s] 64%|██████▍   | 2008/3145 [13:30<08:49,  2.15it/s] 63%|██████▎   | 1986/3145 [13:30<07:33,  2.55it/s] 64%|██████▎   | 2001/3143 [13:36<07:52,  2.41it/s] 64%|██████▍   | 2009/3145 [13:30<08:30,  2.22it/s] 63%|██████▎   | 1967/3145 [13:30<08:21,  2.35it/s] 63%|██████▎   | 1987/3145 [13:30<07:23,  2.61it/s] 64%|██████▎   | 2002/3143 [13:36<07:44,  2.46it/s] 63%|██████▎   | 1988/3145 [13:30<07:13,  2.67it/s] 63%|██████▎   | 1968/3145 [13:30<08:14,  2.38it/s] 64%|██████▍   | 2010/3145 [13:30<08:24,  2.25it/s] 63%|██████▎   | 1969/3145 [13:31<07:14,  2.71it/s] 64%|██████▎   | 2003/3143 [13:37<08:02,  2.36it/s] 63%|██████▎   | 1989/3145 [13:31<07:32,  2.55it/s] 64%|██████▍   | 2011/3145 [13:31<08:16,  2.28it/s] 63%|██████▎   | 1970/3145 [13:31<07:13,  2.71it/s] 64%|██████▍   | 2004/3143 [13:37<07:47,  2.43it/s] 63%|██████▎   | 1971/3145 [13:31<06:29,  3.02it/s] 63%|██████▎   | 1990/3145 [13:31<08:33,  2.25it/s] 64%|██████▍   | 2012/3145 [13:31<08:56,  2.11it/s] 64%|██████▍   | 2005/3143 [13:37<07:40,  2.47it/s] 63%|██████▎   | 1972/3145 [13:32<07:21,  2.66it/s] 64%|██████▍   | 2013/3145 [13:32<08:28,  2.22it/s] 64%|██████▍   | 2006/3143 [13:38<07:37,  2.48it/s] 63%|██████▎   | 1991/3145 [13:32<09:33,  2.01it/s] 63%|██████▎   | 1973/3145 [13:32<07:18,  2.67it/s] 64%|██████▍   | 2014/3145 [13:32<08:11,  2.30it/s] 64%|██████▍   | 2007/3143 [13:38<07:34,  2.50it/s] 63%|██████▎   | 1992/3145 [13:32<08:48,  2.18it/s] 63%|██████▎   | 1974/3145 [13:33<07:52,  2.48it/s] 64%|██████▍   | 2015/3145 [13:33<08:12,  2.30it/s] 63%|██████▎   | 1993/3145 [13:33<07:40,  2.50it/s] 64%|██████▍   | 2008/3143 [13:39<07:43,  2.45it/s] 63%|██████▎   | 1975/3145 [13:33<07:09,  2.73it/s] 63%|██████▎   | 1994/3145 [13:33<07:45,  2.47it/s] 64%|██████▍   | 2016/3145 [13:33<08:13,  2.29it/s] 64%|██████▍   | 2009/3143 [13:39<07:49,  2.42it/s] 63%|██████▎   | 1976/3145 [13:33<07:12,  2.70it/s] 64%|██████▍   | 2017/3145 [13:33<07:00,  2.68it/s] 63%|██████▎   | 1995/3145 [13:33<07:25,  2.58it/s] 64%|██████▍   | 2010/3143 [13:39<07:40,  2.46it/s] 64%|██████▍   | 2018/3145 [13:34<07:08,  2.63it/s] 63%|██████▎   | 1977/3145 [13:34<07:37,  2.55it/s] 63%|██████▎   | 1996/3145 [13:34<08:15,  2.32it/s] 64%|██████▍   | 2011/3143 [13:40<07:55,  2.38it/s] 63%|██████▎   | 1978/3145 [13:34<07:40,  2.54it/s] 64%|██████▍   | 2019/3145 [13:34<07:24,  2.53it/s] 63%|██████▎   | 1997/3145 [13:34<08:02,  2.38it/s] 63%|██████▎   | 1979/3145 [13:34<07:30,  2.59it/s] 64%|██████▍   | 2020/3145 [13:34<07:23,  2.54it/s] 64%|██████▍   | 2012/3143 [13:40<08:30,  2.21it/s] 64%|██████▎   | 1998/3145 [13:35<07:55,  2.41it/s] 63%|██████▎   | 1980/3145 [13:35<07:23,  2.62it/s] 64%|██████▍   | 2021/3145 [13:35<07:20,  2.55it/s] 64%|██████▍   | 2013/3143 [13:41<09:07,  2.06it/s] 64%|██████▎   | 1999/3145 [13:35<07:48,  2.44it/s] 63%|██████▎   | 1981/3145 [13:35<07:14,  2.68it/s] 64%|██████▍   | 2022/3145 [13:35<08:27,  2.21it/s] 64%|██████▍   | 2014/3143 [13:41<09:01,  2.09it/s] 64%|██████▎   | 2000/3145 [13:36<08:02,  2.37it/s] 63%|██████▎   | 1982/3145 [13:36<07:23,  2.62it/s] 64%|██████▍   | 2023/3145 [13:36<08:21,  2.24it/s] 63%|██████▎   | 1983/3145 [13:36<07:19,  2.65it/s] 64%|██████▍   | 2015/3143 [13:42<08:48,  2.13it/s] 64%|██████▎   | 2001/3145 [13:36<08:14,  2.31it/s] 64%|██████▍   | 2024/3145 [13:36<07:50,  2.38it/s] 64%|██████▍   | 2016/3143 [13:42<08:17,  2.26it/s] 63%|██████▎   | 1984/3145 [13:36<07:36,  2.55it/s] 64%|██████▎   | 2002/3145 [13:36<08:13,  2.32it/s] 64%|██████▍   | 2025/3145 [13:37<07:42,  2.42it/s] 64%|██████▍   | 2017/3143 [13:43<08:03,  2.33it/s] 63%|██████▎   | 1985/3145 [13:37<07:38,  2.53it/s] 64%|██████▎   | 2003/3145 [13:37<08:01,  2.37it/s] 64%|██████▍   | 2026/3145 [13:37<07:23,  2.52it/s] 64%|██████▍   | 2018/3143 [13:43<07:45,  2.42it/s] 64%|██████▎   | 2004/3145 [13:37<07:47,  2.44it/s] 63%|██████▎   | 1986/3145 [13:37<08:03,  2.40it/s] 64%|██████▍   | 2027/3145 [13:37<07:06,  2.62it/s] 64%|██████▍   | 2019/3143 [13:43<07:54,  2.37it/s] 64%|██████▍   | 2005/3145 [13:38<07:41,  2.47it/s] 63%|██████▎   | 1987/3145 [13:38<08:07,  2.38it/s] 64%|██████▍   | 2028/3145 [13:38<07:08,  2.61it/s] 64%|██████▍   | 2006/3145 [13:38<07:24,  2.56it/s] 64%|██████▍   | 2020/3143 [13:44<07:48,  2.40it/s] 63%|██████▎   | 1988/3145 [13:38<07:46,  2.48it/s] 65%|██████▍   | 2029/3145 [13:38<07:14,  2.57it/s] 64%|██████▍   | 2007/3145 [13:38<07:25,  2.55it/s] 64%|██████▍   | 2021/3143 [13:44<07:46,  2.40it/s] 63%|██████▎   | 1989/3145 [13:38<07:59,  2.41it/s] 65%|██████▍   | 2030/3145 [13:39<07:12,  2.58it/s] 64%|██████▍   | 2022/3143 [13:45<07:28,  2.50it/s] 64%|██████▍   | 2008/3145 [13:39<07:46,  2.44it/s] 65%|██████▍   | 2031/3145 [13:39<07:33,  2.46it/s] 63%|██████▎   | 1990/3145 [13:39<08:34,  2.24it/s] 64%|██████▍   | 2023/3143 [13:45<07:17,  2.56it/s] 64%|██████▍   | 2009/3145 [13:39<07:37,  2.48it/s] 63%|██████▎   | 1991/3145 [13:39<08:19,  2.31it/s] 65%|██████▍   | 2032/3145 [13:39<07:41,  2.41it/s] 64%|██████▍   | 2024/3143 [13:45<07:17,  2.56it/s] 64%|██████▍   | 2010/3145 [13:40<07:26,  2.54it/s] 63%|██████▎   | 1992/3145 [13:40<07:14,  2.65it/s] 65%|██████▍   | 2033/3145 [13:40<07:46,  2.38it/s] 64%|██████▍   | 2011/3145 [13:40<07:15,  2.60it/s] 64%|██████▍   | 2025/3143 [13:46<07:35,  2.45it/s] 63%|██████▎   | 1993/3145 [13:40<07:09,  2.68it/s] 65%|██████▍   | 2034/3145 [13:40<07:50,  2.36it/s] 64%|██████▍   | 2026/3143 [13:46<07:33,  2.46it/s] 64%|██████▍   | 2012/3145 [13:40<07:34,  2.49it/s] 63%|██████▎   | 1994/3145 [13:40<07:43,  2.48it/s] 65%|██████▍   | 2035/3145 [13:41<07:40,  2.41it/s] 64%|██████▍   | 2027/3143 [13:47<07:23,  2.52it/s] 64%|██████▍   | 2013/3145 [13:41<08:02,  2.35it/s] 63%|██████▎   | 1995/3145 [13:41<07:50,  2.44it/s] 65%|██████▍   | 2036/3145 [13:41<07:53,  2.34it/s] 65%|██████▍   | 2028/3143 [13:47<07:33,  2.46it/s] 63%|██████▎   | 1996/3145 [13:41<07:36,  2.51it/s] 64%|██████▍   | 2014/3145 [13:41<08:12,  2.30it/s] 65%|██████▍   | 2037/3145 [13:41<07:28,  2.47it/s] 65%|██████▍   | 2029/3143 [13:47<07:30,  2.47it/s] 63%|██████▎   | 1997/3145 [13:42<07:52,  2.43it/s] 64%|██████▍   | 2015/3145 [13:42<08:11,  2.30it/s] 65%|██████▍   | 2038/3145 [13:42<07:41,  2.40it/s] 65%|██████▍   | 2030/3143 [13:48<07:21,  2.52it/s] 64%|██████▍   | 2016/3145 [13:42<07:38,  2.46it/s] 64%|██████▎   | 1998/3145 [13:42<08:12,  2.33it/s] 65%|██████▍   | 2031/3143 [13:48<07:21,  2.52it/s] 65%|██████▍   | 2039/3145 [13:42<07:42,  2.39it/s] 64%|██████▍   | 2017/3145 [13:42<07:36,  2.47it/s] 64%|██████▎   | 1999/3145 [13:43<08:16,  2.31it/s] 65%|██████▍   | 2032/3143 [13:48<06:43,  2.75it/s] 65%|██████▍   | 2040/3145 [13:43<07:36,  2.42it/s] 65%|██████▍   | 2033/3143 [13:49<06:03,  3.05it/s] 64%|██████▍   | 2018/3145 [13:43<07:52,  2.38it/s] 64%|██████▎   | 2000/3145 [13:43<08:08,  2.34it/s] 65%|██████▍   | 2041/3145 [13:43<07:42,  2.39it/s] 65%|██████▍   | 2034/3143 [13:49<06:27,  2.86it/s] 64%|██████▍   | 2019/3145 [13:43<07:57,  2.36it/s] 64%|██████▎   | 2001/3145 [13:43<07:59,  2.39it/s] 65%|██████▍   | 2042/3145 [13:44<07:58,  2.31it/s] 65%|██████▍   | 2035/3143 [13:50<06:55,  2.67it/s] 64%|██████▍   | 2020/3145 [13:44<07:35,  2.47it/s] 64%|██████▎   | 2002/3145 [13:44<07:59,  2.38it/s] 65%|██████▍   | 2043/3145 [13:44<07:46,  2.36it/s] 64%|██████▍   | 2021/3145 [13:44<07:32,  2.48it/s] 65%|██████▍   | 2036/3143 [13:50<07:15,  2.54it/s] 64%|██████▎   | 2003/3145 [13:44<08:02,  2.37it/s] 65%|██████▍   | 2044/3145 [13:44<07:52,  2.33it/s] 65%|██████▍   | 2037/3143 [13:50<07:06,  2.59it/s] 64%|██████▍   | 2022/3145 [13:45<07:45,  2.41it/s] 64%|██████▎   | 2004/3145 [13:45<08:16,  2.30it/s] 65%|██████▍   | 2038/3143 [13:51<06:58,  2.64it/s] 65%|██████▌   | 2045/3145 [13:45<07:42,  2.38it/s] 64%|██████▍   | 2023/3145 [13:45<07:51,  2.38it/s] 64%|██████▍   | 2005/3145 [13:45<08:02,  2.36it/s] 65%|██████▍   | 2039/3143 [13:51<07:05,  2.60it/s] 65%|██████▌   | 2046/3145 [13:45<07:37,  2.40it/s] 64%|██████▍   | 2024/3145 [13:45<07:49,  2.39it/s] 64%|██████▍   | 2006/3145 [13:45<07:53,  2.41it/s] 65%|██████▍   | 2040/3143 [13:52<07:01,  2.61it/s] 65%|██████▌   | 2047/3145 [13:46<07:42,  2.37it/s] 64%|██████▍   | 2025/3145 [13:46<07:30,  2.49it/s] 64%|██████▍   | 2007/3145 [13:46<07:43,  2.46it/s] 65%|██████▌   | 2048/3145 [13:46<06:38,  2.76it/s] 65%|██████▍   | 2041/3143 [13:52<07:34,  2.42it/s] 64%|██████▍   | 2026/3145 [13:46<07:29,  2.49it/s] 65%|██████▌   | 2049/3145 [13:46<06:31,  2.80it/s] 64%|██████▍   | 2008/3145 [13:46<07:40,  2.47it/s] 65%|██████▍   | 2042/3143 [13:52<07:27,  2.46it/s] 64%|██████▍   | 2027/3145 [13:47<07:23,  2.52it/s] 64%|██████▍   | 2009/3145 [13:47<07:12,  2.62it/s] 65%|██████▌   | 2050/3145 [13:47<07:03,  2.59it/s] 65%|██████▌   | 2043/3143 [13:53<07:21,  2.49it/s] 64%|██████▍   | 2028/3145 [13:47<07:27,  2.50it/s] 64%|██████▍   | 2010/3145 [13:47<07:40,  2.46it/s] 65%|██████▌   | 2051/3145 [13:47<07:20,  2.48it/s] 65%|██████▍   | 2029/3145 [13:47<07:22,  2.52it/s] 65%|██████▌   | 2044/3143 [13:53<07:40,  2.39it/s] 64%|██████▍   | 2011/3145 [13:47<07:37,  2.48it/s] 65%|██████▌   | 2045/3143 [13:53<06:45,  2.70it/s] 65%|██████▌   | 2052/3145 [13:48<07:29,  2.43it/s] 65%|██████▍   | 2030/3145 [13:48<07:21,  2.53it/s] 64%|██████▍   | 2012/3145 [13:48<07:44,  2.44it/s] 65%|██████▌   | 2046/3143 [13:54<06:46,  2.70it/s] 65%|██████▌   | 2053/3145 [13:48<07:26,  2.45it/s] 65%|██████▍   | 2031/3145 [13:48<07:21,  2.53it/s] 64%|██████▍   | 2013/3145 [13:48<06:51,  2.75it/s] 65%|██████▌   | 2054/3145 [13:48<07:20,  2.47it/s] 65%|██████▌   | 2047/3143 [13:54<07:08,  2.56it/s] 65%|██████▍   | 2032/3145 [13:49<07:15,  2.56it/s] 64%|██████▍   | 2014/3145 [13:49<06:52,  2.74it/s] 65%|██████▌   | 2048/3143 [13:55<07:04,  2.58it/s] 65%|██████▌   | 2055/3145 [13:49<07:18,  2.49it/s] 65%|██████▍   | 2033/3145 [13:49<07:18,  2.54it/s] 64%|██████▍   | 2015/3145 [13:49<07:17,  2.58it/s] 65%|██████▌   | 2056/3145 [13:49<07:22,  2.46it/s] 65%|██████▌   | 2049/3143 [13:55<07:21,  2.48it/s] 65%|██████▍   | 2034/3145 [13:49<07:04,  2.62it/s] 64%|██████▍   | 2016/3145 [13:49<07:15,  2.60it/s] 65%|██████▍   | 2035/3145 [13:50<07:00,  2.64it/s] 65%|██████▌   | 2057/3145 [13:50<07:30,  2.42it/s] 65%|██████▌   | 2050/3143 [13:56<07:35,  2.40it/s] 64%|██████▍   | 2017/3145 [13:50<07:18,  2.58it/s] 65%|██████▌   | 2058/3145 [13:50<06:36,  2.74it/s] 65%|██████▍   | 2036/3145 [13:50<06:52,  2.69it/s] 65%|██████▌   | 2051/3143 [13:56<07:27,  2.44it/s] 64%|██████▍   | 2018/3145 [13:50<07:08,  2.63it/s] 65%|██████▌   | 2059/3145 [13:50<05:53,  3.08it/s] 65%|██████▍   | 2037/3145 [13:50<07:07,  2.59it/s] 65%|██████▌   | 2052/3143 [13:56<07:37,  2.38it/s] 64%|██████▍   | 2019/3145 [13:51<07:32,  2.49it/s] 66%|██████▌   | 2060/3145 [13:51<06:47,  2.66it/s] 65%|██████▍   | 2038/3145 [13:51<06:57,  2.65it/s] 65%|██████▌   | 2053/3143 [13:57<07:31,  2.42it/s] 66%|██████▌   | 2061/3145 [13:51<06:54,  2.61it/s] 64%|██████▍   | 2020/3145 [13:51<07:52,  2.38it/s] 65%|██████▍   | 2039/3145 [13:51<07:03,  2.61it/s] 65%|██████▌   | 2054/3143 [13:57<07:25,  2.45it/s] 66%|██████▌   | 2062/3145 [13:51<06:57,  2.60it/s] 64%|██████▍   | 2021/3145 [13:51<07:56,  2.36it/s] 65%|██████▍   | 2040/3145 [13:52<06:57,  2.64it/s] 65%|██████▌   | 2055/3143 [13:58<06:59,  2.59it/s] 65%|██████▍   | 2041/3145 [13:52<06:45,  2.72it/s] 64%|██████▍   | 2022/3145 [13:52<08:11,  2.28it/s] 66%|██████▌   | 2063/3145 [13:52<07:59,  2.25it/s] 65%|██████▌   | 2056/3143 [13:58<07:02,  2.57it/s] 65%|██████▍   | 2042/3145 [13:52<06:54,  2.66it/s] 64%|██████▍   | 2023/3145 [13:52<08:02,  2.32it/s] 65%|██████▌   | 2057/3143 [13:58<07:19,  2.47it/s] 66%|██████▌   | 2064/3145 [13:52<08:21,  2.15it/s] 65%|██████▍   | 2043/3145 [13:53<06:49,  2.69it/s] 64%|██████▍   | 2024/3145 [13:53<07:48,  2.39it/s] 65%|██████▌   | 2058/3143 [13:59<06:47,  2.66it/s] 66%|██████▌   | 2065/3145 [13:53<08:04,  2.23it/s] 65%|██████▍   | 2044/3145 [13:53<06:47,  2.70it/s] 64%|██████▍   | 2025/3145 [13:53<07:41,  2.43it/s] 66%|██████▌   | 2059/3143 [13:59<07:08,  2.53it/s] 66%|██████▌   | 2066/3145 [13:53<08:04,  2.23it/s] 65%|██████▌   | 2045/3145 [13:53<06:56,  2.64it/s] 64%|██████▍   | 2026/3145 [13:54<07:50,  2.38it/s] 66%|██████▌   | 2060/3143 [13:59<06:51,  2.63it/s] 66%|██████▌   | 2067/3145 [13:54<07:43,  2.33it/s] 65%|██████▌   | 2046/3145 [13:54<07:13,  2.54it/s] 64%|██████▍   | 2027/3145 [13:54<07:30,  2.48it/s] 66%|██████▌   | 2061/3143 [14:00<07:00,  2.57it/s] 66%|██████▌   | 2068/3145 [13:54<07:33,  2.38it/s] 65%|██████▌   | 2047/3145 [13:54<07:05,  2.58it/s] 66%|██████▌   | 2062/3143 [14:00<07:02,  2.56it/s] 64%|██████▍   | 2028/3145 [13:54<07:52,  2.36it/s] 65%|██████▌   | 2048/3145 [13:54<06:13,  2.94it/s] 66%|██████▌   | 2069/3145 [13:55<07:44,  2.32it/s] 66%|██████▌   | 2063/3143 [14:01<07:04,  2.55it/s] 65%|██████▍   | 2029/3145 [13:55<07:40,  2.43it/s] 65%|██████▌   | 2049/3145 [13:55<06:32,  2.79it/s] 66%|██████▌   | 2070/3145 [13:55<07:40,  2.33it/s] 65%|██████▍   | 2030/3145 [13:55<07:48,  2.38it/s] 66%|██████▌   | 2064/3143 [14:01<07:24,  2.43it/s] 65%|██████▌   | 2050/3145 [13:55<06:57,  2.62it/s] 66%|██████▌   | 2071/3145 [13:55<07:18,  2.45it/s] 66%|██████▌   | 2065/3143 [14:02<07:16,  2.47it/s] 65%|██████▌   | 2051/3145 [13:56<07:14,  2.52it/s] 65%|██████▍   | 2031/3145 [13:56<08:24,  2.21it/s] 66%|██████▌   | 2072/3145 [13:56<07:26,  2.40it/s] 66%|██████▌   | 2066/3143 [14:02<06:20,  2.83it/s] 65%|██████▌   | 2052/3145 [13:56<07:15,  2.51it/s] 66%|██████▌   | 2067/3143 [14:02<05:48,  3.09it/s] 65%|██████▍   | 2032/3145 [13:56<08:03,  2.30it/s] 66%|██████▌   | 2073/3145 [13:56<07:20,  2.44it/s] 65%|██████▌   | 2053/3145 [13:57<07:16,  2.50it/s] 66%|██████▌   | 2068/3143 [14:02<06:14,  2.87it/s] 65%|██████▍   | 2033/3145 [13:57<07:55,  2.34it/s] 66%|██████▌   | 2074/3145 [13:57<07:14,  2.46it/s] 66%|██████▌   | 2069/3143 [14:03<06:42,  2.67it/s] 65%|██████▍   | 2034/3145 [13:57<07:43,  2.40it/s] 66%|██████▌   | 2075/3145 [13:57<07:09,  2.49it/s] 65%|██████▌   | 2054/3145 [13:57<08:19,  2.18it/s] 66%|██████▌   | 2070/3143 [14:03<06:44,  2.66it/s] 66%|██████▌   | 2076/3145 [13:57<07:07,  2.50it/s] 65%|██████▍   | 2035/3145 [13:57<07:53,  2.34it/s] 65%|██████▌   | 2055/3145 [13:58<08:09,  2.23it/s] 66%|██████▌   | 2071/3143 [14:04<07:10,  2.49it/s] 66%|██████▌   | 2077/3145 [13:58<07:11,  2.48it/s] 65%|██████▍   | 2036/3145 [13:58<08:25,  2.19it/s] 65%|██████▌   | 2056/3145 [13:58<08:55,  2.03it/s] 66%|██████▌   | 2078/3145 [13:58<07:01,  2.53it/s] 65%|██████▍   | 2037/3145 [13:58<08:13,  2.25it/s] 66%|██████▌   | 2072/3143 [14:04<08:05,  2.20it/s] 66%|██████▌   | 2079/3145 [13:59<06:56,  2.56it/s] 65%|██████▌   | 2057/3145 [13:59<09:36,  1.89it/s] 66%|██████▌   | 2073/3143 [14:05<07:47,  2.29it/s] 65%|██████▍   | 2038/3145 [13:59<08:56,  2.06it/s] 66%|██████▌   | 2080/3145 [13:59<07:27,  2.38it/s] 66%|██████▌   | 2074/3143 [14:05<07:34,  2.35it/s] 65%|██████▌   | 2058/3145 [13:59<09:34,  1.89it/s] 65%|██████▍   | 2039/3145 [13:59<08:18,  2.22it/s] 66%|██████▌   | 2081/3145 [13:59<07:30,  2.36it/s] 66%|██████▌   | 2075/3143 [14:05<07:34,  2.35it/s] 65%|██████▌   | 2059/3145 [14:00<08:35,  2.11it/s] 65%|██████▍   | 2040/3145 [14:00<08:09,  2.26it/s] 66%|██████▌   | 2082/3145 [14:00<07:40,  2.31it/s] 66%|██████▌   | 2060/3145 [14:00<08:09,  2.22it/s] 66%|██████▌   | 2076/3143 [14:06<07:35,  2.34it/s] 65%|██████▍   | 2041/3145 [14:00<08:05,  2.28it/s] 66%|██████▌   | 2083/3145 [14:00<07:16,  2.43it/s] 66%|██████▌   | 2077/3143 [14:06<07:19,  2.43it/s] 66%|██████▌   | 2061/3145 [14:00<08:03,  2.24it/s] 65%|██████▍   | 2042/3145 [14:00<07:18,  2.52it/s] 66%|██████▋   | 2084/3145 [14:01<06:35,  2.68it/s] 66%|██████▌   | 2078/3143 [14:07<07:14,  2.45it/s] 65%|██████▍   | 2043/3145 [14:01<07:11,  2.56it/s] 66%|██████▌   | 2062/3145 [14:01<07:53,  2.29it/s] 66%|██████▋   | 2085/3145 [14:01<06:29,  2.72it/s] 66%|██████▌   | 2063/3145 [14:01<07:23,  2.44it/s] 65%|██████▍   | 2044/3145 [14:01<07:13,  2.54it/s] 66%|██████▋   | 2086/3145 [14:01<06:22,  2.77it/s] 66%|██████▌   | 2079/3143 [14:07<07:42,  2.30it/s] 66%|██████▌   | 2064/3145 [14:02<07:27,  2.42it/s] 65%|██████▌   | 2045/3145 [14:02<07:28,  2.45it/s] 66%|██████▋   | 2087/3145 [14:02<06:36,  2.67it/s] 66%|██████▌   | 2080/3143 [14:08<07:36,  2.33it/s] 66%|██████▌   | 2065/3145 [14:02<07:20,  2.45it/s] 66%|██████▋   | 2088/3145 [14:02<06:42,  2.63it/s] 66%|██████▌   | 2081/3143 [14:08<07:27,  2.37it/s] 65%|██████▌   | 2046/3145 [14:02<07:54,  2.32it/s] 66%|██████▌   | 2066/3145 [14:02<07:27,  2.41it/s] 66%|██████▋   | 2089/3145 [14:02<06:47,  2.59it/s] 65%|██████▌   | 2047/3145 [14:03<07:44,  2.36it/s] 66%|██████▌   | 2082/3143 [14:08<07:47,  2.27it/s] 66%|██████▌   | 2067/3145 [14:03<07:18,  2.46it/s] 66%|██████▋   | 2090/3145 [14:03<07:02,  2.50it/s] 65%|██████▌   | 2048/3145 [14:03<07:40,  2.38it/s] 66%|██████▋   | 2083/3143 [14:09<07:46,  2.27it/s] 66%|██████▌   | 2068/3145 [14:03<07:18,  2.45it/s] 66%|██████▋   | 2091/3145 [14:03<07:06,  2.47it/s] 65%|██████▌   | 2049/3145 [14:03<07:35,  2.40it/s] 66%|██████▋   | 2084/3143 [14:09<07:30,  2.35it/s] 66%|██████▌   | 2069/3145 [14:04<07:29,  2.40it/s] 65%|██████▌   | 2050/3145 [14:04<07:30,  2.43it/s] 67%|██████▋   | 2092/3145 [14:04<07:21,  2.38it/s] 66%|██████▋   | 2085/3143 [14:10<07:38,  2.31it/s] 66%|██████▌   | 2070/3145 [14:04<07:30,  2.39it/s] 67%|██████▋   | 2093/3145 [14:04<07:03,  2.49it/s] 65%|██████▌   | 2051/3145 [14:04<07:50,  2.32it/s] 66%|██████▋   | 2086/3143 [14:10<07:28,  2.36it/s] 66%|██████▌   | 2071/3145 [14:04<07:13,  2.48it/s] 66%|██████▋   | 2087/3143 [14:10<06:28,  2.72it/s] 65%|██████▌   | 2052/3145 [14:05<07:05,  2.57it/s] 67%|██████▋   | 2094/3145 [14:05<07:13,  2.43it/s] 66%|██████▌   | 2072/3145 [14:05<07:01,  2.55it/s] 65%|██████▌   | 2053/3145 [14:05<07:11,  2.53it/s] 66%|██████▋   | 2088/3143 [14:11<06:51,  2.57it/s] 67%|██████▋   | 2095/3145 [14:05<07:20,  2.38it/s] 66%|██████▌   | 2073/3145 [14:05<06:45,  2.64it/s] 66%|██████▋   | 2089/3143 [14:11<06:44,  2.60it/s] 67%|██████▋   | 2096/3145 [14:05<06:56,  2.52it/s] 65%|██████▌   | 2054/3145 [14:05<07:30,  2.42it/s] 66%|██████▌   | 2074/3145 [14:06<06:50,  2.61it/s] 66%|██████▋   | 2090/3143 [14:12<06:47,  2.59it/s] 67%|██████▋   | 2097/3145 [14:06<07:00,  2.49it/s] 65%|██████▌   | 2055/3145 [14:06<07:38,  2.38it/s] 66%|██████▌   | 2075/3145 [14:06<07:06,  2.51it/s] 67%|██████▋   | 2091/3143 [14:12<06:38,  2.64it/s] 67%|██████▋   | 2098/3145 [14:06<06:45,  2.58it/s] 65%|██████▌   | 2056/3145 [14:06<07:39,  2.37it/s] 66%|██████▌   | 2076/3145 [14:06<06:58,  2.55it/s] 67%|██████▋   | 2092/3143 [14:12<06:57,  2.52it/s] 67%|██████▋   | 2099/3145 [14:07<06:59,  2.49it/s] 65%|██████▌   | 2057/3145 [14:07<07:27,  2.43it/s] 66%|██████▌   | 2077/3145 [14:07<06:57,  2.56it/s] 67%|██████▋   | 2100/3145 [14:07<06:10,  2.82it/s] 67%|██████▋   | 2093/3143 [14:13<06:59,  2.50it/s] 65%|██████▌   | 2058/3145 [14:07<07:12,  2.52it/s] 66%|██████▌   | 2078/3145 [14:07<07:09,  2.48it/s] 67%|██████▋   | 2101/3145 [14:07<06:23,  2.72it/s] 67%|██████▋   | 2094/3143 [14:13<07:00,  2.49it/s] 65%|██████▌   | 2059/3145 [14:07<07:10,  2.52it/s] 67%|██████▋   | 2102/3145 [14:08<06:28,  2.69it/s] 66%|██████▌   | 2079/3145 [14:08<07:22,  2.41it/s] 67%|██████▋   | 2095/3143 [14:14<07:07,  2.45it/s] 66%|██████▌   | 2060/3145 [14:08<07:10,  2.52it/s] 67%|██████▋   | 2103/3145 [14:08<06:42,  2.59it/s] 66%|██████▌   | 2080/3145 [14:08<07:36,  2.33it/s] 67%|██████▋   | 2096/3143 [14:14<07:07,  2.45it/s] 66%|██████▌   | 2061/3145 [14:08<07:12,  2.51it/s] 67%|██████▋   | 2104/3145 [14:08<06:41,  2.60it/s] 67%|██████▋   | 2097/3143 [14:14<06:54,  2.52it/s] 66%|██████▌   | 2081/3145 [14:09<07:39,  2.32it/s] 66%|██████▌   | 2062/3145 [14:09<07:05,  2.54it/s] 67%|██████▋   | 2105/3145 [14:09<06:20,  2.73it/s] 67%|██████▋   | 2098/3143 [14:15<06:50,  2.54it/s] 66%|██████▌   | 2082/3145 [14:09<07:37,  2.32it/s] 66%|██████▌   | 2063/3145 [14:09<07:18,  2.47it/s] 67%|██████▋   | 2106/3145 [14:09<06:40,  2.59it/s] 67%|██████▋   | 2107/3145 [14:09<05:49,  2.97it/s] 67%|██████▋   | 2099/3143 [14:15<07:14,  2.40it/s] 66%|██████▌   | 2083/3145 [14:09<07:38,  2.32it/s] 66%|██████▌   | 2064/3145 [14:09<07:35,  2.38it/s] 66%|██████▋   | 2084/3145 [14:10<07:11,  2.46it/s] 67%|██████▋   | 2100/3143 [14:16<07:26,  2.33it/s] 67%|██████▋   | 2108/3145 [14:10<06:55,  2.50it/s] 66%|██████▌   | 2065/3145 [14:10<07:53,  2.28it/s] 66%|██████▋   | 2085/3145 [14:10<07:18,  2.42it/s] 67%|██████▋   | 2101/3143 [14:16<07:18,  2.38it/s] 67%|██████▋   | 2109/3145 [14:10<06:43,  2.57it/s] 66%|██████▌   | 2066/3145 [14:10<07:50,  2.29it/s] 66%|██████▋   | 2086/3145 [14:11<07:03,  2.50it/s] 67%|██████▋   | 2110/3145 [14:11<06:34,  2.63it/s] 67%|██████▋   | 2102/3143 [14:17<07:21,  2.36it/s] 66%|██████▌   | 2067/3145 [14:11<07:39,  2.35it/s] 66%|██████▋   | 2087/3145 [14:11<06:50,  2.58it/s] 67%|██████▋   | 2111/3145 [14:11<06:41,  2.58it/s] 67%|██████▋   | 2103/3143 [14:17<07:14,  2.40it/s] 66%|██████▌   | 2068/3145 [14:11<07:31,  2.38it/s] 66%|██████▋   | 2088/3145 [14:11<06:38,  2.65it/s] 67%|██████▋   | 2104/3143 [14:17<07:00,  2.47it/s] 67%|██████▋   | 2112/3145 [14:11<06:54,  2.49it/s] 66%|██████▌   | 2069/3145 [14:12<07:44,  2.32it/s] 66%|██████▋   | 2089/3145 [14:12<07:01,  2.51it/s] 67%|██████▋   | 2105/3143 [14:18<06:57,  2.48it/s] 67%|██████▋   | 2113/3145 [14:12<07:00,  2.46it/s] 66%|██████▋   | 2090/3145 [14:12<06:47,  2.59it/s] 66%|██████▌   | 2070/3145 [14:12<07:54,  2.27it/s] 67%|██████▋   | 2106/3143 [14:18<07:06,  2.43it/s] 67%|██████▋   | 2114/3145 [14:12<07:07,  2.41it/s] 66%|██████▋   | 2091/3145 [14:13<07:02,  2.49it/s] 66%|██████▌   | 2071/3145 [14:13<08:01,  2.23it/s] 67%|██████▋   | 2107/3143 [14:19<07:02,  2.45it/s] 67%|██████▋   | 2115/3145 [14:13<07:08,  2.40it/s] 66%|██████▌   | 2072/3145 [14:13<06:50,  2.61it/s] 67%|██████▋   | 2092/3145 [14:13<06:59,  2.51it/s] 67%|██████▋   | 2116/3145 [14:13<06:10,  2.77it/s] 67%|██████▋   | 2108/3143 [14:19<06:58,  2.47it/s] 66%|██████▌   | 2073/3145 [14:13<06:50,  2.61it/s] 67%|██████▋   | 2093/3145 [14:13<06:59,  2.51it/s] 67%|██████▋   | 2117/3145 [14:13<06:40,  2.57it/s] 67%|██████▋   | 2109/3143 [14:19<07:12,  2.39it/s] 66%|██████▌   | 2074/3145 [14:14<07:00,  2.55it/s] 67%|██████▋   | 2094/3145 [14:14<06:50,  2.56it/s] 67%|██████▋   | 2118/3145 [14:14<06:47,  2.52it/s] 67%|██████▋   | 2110/3143 [14:20<07:14,  2.38it/s] 66%|██████▌   | 2075/3145 [14:14<07:02,  2.53it/s] 67%|██████▋   | 2095/3145 [14:14<06:50,  2.56it/s] 67%|██████▋   | 2119/3145 [14:14<06:37,  2.58it/s] 67%|██████▋   | 2111/3143 [14:20<07:23,  2.33it/s] 66%|██████▌   | 2076/3145 [14:14<07:06,  2.51it/s] 67%|██████▋   | 2096/3145 [14:14<06:50,  2.56it/s] 67%|██████▋   | 2120/3145 [14:14<06:02,  2.82it/s] 67%|██████▋   | 2112/3143 [14:21<07:24,  2.32it/s] 67%|██████▋   | 2097/3145 [14:15<06:51,  2.55it/s] 66%|██████▌   | 2077/3145 [14:15<07:22,  2.41it/s] 67%|██████▋   | 2121/3145 [14:15<06:16,  2.72it/s] 67%|██████▋   | 2122/3145 [14:15<05:51,  2.91it/s] 66%|██████▌   | 2078/3145 [14:15<07:18,  2.43it/s] 67%|██████▋   | 2098/3145 [14:15<07:15,  2.40it/s] 67%|██████▋   | 2113/3143 [14:21<07:42,  2.23it/s] 68%|██████▊   | 2123/3145 [14:16<06:10,  2.76it/s] 67%|██████▋   | 2099/3145 [14:16<07:10,  2.43it/s] 67%|██████▋   | 2114/3143 [14:22<07:27,  2.30it/s] 66%|██████▌   | 2079/3145 [14:16<07:45,  2.29it/s] 68%|██████▊   | 2124/3145 [14:16<06:19,  2.69it/s] 67%|██████▋   | 2100/3145 [14:16<07:09,  2.43it/s] 67%|██████▋   | 2115/3143 [14:22<07:30,  2.28it/s] 66%|██████▌   | 2080/3145 [14:16<08:02,  2.21it/s] 68%|██████▊   | 2125/3145 [14:16<06:29,  2.62it/s] 67%|██████▋   | 2101/3145 [14:17<07:06,  2.45it/s] 67%|██████▋   | 2116/3143 [14:22<07:09,  2.39it/s] 68%|██████▊   | 2126/3145 [14:17<05:49,  2.92it/s] 66%|██████▌   | 2081/3145 [14:17<07:47,  2.27it/s] 67%|██████▋   | 2117/3143 [14:23<06:14,  2.74it/s] 67%|██████▋   | 2102/3145 [14:17<07:12,  2.41it/s] 68%|██████▊   | 2127/3145 [14:17<06:03,  2.80it/s] 67%|██████▋   | 2118/3143 [14:23<05:36,  3.04it/s] 66%|██████▌   | 2082/3145 [14:17<07:42,  2.30it/s] 68%|██████▊   | 2128/3145 [14:17<06:22,  2.66it/s] 67%|██████▋   | 2103/3145 [14:17<07:34,  2.29it/s] 67%|██████▋   | 2119/3143 [14:23<06:05,  2.80it/s] 66%|██████▌   | 2083/3145 [14:17<07:18,  2.42it/s] 67%|██████▋   | 2120/3143 [14:24<06:13,  2.74it/s] 68%|██████▊   | 2129/3145 [14:18<06:45,  2.51it/s] 67%|██████▋   | 2104/3145 [14:18<07:43,  2.25it/s] 66%|██████▋   | 2084/3145 [14:18<07:37,  2.32it/s] 67%|██████▋   | 2121/3143 [14:24<05:31,  3.08it/s] 68%|██████▊   | 2130/3145 [14:18<06:41,  2.53it/s] 68%|██████▊   | 2122/3143 [14:24<05:00,  3.40it/s] 67%|██████▋   | 2105/3145 [14:18<07:45,  2.23it/s] 66%|██████▋   | 2085/3145 [14:18<07:41,  2.30it/s] 68%|██████▊   | 2123/3143 [14:24<04:50,  3.52it/s] 68%|██████▊   | 2131/3145 [14:19<06:44,  2.51it/s] 67%|██████▋   | 2106/3145 [14:19<07:53,  2.19it/s] 66%|██████▋   | 2086/3145 [14:19<08:09,  2.16it/s] 68%|██████▊   | 2124/3143 [14:25<05:52,  2.89it/s] 68%|██████▊   | 2132/3145 [14:19<06:47,  2.48it/s] 67%|██████▋   | 2107/3145 [14:19<07:39,  2.26it/s] 66%|██████▋   | 2087/3145 [14:19<08:02,  2.19it/s] 68%|██████▊   | 2133/3145 [14:19<06:32,  2.58it/s] 68%|██████▊   | 2125/3143 [14:25<06:41,  2.54it/s] 66%|██████▋   | 2088/3145 [14:20<07:55,  2.22it/s] 67%|██████▋   | 2108/3145 [14:20<08:18,  2.08it/s] 68%|██████▊   | 2134/3145 [14:20<07:02,  2.39it/s] 68%|██████▊   | 2126/3143 [14:26<06:56,  2.44it/s] 66%|██████▋   | 2089/3145 [14:20<07:52,  2.23it/s] 67%|██████▋   | 2109/3145 [14:20<07:56,  2.18it/s] 68%|██████▊   | 2135/3145 [14:20<06:49,  2.47it/s] 68%|██████▊   | 2127/3143 [14:26<07:11,  2.35it/s] 66%|██████▋   | 2090/3145 [14:21<07:41,  2.29it/s] 67%|██████▋   | 2110/3145 [14:21<08:01,  2.15it/s] 68%|██████▊   | 2136/3145 [14:21<06:55,  2.43it/s] 68%|██████▊   | 2128/3143 [14:27<07:02,  2.40it/s] 66%|██████▋   | 2091/3145 [14:21<07:31,  2.33it/s] 68%|██████▊   | 2129/3143 [14:27<06:05,  2.77it/s] 68%|██████▊   | 2137/3145 [14:21<06:58,  2.41it/s] 67%|██████▋   | 2111/3145 [14:21<08:07,  2.12it/s] 68%|██████▊   | 2130/3143 [14:27<05:33,  3.04it/s] 67%|██████▋   | 2092/3145 [14:21<07:20,  2.39it/s] 68%|██████▊   | 2138/3145 [14:22<06:48,  2.46it/s] 67%|██████▋   | 2112/3145 [14:22<08:10,  2.10it/s] 68%|██████▊   | 2131/3143 [14:28<06:12,  2.71it/s] 67%|██████▋   | 2093/3145 [14:22<07:28,  2.35it/s] 68%|██████▊   | 2139/3145 [14:22<06:45,  2.48it/s] 68%|██████▊   | 2132/3143 [14:28<05:34,  3.02it/s] 67%|██████▋   | 2113/3145 [14:22<07:40,  2.24it/s] 67%|██████▋   | 2094/3145 [14:22<07:04,  2.48it/s] 68%|██████▊   | 2140/3145 [14:22<06:31,  2.57it/s] 68%|██████▊   | 2133/3143 [14:28<05:48,  2.90it/s] 67%|██████▋   | 2114/3145 [14:23<07:51,  2.19it/s] 68%|██████▊   | 2141/3145 [14:23<06:24,  2.61it/s] 67%|██████▋   | 2095/3145 [14:23<07:16,  2.41it/s] 68%|██████▊   | 2134/3143 [14:29<05:57,  2.82it/s] 67%|██████▋   | 2115/3145 [14:23<07:32,  2.28it/s] 68%|██████▊   | 2142/3145 [14:23<06:32,  2.56it/s] 67%|██████▋   | 2096/3145 [14:23<07:26,  2.35it/s] 68%|██████▊   | 2135/3143 [14:29<06:02,  2.78it/s] 67%|██████▋   | 2116/3145 [14:23<07:45,  2.21it/s] 67%|██████▋   | 2097/3145 [14:24<07:21,  2.37it/s] 68%|██████▊   | 2143/3145 [14:24<06:51,  2.43it/s] 68%|██████▊   | 2136/3143 [14:29<06:22,  2.63it/s] 67%|██████▋   | 2117/3145 [14:24<07:51,  2.18it/s] 68%|██████▊   | 2144/3145 [14:24<06:49,  2.44it/s] 67%|██████▋   | 2098/3145 [14:24<07:32,  2.31it/s] 68%|██████▊   | 2137/3143 [14:30<06:29,  2.58it/s] 67%|██████▋   | 2118/3145 [14:24<06:53,  2.48it/s] 68%|██████▊   | 2138/3143 [14:30<06:21,  2.63it/s] 68%|██████▊   | 2145/3145 [14:24<07:00,  2.38it/s] 67%|██████▋   | 2099/3145 [14:24<07:26,  2.34it/s] 67%|██████▋   | 2119/3145 [14:25<06:50,  2.50it/s] 68%|██████▊   | 2139/3143 [14:31<06:16,  2.66it/s] 68%|██████▊   | 2146/3145 [14:25<06:42,  2.48it/s] 67%|██████▋   | 2100/3145 [14:25<07:07,  2.44it/s] 67%|██████▋   | 2120/3145 [14:25<06:43,  2.54it/s] 68%|██████▊   | 2140/3143 [14:31<06:13,  2.68it/s] 68%|██████▊   | 2147/3145 [14:25<06:41,  2.49it/s] 67%|██████▋   | 2101/3145 [14:25<07:04,  2.46it/s] 67%|██████▋   | 2121/3145 [14:25<06:44,  2.53it/s] 68%|██████▊   | 2141/3143 [14:31<06:12,  2.69it/s] 68%|██████▊   | 2148/3145 [14:26<06:27,  2.57it/s] 67%|██████▋   | 2102/3145 [14:26<06:51,  2.53it/s] 67%|██████▋   | 2122/3145 [14:26<06:35,  2.58it/s] 68%|██████▊   | 2149/3145 [14:26<06:00,  2.76it/s] 68%|██████▊   | 2142/3143 [14:32<06:24,  2.60it/s] 67%|██████▋   | 2103/3145 [14:26<06:50,  2.54it/s] 68%|██████▊   | 2123/3145 [14:26<06:38,  2.56it/s] 68%|██████▊   | 2150/3145 [14:26<06:00,  2.76it/s] 67%|██████▋   | 2104/3145 [14:26<06:51,  2.53it/s] 68%|██████▊   | 2143/3143 [14:32<06:49,  2.44it/s] 68%|██████▊   | 2151/3145 [14:26<05:28,  3.03it/s] 68%|██████▊   | 2124/3145 [14:27<06:34,  2.59it/s] 68%|██████▊   | 2144/3143 [14:33<07:00,  2.38it/s] 68%|██████▊   | 2152/3145 [14:27<05:43,  2.89it/s] 67%|██████▋   | 2105/3145 [14:27<07:16,  2.38it/s] 68%|██████▊   | 2125/3145 [14:27<06:31,  2.61it/s] 68%|██████▊   | 2145/3143 [14:33<06:49,  2.44it/s] 68%|██████▊   | 2153/3145 [14:27<05:52,  2.81it/s] 67%|██████▋   | 2106/3145 [14:27<07:25,  2.33it/s] 68%|██████▊   | 2126/3145 [14:27<06:46,  2.51it/s] 68%|██████▊   | 2146/3143 [14:33<06:49,  2.44it/s] 68%|██████▊   | 2154/3145 [14:28<06:19,  2.61it/s] 67%|██████▋   | 2107/3145 [14:28<07:22,  2.35it/s] 68%|██████▊   | 2127/3145 [14:28<06:58,  2.43it/s] 68%|██████▊   | 2147/3143 [14:34<06:05,  2.72it/s] 69%|██████▊   | 2155/3145 [14:28<06:45,  2.44it/s] 67%|██████▋   | 2108/3145 [14:28<07:28,  2.31it/s] 68%|██████▊   | 2128/3145 [14:28<07:11,  2.36it/s] 68%|██████▊   | 2148/3143 [14:34<06:25,  2.58it/s] 69%|██████▊   | 2156/3145 [14:29<06:46,  2.44it/s] 67%|██████▋   | 2109/3145 [14:29<07:19,  2.36it/s] 68%|██████▊   | 2129/3145 [14:29<06:57,  2.43it/s] 68%|██████▊   | 2149/3143 [14:35<06:11,  2.67it/s] 68%|██████▊   | 2150/3143 [14:35<05:34,  2.97it/s] 69%|██████▊   | 2157/3145 [14:29<06:45,  2.44it/s] 67%|██████▋   | 2110/3145 [14:29<07:12,  2.39it/s] 68%|██████▊   | 2130/3145 [14:29<07:00,  2.41it/s] 68%|██████▊   | 2151/3143 [14:35<05:52,  2.81it/s] 69%|██████▊   | 2158/3145 [14:29<06:51,  2.40it/s] 67%|██████▋   | 2111/3145 [14:29<07:27,  2.31it/s] 68%|██████▊   | 2131/3145 [14:29<07:01,  2.41it/s] 68%|██████▊   | 2152/3143 [14:35<05:14,  3.15it/s] 67%|██████▋   | 2112/3145 [14:30<07:04,  2.44it/s] 69%|██████▊   | 2159/3145 [14:30<06:52,  2.39it/s] 68%|██████▊   | 2132/3145 [14:30<06:58,  2.42it/s] 69%|██████▊   | 2153/3143 [14:36<05:49,  2.84it/s] 69%|██████▊   | 2160/3145 [14:30<06:36,  2.48it/s] 68%|██████▊   | 2133/3145 [14:30<06:45,  2.49it/s] 67%|██████▋   | 2113/3145 [14:30<07:21,  2.34it/s] 69%|██████▊   | 2154/3143 [14:36<05:53,  2.80it/s] 68%|██████▊   | 2134/3145 [14:31<06:29,  2.60it/s] 69%|██████▊   | 2161/3145 [14:31<06:34,  2.49it/s] 67%|██████▋   | 2114/3145 [14:31<07:32,  2.28it/s] 69%|██████▊   | 2155/3143 [14:37<06:15,  2.63it/s] 68%|██████▊   | 2135/3145 [14:31<05:56,  2.83it/s] 69%|██████▊   | 2162/3145 [14:31<06:39,  2.46it/s] 67%|██████▋   | 2115/3145 [14:31<07:11,  2.39it/s] 68%|██████▊   | 2136/3145 [14:31<06:11,  2.71it/s] 69%|██████▊   | 2156/3143 [14:37<06:45,  2.44it/s] 69%|██████▉   | 2163/3145 [14:31<06:32,  2.50it/s] 68%|██████▊   | 2137/3145 [14:31<05:33,  3.03it/s] 67%|██████▋   | 2116/3145 [14:32<07:19,  2.34it/s] 69%|██████▊   | 2157/3143 [14:38<06:39,  2.47it/s] 69%|██████▉   | 2164/3145 [14:32<06:34,  2.49it/s] 68%|██████▊   | 2138/3145 [14:32<05:59,  2.80it/s] 67%|██████▋   | 2117/3145 [14:32<07:24,  2.32it/s] 69%|██████▊   | 2158/3143 [14:38<06:37,  2.48it/s] 69%|██████▉   | 2165/3145 [14:32<06:30,  2.51it/s] 68%|██████▊   | 2139/3145 [14:32<06:16,  2.67it/s] 67%|██████▋   | 2118/3145 [14:32<07:31,  2.27it/s] 69%|██████▊   | 2159/3143 [14:38<06:43,  2.44it/s] 69%|██████▉   | 2166/3145 [14:33<06:30,  2.51it/s] 68%|██████▊   | 2140/3145 [14:33<05:38,  2.97it/s] 67%|██████▋   | 2119/3145 [14:33<07:16,  2.35it/s] 69%|██████▊   | 2160/3143 [14:39<06:41,  2.45it/s] 69%|██████▉   | 2167/3145 [14:33<06:25,  2.54it/s] 68%|██████▊   | 2141/3145 [14:33<05:57,  2.81it/s] 67%|██████▋   | 2120/3145 [14:33<07:06,  2.40it/s] 69%|██████▉   | 2161/3143 [14:39<06:38,  2.46it/s] 69%|██████▉   | 2168/3145 [14:33<06:20,  2.57it/s] 68%|██████▊   | 2142/3145 [14:33<06:09,  2.71it/s] 69%|██████▉   | 2162/3143 [14:39<06:22,  2.56it/s] 67%|██████▋   | 2121/3145 [14:34<07:00,  2.43it/s] 68%|██████▊   | 2143/3145 [14:34<06:05,  2.74it/s] 69%|██████▉   | 2169/3145 [14:34<06:32,  2.48it/s] 69%|██████▉   | 2163/3143 [14:40<06:38,  2.46it/s] 67%|██████▋   | 2122/3145 [14:34<07:13,  2.36it/s] 68%|██████▊   | 2144/3145 [14:34<06:15,  2.67it/s] 69%|██████▉   | 2170/3145 [14:34<06:37,  2.45it/s] 69%|██████▉   | 2164/3143 [14:40<06:36,  2.47it/s] 68%|██████▊   | 2145/3145 [14:34<06:09,  2.71it/s] 68%|██████▊   | 2123/3145 [14:34<07:10,  2.37it/s] 69%|██████▉   | 2171/3145 [14:35<06:44,  2.41it/s] 68%|██████▊   | 2146/3145 [14:35<06:10,  2.69it/s] 69%|██████▉   | 2165/3143 [14:41<06:46,  2.41it/s] 68%|██████▊   | 2124/3145 [14:35<07:20,  2.32it/s] 69%|██████▉   | 2172/3145 [14:35<06:32,  2.48it/s] 68%|██████▊   | 2125/3145 [14:35<06:28,  2.62it/s] 68%|██████▊   | 2147/3145 [14:35<06:19,  2.63it/s] 69%|██████▉   | 2166/3143 [14:41<06:51,  2.37it/s] 69%|██████▉   | 2173/3145 [14:35<06:28,  2.50it/s] 68%|██████▊   | 2148/3145 [14:36<06:23,  2.60it/s] 68%|██████▊   | 2126/3145 [14:36<06:49,  2.49it/s] 69%|██████▉   | 2167/3143 [14:42<06:34,  2.48it/s] 69%|██████▉   | 2174/3145 [14:36<06:25,  2.52it/s] 68%|██████▊   | 2149/3145 [14:36<06:25,  2.58it/s] 69%|██████▉   | 2168/3143 [14:42<06:26,  2.52it/s] 68%|██████▊   | 2127/3145 [14:36<07:04,  2.40it/s] 69%|██████▉   | 2175/3145 [14:36<07:16,  2.22it/s] 68%|██████▊   | 2150/3145 [14:36<06:30,  2.55it/s] 68%|██████▊   | 2128/3145 [14:36<06:51,  2.47it/s] 69%|██████▉   | 2169/3143 [14:42<06:33,  2.48it/s] 69%|██████▉   | 2176/3145 [14:37<07:02,  2.29it/s] 68%|██████▊   | 2151/3145 [14:37<06:45,  2.45it/s] 69%|██████▉   | 2170/3143 [14:43<06:29,  2.50it/s] 68%|██████▊   | 2129/3145 [14:37<06:52,  2.46it/s] 69%|██████▉   | 2177/3145 [14:37<06:33,  2.46it/s] 68%|██████▊   | 2152/3145 [14:37<06:24,  2.59it/s] 68%|██████▊   | 2130/3145 [14:37<06:58,  2.43it/s] 69%|██████▉   | 2171/3143 [14:43<06:54,  2.34it/s] 69%|██████▉   | 2178/3145 [14:38<06:40,  2.41it/s] 68%|██████▊   | 2153/3145 [14:38<06:24,  2.58it/s] 68%|██████▊   | 2131/3145 [14:38<07:08,  2.36it/s] 69%|██████▉   | 2172/3143 [14:44<06:48,  2.37it/s] 69%|██████▉   | 2179/3145 [14:38<06:21,  2.53it/s] 69%|██████▉   | 2173/3143 [14:44<05:53,  2.75it/s] 68%|██████▊   | 2154/3145 [14:38<06:44,  2.45it/s] 68%|██████▊   | 2132/3145 [14:38<06:50,  2.47it/s] 69%|██████▉   | 2174/3143 [14:44<06:04,  2.66it/s] 69%|██████▉   | 2180/3145 [14:38<07:14,  2.22it/s] 69%|██████▊   | 2155/3145 [14:38<06:41,  2.47it/s] 68%|██████▊   | 2133/3145 [14:38<06:34,  2.57it/s] 69%|██████▉   | 2175/3143 [14:45<06:01,  2.68it/s] 68%|██████▊   | 2134/3145 [14:39<06:15,  2.69it/s] 69%|██████▉   | 2181/3145 [14:39<07:02,  2.28it/s] 69%|██████▊   | 2156/3145 [14:39<06:55,  2.38it/s] 69%|██████▉   | 2176/3143 [14:45<06:15,  2.58it/s] 68%|██████▊   | 2135/3145 [14:39<06:36,  2.55it/s] 69%|██████▉   | 2182/3145 [14:39<06:54,  2.32it/s] 69%|██████▊   | 2157/3145 [14:39<06:58,  2.36it/s] 69%|██████▉   | 2177/3143 [14:45<06:19,  2.55it/s] 68%|██████▊   | 2136/3145 [14:40<06:38,  2.53it/s] 69%|██████▊   | 2158/3145 [14:40<06:38,  2.48it/s] 69%|██████▉   | 2183/3145 [14:40<07:19,  2.19it/s] 69%|██████▉   | 2178/3143 [14:46<06:32,  2.46it/s] 68%|██████▊   | 2137/3145 [14:40<06:42,  2.51it/s] 69%|██████▊   | 2159/3145 [14:40<06:46,  2.43it/s] 69%|██████▉   | 2184/3145 [14:40<07:38,  2.09it/s] 69%|██████▉   | 2179/3143 [14:46<06:25,  2.50it/s] 68%|██████▊   | 2138/3145 [14:40<06:42,  2.50it/s] 69%|██████▊   | 2160/3145 [14:41<06:40,  2.46it/s] 69%|██████▉   | 2185/3145 [14:41<07:39,  2.09it/s] 68%|██████▊   | 2139/3145 [14:41<06:37,  2.53it/s] 69%|██████▉   | 2180/3143 [14:47<06:38,  2.42it/s] 69%|██████▊   | 2161/3145 [14:41<06:36,  2.48it/s] 70%|██████▉   | 2186/3145 [14:41<06:34,  2.43it/s] 70%|██████▉   | 2187/3145 [14:41<05:39,  2.82it/s] 69%|██████▉   | 2181/3143 [14:47<06:34,  2.44it/s] 68%|██████▊   | 2140/3145 [14:41<06:50,  2.45it/s] 69%|██████▊   | 2162/3145 [14:41<06:31,  2.51it/s] 70%|██████▉   | 2188/3145 [14:42<05:13,  3.05it/s] 69%|██████▉   | 2163/3145 [14:42<06:18,  2.59it/s] 69%|██████▉   | 2182/3143 [14:48<06:30,  2.46it/s] 68%|██████▊   | 2141/3145 [14:42<07:09,  2.34it/s] 70%|██████▉   | 2189/3145 [14:42<05:24,  2.95it/s] 69%|██████▉   | 2164/3145 [14:42<06:33,  2.49it/s] 68%|██████▊   | 2142/3145 [14:42<07:06,  2.35it/s] 69%|██████▉   | 2183/3143 [14:48<07:18,  2.19it/s] 70%|██████▉   | 2190/3145 [14:42<05:43,  2.78it/s] 69%|██████▉   | 2184/3143 [14:48<06:18,  2.53it/s] 69%|██████▉   | 2165/3145 [14:43<06:50,  2.39it/s] 68%|██████▊   | 2143/3145 [14:43<07:10,  2.33it/s] 70%|██████▉   | 2191/3145 [14:43<05:52,  2.70it/s] 70%|██████▉   | 2185/3143 [14:49<06:20,  2.52it/s] 69%|██████▉   | 2166/3145 [14:43<06:55,  2.35it/s] 68%|██████▊   | 2144/3145 [14:43<07:02,  2.37it/s] 70%|██████▉   | 2192/3145 [14:43<06:09,  2.58it/s] 70%|██████▉   | 2186/3143 [14:49<06:28,  2.46it/s] 69%|██████▉   | 2167/3145 [14:43<06:43,  2.42it/s] 68%|██████▊   | 2145/3145 [14:43<07:07,  2.34it/s] 70%|██████▉   | 2193/3145 [14:44<06:16,  2.53it/s] 70%|██████▉   | 2187/3143 [14:50<06:28,  2.46it/s] 69%|██████▉   | 2168/3145 [14:44<06:35,  2.47it/s] 68%|██████▊   | 2146/3145 [14:44<07:11,  2.32it/s] 70%|██████▉   | 2194/3145 [14:44<06:27,  2.46it/s] 70%|██████▉   | 2188/3143 [14:50<06:26,  2.47it/s] 69%|██████▉   | 2169/3145 [14:44<06:33,  2.48it/s] 68%|██████▊   | 2147/3145 [14:44<07:23,  2.25it/s] 70%|██████▉   | 2195/3145 [14:44<06:34,  2.41it/s] 69%|██████▉   | 2170/3145 [14:45<06:25,  2.53it/s] 70%|██████▉   | 2189/3143 [14:51<06:45,  2.35it/s] 68%|██████▊   | 2148/3145 [14:45<07:24,  2.24it/s] 70%|██████▉   | 2196/3145 [14:45<06:39,  2.37it/s] 69%|██████▉   | 2171/3145 [14:45<06:35,  2.46it/s] 70%|██████▉   | 2190/3143 [14:51<06:29,  2.44it/s] 70%|██████▉   | 2197/3145 [14:45<06:30,  2.43it/s] 68%|██████▊   | 2149/3145 [14:45<07:32,  2.20it/s] 69%|██████▉   | 2172/3145 [14:45<06:36,  2.45it/s] 70%|██████▉   | 2191/3143 [14:51<07:10,  2.21it/s] 70%|██████▉   | 2198/3145 [14:46<06:24,  2.47it/s] 68%|██████▊   | 2150/3145 [14:46<07:38,  2.17it/s] 69%|██████▉   | 2173/3145 [14:46<06:34,  2.46it/s] 70%|██████▉   | 2192/3143 [14:52<06:48,  2.33it/s] 70%|██████▉   | 2199/3145 [14:46<06:19,  2.49it/s] 69%|██████▉   | 2174/3145 [14:46<05:46,  2.80it/s] 70%|██████▉   | 2193/3143 [14:52<06:39,  2.38it/s] 70%|██████▉   | 2200/3145 [14:46<06:15,  2.52it/s] 68%|██████▊   | 2151/3145 [14:46<08:30,  1.95it/s] 69%|██████▉   | 2175/3145 [14:46<06:10,  2.62it/s] 68%|██████▊   | 2152/3145 [14:47<07:11,  2.30it/s] 70%|██████▉   | 2194/3143 [14:53<06:30,  2.43it/s] 69%|██████▉   | 2176/3145 [14:47<06:24,  2.52it/s] 70%|██████▉   | 2201/3145 [14:47<07:17,  2.16it/s] 68%|██████▊   | 2153/3145 [14:47<07:00,  2.36it/s] 70%|██████▉   | 2195/3143 [14:53<06:43,  2.35it/s] 69%|██████▉   | 2177/3145 [14:47<06:18,  2.56it/s] 70%|███████   | 2202/3145 [14:47<07:12,  2.18it/s] 68%|██████▊   | 2154/3145 [14:47<06:58,  2.37it/s] 70%|██████▉   | 2196/3143 [14:53<06:36,  2.39it/s] 69%|██████▉   | 2178/3145 [14:48<07:02,  2.29it/s] 69%|██████▊   | 2155/3145 [14:48<06:47,  2.43it/s] 70%|███████   | 2203/3145 [14:48<06:56,  2.26it/s] 70%|██████▉   | 2197/3143 [14:54<06:30,  2.42it/s] 69%|██████▊   | 2156/3145 [14:48<06:19,  2.61it/s] 69%|██████▉   | 2179/3145 [14:48<07:01,  2.29it/s] 70%|███████   | 2204/3145 [14:48<06:47,  2.31it/s] 70%|██████▉   | 2198/3143 [14:54<06:32,  2.41it/s] 69%|██████▊   | 2157/3145 [14:49<06:22,  2.58it/s] 69%|██████▉   | 2180/3145 [14:49<06:46,  2.38it/s] 70%|███████   | 2205/3145 [14:49<06:42,  2.34it/s] 70%|██████▉   | 2199/3143 [14:55<06:15,  2.51it/s] 69%|██████▊   | 2158/3145 [14:49<06:39,  2.47it/s] 69%|██████▉   | 2181/3145 [14:49<06:45,  2.38it/s] 70%|███████   | 2206/3145 [14:49<06:42,  2.33it/s] 70%|██████▉   | 2200/3143 [14:55<06:21,  2.47it/s] 69%|██████▊   | 2159/3145 [14:49<06:28,  2.54it/s] 70%|███████   | 2207/3145 [14:50<06:38,  2.35it/s] 69%|██████▉   | 2182/3145 [14:50<07:01,  2.28it/s] 70%|███████   | 2201/3143 [14:56<06:36,  2.38it/s] 69%|██████▊   | 2160/3145 [14:50<06:30,  2.53it/s] 70%|███████   | 2208/3145 [14:50<06:32,  2.39it/s] 69%|██████▉   | 2183/3145 [14:50<07:29,  2.14it/s] 70%|███████   | 2202/3143 [14:56<06:53,  2.28it/s] 69%|██████▊   | 2161/3145 [14:50<06:43,  2.44it/s] 70%|███████   | 2209/3145 [14:50<06:26,  2.42it/s] 69%|██████▉   | 2184/3145 [14:51<07:17,  2.20it/s] 70%|███████   | 2203/3143 [14:56<06:59,  2.24it/s] 69%|██████▊   | 2162/3145 [14:51<06:39,  2.46it/s] 70%|███████   | 2210/3145 [14:51<06:25,  2.43it/s] 69%|██████▉   | 2185/3145 [14:51<07:10,  2.23it/s] 70%|███████   | 2204/3143 [14:57<06:44,  2.32it/s] 69%|██████▉   | 2163/3145 [14:51<06:50,  2.39it/s] 70%|███████   | 2211/3145 [14:51<06:29,  2.40it/s] 70%|███████   | 2205/3143 [14:57<05:51,  2.67it/s] 70%|██████▉   | 2186/3145 [14:51<06:51,  2.33it/s] 70%|███████   | 2206/3143 [14:57<05:15,  2.97it/s] 69%|██████▉   | 2164/3145 [14:51<06:46,  2.41it/s] 70%|███████   | 2212/3145 [14:52<06:20,  2.45it/s] 70%|███████   | 2207/3143 [14:58<05:07,  3.05it/s] 70%|██████▉   | 2187/3145 [14:52<06:52,  2.32it/s] 69%|██████▉   | 2165/3145 [14:52<06:44,  2.42it/s] 70%|███████   | 2213/3145 [14:52<06:13,  2.49it/s] 70%|██████▉   | 2188/3145 [14:52<06:28,  2.46it/s] 70%|███████   | 2208/3143 [14:58<05:27,  2.85it/s] 70%|███████   | 2214/3145 [14:52<06:03,  2.56it/s] 69%|██████▉   | 2166/3145 [14:52<06:49,  2.39it/s] 70%|██████▉   | 2189/3145 [14:53<06:35,  2.42it/s] 70%|███████   | 2209/3143 [14:58<05:51,  2.66it/s] 70%|███████   | 2215/3145 [14:53<05:54,  2.62it/s] 69%|██████▉   | 2167/3145 [14:53<06:31,  2.50it/s] 70%|██████▉   | 2190/3145 [14:53<06:28,  2.46it/s] 70%|███████   | 2210/3143 [14:59<05:58,  2.60it/s] 69%|██████▉   | 2168/3145 [14:53<06:25,  2.54it/s] 70%|███████   | 2216/3145 [14:53<06:00,  2.58it/s] 70%|██████▉   | 2191/3145 [14:53<06:10,  2.58it/s] 70%|███████   | 2211/3143 [14:59<06:10,  2.52it/s] 70%|███████   | 2217/3145 [14:53<06:02,  2.56it/s] 69%|██████▉   | 2169/3145 [14:53<06:38,  2.45it/s] 70%|██████▉   | 2192/3145 [14:54<06:08,  2.58it/s] 70%|███████   | 2212/3143 [15:00<06:01,  2.57it/s] 71%|███████   | 2218/3145 [14:54<06:03,  2.55it/s] 69%|██████▉   | 2170/3145 [14:54<06:47,  2.39it/s] 70%|██████▉   | 2193/3145 [14:54<06:00,  2.64it/s] 70%|███████   | 2213/3143 [15:00<06:07,  2.53it/s] 71%|███████   | 2219/3145 [14:54<06:24,  2.41it/s] 69%|██████▉   | 2171/3145 [14:54<06:43,  2.41it/s] 70%|██████▉   | 2194/3145 [14:54<06:08,  2.58it/s] 70%|███████   | 2214/3143 [15:00<06:03,  2.56it/s] 69%|██████▉   | 2172/3145 [14:55<06:43,  2.41it/s] 71%|███████   | 2220/3145 [14:55<06:31,  2.36it/s] 70%|██████▉   | 2195/3145 [14:55<06:19,  2.50it/s] 70%|███████   | 2215/3143 [15:01<05:59,  2.58it/s] 69%|██████▉   | 2173/3145 [14:55<06:08,  2.64it/s] 71%|███████   | 2221/3145 [14:55<06:39,  2.31it/s] 70%|██████▉   | 2196/3145 [14:55<06:35,  2.40it/s] 71%|███████   | 2216/3143 [15:01<06:00,  2.57it/s] 69%|██████▉   | 2174/3145 [14:55<06:14,  2.59it/s] 71%|███████   | 2222/3145 [14:56<06:33,  2.35it/s] 70%|██████▉   | 2197/3145 [14:56<06:20,  2.49it/s] 71%|███████   | 2217/3143 [15:02<06:13,  2.48it/s] 69%|██████▉   | 2175/3145 [14:56<06:21,  2.54it/s] 71%|███████   | 2223/3145 [14:56<06:23,  2.41it/s] 70%|██████▉   | 2198/3145 [14:56<06:19,  2.50it/s] 71%|███████   | 2218/3143 [15:02<06:23,  2.41it/s] 69%|██████▉   | 2176/3145 [14:56<06:27,  2.50it/s] 71%|███████   | 2224/3145 [14:56<06:14,  2.46it/s] 70%|██████▉   | 2199/3145 [14:57<06:47,  2.32it/s] 71%|███████   | 2225/3145 [14:57<06:05,  2.52it/s] 71%|███████   | 2219/3143 [15:03<07:23,  2.08it/s] 69%|██████▉   | 2177/3145 [14:57<07:19,  2.20it/s] 70%|██████▉   | 2200/3145 [14:57<06:39,  2.37it/s] 71%|███████   | 2226/3145 [14:57<06:02,  2.53it/s] 69%|██████▉   | 2178/3145 [14:57<07:07,  2.26it/s] 71%|███████   | 2220/3143 [15:03<07:50,  1.96it/s] 70%|██████▉   | 2201/3145 [14:57<06:53,  2.28it/s] 71%|███████   | 2227/3145 [14:58<06:01,  2.54it/s] 69%|██████▉   | 2179/3145 [14:58<07:11,  2.24it/s] 71%|███████   | 2221/3143 [15:04<07:19,  2.10it/s] 70%|███████   | 2202/3145 [14:58<06:41,  2.35it/s] 71%|███████   | 2228/3145 [14:58<05:53,  2.59it/s] 69%|██████▉   | 2180/3145 [14:58<07:08,  2.25it/s] 71%|███████   | 2222/3143 [15:04<06:42,  2.29it/s] 70%|███████   | 2203/3145 [14:58<06:36,  2.37it/s] 71%|███████   | 2229/3145 [14:58<06:02,  2.53it/s] 69%|██████▉   | 2181/3145 [14:59<06:51,  2.34it/s] 71%|███████   | 2223/3143 [15:05<06:44,  2.28it/s] 70%|███████   | 2204/3145 [14:59<06:42,  2.34it/s] 71%|███████   | 2230/3145 [14:59<05:52,  2.60it/s] 69%|██████▉   | 2182/3145 [14:59<06:14,  2.57it/s] 71%|███████   | 2224/3143 [15:05<06:31,  2.35it/s] 70%|███████   | 2205/3145 [14:59<06:33,  2.39it/s] 71%|███████   | 2231/3145 [14:59<05:56,  2.56it/s] 69%|██████▉   | 2183/3145 [14:59<06:08,  2.61it/s] 71%|███████   | 2232/3145 [14:59<05:25,  2.80it/s] 71%|███████   | 2225/3143 [15:05<06:22,  2.40it/s] 70%|███████   | 2206/3145 [15:00<06:39,  2.35it/s] 69%|██████▉   | 2184/3145 [15:00<07:11,  2.23it/s] 71%|███████   | 2233/3145 [15:00<05:47,  2.62it/s] 71%|███████   | 2226/3143 [15:06<06:29,  2.36it/s] 70%|███████   | 2207/3145 [15:00<06:20,  2.47it/s] 69%|██████▉   | 2185/3145 [15:00<06:54,  2.32it/s] 70%|███████   | 2208/3145 [15:00<06:06,  2.56it/s] 71%|███████   | 2234/3145 [15:00<05:56,  2.56it/s] 71%|███████   | 2227/3143 [15:06<06:23,  2.39it/s] 71%|███████   | 2228/3143 [15:06<05:34,  2.74it/s] 70%|███████   | 2209/3145 [15:01<06:16,  2.49it/s] 71%|███████   | 2235/3145 [15:01<06:10,  2.46it/s] 70%|██████▉   | 2186/3145 [15:01<07:44,  2.07it/s] 71%|███████   | 2229/3143 [15:07<05:36,  2.72it/s] 71%|███████   | 2236/3145 [15:01<06:11,  2.45it/s] 70%|███████   | 2210/3145 [15:01<06:44,  2.31it/s] 71%|███████   | 2230/3143 [15:07<05:41,  2.68it/s] 70%|██████▉   | 2187/3145 [15:01<08:18,  1.92it/s] 71%|███████   | 2237/3145 [15:02<06:13,  2.43it/s] 70%|███████   | 2211/3145 [15:02<06:31,  2.39it/s] 71%|███████   | 2231/3143 [15:08<05:33,  2.73it/s] 70%|███████   | 2212/3145 [15:02<06:17,  2.47it/s] 70%|██████▉   | 2188/3145 [15:02<08:16,  1.93it/s] 71%|███████   | 2232/3143 [15:08<05:32,  2.74it/s] 71%|███████   | 2238/3145 [15:02<07:00,  2.16it/s] 70%|██████▉   | 2189/3145 [15:02<07:24,  2.15it/s] 71%|███████   | 2233/3143 [15:08<05:40,  2.67it/s] 70%|███████   | 2213/3145 [15:02<06:36,  2.35it/s] 71%|███████   | 2239/3145 [15:02<06:32,  2.31it/s] 70%|██████▉   | 2190/3145 [15:03<06:29,  2.45it/s] 71%|███████   | 2240/3145 [15:03<05:32,  2.72it/s] 71%|███████   | 2234/3143 [15:09<05:56,  2.55it/s] 70%|███████   | 2214/3145 [15:03<06:39,  2.33it/s] 71%|███████▏  | 2241/3145 [15:03<05:34,  2.70it/s] 70%|██████▉   | 2191/3145 [15:03<07:25,  2.14it/s] 70%|███████   | 2215/3145 [15:03<06:22,  2.43it/s] 71%|███████   | 2235/3143 [15:09<06:10,  2.45it/s] 71%|███████▏  | 2242/3145 [15:03<05:37,  2.68it/s] 70%|███████   | 2216/3145 [15:04<06:16,  2.47it/s] 71%|███████   | 2236/3143 [15:10<06:16,  2.41it/s] 70%|██████▉   | 2192/3145 [15:04<07:53,  2.01it/s] 71%|███████▏  | 2243/3145 [15:04<05:33,  2.70it/s] 70%|███████   | 2217/3145 [15:04<06:22,  2.42it/s] 71%|███████   | 2237/3143 [15:10<06:07,  2.47it/s] 71%|███████▏  | 2244/3145 [15:04<05:32,  2.71it/s] 70%|██████▉   | 2193/3145 [15:04<08:02,  1.97it/s] 71%|███████   | 2218/3145 [15:04<06:14,  2.48it/s] 71%|███████   | 2238/3143 [15:10<06:03,  2.49it/s] 70%|██████▉   | 2194/3145 [15:05<07:24,  2.14it/s] 71%|███████   | 2219/3145 [15:05<05:36,  2.76it/s] 71%|███████▏  | 2245/3145 [15:05<06:35,  2.28it/s] 71%|███████   | 2239/3143 [15:11<05:58,  2.52it/s] 71%|███████   | 2220/3145 [15:05<04:56,  3.12it/s] 70%|██████▉   | 2195/3145 [15:05<07:44,  2.05it/s] 71%|███████▏  | 2246/3145 [15:05<06:35,  2.27it/s] 71%|███████▏  | 2240/3143 [15:11<06:04,  2.47it/s] 71%|███████   | 2221/3145 [15:05<05:14,  2.93it/s] 70%|██████▉   | 2196/3145 [15:06<07:29,  2.11it/s] 71%|███████▏  | 2247/3145 [15:06<06:24,  2.33it/s] 71%|███████   | 2222/3145 [15:06<05:29,  2.81it/s] 71%|███████▏  | 2241/3143 [15:12<06:12,  2.42it/s] 71%|███████▏  | 2248/3145 [15:06<06:27,  2.31it/s] 70%|██████▉   | 2197/3145 [15:06<07:21,  2.15it/s] 71%|███████▏  | 2242/3143 [15:12<06:15,  2.40it/s] 71%|███████   | 2223/3145 [15:06<05:59,  2.57it/s] 71%|███████▏  | 2243/3143 [15:12<05:32,  2.71it/s] 72%|███████▏  | 2249/3145 [15:06<06:07,  2.44it/s] 70%|██████▉   | 2198/3145 [15:06<07:01,  2.25it/s] 71%|███████   | 2224/3145 [15:07<06:13,  2.47it/s] 71%|███████▏  | 2244/3143 [15:13<05:49,  2.58it/s] 70%|██████▉   | 2199/3145 [15:07<06:40,  2.36it/s] 72%|███████▏  | 2250/3145 [15:07<06:14,  2.39it/s] 71%|███████   | 2225/3145 [15:07<06:19,  2.42it/s] 72%|███████▏  | 2251/3145 [15:07<05:54,  2.52it/s] 71%|███████▏  | 2245/3143 [15:13<06:07,  2.44it/s] 70%|██████▉   | 2200/3145 [15:07<07:24,  2.13it/s] 71%|███████   | 2226/3145 [15:08<06:28,  2.37it/s] 71%|███████▏  | 2246/3143 [15:13<05:27,  2.74it/s] 71%|███████▏  | 2247/3143 [15:14<04:50,  3.08it/s] 72%|███████▏  | 2252/3145 [15:08<06:49,  2.18it/s] 70%|██████▉   | 2201/3145 [15:08<07:04,  2.22it/s] 71%|███████   | 2227/3145 [15:08<06:18,  2.43it/s] 72%|███████▏  | 2248/3143 [15:14<05:07,  2.91it/s] 72%|███████▏  | 2253/3145 [15:08<06:28,  2.29it/s] 70%|███████   | 2202/3145 [15:08<06:59,  2.25it/s] 71%|███████   | 2228/3145 [15:08<06:23,  2.39it/s] 72%|███████▏  | 2249/3143 [15:14<05:08,  2.89it/s] 72%|███████▏  | 2254/3145 [15:09<06:23,  2.32it/s] 70%|███████   | 2203/3145 [15:09<06:59,  2.25it/s] 71%|███████   | 2229/3145 [15:09<06:17,  2.43it/s] 72%|███████▏  | 2250/3143 [15:15<05:24,  2.75it/s] 72%|███████▏  | 2255/3145 [15:09<06:08,  2.42it/s] 71%|███████   | 2230/3145 [15:09<06:10,  2.47it/s] 72%|███████▏  | 2251/3143 [15:15<04:49,  3.08it/s] 70%|███████   | 2204/3145 [15:09<07:41,  2.04it/s] 72%|███████▏  | 2256/3145 [15:09<06:07,  2.42it/s] 72%|███████▏  | 2252/3143 [15:15<05:08,  2.89it/s] 71%|███████   | 2231/3145 [15:10<06:55,  2.20it/s] 70%|███████   | 2205/3145 [15:10<07:34,  2.07it/s] 72%|███████▏  | 2257/3145 [15:10<06:15,  2.37it/s] 72%|███████▏  | 2253/3143 [15:16<05:22,  2.76it/s] 72%|███████▏  | 2258/3145 [15:10<05:22,  2.75it/s] 71%|███████   | 2232/3145 [15:10<06:51,  2.22it/s] 70%|███████   | 2206/3145 [15:10<07:21,  2.13it/s] 72%|███████▏  | 2254/3143 [15:16<05:31,  2.68it/s] 72%|███████▏  | 2259/3145 [15:10<05:21,  2.75it/s] 71%|███████   | 2233/3145 [15:11<06:40,  2.28it/s] 70%|███████   | 2207/3145 [15:11<06:46,  2.31it/s] 72%|███████▏  | 2255/3143 [15:17<05:38,  2.62it/s] 72%|███████▏  | 2260/3145 [15:11<05:32,  2.66it/s] 71%|███████   | 2234/3145 [15:11<06:24,  2.37it/s] 70%|███████   | 2208/3145 [15:11<06:39,  2.34it/s] 72%|███████▏  | 2256/3143 [15:17<05:25,  2.73it/s] 72%|███████▏  | 2261/3145 [15:11<05:03,  2.91it/s] 70%|███████   | 2209/3145 [15:11<05:42,  2.73it/s] 71%|███████   | 2235/3145 [15:11<06:27,  2.35it/s] 72%|███████▏  | 2257/3143 [15:17<05:32,  2.66it/s] 72%|███████▏  | 2262/3145 [15:12<05:17,  2.78it/s] 70%|███████   | 2210/3145 [15:12<06:03,  2.58it/s] 71%|███████   | 2236/3145 [15:12<06:00,  2.52it/s] 72%|███████▏  | 2258/3143 [15:18<05:38,  2.62it/s] 72%|███████▏  | 2263/3145 [15:12<05:22,  2.73it/s] 70%|███████   | 2211/3145 [15:12<06:00,  2.59it/s] 71%|███████   | 2237/3145 [15:12<06:11,  2.44it/s] 72%|███████▏  | 2264/3145 [15:12<05:26,  2.70it/s] 72%|███████▏  | 2259/3143 [15:18<05:48,  2.54it/s] 70%|███████   | 2212/3145 [15:12<06:15,  2.49it/s] 71%|███████   | 2238/3145 [15:13<06:17,  2.40it/s] 72%|███████▏  | 2260/3143 [15:19<05:57,  2.47it/s] 72%|███████▏  | 2265/3145 [15:13<06:02,  2.43it/s] 70%|███████   | 2213/3145 [15:13<06:09,  2.52it/s] 71%|███████   | 2239/3145 [15:13<06:20,  2.38it/s] 72%|███████▏  | 2266/3145 [15:13<05:48,  2.52it/s] 71%|███████   | 2240/3145 [15:13<05:36,  2.69it/s] 70%|███████   | 2214/3145 [15:13<06:11,  2.50it/s] 72%|███████▏  | 2261/3143 [15:19<06:42,  2.19it/s] 72%|███████▏  | 2262/3143 [15:19<05:42,  2.57it/s] 72%|███████▏  | 2267/3145 [15:14<05:49,  2.51it/s] 71%|███████▏  | 2241/3145 [15:14<05:41,  2.65it/s] 70%|███████   | 2215/3145 [15:14<06:22,  2.43it/s] 72%|███████▏  | 2268/3145 [15:14<05:45,  2.54it/s] 72%|███████▏  | 2263/3143 [15:20<06:04,  2.42it/s] 71%|███████▏  | 2242/3145 [15:14<05:55,  2.54it/s] 70%|███████   | 2216/3145 [15:14<06:17,  2.46it/s] 72%|███████▏  | 2269/3145 [15:14<05:36,  2.60it/s] 71%|███████▏  | 2243/3145 [15:14<05:47,  2.60it/s] 72%|███████▏  | 2264/3143 [15:20<06:06,  2.40it/s] 70%|███████   | 2217/3145 [15:15<06:28,  2.39it/s] 72%|███████▏  | 2270/3145 [15:15<05:39,  2.58it/s] 71%|███████▏  | 2244/3145 [15:15<05:49,  2.58it/s] 72%|███████▏  | 2265/3143 [15:21<06:12,  2.35it/s] 71%|███████   | 2218/3145 [15:15<06:14,  2.48it/s] 72%|███████▏  | 2271/3145 [15:15<05:41,  2.56it/s] 71%|███████▏  | 2245/3145 [15:15<05:40,  2.64it/s] 71%|███████   | 2219/3145 [15:15<06:12,  2.49it/s] 72%|███████▏  | 2266/3143 [15:21<06:22,  2.29it/s] 72%|███████▏  | 2272/3145 [15:16<05:51,  2.48it/s] 71%|███████▏  | 2246/3145 [15:16<05:54,  2.54it/s] 71%|███████   | 2220/3145 [15:16<06:05,  2.53it/s] 72%|███████▏  | 2267/3143 [15:22<06:24,  2.28it/s] 72%|███████▏  | 2273/3145 [15:16<05:59,  2.43it/s] 71%|███████▏  | 2247/3145 [15:16<05:58,  2.50it/s] 71%|███████   | 2221/3145 [15:16<06:05,  2.52it/s] 72%|███████▏  | 2268/3143 [15:22<06:17,  2.32it/s] 71%|███████▏  | 2248/3145 [15:16<05:47,  2.58it/s] 72%|███████▏  | 2274/3145 [15:16<06:09,  2.35it/s] 71%|███████   | 2222/3145 [15:16<06:10,  2.49it/s] 72%|███████▏  | 2249/3145 [15:17<05:09,  2.90it/s] 72%|███████▏  | 2269/3143 [15:23<06:25,  2.27it/s] 72%|███████▏  | 2275/3145 [15:17<06:13,  2.33it/s] 71%|███████   | 2223/3145 [15:17<06:12,  2.47it/s] 72%|███████▏  | 2250/3145 [15:17<05:22,  2.78it/s] 72%|███████▏  | 2270/3143 [15:23<06:24,  2.27it/s] 72%|███████▏  | 2276/3145 [15:17<06:13,  2.33it/s] 71%|███████   | 2224/3145 [15:17<06:15,  2.45it/s] 72%|███████▏  | 2251/3145 [15:17<05:37,  2.65it/s] 72%|███████▏  | 2277/3145 [15:18<05:20,  2.71it/s] 72%|███████▏  | 2271/3143 [15:23<06:29,  2.24it/s] 71%|███████   | 2225/3145 [15:18<06:19,  2.42it/s] 72%|███████▏  | 2252/3145 [15:18<05:37,  2.65it/s] 72%|███████▏  | 2278/3145 [15:18<05:23,  2.68it/s] 72%|███████▏  | 2272/3143 [15:24<06:29,  2.23it/s] 71%|███████   | 2226/3145 [15:18<06:27,  2.37it/s] 72%|███████▏  | 2253/3145 [15:18<05:41,  2.61it/s] 72%|███████▏  | 2279/3145 [15:18<05:29,  2.63it/s] 71%|███████   | 2227/3145 [15:18<05:32,  2.76it/s] 72%|███████▏  | 2273/3143 [15:24<06:19,  2.29it/s] 72%|███████▏  | 2254/3145 [15:19<05:38,  2.63it/s] 72%|███████▏  | 2280/3145 [15:19<05:22,  2.68it/s] 71%|███████   | 2228/3145 [15:19<05:52,  2.60it/s] 72%|███████▏  | 2274/3143 [15:25<06:19,  2.29it/s] 72%|███████▏  | 2255/3145 [15:19<05:52,  2.53it/s] 73%|███████▎  | 2281/3145 [15:19<05:35,  2.58it/s] 71%|███████   | 2229/3145 [15:19<05:48,  2.63it/s] 72%|███████▏  | 2275/3143 [15:25<06:46,  2.13it/s] 73%|███████▎  | 2282/3145 [15:19<05:27,  2.64it/s] 72%|███████▏  | 2256/3145 [15:19<06:07,  2.42it/s] 71%|███████   | 2230/3145 [15:20<06:01,  2.53it/s] 72%|███████▏  | 2276/3143 [15:26<06:18,  2.29it/s] 73%|███████▎  | 2283/3145 [15:20<05:32,  2.59it/s] 72%|███████▏  | 2257/3145 [15:20<05:56,  2.49it/s] 71%|███████   | 2231/3145 [15:20<06:10,  2.47it/s] 72%|███████▏  | 2277/3143 [15:26<05:48,  2.49it/s] 73%|███████▎  | 2284/3145 [15:20<05:31,  2.60it/s] 72%|███████▏  | 2258/3145 [15:20<06:40,  2.22it/s] 71%|███████   | 2232/3145 [15:20<06:00,  2.53it/s] 72%|███████▏  | 2278/3143 [15:26<05:50,  2.47it/s] 73%|███████▎  | 2285/3145 [15:21<05:33,  2.58it/s] 72%|███████▏  | 2259/3145 [15:21<05:53,  2.50it/s] 71%|███████   | 2233/3145 [15:21<06:01,  2.52it/s] 73%|███████▎  | 2279/3143 [15:27<05:52,  2.45it/s] 73%|███████▎  | 2286/3145 [15:21<05:32,  2.58it/s] 72%|███████▏  | 2260/3145 [15:21<05:48,  2.54it/s] 71%|███████   | 2234/3145 [15:21<05:53,  2.58it/s] 73%|███████▎  | 2287/3145 [15:21<05:25,  2.63it/s] 73%|███████▎  | 2280/3143 [15:27<06:21,  2.26it/s] 72%|███████▏  | 2261/3145 [15:22<06:02,  2.44it/s] 71%|███████   | 2235/3145 [15:22<05:58,  2.54it/s] 73%|███████▎  | 2288/3145 [15:22<05:44,  2.49it/s] 73%|███████▎  | 2281/3143 [15:28<06:10,  2.33it/s] 72%|███████▏  | 2262/3145 [15:22<05:55,  2.48it/s] 71%|███████   | 2236/3145 [15:22<06:09,  2.46it/s] 73%|███████▎  | 2289/3145 [15:22<05:41,  2.51it/s] 73%|███████▎  | 2282/3143 [15:28<06:07,  2.34it/s] 72%|███████▏  | 2263/3145 [15:22<05:50,  2.51it/s] 71%|███████   | 2237/3145 [15:22<06:15,  2.42it/s] 73%|███████▎  | 2290/3145 [15:23<05:46,  2.47it/s] 73%|███████▎  | 2283/3143 [15:29<05:53,  2.43it/s] 72%|███████▏  | 2264/3145 [15:23<05:47,  2.54it/s] 71%|███████   | 2238/3145 [15:23<06:02,  2.50it/s] 73%|███████▎  | 2291/3145 [15:23<05:48,  2.45it/s] 72%|███████▏  | 2265/3145 [15:23<05:44,  2.55it/s] 73%|███████▎  | 2284/3143 [15:29<06:15,  2.28it/s] 71%|███████   | 2239/3145 [15:23<06:04,  2.48it/s] 73%|███████▎  | 2292/3145 [15:23<05:50,  2.44it/s] 72%|███████▏  | 2266/3145 [15:24<05:55,  2.47it/s] 73%|███████▎  | 2285/3143 [15:29<06:06,  2.34it/s] 71%|███████   | 2240/3145 [15:24<06:04,  2.48it/s] 73%|███████▎  | 2293/3145 [15:24<05:46,  2.46it/s] 72%|███████▏  | 2267/3145 [15:24<05:50,  2.51it/s] 73%|███████▎  | 2286/3143 [15:30<05:57,  2.39it/s] 71%|███████▏  | 2241/3145 [15:24<05:54,  2.55it/s] 73%|███████▎  | 2294/3145 [15:24<05:35,  2.54it/s] 72%|███████▏  | 2268/3145 [15:24<05:59,  2.44it/s] 73%|███████▎  | 2287/3143 [15:30<06:09,  2.32it/s] 71%|███████▏  | 2242/3145 [15:24<05:54,  2.55it/s] 73%|███████▎  | 2295/3145 [15:25<05:35,  2.53it/s] 72%|███████▏  | 2269/3145 [15:25<05:55,  2.46it/s] 73%|███████▎  | 2288/3143 [15:31<05:51,  2.43it/s] 71%|███████▏  | 2243/3145 [15:25<05:57,  2.52it/s] 73%|███████▎  | 2296/3145 [15:25<05:46,  2.45it/s] 71%|███████▏  | 2244/3145 [15:25<05:54,  2.54it/s] 73%|███████▎  | 2289/3143 [15:31<06:14,  2.28it/s] 72%|███████▏  | 2270/3145 [15:25<06:51,  2.13it/s] 73%|███████▎  | 2297/3145 [15:25<05:52,  2.40it/s] 71%|███████▏  | 2245/3145 [15:26<05:58,  2.51it/s] 73%|███████▎  | 2290/3143 [15:32<06:09,  2.31it/s] 72%|███████▏  | 2271/3145 [15:26<06:35,  2.21it/s] 73%|███████▎  | 2298/3145 [15:26<05:51,  2.41it/s] 71%|███████▏  | 2246/3145 [15:26<05:57,  2.51it/s] 73%|███████▎  | 2291/3143 [15:32<06:05,  2.33it/s] 72%|███████▏  | 2272/3145 [15:26<06:29,  2.24it/s] 73%|███████▎  | 2299/3145 [15:26<05:44,  2.46it/s] 71%|███████▏  | 2247/3145 [15:26<05:54,  2.54it/s] 73%|███████▎  | 2292/3143 [15:32<06:14,  2.27it/s] 72%|███████▏  | 2273/3145 [15:27<06:25,  2.26it/s] 73%|███████▎  | 2300/3145 [15:27<05:49,  2.42it/s] 71%|███████▏  | 2248/3145 [15:27<06:07,  2.44it/s] 73%|███████▎  | 2293/3143 [15:33<06:04,  2.33it/s] 72%|███████▏  | 2274/3145 [15:27<06:11,  2.34it/s] 73%|███████▎  | 2301/3145 [15:27<05:40,  2.48it/s] 72%|███████▏  | 2249/3145 [15:27<06:21,  2.35it/s] 73%|███████▎  | 2294/3143 [15:33<05:51,  2.42it/s] 72%|███████▏  | 2275/3145 [15:27<06:13,  2.33it/s] 73%|███████▎  | 2302/3145 [15:27<05:38,  2.49it/s] 73%|███████▎  | 2295/3143 [15:34<05:46,  2.45it/s] 72%|███████▏  | 2250/3145 [15:28<06:32,  2.28it/s] 72%|███████▏  | 2276/3145 [15:28<05:59,  2.42it/s] 73%|███████▎  | 2303/3145 [15:28<05:49,  2.41it/s] 72%|███████▏  | 2251/3145 [15:28<06:17,  2.37it/s] 73%|███████▎  | 2296/3143 [15:34<05:50,  2.42it/s] 72%|███████▏  | 2277/3145 [15:28<06:04,  2.38it/s] 73%|███████▎  | 2304/3145 [15:28<05:58,  2.34it/s] 73%|███████▎  | 2297/3143 [15:35<05:58,  2.36it/s] 72%|███████▏  | 2252/3145 [15:29<06:24,  2.32it/s] 72%|███████▏  | 2278/3145 [15:29<05:54,  2.44it/s] 73%|███████▎  | 2305/3145 [15:29<06:01,  2.32it/s] 73%|███████▎  | 2298/3143 [15:35<06:03,  2.32it/s] 72%|███████▏  | 2253/3145 [15:29<06:30,  2.28it/s] 72%|███████▏  | 2279/3145 [15:29<06:14,  2.32it/s] 73%|███████▎  | 2306/3145 [15:29<06:07,  2.28it/s] 72%|███████▏  | 2254/3145 [15:29<05:41,  2.61it/s] 72%|███████▏  | 2280/3145 [15:30<06:00,  2.40it/s] 73%|███████▎  | 2299/3143 [15:35<06:13,  2.26it/s] 73%|███████▎  | 2307/3145 [15:30<06:06,  2.29it/s] 72%|███████▏  | 2255/3145 [15:30<05:41,  2.61it/s] 73%|███████▎  | 2281/3145 [15:30<05:45,  2.50it/s] 73%|███████▎  | 2300/3143 [15:36<06:02,  2.32it/s] 73%|███████▎  | 2308/3145 [15:30<05:51,  2.38it/s] 72%|███████▏  | 2256/3145 [15:30<05:50,  2.53it/s] 73%|███████▎  | 2282/3145 [15:30<05:40,  2.53it/s] 73%|███████▎  | 2301/3143 [15:36<05:56,  2.36it/s] 72%|███████▏  | 2257/3145 [15:30<05:11,  2.85it/s] 73%|███████▎  | 2309/3145 [15:31<05:55,  2.35it/s] 73%|███████▎  | 2283/3145 [15:31<05:30,  2.61it/s] 73%|███████▎  | 2302/3143 [15:37<05:40,  2.47it/s] 72%|███████▏  | 2258/3145 [15:31<05:27,  2.71it/s] 73%|███████▎  | 2310/3145 [15:31<05:48,  2.40it/s] 73%|███████▎  | 2284/3145 [15:31<05:32,  2.59it/s] 73%|███████▎  | 2303/3143 [15:37<05:47,  2.42it/s] 72%|███████▏  | 2259/3145 [15:31<05:27,  2.71it/s] 73%|███████▎  | 2285/3145 [15:31<05:35,  2.57it/s] 73%|███████▎  | 2311/3145 [15:31<05:58,  2.33it/s] 73%|███████▎  | 2304/3143 [15:37<05:43,  2.44it/s] 72%|███████▏  | 2260/3145 [15:32<05:38,  2.62it/s] 73%|███████▎  | 2286/3145 [15:32<05:32,  2.58it/s] 74%|███████▎  | 2312/3145 [15:32<05:51,  2.37it/s] 73%|███████▎  | 2305/3143 [15:38<05:31,  2.53it/s] 72%|███████▏  | 2261/3145 [15:32<05:45,  2.56it/s] 74%|███████▎  | 2313/3145 [15:32<05:39,  2.45it/s] 73%|███████▎  | 2287/3145 [15:32<05:44,  2.49it/s] 73%|███████▎  | 2306/3143 [15:38<05:33,  2.51it/s] 72%|███████▏  | 2262/3145 [15:32<05:50,  2.52it/s] 74%|███████▎  | 2314/3145 [15:33<05:35,  2.48it/s] 73%|███████▎  | 2288/3145 [15:33<05:43,  2.49it/s] 73%|███████▎  | 2307/3143 [15:39<05:34,  2.50it/s] 72%|███████▏  | 2263/3145 [15:33<06:38,  2.21it/s] 74%|███████▎  | 2315/3145 [15:33<05:47,  2.39it/s] 73%|███████▎  | 2289/3145 [15:33<05:58,  2.38it/s] 73%|███████▎  | 2308/3143 [15:39<05:24,  2.57it/s] 74%|███████▎  | 2316/3145 [15:33<05:51,  2.36it/s] 73%|███████▎  | 2309/3143 [15:39<05:28,  2.54it/s] 73%|███████▎  | 2290/3145 [15:33<05:58,  2.39it/s] 72%|███████▏  | 2264/3145 [15:34<07:18,  2.01it/s] 74%|███████▎  | 2317/3145 [15:34<05:35,  2.47it/s] 73%|███████▎  | 2310/3143 [15:40<05:26,  2.55it/s] 73%|███████▎  | 2291/3145 [15:34<05:51,  2.43it/s] 72%|███████▏  | 2265/3145 [15:34<06:32,  2.24it/s] 74%|███████▎  | 2318/3145 [15:34<05:32,  2.49it/s] 72%|███████▏  | 2266/3145 [15:34<05:55,  2.47it/s] 73%|███████▎  | 2292/3145 [15:34<05:37,  2.53it/s] 74%|███████▎  | 2311/3143 [15:40<05:29,  2.52it/s] 72%|███████▏  | 2267/3145 [15:35<05:51,  2.50it/s] 74%|███████▎  | 2319/3145 [15:35<05:42,  2.41it/s] 74%|███████▎  | 2312/3143 [15:41<05:24,  2.56it/s] 73%|███████▎  | 2293/3145 [15:35<05:48,  2.44it/s] 74%|███████▍  | 2320/3145 [15:35<04:58,  2.77it/s] 72%|███████▏  | 2268/3145 [15:35<05:29,  2.66it/s] 73%|███████▎  | 2294/3145 [15:35<05:34,  2.54it/s] 74%|███████▎  | 2313/3143 [15:41<05:31,  2.50it/s] 72%|███████▏  | 2269/3145 [15:35<05:07,  2.85it/s] 74%|███████▍  | 2321/3145 [15:35<05:06,  2.68it/s] 73%|███████▎  | 2295/3145 [15:35<05:36,  2.53it/s] 74%|███████▎  | 2314/3143 [15:41<05:47,  2.39it/s] 72%|███████▏  | 2270/3145 [15:36<05:13,  2.79it/s] 74%|███████▍  | 2322/3145 [15:36<05:21,  2.56it/s] 73%|███████▎  | 2296/3145 [15:36<05:46,  2.45it/s] 74%|███████▎  | 2315/3143 [15:42<05:41,  2.42it/s] 74%|███████▍  | 2323/3145 [15:36<04:41,  2.92it/s] 72%|███████▏  | 2271/3145 [15:36<05:24,  2.69it/s] 73%|███████▎  | 2297/3145 [15:36<04:59,  2.83it/s] 74%|███████▍  | 2324/3145 [15:36<04:19,  3.16it/s] 74%|███████▎  | 2316/3143 [15:42<05:30,  2.50it/s] 72%|███████▏  | 2272/3145 [15:36<05:23,  2.70it/s] 74%|███████▍  | 2325/3145 [15:36<04:02,  3.38it/s] 73%|███████▎  | 2298/3145 [15:37<05:09,  2.73it/s] 74%|███████▎  | 2317/3143 [15:43<05:31,  2.49it/s] 72%|███████▏  | 2273/3145 [15:37<05:30,  2.64it/s] 74%|███████▍  | 2326/3145 [15:37<04:37,  2.96it/s] 73%|███████▎  | 2299/3145 [15:37<05:17,  2.67it/s] 74%|███████▍  | 2318/3143 [15:43<05:26,  2.52it/s] 73%|███████▎  | 2300/3145 [15:37<04:46,  2.95it/s] 72%|███████▏  | 2274/3145 [15:37<05:56,  2.44it/s] 74%|███████▍  | 2327/3145 [15:37<04:59,  2.73it/s] 74%|███████▍  | 2319/3143 [15:43<05:26,  2.52it/s] 73%|███████▎  | 2301/3145 [15:38<04:58,  2.83it/s] 72%|███████▏  | 2275/3145 [15:38<06:01,  2.41it/s] 74%|███████▍  | 2328/3145 [15:38<05:06,  2.66it/s] 74%|███████▍  | 2320/3143 [15:44<05:29,  2.50it/s] 73%|███████▎  | 2302/3145 [15:38<05:13,  2.69it/s] 74%|███████▍  | 2329/3145 [15:38<05:06,  2.66it/s] 72%|███████▏  | 2276/3145 [15:38<06:03,  2.39it/s] 74%|███████▍  | 2321/3143 [15:44<05:24,  2.53it/s] 73%|███████▎  | 2303/3145 [15:38<05:15,  2.67it/s] 72%|███████▏  | 2277/3145 [15:38<05:44,  2.52it/s] 74%|███████▍  | 2330/3145 [15:39<05:26,  2.50it/s] 74%|███████▍  | 2322/3143 [15:45<05:26,  2.51it/s] 72%|███████▏  | 2278/3145 [15:39<05:42,  2.53it/s] 73%|███████▎  | 2304/3145 [15:39<06:04,  2.31it/s] 74%|███████▍  | 2331/3145 [15:39<05:36,  2.42it/s] 74%|███████▍  | 2323/3143 [15:45<05:28,  2.50it/s] 72%|███████▏  | 2279/3145 [15:39<05:06,  2.82it/s] 74%|███████▍  | 2332/3145 [15:39<05:26,  2.49it/s] 72%|███████▏  | 2280/3145 [15:39<05:09,  2.80it/s] 73%|███████▎  | 2305/3145 [15:39<06:38,  2.11it/s] 74%|███████▍  | 2324/3143 [15:45<05:43,  2.38it/s] 74%|███████▍  | 2333/3145 [15:40<04:47,  2.83it/s] 73%|███████▎  | 2281/3145 [15:40<05:17,  2.72it/s] 73%|███████▎  | 2306/3145 [15:40<06:37,  2.11it/s] 74%|███████▍  | 2334/3145 [15:40<04:56,  2.73it/s] 74%|███████▍  | 2325/3143 [15:46<05:54,  2.31it/s] 73%|███████▎  | 2282/3145 [15:40<05:39,  2.54it/s] 74%|███████▍  | 2326/3143 [15:46<05:49,  2.34it/s] 73%|███████▎  | 2307/3145 [15:40<06:44,  2.07it/s] 74%|███████▍  | 2335/3145 [15:41<05:30,  2.45it/s] 73%|███████▎  | 2283/3145 [15:41<05:52,  2.45it/s] 73%|███████▎  | 2308/3145 [15:41<06:07,  2.28it/s] 74%|███████▍  | 2327/3143 [15:47<06:00,  2.26it/s] 74%|███████▍  | 2336/3145 [15:41<05:36,  2.41it/s] 73%|███████▎  | 2309/3145 [15:41<05:36,  2.48it/s] 73%|███████▎  | 2284/3145 [15:41<05:48,  2.47it/s] 74%|███████▍  | 2328/3143 [15:47<05:50,  2.33it/s] 74%|███████▍  | 2337/3145 [15:41<05:33,  2.42it/s] 73%|███████▎  | 2310/3145 [15:41<05:31,  2.52it/s] 73%|███████▎  | 2285/3145 [15:42<05:57,  2.41it/s] 74%|███████▍  | 2338/3145 [15:42<05:30,  2.44it/s] 74%|███████▍  | 2329/3143 [15:48<05:58,  2.27it/s] 73%|███████▎  | 2311/3145 [15:42<05:10,  2.68it/s] 73%|███████▎  | 2286/3145 [15:42<06:05,  2.35it/s] 74%|███████▍  | 2339/3145 [15:42<05:25,  2.48it/s] 74%|███████▍  | 2330/3143 [15:48<05:39,  2.40it/s] 74%|███████▎  | 2312/3145 [15:42<05:30,  2.52it/s] 73%|███████▎  | 2287/3145 [15:42<05:44,  2.49it/s] 74%|███████▍  | 2331/3143 [15:48<05:21,  2.52it/s] 74%|███████▍  | 2340/3145 [15:43<05:22,  2.50it/s] 74%|███████▎  | 2313/3145 [15:43<05:11,  2.67it/s] 73%|███████▎  | 2288/3145 [15:43<05:55,  2.41it/s] 74%|███████▍  | 2332/3143 [15:49<05:31,  2.45it/s] 74%|███████▎  | 2314/3145 [15:43<05:07,  2.71it/s] 74%|███████▍  | 2341/3145 [15:43<06:01,  2.22it/s] 74%|███████▎  | 2315/3145 [15:43<04:40,  2.95it/s] 73%|███████▎  | 2289/3145 [15:43<06:02,  2.36it/s] 74%|███████▍  | 2333/3143 [15:49<05:15,  2.56it/s] 74%|███████▍  | 2342/3145 [15:44<06:00,  2.23it/s] 74%|███████▎  | 2316/3145 [15:44<04:58,  2.78it/s] 73%|███████▎  | 2290/3145 [15:44<06:05,  2.34it/s] 74%|███████▍  | 2334/3143 [15:50<05:36,  2.40it/s] 74%|███████▍  | 2335/3143 [15:50<04:48,  2.80it/s] 74%|███████▍  | 2343/3145 [15:44<05:56,  2.25it/s] 73%|███████▎  | 2291/3145 [15:44<05:50,  2.43it/s] 74%|███████▎  | 2317/3145 [15:44<05:26,  2.53it/s] 75%|███████▍  | 2344/3145 [15:44<05:47,  2.30it/s] 74%|███████▍  | 2336/3143 [15:50<05:18,  2.54it/s] 73%|███████▎  | 2292/3145 [15:45<06:05,  2.33it/s] 74%|███████▎  | 2318/3145 [15:45<05:45,  2.40it/s] 74%|███████▍  | 2337/3143 [15:51<04:36,  2.91it/s] 75%|███████▍  | 2345/3145 [15:45<05:36,  2.38it/s] 74%|███████▍  | 2338/3143 [15:51<04:09,  3.23it/s] 74%|███████▎  | 2319/3145 [15:45<05:40,  2.43it/s] 73%|███████▎  | 2293/3145 [15:45<06:25,  2.21it/s] 75%|███████▍  | 2346/3145 [15:45<05:40,  2.35it/s] 74%|███████▍  | 2339/3143 [15:51<04:28,  3.00it/s] 74%|███████▍  | 2320/3145 [15:45<05:41,  2.42it/s] 73%|███████▎  | 2294/3145 [15:45<06:11,  2.29it/s] 75%|███████▍  | 2347/3145 [15:46<05:42,  2.33it/s] 74%|███████▍  | 2340/3143 [15:52<04:35,  2.92it/s] 74%|███████▍  | 2321/3145 [15:46<05:34,  2.47it/s] 75%|███████▍  | 2348/3145 [15:46<04:57,  2.68it/s] 73%|███████▎  | 2295/3145 [15:46<06:18,  2.25it/s] 74%|███████▍  | 2341/3143 [15:52<04:58,  2.69it/s] 74%|███████▍  | 2322/3145 [15:46<05:45,  2.38it/s] 75%|███████▍  | 2349/3145 [15:46<04:52,  2.72it/s] 73%|███████▎  | 2296/3145 [15:46<06:18,  2.24it/s] 75%|███████▍  | 2342/3143 [15:52<05:13,  2.55it/s] 74%|███████▍  | 2323/3145 [15:47<05:44,  2.38it/s] 75%|███████▍  | 2350/3145 [15:47<05:11,  2.55it/s] 73%|███████▎  | 2297/3145 [15:47<06:08,  2.30it/s] 75%|███████▍  | 2343/3143 [15:53<05:21,  2.49it/s] 74%|███████▍  | 2324/3145 [15:47<05:46,  2.37it/s] 75%|███████▍  | 2351/3145 [15:47<05:25,  2.44it/s] 73%|███████▎  | 2298/3145 [15:47<06:09,  2.30it/s] 75%|███████▍  | 2344/3143 [15:53<05:16,  2.53it/s] 75%|███████▍  | 2352/3145 [15:48<05:12,  2.54it/s] 74%|███████▍  | 2325/3145 [15:48<06:33,  2.08it/s] 73%|███████▎  | 2299/3145 [15:48<06:17,  2.24it/s] 75%|███████▍  | 2345/3143 [15:54<05:09,  2.58it/s] 75%|███████▍  | 2353/3145 [15:48<05:05,  2.59it/s] 73%|███████▎  | 2300/3145 [15:48<06:04,  2.32it/s] 75%|███████▍  | 2346/3143 [15:54<05:09,  2.58it/s] 74%|███████▍  | 2326/3145 [15:48<06:23,  2.13it/s] 75%|███████▍  | 2354/3145 [15:48<05:07,  2.57it/s] 75%|███████▍  | 2347/3143 [15:54<05:07,  2.59it/s] 73%|███████▎  | 2301/3145 [15:48<05:57,  2.36it/s] 74%|███████▍  | 2327/3145 [15:49<06:27,  2.11it/s] 75%|███████▍  | 2355/3145 [15:49<05:23,  2.44it/s] 75%|███████▍  | 2348/3143 [15:55<05:09,  2.57it/s] 73%|███████▎  | 2302/3145 [15:49<06:05,  2.31it/s] 74%|███████▍  | 2328/3145 [15:49<06:38,  2.05it/s] 75%|███████▍  | 2356/3145 [15:49<05:19,  2.47it/s] 75%|███████▍  | 2349/3143 [15:55<05:08,  2.57it/s] 73%|███████▎  | 2303/3145 [15:49<06:01,  2.33it/s] 75%|███████▍  | 2357/3145 [15:50<05:19,  2.47it/s] 74%|███████▍  | 2329/3145 [15:50<06:18,  2.15it/s] 73%|███████▎  | 2304/3145 [15:50<05:49,  2.41it/s] 75%|███████▍  | 2350/3143 [15:56<06:03,  2.18it/s] 75%|███████▍  | 2358/3145 [15:50<05:26,  2.41it/s] 74%|███████▍  | 2330/3145 [15:50<06:12,  2.19it/s] 73%|███████▎  | 2305/3145 [15:50<05:46,  2.42it/s] 75%|███████▍  | 2351/3143 [15:56<06:02,  2.19it/s] 75%|███████▌  | 2359/3145 [15:50<05:25,  2.42it/s] 74%|███████▍  | 2331/3145 [15:50<06:09,  2.20it/s] 73%|███████▎  | 2306/3145 [15:51<05:46,  2.42it/s] 74%|███████▍  | 2332/3145 [15:51<05:28,  2.47it/s] 75%|███████▍  | 2352/3143 [15:57<05:50,  2.26it/s] 75%|███████▌  | 2360/3145 [15:51<05:28,  2.39it/s] 73%|███████▎  | 2307/3145 [15:51<05:53,  2.37it/s] 75%|███████▍  | 2353/3143 [15:57<05:34,  2.36it/s] 75%|███████▌  | 2361/3145 [15:51<05:20,  2.45it/s] 74%|███████▍  | 2333/3145 [15:51<06:09,  2.20it/s] 75%|███████▍  | 2354/3143 [15:57<04:51,  2.71it/s] 73%|███████▎  | 2308/3145 [15:51<05:51,  2.38it/s] 75%|███████▌  | 2362/3145 [15:52<05:18,  2.46it/s] 75%|███████▍  | 2355/3143 [15:58<04:23,  2.99it/s] 73%|███████▎  | 2309/3145 [15:52<05:40,  2.45it/s] 74%|███████▍  | 2334/3145 [15:52<06:37,  2.04it/s] 75%|███████▍  | 2356/3143 [15:58<04:24,  2.98it/s] 75%|███████▌  | 2363/3145 [15:52<05:18,  2.46it/s] 73%|███████▎  | 2310/3145 [15:52<05:53,  2.36it/s] 74%|███████▍  | 2335/3145 [15:52<06:19,  2.13it/s] 75%|███████▍  | 2357/3143 [15:58<04:38,  2.82it/s] 75%|███████▌  | 2364/3145 [15:52<05:09,  2.52it/s] 74%|███████▍  | 2336/3145 [15:53<05:46,  2.33it/s] 73%|███████▎  | 2311/3145 [15:53<05:49,  2.38it/s] 75%|███████▌  | 2358/3143 [15:59<04:37,  2.83it/s] 75%|███████▌  | 2365/3145 [15:53<05:03,  2.57it/s] 74%|███████▍  | 2337/3145 [15:53<05:29,  2.45it/s] 74%|███████▎  | 2312/3145 [15:53<05:23,  2.57it/s] 75%|███████▌  | 2366/3145 [15:53<05:06,  2.54it/s] 75%|███████▌  | 2359/3143 [15:59<05:24,  2.41it/s] 74%|███████▍  | 2338/3145 [15:53<05:31,  2.43it/s] 74%|███████▎  | 2313/3145 [15:53<05:29,  2.52it/s] 75%|███████▌  | 2367/3145 [15:54<05:08,  2.52it/s] 75%|███████▌  | 2360/3143 [16:00<05:21,  2.44it/s] 74%|███████▍  | 2339/3145 [15:54<05:23,  2.49it/s] 74%|███████▎  | 2314/3145 [15:54<05:39,  2.45it/s] 75%|███████▌  | 2368/3145 [15:54<05:21,  2.41it/s] 75%|███████▌  | 2361/3143 [16:00<05:19,  2.45it/s] 74%|███████▍  | 2340/3145 [15:54<05:15,  2.55it/s] 74%|███████▎  | 2315/3145 [15:54<05:42,  2.42it/s] 75%|███████▌  | 2369/3145 [15:54<05:16,  2.45it/s] 75%|███████▌  | 2362/3143 [16:00<05:21,  2.43it/s] 74%|███████▍  | 2341/3145 [15:55<05:24,  2.48it/s] 74%|███████▎  | 2316/3145 [15:55<05:37,  2.46it/s] 75%|███████▌  | 2370/3145 [15:55<05:21,  2.41it/s] 74%|███████▍  | 2342/3145 [15:55<05:13,  2.56it/s] 75%|███████▌  | 2363/3143 [16:01<05:25,  2.39it/s] 74%|███████▎  | 2317/3145 [15:55<05:33,  2.48it/s] 75%|███████▌  | 2371/3145 [15:55<05:16,  2.44it/s] 74%|███████▍  | 2343/3145 [15:55<05:03,  2.65it/s] 75%|███████▌  | 2364/3143 [16:01<05:16,  2.46it/s] 74%|███████▎  | 2318/3145 [15:55<05:43,  2.41it/s] 75%|███████▍  | 2344/3145 [15:56<05:16,  2.53it/s] 75%|███████▌  | 2372/3145 [15:56<05:38,  2.28it/s] 75%|███████▌  | 2365/3143 [16:02<05:26,  2.38it/s] 74%|███████▎  | 2319/3145 [15:56<05:50,  2.36it/s] 75%|███████▍  | 2345/3145 [15:56<05:14,  2.55it/s] 75%|███████▌  | 2366/3143 [16:02<05:22,  2.41it/s] 75%|███████▌  | 2373/3145 [15:56<05:46,  2.23it/s] 75%|███████▍  | 2346/3145 [15:56<04:33,  2.92it/s] 74%|███████▍  | 2320/3145 [15:56<05:39,  2.43it/s] 75%|███████▌  | 2367/3143 [16:02<05:23,  2.40it/s] 75%|███████▌  | 2374/3145 [15:57<05:48,  2.21it/s] 74%|███████▍  | 2321/3145 [15:57<05:53,  2.33it/s] 75%|███████▍  | 2347/3145 [15:57<05:07,  2.59it/s] 75%|███████▌  | 2368/3143 [16:03<05:24,  2.39it/s] 76%|███████▌  | 2375/3145 [15:57<05:38,  2.28it/s] 74%|███████▍  | 2322/3145 [15:57<05:35,  2.45it/s] 75%|███████▍  | 2348/3145 [15:57<05:33,  2.39it/s] 76%|███████▌  | 2376/3145 [15:57<05:16,  2.43it/s] 75%|███████▌  | 2369/3143 [16:03<05:34,  2.32it/s] 74%|███████▍  | 2323/3145 [15:58<05:25,  2.53it/s] 75%|███████▍  | 2349/3145 [15:58<05:23,  2.46it/s] 75%|███████▌  | 2370/3143 [16:04<05:17,  2.43it/s] 76%|███████▌  | 2377/3145 [15:58<05:29,  2.33it/s] 74%|███████▍  | 2324/3145 [15:58<05:24,  2.53it/s] 75%|███████▍  | 2350/3145 [15:58<05:30,  2.41it/s] 75%|███████▌  | 2371/3143 [16:04<05:13,  2.46it/s] 76%|███████▌  | 2378/3145 [15:58<05:19,  2.40it/s] 74%|███████▍  | 2325/3145 [15:58<05:19,  2.57it/s] 75%|███████▍  | 2351/3145 [15:59<05:34,  2.38it/s] 75%|███████▌  | 2372/3143 [16:04<05:00,  2.56it/s] 76%|███████▌  | 2379/3145 [15:59<05:00,  2.55it/s] 74%|███████▍  | 2326/3145 [15:59<05:34,  2.45it/s] 75%|███████▍  | 2352/3145 [15:59<05:30,  2.40it/s] 76%|███████▌  | 2373/3143 [16:05<05:04,  2.53it/s] 76%|███████▌  | 2380/3145 [15:59<05:09,  2.47it/s] 74%|███████▍  | 2327/3145 [15:59<05:30,  2.47it/s] 76%|███████▌  | 2374/3143 [16:05<04:59,  2.56it/s] 75%|███████▍  | 2353/3145 [15:59<05:34,  2.37it/s] 76%|███████▌  | 2381/3145 [15:59<05:15,  2.42it/s] 74%|███████▍  | 2328/3145 [16:00<05:31,  2.46it/s] 75%|███████▍  | 2354/3145 [16:00<05:29,  2.40it/s] 76%|███████▌  | 2375/3143 [16:06<05:22,  2.38it/s] 76%|███████▌  | 2382/3145 [16:00<05:10,  2.45it/s] 74%|███████▍  | 2329/3145 [16:00<05:28,  2.48it/s] 75%|███████▍  | 2355/3145 [16:00<05:23,  2.44it/s] 76%|███████▌  | 2376/3143 [16:06<05:08,  2.49it/s] 76%|███████▌  | 2383/3145 [16:00<05:17,  2.40it/s] 74%|███████▍  | 2330/3145 [16:00<05:28,  2.48it/s] 75%|███████▍  | 2356/3145 [16:00<04:43,  2.78it/s] 76%|███████▌  | 2377/3143 [16:07<05:07,  2.49it/s] 76%|███████▌  | 2384/3145 [16:01<05:21,  2.37it/s] 74%|███████▍  | 2331/3145 [16:01<05:35,  2.43it/s] 75%|███████▍  | 2357/3145 [16:01<04:45,  2.76it/s] 76%|███████▌  | 2378/3143 [16:07<04:59,  2.56it/s] 76%|███████▌  | 2385/3145 [16:01<05:09,  2.46it/s] 74%|███████▍  | 2332/3145 [16:01<05:21,  2.53it/s] 75%|███████▍  | 2358/3145 [16:01<04:56,  2.65it/s] 76%|███████▌  | 2386/3145 [16:01<04:31,  2.80it/s] 76%|███████▌  | 2379/3143 [16:07<05:09,  2.47it/s] 74%|███████▍  | 2333/3145 [16:02<05:28,  2.47it/s] 75%|███████▌  | 2359/3145 [16:02<04:51,  2.70it/s] 76%|███████▌  | 2387/3145 [16:02<04:48,  2.63it/s] 76%|███████▌  | 2380/3143 [16:08<05:17,  2.40it/s] 74%|███████▍  | 2334/3145 [16:02<05:18,  2.55it/s] 75%|███████▌  | 2360/3145 [16:02<05:08,  2.55it/s] 76%|███████▌  | 2388/3145 [16:02<04:50,  2.60it/s] 76%|███████▌  | 2381/3143 [16:08<05:23,  2.35it/s] 74%|███████▍  | 2335/3145 [16:02<05:30,  2.45it/s] 75%|███████▌  | 2361/3145 [16:02<05:08,  2.54it/s] 76%|███████▌  | 2389/3145 [16:03<05:00,  2.52it/s] 76%|███████▌  | 2382/3143 [16:09<05:02,  2.52it/s] 74%|███████▍  | 2336/3145 [16:03<05:29,  2.45it/s] 75%|███████▌  | 2362/3145 [16:03<05:17,  2.47it/s] 76%|███████▌  | 2390/3145 [16:03<04:58,  2.53it/s] 76%|███████▌  | 2383/3143 [16:09<05:04,  2.49it/s] 74%|███████▍  | 2337/3145 [16:03<05:31,  2.44it/s] 75%|███████▌  | 2363/3145 [16:03<05:12,  2.50it/s] 76%|███████▌  | 2384/3143 [16:09<04:30,  2.81it/s] 76%|███████▌  | 2391/3145 [16:03<05:06,  2.46it/s] 75%|███████▌  | 2364/3145 [16:04<05:12,  2.50it/s] 74%|███████▍  | 2338/3145 [16:04<05:44,  2.34it/s] 76%|███████▌  | 2385/3143 [16:10<04:49,  2.62it/s] 76%|███████▌  | 2392/3145 [16:04<05:15,  2.38it/s] 75%|███████▌  | 2365/3145 [16:04<05:08,  2.52it/s] 74%|███████▍  | 2339/3145 [16:04<05:36,  2.39it/s] 76%|███████▌  | 2386/3143 [16:10<05:13,  2.42it/s] 76%|███████▌  | 2393/3145 [16:04<05:28,  2.29it/s] 75%|███████▌  | 2366/3145 [16:04<05:10,  2.51it/s] 74%|███████▍  | 2340/3145 [16:04<05:43,  2.34it/s] 76%|███████▌  | 2387/3143 [16:11<05:17,  2.38it/s] 75%|███████▌  | 2367/3145 [16:05<05:14,  2.47it/s] 74%|███████▍  | 2341/3145 [16:05<05:38,  2.38it/s] 76%|███████▌  | 2394/3145 [16:05<06:00,  2.08it/s] 76%|███████▌  | 2388/3143 [16:11<05:00,  2.51it/s] 75%|███████▌  | 2368/3145 [16:05<05:08,  2.52it/s] 76%|███████▌  | 2395/3145 [16:05<05:28,  2.28it/s] 74%|███████▍  | 2342/3145 [16:05<05:48,  2.30it/s] 76%|███████▌  | 2389/3143 [16:11<05:04,  2.47it/s] 75%|███████▌  | 2369/3145 [16:06<05:02,  2.56it/s] 76%|███████▌  | 2396/3145 [16:06<05:16,  2.36it/s] 76%|███████▌  | 2390/3143 [16:12<05:07,  2.45it/s] 75%|███████▌  | 2370/3145 [16:06<04:51,  2.66it/s] 74%|███████▍  | 2343/3145 [16:06<06:29,  2.06it/s] 76%|███████▌  | 2397/3145 [16:06<05:10,  2.41it/s] 76%|███████▌  | 2391/3143 [16:12<05:04,  2.47it/s] 75%|███████▌  | 2371/3145 [16:06<05:07,  2.52it/s] 75%|███████▍  | 2344/3145 [16:06<06:35,  2.02it/s] 76%|███████▌  | 2398/3145 [16:07<05:15,  2.37it/s] 75%|███████▌  | 2372/3145 [16:07<04:57,  2.60it/s] 76%|███████▌  | 2392/3143 [16:13<05:27,  2.29it/s] 75%|███████▍  | 2345/3145 [16:07<06:26,  2.07it/s] 76%|███████▋  | 2399/3145 [16:07<05:20,  2.33it/s] 75%|███████▌  | 2373/3145 [16:07<04:57,  2.60it/s] 76%|███████▌  | 2393/3143 [16:13<05:27,  2.29it/s] 75%|███████▍  | 2346/3145 [16:07<06:01,  2.21it/s] 76%|███████▋  | 2400/3145 [16:07<05:27,  2.28it/s] 75%|███████▌  | 2374/3145 [16:08<04:59,  2.58it/s] 76%|███████▌  | 2394/3143 [16:13<05:12,  2.40it/s] 75%|███████▍  | 2347/3145 [16:08<05:41,  2.34it/s] 76%|███████▋  | 2401/3145 [16:08<05:09,  2.40it/s] 76%|███████▌  | 2395/3143 [16:14<04:40,  2.67it/s] 76%|███████▌  | 2375/3145 [16:08<05:05,  2.52it/s] 75%|███████▍  | 2348/3145 [16:08<05:34,  2.38it/s] 76%|███████▋  | 2402/3145 [16:08<05:17,  2.34it/s] 76%|███████▌  | 2396/3143 [16:14<04:48,  2.59it/s] 76%|███████▌  | 2376/3145 [16:09<05:44,  2.23it/s] 75%|███████▍  | 2349/3145 [16:09<05:39,  2.34it/s] 76%|███████▋  | 2403/3145 [16:09<05:11,  2.38it/s] 76%|███████▋  | 2397/3143 [16:15<05:05,  2.45it/s] 76%|███████▌  | 2377/3145 [16:09<05:41,  2.25it/s] 75%|███████▍  | 2350/3145 [16:09<05:44,  2.31it/s] 76%|███████▋  | 2404/3145 [16:09<05:19,  2.32it/s] 76%|███████▋  | 2398/3143 [16:15<05:00,  2.48it/s] 76%|███████▌  | 2378/3145 [16:09<05:27,  2.34it/s] 75%|███████▍  | 2351/3145 [16:09<05:24,  2.45it/s] 76%|███████▋  | 2405/3145 [16:10<05:12,  2.37it/s] 76%|███████▋  | 2399/3143 [16:15<05:10,  2.40it/s] 76%|███████▌  | 2379/3145 [16:10<05:27,  2.34it/s] 75%|███████▍  | 2352/3145 [16:10<05:30,  2.40it/s] 77%|███████▋  | 2406/3145 [16:10<05:06,  2.41it/s] 76%|███████▋  | 2400/3143 [16:16<05:06,  2.43it/s] 76%|███████▌  | 2380/3145 [16:10<05:19,  2.40it/s] 75%|███████▍  | 2353/3145 [16:10<05:20,  2.47it/s] 77%|███████▋  | 2407/3145 [16:10<05:06,  2.40it/s] 76%|███████▋  | 2401/3143 [16:16<05:08,  2.40it/s] 76%|███████▌  | 2381/3145 [16:11<05:13,  2.44it/s] 75%|███████▍  | 2354/3145 [16:11<05:22,  2.46it/s] 77%|███████▋  | 2408/3145 [16:11<05:13,  2.35it/s] 76%|███████▋  | 2402/3143 [16:17<05:13,  2.37it/s] 75%|███████▍  | 2355/3145 [16:11<05:25,  2.43it/s] 76%|███████▌  | 2382/3145 [16:11<05:34,  2.28it/s] 76%|███████▋  | 2403/3143 [16:17<05:00,  2.46it/s] 77%|███████▋  | 2409/3145 [16:11<05:25,  2.26it/s] 75%|███████▍  | 2356/3145 [16:11<05:13,  2.52it/s] 76%|███████▌  | 2383/3145 [16:12<05:50,  2.17it/s] 77%|███████▋  | 2410/3145 [16:12<05:04,  2.42it/s] 75%|███████▍  | 2357/3145 [16:12<05:15,  2.50it/s] 76%|███████▋  | 2404/3143 [16:18<05:45,  2.14it/s] 76%|███████▌  | 2384/3145 [16:12<05:33,  2.28it/s] 77%|███████▋  | 2411/3145 [16:12<05:11,  2.36it/s] 75%|███████▍  | 2358/3145 [16:12<05:14,  2.50it/s] 76%|███████▌  | 2385/3145 [16:12<05:18,  2.38it/s] 77%|███████▋  | 2405/3143 [16:18<05:58,  2.06it/s] 77%|███████▋  | 2412/3145 [16:12<05:12,  2.35it/s] 75%|███████▌  | 2359/3145 [16:13<05:13,  2.51it/s] 76%|███████▌  | 2386/3145 [16:13<05:15,  2.41it/s] 77%|███████▋  | 2406/3143 [16:19<06:18,  1.95it/s] 77%|███████▋  | 2413/3145 [16:13<05:19,  2.29it/s] 75%|███████▌  | 2360/3145 [16:13<05:15,  2.49it/s] 76%|███████▌  | 2387/3145 [16:13<04:59,  2.53it/s] 77%|███████▋  | 2407/3143 [16:19<05:52,  2.09it/s] 77%|███████▋  | 2414/3145 [16:13<05:09,  2.36it/s] 75%|███████▌  | 2361/3145 [16:13<05:26,  2.40it/s] 76%|███████▌  | 2388/3145 [16:14<05:11,  2.43it/s] 77%|███████▋  | 2408/3143 [16:19<05:02,  2.43it/s] 75%|███████▌  | 2362/3145 [16:14<05:12,  2.50it/s] 77%|███████▋  | 2415/3145 [16:14<05:23,  2.26it/s] 77%|███████▋  | 2409/3143 [16:20<05:08,  2.38it/s] 76%|███████▌  | 2389/3145 [16:14<05:42,  2.20it/s] 75%|███████▌  | 2363/3145 [16:14<05:14,  2.49it/s] 77%|███████▋  | 2416/3145 [16:14<05:31,  2.20it/s] 77%|███████▋  | 2410/3143 [16:20<05:03,  2.42it/s] 75%|███████▌  | 2364/3145 [16:15<05:06,  2.55it/s] 76%|███████▌  | 2390/3145 [16:15<06:04,  2.07it/s] 77%|███████▋  | 2417/3145 [16:15<05:18,  2.28it/s] 77%|███████▋  | 2411/3143 [16:21<04:59,  2.44it/s] 75%|███████▌  | 2365/3145 [16:15<05:01,  2.59it/s] 76%|███████▌  | 2391/3145 [16:15<05:52,  2.14it/s] 77%|███████▋  | 2418/3145 [16:15<05:06,  2.37it/s] 77%|███████▋  | 2412/3143 [16:21<04:47,  2.54it/s] 75%|███████▌  | 2366/3145 [16:15<05:07,  2.54it/s] 76%|███████▌  | 2392/3145 [16:15<05:28,  2.29it/s] 77%|███████▋  | 2419/3145 [16:15<04:56,  2.45it/s] 77%|███████▋  | 2413/3143 [16:21<04:49,  2.52it/s] 75%|███████▌  | 2367/3145 [16:16<05:09,  2.51it/s] 76%|███████▌  | 2393/3145 [16:16<05:16,  2.37it/s] 77%|███████▋  | 2420/3145 [16:16<04:46,  2.53it/s] 77%|███████▋  | 2414/3143 [16:22<04:54,  2.48it/s] 77%|███████▋  | 2421/3145 [16:16<04:13,  2.85it/s] 75%|███████▌  | 2368/3145 [16:16<05:01,  2.58it/s] 76%|███████▌  | 2394/3145 [16:16<05:10,  2.42it/s] 77%|███████▋  | 2415/3143 [16:22<04:54,  2.47it/s] 77%|███████▋  | 2422/3145 [16:16<04:23,  2.74it/s] 76%|███████▌  | 2395/3145 [16:17<04:54,  2.55it/s] 75%|███████▌  | 2369/3145 [16:17<05:20,  2.42it/s] 77%|███████▋  | 2416/3143 [16:23<04:57,  2.44it/s] 77%|███████▋  | 2423/3145 [16:17<04:37,  2.60it/s] 76%|███████▌  | 2396/3145 [16:17<04:59,  2.50it/s] 75%|███████▌  | 2370/3145 [16:17<05:17,  2.44it/s] 77%|███████▋  | 2417/3143 [16:23<04:49,  2.51it/s] 77%|███████▋  | 2424/3145 [16:17<04:32,  2.65it/s] 76%|███████▌  | 2397/3145 [16:17<05:06,  2.44it/s] 75%|███████▌  | 2371/3145 [16:17<05:17,  2.44it/s] 77%|███████▋  | 2418/3143 [16:23<04:16,  2.83it/s] 77%|███████▋  | 2425/3145 [16:18<04:15,  2.82it/s] 76%|███████▌  | 2398/3145 [16:18<04:54,  2.53it/s] 75%|███████▌  | 2372/3145 [16:18<05:16,  2.44it/s] 77%|███████▋  | 2419/3143 [16:24<04:34,  2.64it/s] 77%|███████▋  | 2426/3145 [16:18<04:23,  2.72it/s] 76%|███████▋  | 2399/3145 [16:18<04:37,  2.68it/s] 75%|███████▌  | 2373/3145 [16:18<05:18,  2.42it/s] 77%|███████▋  | 2420/3143 [16:24<04:46,  2.52it/s] 77%|███████▋  | 2427/3145 [16:18<04:44,  2.53it/s] 76%|███████▋  | 2400/3145 [16:18<04:41,  2.65it/s] 75%|███████▌  | 2374/3145 [16:19<05:09,  2.49it/s] 77%|███████▋  | 2421/3143 [16:25<04:47,  2.51it/s] 77%|███████▋  | 2428/3145 [16:19<04:48,  2.48it/s] 76%|███████▋  | 2401/3145 [16:19<04:46,  2.60it/s] 76%|███████▌  | 2375/3145 [16:19<05:14,  2.45it/s] 77%|███████▋  | 2422/3143 [16:25<04:46,  2.52it/s] 77%|███████▋  | 2429/3145 [16:19<04:46,  2.50it/s] 76%|███████▋  | 2402/3145 [16:19<04:50,  2.56it/s] 76%|███████▌  | 2376/3145 [16:19<05:10,  2.47it/s] 77%|███████▋  | 2423/3143 [16:25<04:51,  2.47it/s] 77%|███████▋  | 2430/3145 [16:20<04:45,  2.50it/s] 76%|███████▋  | 2403/3145 [16:20<04:51,  2.55it/s] 77%|███████▋  | 2424/3143 [16:26<04:13,  2.84it/s] 76%|███████▌  | 2377/3145 [16:20<05:06,  2.50it/s] 77%|███████▋  | 2431/3145 [16:20<04:37,  2.58it/s] 77%|███████▋  | 2425/3143 [16:26<04:18,  2.78it/s] 76%|███████▋  | 2404/3145 [16:20<05:05,  2.43it/s] 76%|███████▌  | 2378/3145 [16:20<05:21,  2.39it/s] 77%|███████▋  | 2432/3145 [16:20<04:45,  2.50it/s] 77%|███████▋  | 2426/3143 [16:26<04:20,  2.75it/s] 76%|███████▋  | 2405/3145 [16:21<05:15,  2.34it/s] 76%|███████▌  | 2379/3145 [16:21<05:12,  2.45it/s] 77%|███████▋  | 2433/3145 [16:21<04:52,  2.43it/s] 77%|███████▋  | 2427/3143 [16:27<04:29,  2.66it/s] 77%|███████▋  | 2406/3145 [16:21<05:16,  2.34it/s] 76%|███████▌  | 2380/3145 [16:21<05:20,  2.39it/s] 77%|███████▋  | 2434/3145 [16:21<04:32,  2.61it/s] 77%|███████▋  | 2428/3143 [16:27<04:35,  2.60it/s] 77%|███████▋  | 2407/3145 [16:21<05:02,  2.44it/s] 77%|███████▋  | 2435/3145 [16:22<04:29,  2.64it/s] 77%|███████▋  | 2408/3145 [16:22<04:24,  2.78it/s] 76%|███████▌  | 2381/3145 [16:22<05:46,  2.21it/s] 77%|███████▋  | 2429/3143 [16:28<04:31,  2.63it/s] 77%|███████▋  | 2436/3145 [16:22<04:24,  2.68it/s] 77%|███████▋  | 2409/3145 [16:22<04:30,  2.72it/s] 76%|███████▌  | 2382/3145 [16:22<05:43,  2.22it/s] 77%|███████▋  | 2430/3143 [16:28<05:14,  2.26it/s] 77%|███████▋  | 2437/3145 [16:22<04:22,  2.69it/s] 77%|███████▋  | 2410/3145 [16:22<04:26,  2.76it/s] 76%|███████▌  | 2383/3145 [16:23<05:41,  2.23it/s] 78%|███████▊  | 2438/3145 [16:23<04:20,  2.71it/s] 77%|███████▋  | 2431/3143 [16:29<05:05,  2.33it/s] 77%|███████▋  | 2411/3145 [16:23<04:20,  2.82it/s] 76%|███████▌  | 2384/3145 [16:23<05:46,  2.20it/s] 78%|███████▊  | 2439/3145 [16:23<04:33,  2.58it/s] 77%|███████▋  | 2432/3143 [16:29<05:03,  2.34it/s] 77%|███████▋  | 2412/3145 [16:23<04:44,  2.57it/s] 76%|███████▌  | 2385/3145 [16:23<04:59,  2.54it/s] 78%|███████▊  | 2440/3145 [16:23<04:25,  2.65it/s] 77%|███████▋  | 2433/3143 [16:29<04:58,  2.38it/s] 77%|███████▋  | 2413/3145 [16:24<04:48,  2.54it/s] 76%|███████▌  | 2386/3145 [16:24<04:58,  2.54it/s] 77%|███████▋  | 2414/3145 [16:24<04:42,  2.59it/s] 78%|███████▊  | 2441/3145 [16:24<05:07,  2.29it/s] 76%|███████▌  | 2387/3145 [16:24<04:58,  2.54it/s] 77%|███████▋  | 2434/3143 [16:30<05:32,  2.13it/s] 77%|███████▋  | 2415/3145 [16:24<04:44,  2.57it/s] 78%|███████▊  | 2442/3145 [16:24<04:55,  2.38it/s] 77%|███████▋  | 2435/3143 [16:30<05:16,  2.24it/s] 76%|███████▌  | 2388/3145 [16:25<05:51,  2.16it/s] 77%|███████▋  | 2416/3145 [16:25<04:45,  2.56it/s] 78%|███████▊  | 2443/3145 [16:25<04:54,  2.38it/s] 78%|███████▊  | 2436/3143 [16:31<05:11,  2.27it/s] 76%|███████▌  | 2389/3145 [16:25<05:36,  2.24it/s] 78%|███████▊  | 2444/3145 [16:25<04:44,  2.47it/s] 77%|███████▋  | 2417/3145 [16:25<05:25,  2.24it/s] 78%|███████▊  | 2437/3143 [16:31<05:18,  2.22it/s] 78%|███████▊  | 2445/3145 [16:26<04:49,  2.42it/s] 76%|███████▌  | 2390/3145 [16:26<06:11,  2.03it/s] 77%|███████▋  | 2418/3145 [16:26<05:11,  2.33it/s] 78%|███████▊  | 2438/3143 [16:32<05:07,  2.29it/s] 78%|███████▊  | 2446/3145 [16:26<04:52,  2.39it/s] 77%|███████▋  | 2419/3145 [16:26<04:57,  2.44it/s] 76%|███████▌  | 2391/3145 [16:26<05:55,  2.12it/s] 78%|███████▊  | 2439/3143 [16:32<05:05,  2.30it/s] 78%|███████▊  | 2447/3145 [16:26<04:43,  2.47it/s] 77%|███████▋  | 2420/3145 [16:26<04:49,  2.50it/s] 76%|███████▌  | 2392/3145 [16:27<05:49,  2.15it/s] 78%|███████▊  | 2440/3143 [16:32<04:58,  2.35it/s] 78%|███████▊  | 2448/3145 [16:27<04:38,  2.50it/s] 78%|███████▊  | 2441/3143 [16:33<04:48,  2.43it/s] 77%|███████▋  | 2421/3145 [16:27<05:22,  2.24it/s] 78%|███████▊  | 2449/3145 [16:27<04:03,  2.86it/s] 76%|███████▌  | 2393/3145 [16:27<06:15,  2.00it/s] 77%|███████▋  | 2422/3145 [16:27<05:09,  2.34it/s] 78%|███████▊  | 2442/3143 [16:33<04:49,  2.42it/s] 78%|███████▊  | 2450/3145 [16:27<04:20,  2.67it/s] 76%|███████▌  | 2394/3145 [16:28<05:58,  2.09it/s] 77%|███████▋  | 2423/3145 [16:28<04:55,  2.44it/s] 78%|███████▊  | 2443/3143 [16:34<04:40,  2.50it/s] 78%|███████▊  | 2451/3145 [16:28<04:27,  2.60it/s] 76%|███████▌  | 2395/3145 [16:28<05:36,  2.23it/s] 77%|███████▋  | 2424/3145 [16:28<04:52,  2.46it/s] 78%|███████▊  | 2444/3143 [16:34<04:45,  2.45it/s] 78%|███████▊  | 2452/3145 [16:28<04:35,  2.51it/s] 76%|███████▌  | 2396/3145 [16:28<05:36,  2.23it/s] 77%|███████▋  | 2425/3145 [16:29<04:50,  2.48it/s] 78%|███████▊  | 2445/3143 [16:35<04:57,  2.35it/s] 78%|███████▊  | 2453/3145 [16:29<04:44,  2.43it/s] 76%|███████▌  | 2397/3145 [16:29<05:25,  2.30it/s] 77%|███████▋  | 2426/3145 [16:29<04:55,  2.43it/s] 78%|███████▊  | 2454/3145 [16:29<04:30,  2.56it/s] 78%|███████▊  | 2446/3143 [16:35<04:59,  2.32it/s] 76%|███████▌  | 2398/3145 [16:29<05:27,  2.28it/s] 77%|███████▋  | 2427/3145 [16:29<04:45,  2.51it/s] 78%|███████▊  | 2455/3145 [16:29<04:31,  2.54it/s] 78%|███████▊  | 2447/3143 [16:35<05:07,  2.26it/s] 77%|███████▋  | 2428/3145 [16:30<04:33,  2.62it/s] 76%|███████▋  | 2399/3145 [16:30<05:39,  2.20it/s] 78%|███████▊  | 2456/3145 [16:30<04:44,  2.42it/s] 78%|███████▊  | 2448/3143 [16:36<05:11,  2.23it/s] 76%|███████▋  | 2400/3145 [16:30<05:09,  2.40it/s] 77%|███████▋  | 2429/3145 [16:30<04:45,  2.51it/s] 76%|███████▋  | 2401/3145 [16:30<04:45,  2.61it/s] 78%|███████▊  | 2457/3145 [16:30<04:53,  2.34it/s] 78%|███████▊  | 2449/3143 [16:36<05:06,  2.26it/s] 77%|███████▋  | 2430/3145 [16:31<05:01,  2.37it/s] 76%|███████▋  | 2402/3145 [16:31<04:51,  2.55it/s] 78%|███████▊  | 2450/3143 [16:37<04:51,  2.37it/s] 78%|███████▊  | 2458/3145 [16:31<05:03,  2.26it/s] 77%|███████▋  | 2431/3145 [16:31<04:51,  2.45it/s] 76%|███████▋  | 2403/3145 [16:31<04:41,  2.64it/s] 78%|███████▊  | 2451/3143 [16:37<04:48,  2.40it/s] 78%|███████▊  | 2459/3145 [16:31<04:52,  2.35it/s] 77%|███████▋  | 2432/3145 [16:31<04:45,  2.50it/s] 76%|███████▋  | 2404/3145 [16:32<04:56,  2.50it/s] 78%|███████▊  | 2460/3145 [16:32<04:45,  2.40it/s] 78%|███████▊  | 2452/3143 [16:38<04:49,  2.38it/s] 77%|███████▋  | 2433/3145 [16:32<04:44,  2.51it/s] 76%|███████▋  | 2405/3145 [16:32<05:03,  2.44it/s] 78%|███████▊  | 2461/3145 [16:32<04:34,  2.49it/s] 78%|███████▊  | 2453/3143 [16:38<04:38,  2.48it/s] 77%|███████▋  | 2434/3145 [16:32<04:42,  2.52it/s] 78%|███████▊  | 2462/3145 [16:32<04:32,  2.50it/s] 78%|███████▊  | 2454/3143 [16:38<04:39,  2.46it/s] 77%|███████▋  | 2406/3145 [16:32<05:21,  2.30it/s] 77%|███████▋  | 2435/3145 [16:33<04:41,  2.52it/s] 78%|███████▊  | 2455/3143 [16:39<04:34,  2.51it/s] 77%|███████▋  | 2436/3145 [16:33<04:40,  2.52it/s] 77%|███████▋  | 2407/3145 [16:33<05:22,  2.29it/s] 78%|███████▊  | 2463/3145 [16:33<05:08,  2.21it/s] 78%|███████▊  | 2456/3143 [16:39<04:27,  2.56it/s] 77%|███████▋  | 2437/3145 [16:33<04:54,  2.40it/s] 78%|███████▊  | 2464/3145 [16:33<05:04,  2.24it/s] 77%|███████▋  | 2408/3145 [16:34<05:59,  2.05it/s] 78%|███████▊  | 2457/3143 [16:40<04:37,  2.48it/s] 78%|███████▊  | 2465/3145 [16:34<04:19,  2.62it/s] 78%|███████▊  | 2438/3145 [16:34<05:26,  2.16it/s] 77%|███████▋  | 2409/3145 [16:34<05:51,  2.10it/s] 78%|███████▊  | 2458/3143 [16:40<04:36,  2.48it/s] 78%|███████▊  | 2466/3145 [16:34<04:19,  2.62it/s] 77%|███████▋  | 2410/3145 [16:34<05:26,  2.25it/s] 78%|███████▊  | 2439/3145 [16:34<05:11,  2.26it/s] 78%|███████▊  | 2459/3143 [16:40<04:34,  2.49it/s] 78%|███████▊  | 2467/3145 [16:34<04:24,  2.56it/s] 78%|███████▊  | 2440/3145 [16:35<05:03,  2.32it/s] 78%|███████▊  | 2460/3143 [16:41<04:26,  2.56it/s] 77%|███████▋  | 2411/3145 [16:35<05:28,  2.23it/s] 78%|███████▊  | 2468/3145 [16:35<04:25,  2.55it/s] 78%|███████▊  | 2461/3143 [16:41<04:28,  2.54it/s] 79%|███████▊  | 2469/3145 [16:35<04:24,  2.55it/s] 78%|███████▊  | 2441/3145 [16:35<05:19,  2.20it/s] 77%|███████▋  | 2412/3145 [16:35<05:34,  2.19it/s] 79%|███████▊  | 2470/3145 [16:36<04:14,  2.65it/s] 78%|███████▊  | 2462/3143 [16:41<04:29,  2.53it/s] 78%|███████▊  | 2442/3145 [16:36<05:04,  2.31it/s] 77%|███████▋  | 2413/3145 [16:36<05:27,  2.24it/s] 77%|███████▋  | 2414/3145 [16:36<04:45,  2.56it/s] 78%|███████▊  | 2463/3143 [16:42<04:35,  2.47it/s] 79%|███████▊  | 2471/3145 [16:36<04:30,  2.49it/s] 78%|███████▊  | 2443/3145 [16:36<05:02,  2.32it/s] 79%|███████▊  | 2472/3145 [16:36<04:23,  2.56it/s] 78%|███████▊  | 2464/3143 [16:42<04:32,  2.49it/s] 77%|███████▋  | 2415/3145 [16:36<04:59,  2.43it/s] 78%|███████▊  | 2444/3145 [16:36<04:53,  2.39it/s] 79%|███████▊  | 2473/3145 [16:37<04:22,  2.56it/s] 78%|███████▊  | 2465/3143 [16:43<04:28,  2.52it/s] 78%|███████▊  | 2445/3145 [16:37<04:42,  2.47it/s] 77%|███████▋  | 2416/3145 [16:37<05:08,  2.37it/s] 78%|███████▊  | 2446/3145 [16:37<04:32,  2.57it/s] 79%|███████▊  | 2474/3145 [16:37<04:31,  2.47it/s] 77%|███████▋  | 2417/3145 [16:37<05:00,  2.42it/s] 78%|███████▊  | 2466/3143 [16:43<04:43,  2.38it/s] 78%|███████▊  | 2447/3145 [16:38<04:32,  2.56it/s] 77%|███████▋  | 2418/3145 [16:38<04:49,  2.51it/s] 79%|███████▊  | 2475/3145 [16:38<04:34,  2.44it/s] 78%|███████▊  | 2467/3143 [16:44<04:46,  2.36it/s] 78%|███████▊  | 2448/3145 [16:38<04:36,  2.52it/s] 77%|███████▋  | 2419/3145 [16:38<04:46,  2.53it/s] 79%|███████▊  | 2476/3145 [16:38<04:33,  2.45it/s] 79%|███████▊  | 2468/3143 [16:44<04:42,  2.39it/s] 77%|███████▋  | 2420/3145 [16:38<04:48,  2.51it/s] 79%|███████▉  | 2477/3145 [16:38<04:31,  2.46it/s] 78%|███████▊  | 2449/3145 [16:38<04:50,  2.40it/s] 79%|███████▊  | 2469/3143 [16:44<04:44,  2.37it/s] 77%|███████▋  | 2421/3145 [16:39<04:43,  2.56it/s] 79%|███████▉  | 2478/3145 [16:39<04:30,  2.47it/s] 78%|███████▊  | 2450/3145 [16:39<04:48,  2.41it/s] 79%|███████▊  | 2470/3143 [16:45<04:31,  2.48it/s] 77%|███████▋  | 2422/3145 [16:39<04:36,  2.61it/s] 79%|███████▉  | 2479/3145 [16:39<04:18,  2.58it/s] 78%|███████▊  | 2451/3145 [16:39<04:38,  2.49it/s] 79%|███████▊  | 2471/3143 [16:45<04:30,  2.48it/s] 77%|███████▋  | 2423/3145 [16:40<04:36,  2.62it/s] 79%|███████▉  | 2480/3145 [16:40<04:17,  2.58it/s] 78%|███████▊  | 2452/3145 [16:40<04:42,  2.46it/s] 79%|███████▊  | 2472/3143 [16:46<04:41,  2.38it/s] 77%|███████▋  | 2424/3145 [16:40<04:58,  2.42it/s] 79%|███████▉  | 2481/3145 [16:40<04:31,  2.44it/s] 78%|███████▊  | 2453/3145 [16:40<04:46,  2.41it/s] 79%|███████▊  | 2473/3143 [16:46<04:38,  2.41it/s] 77%|███████▋  | 2425/3145 [16:40<04:21,  2.75it/s] 78%|███████▊  | 2454/3145 [16:40<04:39,  2.47it/s] 79%|███████▉  | 2482/3145 [16:40<04:35,  2.41it/s] 77%|███████▋  | 2426/3145 [16:41<03:57,  3.03it/s] 79%|███████▊  | 2474/3143 [16:46<04:29,  2.48it/s] 78%|███████▊  | 2455/3145 [16:41<04:31,  2.54it/s] 79%|███████▉  | 2483/3145 [16:41<04:39,  2.37it/s] 77%|███████▋  | 2427/3145 [16:41<04:17,  2.79it/s] 79%|███████▊  | 2475/3143 [16:47<05:05,  2.19it/s] 77%|███████▋  | 2428/3145 [16:41<03:46,  3.17it/s] 78%|███████▊  | 2456/3145 [16:41<04:47,  2.39it/s] 79%|███████▉  | 2484/3145 [16:41<04:41,  2.35it/s] 77%|███████▋  | 2429/3145 [16:42<04:14,  2.82it/s] 79%|███████▉  | 2476/3143 [16:48<05:15,  2.11it/s] 78%|███████▊  | 2457/3145 [16:42<04:40,  2.45it/s] 79%|███████▉  | 2485/3145 [16:42<04:35,  2.40it/s] 77%|███████▋  | 2430/3145 [16:42<04:23,  2.71it/s] 78%|███████▊  | 2458/3145 [16:42<04:46,  2.40it/s] 79%|███████▉  | 2477/3143 [16:48<05:23,  2.06it/s] 79%|███████▉  | 2486/3145 [16:42<04:36,  2.38it/s] 77%|███████▋  | 2431/3145 [16:42<04:26,  2.68it/s] 78%|███████▊  | 2459/3145 [16:43<04:41,  2.43it/s] 79%|███████▉  | 2478/3143 [16:49<05:17,  2.09it/s] 79%|███████▉  | 2487/3145 [16:43<04:39,  2.36it/s] 77%|███████▋  | 2432/3145 [16:43<04:32,  2.61it/s] 78%|███████▊  | 2460/3145 [16:43<04:38,  2.46it/s] 79%|███████▉  | 2479/3143 [16:49<04:56,  2.24it/s] 79%|███████▉  | 2488/3145 [16:43<04:42,  2.33it/s] 77%|███████▋  | 2433/3145 [16:43<04:47,  2.48it/s] 78%|███████▊  | 2461/3145 [16:43<04:29,  2.54it/s] 79%|███████▉  | 2480/3143 [16:49<04:48,  2.30it/s] 79%|███████▉  | 2489/3145 [16:43<04:42,  2.32it/s] 77%|███████▋  | 2434/3145 [16:44<04:18,  2.75it/s] 78%|███████▊  | 2462/3145 [16:44<04:20,  2.63it/s] 77%|███████▋  | 2435/3145 [16:44<03:56,  3.01it/s] 79%|███████▉  | 2481/3143 [16:50<04:40,  2.36it/s] 79%|███████▉  | 2490/3145 [16:44<04:39,  2.34it/s] 78%|███████▊  | 2463/3145 [16:44<04:28,  2.54it/s] 79%|███████▉  | 2482/3143 [16:50<04:37,  2.39it/s] 77%|███████▋  | 2436/3145 [16:44<04:23,  2.69it/s] 79%|███████▉  | 2491/3145 [16:44<04:40,  2.33it/s] 78%|███████▊  | 2464/3145 [16:44<04:29,  2.53it/s] 79%|███████▉  | 2483/3143 [16:51<04:39,  2.36it/s] 77%|███████▋  | 2437/3145 [16:45<04:33,  2.59it/s] 79%|███████▉  | 2492/3145 [16:45<04:37,  2.36it/s] 78%|███████▊  | 2465/3145 [16:45<04:28,  2.53it/s] 79%|███████▉  | 2484/3143 [16:51<04:41,  2.34it/s] 78%|███████▊  | 2438/3145 [16:45<04:36,  2.56it/s] 79%|███████▉  | 2493/3145 [16:45<04:43,  2.30it/s] 78%|███████▊  | 2466/3145 [16:45<04:40,  2.42it/s] 78%|███████▊  | 2439/3145 [16:45<04:33,  2.58it/s] 79%|███████▉  | 2485/3143 [16:51<04:43,  2.32it/s] 79%|███████▉  | 2494/3145 [16:46<04:37,  2.35it/s] 78%|███████▊  | 2467/3145 [16:46<04:43,  2.39it/s] 79%|███████▉  | 2486/3143 [16:52<04:36,  2.37it/s] 78%|███████▊  | 2440/3145 [16:46<04:54,  2.39it/s] 79%|███████▉  | 2495/3145 [16:46<04:24,  2.46it/s] 78%|███████▊  | 2468/3145 [16:46<04:58,  2.27it/s] 79%|███████▉  | 2496/3145 [16:46<04:14,  2.55it/s] 79%|███████▉  | 2487/3143 [16:52<04:39,  2.34it/s] 78%|███████▊  | 2441/3145 [16:46<04:59,  2.35it/s] 79%|███████▊  | 2469/3145 [16:47<04:49,  2.34it/s] 78%|███████▊  | 2442/3145 [16:47<04:54,  2.39it/s] 79%|███████▉  | 2488/3143 [16:53<04:42,  2.32it/s] 79%|███████▉  | 2497/3145 [16:47<04:33,  2.37it/s] 79%|███████▊  | 2470/3145 [16:47<04:44,  2.37it/s] 79%|███████▉  | 2498/3145 [16:47<04:20,  2.48it/s] 78%|███████▊  | 2443/3145 [16:47<04:51,  2.41it/s] 79%|███████▉  | 2489/3143 [16:53<04:43,  2.31it/s] 79%|███████▊  | 2471/3145 [16:47<04:19,  2.59it/s] 79%|███████▉  | 2499/3145 [16:48<04:19,  2.49it/s] 79%|███████▉  | 2490/3143 [16:54<04:36,  2.36it/s] 78%|███████▊  | 2444/3145 [16:48<04:56,  2.36it/s] 79%|███████▊  | 2472/3145 [16:48<04:21,  2.58it/s] 79%|███████▉  | 2491/3143 [16:54<04:22,  2.48it/s] 79%|███████▉  | 2500/3145 [16:48<04:29,  2.39it/s] 78%|███████▊  | 2445/3145 [16:48<04:58,  2.34it/s] 79%|███████▊  | 2473/3145 [16:48<04:18,  2.60it/s] 80%|███████▉  | 2501/3145 [16:48<04:18,  2.49it/s] 79%|███████▉  | 2492/3143 [16:54<04:28,  2.42it/s] 79%|███████▊  | 2474/3145 [16:49<04:20,  2.58it/s] 78%|███████▊  | 2446/3145 [16:49<05:06,  2.28it/s] 80%|███████▉  | 2502/3145 [16:49<04:10,  2.57it/s] 79%|███████▉  | 2493/3143 [16:55<04:22,  2.48it/s] 79%|███████▊  | 2475/3145 [16:49<04:13,  2.64it/s] 78%|███████▊  | 2447/3145 [16:49<04:52,  2.39it/s] 80%|███████▉  | 2503/3145 [16:49<04:04,  2.62it/s] 79%|███████▉  | 2494/3143 [16:55<04:19,  2.50it/s] 79%|███████▊  | 2476/3145 [16:49<04:14,  2.63it/s] 78%|███████▊  | 2448/3145 [16:49<05:01,  2.31it/s] 80%|███████▉  | 2504/3145 [16:50<04:09,  2.56it/s] 79%|███████▉  | 2495/3143 [16:56<04:27,  2.43it/s] 79%|███████▉  | 2477/3145 [16:50<04:19,  2.58it/s] 78%|███████▊  | 2449/3145 [16:50<05:04,  2.28it/s] 80%|███████▉  | 2505/3145 [16:50<04:26,  2.41it/s] 79%|███████▉  | 2496/3143 [16:56<04:22,  2.46it/s] 79%|███████▉  | 2478/3145 [16:50<04:20,  2.56it/s] 78%|███████▊  | 2450/3145 [16:50<04:51,  2.38it/s] 79%|███████▉  | 2497/3143 [16:56<03:49,  2.81it/s] 79%|███████▉  | 2479/3145 [16:50<04:15,  2.61it/s] 80%|███████▉  | 2506/3145 [16:50<04:28,  2.38it/s] 78%|███████▊  | 2451/3145 [16:51<04:40,  2.47it/s] 79%|███████▉  | 2498/3143 [16:57<04:05,  2.63it/s] 80%|███████▉  | 2507/3145 [16:51<04:24,  2.42it/s] 79%|███████▉  | 2480/3145 [16:51<04:24,  2.51it/s] 78%|███████▊  | 2452/3145 [16:51<04:40,  2.47it/s] 80%|███████▉  | 2499/3143 [16:57<04:15,  2.52it/s] 80%|███████▉  | 2508/3145 [16:51<04:28,  2.37it/s] 79%|███████▉  | 2481/3145 [16:51<04:33,  2.43it/s] 80%|███████▉  | 2500/3143 [16:57<03:40,  2.92it/s] 78%|███████▊  | 2453/3145 [16:51<04:36,  2.51it/s] 79%|███████▉  | 2482/3145 [16:52<04:25,  2.50it/s] 80%|███████▉  | 2509/3145 [16:52<04:32,  2.33it/s] 80%|███████▉  | 2501/3143 [16:58<03:59,  2.69it/s] 78%|███████▊  | 2454/3145 [16:52<04:42,  2.45it/s] 79%|███████▉  | 2483/3145 [16:52<04:16,  2.58it/s] 80%|███████▉  | 2510/3145 [16:52<04:21,  2.42it/s] 78%|███████▊  | 2455/3145 [16:52<04:41,  2.45it/s] 80%|███████▉  | 2502/3143 [16:58<04:14,  2.52it/s] 79%|███████▉  | 2484/3145 [16:52<04:17,  2.57it/s] 80%|███████▉  | 2511/3145 [16:53<04:17,  2.46it/s] 78%|███████▊  | 2456/3145 [16:53<04:35,  2.50it/s] 80%|███████▉  | 2503/3143 [16:59<04:07,  2.59it/s] 79%|███████▉  | 2485/3145 [16:53<04:11,  2.63it/s] 80%|███████▉  | 2512/3145 [16:53<04:16,  2.47it/s] 78%|███████▊  | 2457/3145 [16:53<04:32,  2.53it/s] 80%|███████▉  | 2504/3143 [16:59<04:12,  2.53it/s] 79%|███████▉  | 2486/3145 [16:53<04:20,  2.53it/s] 80%|███████▉  | 2513/3145 [16:53<04:12,  2.50it/s] 78%|███████▊  | 2458/3145 [16:53<04:25,  2.59it/s] 80%|███████▉  | 2505/3143 [16:59<04:05,  2.60it/s] 79%|███████▉  | 2487/3145 [16:54<04:17,  2.55it/s] 80%|███████▉  | 2514/3145 [16:54<04:11,  2.51it/s] 78%|███████▊  | 2459/3145 [16:54<04:36,  2.48it/s] 80%|███████▉  | 2506/3143 [17:00<04:29,  2.37it/s] 79%|███████▉  | 2488/3145 [16:54<04:15,  2.57it/s] 80%|███████▉  | 2515/3145 [16:54<04:11,  2.51it/s] 78%|███████▊  | 2460/3145 [16:54<04:39,  2.45it/s] 79%|███████▉  | 2489/3145 [16:54<04:07,  2.65it/s] 80%|███████▉  | 2507/3143 [17:00<04:37,  2.29it/s] 80%|████████  | 2516/3145 [16:55<04:16,  2.45it/s] 78%|███████▊  | 2461/3145 [16:55<04:44,  2.40it/s] 80%|███████▉  | 2508/3143 [17:01<04:26,  2.38it/s] 79%|███████▉  | 2490/3145 [16:55<04:14,  2.57it/s] 80%|████████  | 2517/3145 [16:55<04:22,  2.39it/s] 78%|███████▊  | 2462/3145 [16:55<04:41,  2.43it/s] 79%|███████▉  | 2491/3145 [16:55<03:59,  2.73it/s] 80%|███████▉  | 2509/3143 [17:01<04:29,  2.35it/s] 79%|███████▉  | 2492/3145 [16:55<03:31,  3.09it/s] 80%|████████  | 2518/3145 [16:55<04:24,  2.37it/s] 78%|███████▊  | 2463/3145 [16:55<04:46,  2.38it/s] 79%|███████▉  | 2493/3145 [16:56<03:11,  3.40it/s] 80%|███████▉  | 2510/3143 [17:01<04:22,  2.41it/s] 80%|████████  | 2519/3145 [16:56<04:20,  2.40it/s] 79%|███████▉  | 2494/3145 [16:56<03:30,  3.09it/s] 78%|███████▊  | 2464/3145 [16:56<04:49,  2.35it/s] 80%|███████▉  | 2511/3143 [17:02<04:17,  2.45it/s] 79%|███████▉  | 2495/3145 [16:56<03:18,  3.28it/s] 80%|████████  | 2520/3145 [16:56<04:28,  2.33it/s] 78%|███████▊  | 2465/3145 [16:56<04:45,  2.38it/s] 80%|███████▉  | 2512/3143 [17:02<04:09,  2.53it/s] 79%|███████▉  | 2496/3145 [16:56<03:18,  3.28it/s] 80%|████████  | 2521/3145 [16:57<04:16,  2.43it/s] 78%|███████▊  | 2466/3145 [16:57<04:42,  2.40it/s] 80%|███████▉  | 2513/3143 [17:03<04:09,  2.53it/s] 80%|████████  | 2522/3145 [16:57<04:10,  2.49it/s] 79%|███████▉  | 2497/3145 [16:57<04:10,  2.58it/s] 78%|███████▊  | 2467/3145 [16:57<04:36,  2.45it/s] 80%|███████▉  | 2514/3143 [17:03<04:23,  2.39it/s] 79%|███████▉  | 2498/3145 [16:57<04:13,  2.56it/s] 80%|████████  | 2523/3145 [16:57<04:20,  2.39it/s] 78%|███████▊  | 2468/3145 [16:57<04:29,  2.52it/s] 80%|████████  | 2515/3143 [17:03<04:14,  2.47it/s] 80%|████████  | 2524/3145 [16:58<04:10,  2.48it/s] 79%|███████▉  | 2499/3145 [16:58<04:17,  2.51it/s] 79%|███████▊  | 2469/3145 [16:58<04:32,  2.48it/s] 80%|████████  | 2516/3143 [17:04<04:13,  2.48it/s] 80%|████████  | 2525/3145 [16:58<04:09,  2.48it/s] 79%|███████▉  | 2500/3145 [16:58<04:11,  2.56it/s] 79%|███████▊  | 2470/3145 [16:58<04:32,  2.48it/s] 80%|████████  | 2517/3143 [17:04<04:20,  2.41it/s] 80%|███████▉  | 2501/3145 [16:59<04:05,  2.62it/s] 80%|████████  | 2526/3145 [16:59<04:03,  2.54it/s] 79%|███████▊  | 2471/3145 [16:59<04:24,  2.55it/s] 80%|████████  | 2518/3143 [17:05<04:16,  2.44it/s] 80%|████████  | 2527/3145 [16:59<03:59,  2.58it/s] 80%|███████▉  | 2502/3145 [16:59<04:13,  2.54it/s] 79%|███████▊  | 2472/3145 [16:59<04:29,  2.50it/s] 80%|████████  | 2519/3143 [17:05<04:07,  2.52it/s] 80%|████████  | 2528/3145 [16:59<03:34,  2.87it/s] 80%|███████▉  | 2503/3145 [16:59<04:25,  2.42it/s] 79%|███████▊  | 2473/3145 [16:59<04:31,  2.47it/s] 80%|████████  | 2520/3143 [17:06<04:14,  2.45it/s] 80%|████████  | 2529/3145 [17:00<03:50,  2.68it/s] 80%|███████▉  | 2504/3145 [17:00<04:27,  2.39it/s] 79%|███████▊  | 2474/3145 [17:00<04:32,  2.47it/s] 80%|████████  | 2521/3143 [17:06<04:21,  2.38it/s] 80%|████████  | 2530/3145 [17:00<04:01,  2.55it/s] 80%|███████▉  | 2505/3145 [17:00<04:25,  2.41it/s] 79%|███████▊  | 2475/3145 [17:00<04:34,  2.44it/s] 80%|████████  | 2531/3145 [17:00<03:52,  2.64it/s] 80%|████████  | 2522/3143 [17:06<04:31,  2.29it/s] 81%|████████  | 2532/3145 [17:01<03:24,  3.00it/s] 80%|███████▉  | 2506/3145 [17:01<04:21,  2.44it/s] 79%|███████▊  | 2476/3145 [17:01<04:39,  2.39it/s] 80%|████████  | 2523/3143 [17:07<04:21,  2.37it/s] 81%|████████  | 2533/3145 [17:01<03:35,  2.85it/s] 80%|███████▉  | 2507/3145 [17:01<04:18,  2.47it/s] 79%|███████▉  | 2477/3145 [17:01<04:37,  2.40it/s] 80%|████████  | 2524/3143 [17:07<04:30,  2.29it/s] 79%|███████▉  | 2478/3145 [17:01<04:02,  2.76it/s] 81%|████████  | 2534/3145 [17:01<03:41,  2.76it/s] 80%|███████▉  | 2508/3145 [17:02<04:36,  2.30it/s] 81%|████████  | 2535/3145 [17:02<03:46,  2.70it/s] 80%|████████  | 2525/3143 [17:08<04:31,  2.28it/s] 79%|███████▉  | 2479/3145 [17:02<04:20,  2.55it/s] 80%|███████▉  | 2509/3145 [17:02<04:44,  2.24it/s] 81%|████████  | 2536/3145 [17:02<03:59,  2.54it/s] 80%|████████  | 2526/3143 [17:08<04:32,  2.26it/s] 79%|███████▉  | 2480/3145 [17:02<04:31,  2.45it/s] 80%|███████▉  | 2510/3145 [17:03<04:39,  2.27it/s] 80%|████████  | 2527/3143 [17:09<04:20,  2.37it/s] 81%|████████  | 2537/3145 [17:03<03:57,  2.56it/s] 79%|███████▉  | 2481/3145 [17:03<04:29,  2.47it/s] 80%|███████▉  | 2511/3145 [17:03<04:32,  2.33it/s] 80%|████████  | 2528/3143 [17:09<04:15,  2.41it/s] 81%|████████  | 2538/3145 [17:03<04:01,  2.51it/s] 79%|███████▉  | 2482/3145 [17:03<04:39,  2.37it/s] 80%|███████▉  | 2512/3145 [17:03<04:31,  2.33it/s] 80%|████████  | 2529/3143 [17:09<04:10,  2.45it/s] 81%|████████  | 2539/3145 [17:04<04:05,  2.47it/s] 79%|███████▉  | 2483/3145 [17:04<04:44,  2.32it/s] 80%|███████▉  | 2513/3145 [17:04<04:15,  2.48it/s] 80%|████████  | 2530/3143 [17:10<03:52,  2.64it/s] 81%|████████  | 2540/3145 [17:04<04:02,  2.50it/s] 79%|███████▉  | 2484/3145 [17:04<04:22,  2.52it/s] 80%|███████▉  | 2514/3145 [17:04<04:24,  2.39it/s] 81%|████████  | 2531/3143 [17:10<03:57,  2.57it/s] 81%|████████  | 2541/3145 [17:04<03:56,  2.56it/s] 79%|███████▉  | 2485/3145 [17:04<04:22,  2.52it/s] 81%|████████  | 2532/3143 [17:10<03:43,  2.74it/s] 80%|███████▉  | 2515/3145 [17:05<04:19,  2.43it/s] 81%|████████  | 2542/3145 [17:05<04:06,  2.45it/s] 79%|███████▉  | 2486/3145 [17:05<04:24,  2.50it/s] 81%|████████  | 2533/3143 [17:11<03:49,  2.66it/s] 80%|████████  | 2516/3145 [17:05<04:15,  2.46it/s] 81%|████████  | 2543/3145 [17:05<03:57,  2.54it/s] 79%|███████▉  | 2487/3145 [17:05<04:25,  2.47it/s] 81%|████████  | 2534/3143 [17:11<03:50,  2.65it/s] 80%|████████  | 2517/3145 [17:05<04:15,  2.46it/s] 81%|████████  | 2544/3145 [17:05<03:54,  2.56it/s] 79%|███████▉  | 2488/3145 [17:06<04:25,  2.47it/s] 81%|████████  | 2535/3143 [17:12<04:03,  2.49it/s] 80%|████████  | 2518/3145 [17:06<04:12,  2.48it/s] 81%|████████  | 2545/3145 [17:06<03:55,  2.54it/s] 79%|███████▉  | 2489/3145 [17:06<04:42,  2.32it/s] 80%|████████  | 2519/3145 [17:06<04:03,  2.57it/s] 81%|████████  | 2536/3143 [17:12<04:02,  2.50it/s] 81%|████████  | 2546/3145 [17:06<04:06,  2.43it/s] 80%|████████  | 2520/3145 [17:06<03:46,  2.76it/s] 79%|███████▉  | 2490/3145 [17:06<04:37,  2.36it/s] 81%|████████  | 2537/3143 [17:12<03:58,  2.54it/s] 81%|████████  | 2547/3145 [17:07<04:03,  2.45it/s] 80%|████████  | 2521/3145 [17:07<03:43,  2.79it/s] 81%|████████  | 2538/3143 [17:13<03:53,  2.59it/s] 79%|███████▉  | 2491/3145 [17:07<04:42,  2.31it/s] 80%|████████  | 2522/3145 [17:07<03:41,  2.81it/s] 81%|████████  | 2548/3145 [17:07<04:07,  2.41it/s] 81%|████████  | 2539/3143 [17:13<03:54,  2.58it/s] 79%|███████▉  | 2492/3145 [17:07<04:32,  2.40it/s] 81%|████████  | 2549/3145 [17:08<03:57,  2.50it/s] 80%|████████  | 2523/3145 [17:08<03:56,  2.64it/s] 81%|████████  | 2540/3143 [17:14<03:57,  2.53it/s] 79%|███████▉  | 2493/3145 [17:08<04:31,  2.40it/s] 81%|████████  | 2550/3145 [17:08<03:59,  2.48it/s] 80%|████████  | 2524/3145 [17:08<04:08,  2.50it/s] 81%|████████  | 2541/3143 [17:14<03:59,  2.51it/s] 79%|███████▉  | 2494/3145 [17:08<04:26,  2.44it/s] 81%|████████  | 2551/3145 [17:08<03:31,  2.81it/s] 80%|████████  | 2525/3145 [17:08<04:14,  2.44it/s] 79%|███████▉  | 2495/3145 [17:08<04:23,  2.47it/s] 81%|████████  | 2552/3145 [17:09<03:30,  2.81it/s] 81%|████████  | 2542/3143 [17:14<04:10,  2.40it/s] 80%|████████  | 2526/3145 [17:09<04:14,  2.43it/s] 81%|████████  | 2543/3143 [17:15<03:50,  2.60it/s] 79%|███████▉  | 2496/3145 [17:09<04:20,  2.49it/s] 81%|████████  | 2553/3145 [17:09<03:46,  2.61it/s] 80%|████████  | 2527/3145 [17:09<04:20,  2.37it/s] 81%|████████  | 2544/3143 [17:15<04:00,  2.49it/s] 79%|███████▉  | 2497/3145 [17:09<04:25,  2.44it/s] 81%|████████  | 2554/3145 [17:09<03:54,  2.52it/s] 81%|████████  | 2545/3143 [17:16<04:06,  2.42it/s] 80%|████████  | 2528/3145 [17:10<04:30,  2.28it/s] 81%|████████  | 2555/3145 [17:10<03:51,  2.55it/s] 79%|███████▉  | 2498/3145 [17:10<04:54,  2.20it/s] 81%|████████▏ | 2556/3145 [17:10<03:40,  2.68it/s] 81%|████████  | 2546/3143 [17:16<04:04,  2.44it/s] 80%|████████  | 2529/3145 [17:10<04:38,  2.22it/s] 79%|███████▉  | 2499/3145 [17:10<04:43,  2.28it/s] 81%|████████▏ | 2557/3145 [17:10<03:37,  2.70it/s] 81%|████████  | 2547/3143 [17:16<04:09,  2.39it/s] 80%|████████  | 2530/3145 [17:11<04:20,  2.36it/s] 79%|███████▉  | 2500/3145 [17:11<04:43,  2.28it/s] 81%|████████▏ | 2558/3145 [17:11<03:45,  2.60it/s] 81%|████████  | 2548/3143 [17:17<04:05,  2.43it/s] 80%|████████  | 2531/3145 [17:11<04:22,  2.34it/s] 80%|███████▉  | 2501/3145 [17:11<04:43,  2.27it/s] 81%|████████▏ | 2559/3145 [17:11<03:46,  2.59it/s] 81%|████████  | 2549/3143 [17:17<04:02,  2.45it/s] 81%|████████  | 2532/3145 [17:11<04:14,  2.41it/s] 80%|███████▉  | 2502/3145 [17:12<04:40,  2.29it/s] 81%|████████▏ | 2560/3145 [17:12<03:41,  2.64it/s] 81%|████████  | 2550/3143 [17:18<03:53,  2.54it/s] 81%|████████  | 2533/3145 [17:12<04:10,  2.44it/s] 80%|███████▉  | 2503/3145 [17:12<04:38,  2.31it/s] 81%|████████▏ | 2561/3145 [17:12<03:49,  2.55it/s] 81%|████████  | 2551/3143 [17:18<03:51,  2.56it/s] 81%|████████  | 2534/3145 [17:12<04:04,  2.50it/s] 80%|███████▉  | 2504/3145 [17:12<04:29,  2.38it/s] 81%|████████▏ | 2562/3145 [17:13<03:56,  2.47it/s] 81%|████████  | 2535/3145 [17:13<04:07,  2.46it/s] 81%|████████  | 2552/3143 [17:19<04:13,  2.33it/s] 80%|███████▉  | 2505/3145 [17:13<03:59,  2.67it/s] 81%|████████▏ | 2563/3145 [17:13<03:56,  2.46it/s] 81%|████████  | 2536/3145 [17:13<04:05,  2.48it/s] 81%|████████  | 2553/3143 [17:19<04:24,  2.23it/s] 80%|███████▉  | 2506/3145 [17:13<04:11,  2.54it/s] 82%|████████▏ | 2564/3145 [17:13<03:48,  2.54it/s] 81%|████████  | 2537/3145 [17:13<03:57,  2.56it/s] 80%|███████▉  | 2507/3145 [17:14<04:22,  2.43it/s] 81%|████████▏ | 2554/3143 [17:19<04:24,  2.22it/s] 82%|████████▏ | 2565/3145 [17:14<03:42,  2.60it/s] 81%|████████  | 2538/3145 [17:14<04:04,  2.49it/s] 80%|███████▉  | 2508/3145 [17:14<03:51,  2.75it/s] 81%|████████▏ | 2555/3143 [17:20<04:16,  2.29it/s] 82%|████████▏ | 2566/3145 [17:14<03:40,  2.63it/s] 81%|████████  | 2539/3145 [17:14<04:04,  2.48it/s] 80%|███████▉  | 2509/3145 [17:14<03:55,  2.70it/s] 82%|████████▏ | 2567/3145 [17:14<03:38,  2.64it/s] 81%|████████▏ | 2556/3143 [17:20<04:15,  2.30it/s] 81%|████████  | 2540/3145 [17:15<04:02,  2.50it/s] 80%|███████▉  | 2510/3145 [17:15<04:06,  2.57it/s] 82%|████████▏ | 2568/3145 [17:15<03:50,  2.50it/s] 81%|████████▏ | 2557/3143 [17:21<04:16,  2.28it/s] 81%|████████  | 2541/3145 [17:15<03:59,  2.53it/s] 80%|███████▉  | 2511/3145 [17:15<04:11,  2.52it/s] 81%|████████▏ | 2558/3143 [17:21<03:39,  2.66it/s] 82%|████████▏ | 2569/3145 [17:15<03:56,  2.44it/s] 81%|████████  | 2542/3145 [17:15<03:53,  2.58it/s] 80%|███████▉  | 2512/3145 [17:15<04:08,  2.55it/s] 81%|████████▏ | 2559/3143 [17:21<03:43,  2.61it/s] 81%|████████  | 2543/3145 [17:16<03:48,  2.64it/s] 82%|████████▏ | 2570/3145 [17:16<04:00,  2.39it/s] 81%|████████▏ | 2560/3143 [17:22<03:43,  2.60it/s] 80%|███████▉  | 2513/3145 [17:16<04:17,  2.45it/s] 81%|████████  | 2544/3145 [17:16<03:46,  2.65it/s] 82%|████████▏ | 2571/3145 [17:16<03:58,  2.41it/s] 81%|████████▏ | 2561/3143 [17:22<03:46,  2.56it/s] 80%|███████▉  | 2514/3145 [17:16<04:24,  2.39it/s] 81%|████████  | 2545/3145 [17:16<03:49,  2.62it/s] 82%|████████▏ | 2572/3145 [17:17<04:00,  2.38it/s] 82%|████████▏ | 2562/3143 [17:23<03:43,  2.60it/s] 80%|███████▉  | 2515/3145 [17:17<04:41,  2.24it/s] 81%|████████  | 2546/3145 [17:17<03:57,  2.52it/s] 82%|████████▏ | 2573/3145 [17:17<03:50,  2.48it/s] 82%|████████▏ | 2563/3143 [17:23<03:45,  2.57it/s] 80%|████████  | 2516/3145 [17:17<04:30,  2.33it/s] 81%|████████  | 2547/3145 [17:17<04:04,  2.44it/s] 82%|████████▏ | 2574/3145 [17:17<03:57,  2.40it/s] 82%|████████▏ | 2564/3143 [17:23<03:46,  2.55it/s] 80%|████████  | 2517/3145 [17:18<04:25,  2.37it/s] 82%|████████▏ | 2575/3145 [17:18<03:53,  2.44it/s] 81%|████████  | 2548/3145 [17:18<04:09,  2.40it/s] 82%|████████▏ | 2565/3143 [17:24<03:53,  2.47it/s] 82%|████████▏ | 2576/3145 [17:18<03:25,  2.76it/s] 80%|████████  | 2518/3145 [17:18<04:42,  2.22it/s] 81%|████████  | 2549/3145 [17:18<04:12,  2.36it/s] 82%|████████▏ | 2577/3145 [17:18<03:09,  2.99it/s] 82%|████████▏ | 2566/3143 [17:24<03:52,  2.48it/s] 80%|████████  | 2519/3145 [17:19<04:26,  2.35it/s] 81%|████████  | 2550/3145 [17:19<04:02,  2.45it/s] 82%|████████▏ | 2578/3145 [17:19<03:13,  2.93it/s] 82%|████████▏ | 2567/3143 [17:25<03:49,  2.51it/s] 80%|████████  | 2520/3145 [17:19<04:22,  2.38it/s] 81%|████████  | 2551/3145 [17:19<04:06,  2.41it/s] 82%|████████▏ | 2579/3145 [17:19<03:26,  2.74it/s] 82%|████████▏ | 2568/3143 [17:25<03:49,  2.50it/s] 81%|████████  | 2552/3145 [17:19<04:06,  2.40it/s] 82%|████████▏ | 2580/3145 [17:19<03:37,  2.60it/s] 82%|████████▏ | 2569/3143 [17:25<03:56,  2.43it/s] 80%|████████  | 2521/3145 [17:20<04:52,  2.13it/s] 82%|████████▏ | 2570/3143 [17:26<03:29,  2.74it/s] 81%|████████  | 2553/3145 [17:20<04:10,  2.36it/s] 82%|████████▏ | 2581/3145 [17:20<03:40,  2.56it/s] 80%|████████  | 2522/3145 [17:20<04:57,  2.10it/s] 82%|████████▏ | 2571/3143 [17:26<03:46,  2.53it/s] 81%|████████  | 2554/3145 [17:20<04:10,  2.36it/s] 82%|████████▏ | 2582/3145 [17:20<03:49,  2.46it/s] 80%|████████  | 2523/3145 [17:20<04:49,  2.15it/s] 82%|████████▏ | 2572/3143 [17:27<03:46,  2.52it/s] 81%|████████  | 2555/3145 [17:21<04:03,  2.42it/s] 82%|████████▏ | 2583/3145 [17:21<03:46,  2.48it/s] 82%|████████▏ | 2573/3143 [17:27<03:18,  2.87it/s] 80%|████████  | 2524/3145 [17:21<04:42,  2.20it/s] 81%|████████▏ | 2556/3145 [17:21<03:53,  2.52it/s] 82%|████████▏ | 2584/3145 [17:21<03:44,  2.50it/s] 82%|████████▏ | 2574/3143 [17:27<03:18,  2.87it/s] 81%|████████▏ | 2557/3145 [17:21<03:31,  2.78it/s] 80%|████████  | 2525/3145 [17:21<04:41,  2.20it/s] 82%|████████▏ | 2585/3145 [17:21<03:39,  2.55it/s] 82%|████████▏ | 2575/3143 [17:27<03:23,  2.79it/s] 81%|████████▏ | 2558/3145 [17:22<03:40,  2.67it/s] 80%|████████  | 2526/3145 [17:22<04:39,  2.21it/s] 82%|████████▏ | 2586/3145 [17:22<03:34,  2.61it/s] 82%|████████▏ | 2576/3143 [17:28<03:36,  2.61it/s] 81%|████████▏ | 2559/3145 [17:22<03:44,  2.61it/s] 80%|████████  | 2527/3145 [17:22<04:30,  2.28it/s] 82%|████████▏ | 2587/3145 [17:22<03:39,  2.54it/s] 82%|████████▏ | 2577/3143 [17:28<03:39,  2.58it/s] 81%|████████▏ | 2560/3145 [17:23<03:44,  2.60it/s] 80%|████████  | 2528/3145 [17:23<04:31,  2.27it/s] 82%|████████▏ | 2588/3145 [17:23<03:39,  2.54it/s] 82%|████████▏ | 2578/3143 [17:29<03:46,  2.49it/s] 81%|████████▏ | 2561/3145 [17:23<03:56,  2.47it/s] 80%|████████  | 2529/3145 [17:23<04:20,  2.36it/s] 82%|████████▏ | 2589/3145 [17:23<03:35,  2.58it/s] 82%|████████▏ | 2579/3143 [17:29<03:51,  2.44it/s] 81%|████████▏ | 2562/3145 [17:23<03:58,  2.45it/s] 82%|████████▏ | 2590/3145 [17:23<03:31,  2.63it/s] 80%|████████  | 2530/3145 [17:23<04:23,  2.34it/s] 82%|████████▏ | 2580/3143 [17:30<03:43,  2.51it/s] 81%|████████▏ | 2563/3145 [17:24<03:54,  2.49it/s] 82%|████████▏ | 2591/3145 [17:24<03:35,  2.57it/s] 80%|████████  | 2531/3145 [17:24<04:18,  2.37it/s] 82%|████████▏ | 2581/3143 [17:30<03:41,  2.54it/s] 82%|████████▏ | 2564/3145 [17:24<03:48,  2.55it/s] 82%|████████▏ | 2592/3145 [17:24<03:31,  2.62it/s] 81%|████████  | 2532/3145 [17:24<04:15,  2.40it/s] 82%|████████▏ | 2582/3143 [17:30<03:39,  2.55it/s] 82%|████████▏ | 2593/3145 [17:25<03:34,  2.57it/s] 82%|████████▏ | 2565/3145 [17:25<03:55,  2.46it/s] 81%|████████  | 2533/3145 [17:25<04:13,  2.42it/s] 82%|████████▏ | 2583/3143 [17:31<03:41,  2.52it/s] 82%|████████▏ | 2594/3145 [17:25<03:35,  2.56it/s] 82%|████████▏ | 2566/3145 [17:25<04:02,  2.39it/s] 81%|████████  | 2534/3145 [17:25<04:17,  2.38it/s] 82%|████████▏ | 2584/3143 [17:31<03:41,  2.53it/s] 83%|████████▎ | 2595/3145 [17:25<03:35,  2.55it/s] 82%|████████▏ | 2567/3145 [17:25<04:05,  2.35it/s] 81%|████████  | 2535/3145 [17:26<04:21,  2.34it/s] 83%|████████▎ | 2596/3145 [17:26<03:28,  2.63it/s] 82%|████████▏ | 2585/3143 [17:32<03:56,  2.36it/s] 82%|████████▏ | 2568/3145 [17:26<04:04,  2.36it/s] 81%|████████  | 2536/3145 [17:26<04:35,  2.21it/s] 82%|████████▏ | 2586/3143 [17:32<03:45,  2.47it/s] 83%|████████▎ | 2597/3145 [17:26<03:40,  2.49it/s] 82%|████████▏ | 2569/3145 [17:26<03:55,  2.45it/s] 81%|████████  | 2537/3145 [17:26<04:25,  2.29it/s] 82%|████████▏ | 2587/3143 [17:32<03:41,  2.51it/s] 83%|████████▎ | 2598/3145 [17:27<03:41,  2.47it/s] 82%|████████▏ | 2570/3145 [17:27<04:22,  2.19it/s] 81%|████████  | 2538/3145 [17:27<04:12,  2.40it/s] 82%|████████▏ | 2588/3143 [17:33<03:40,  2.51it/s] 83%|████████▎ | 2599/3145 [17:27<03:46,  2.41it/s] 82%|████████▏ | 2571/3145 [17:27<04:12,  2.27it/s] 81%|████████  | 2539/3145 [17:27<04:15,  2.37it/s] 82%|████████▏ | 2589/3143 [17:33<03:42,  2.49it/s] 83%|████████▎ | 2600/3145 [17:27<03:29,  2.61it/s] 82%|████████▏ | 2572/3145 [17:28<03:56,  2.43it/s] 81%|████████  | 2540/3145 [17:28<04:09,  2.42it/s] 82%|████████▏ | 2590/3143 [17:34<03:41,  2.50it/s] 83%|████████▎ | 2601/3145 [17:28<03:41,  2.46it/s] 81%|████████  | 2541/3145 [17:28<04:02,  2.49it/s] 82%|████████▏ | 2591/3143 [17:34<03:35,  2.56it/s] 82%|████████▏ | 2573/3145 [17:28<04:12,  2.26it/s] 83%|████████▎ | 2602/3145 [17:28<03:48,  2.38it/s] 81%|████████  | 2542/3145 [17:28<03:28,  2.89it/s] 82%|████████▏ | 2592/3143 [17:34<03:31,  2.60it/s] 82%|████████▏ | 2574/3145 [17:28<04:01,  2.37it/s] 83%|████████▎ | 2603/3145 [17:29<03:42,  2.43it/s] 81%|████████  | 2543/3145 [17:29<03:41,  2.72it/s] 83%|████████▎ | 2593/3143 [17:35<03:27,  2.65it/s] 82%|████████▏ | 2575/3145 [17:29<04:02,  2.35it/s] 83%|████████▎ | 2604/3145 [17:29<03:35,  2.51it/s] 81%|████████  | 2544/3145 [17:29<03:45,  2.66it/s] 83%|████████▎ | 2594/3143 [17:35<03:28,  2.63it/s] 83%|████████▎ | 2605/3145 [17:29<03:16,  2.74it/s] 82%|████████▏ | 2576/3145 [17:29<03:57,  2.40it/s] 81%|████████  | 2545/3145 [17:30<04:03,  2.46it/s] 83%|████████▎ | 2595/3143 [17:35<03:27,  2.64it/s] 83%|████████▎ | 2606/3145 [17:30<03:13,  2.79it/s] 82%|████████▏ | 2577/3145 [17:30<03:58,  2.38it/s] 83%|████████▎ | 2596/3143 [17:36<03:27,  2.64it/s] 81%|████████  | 2546/3145 [17:30<04:09,  2.40it/s] 83%|████████▎ | 2607/3145 [17:30<03:13,  2.78it/s] 82%|████████▏ | 2578/3145 [17:30<03:59,  2.36it/s] 83%|████████▎ | 2597/3143 [17:36<03:31,  2.58it/s] 83%|████████▎ | 2608/3145 [17:30<03:19,  2.69it/s] 81%|████████  | 2547/3145 [17:30<04:18,  2.31it/s] 82%|████████▏ | 2579/3145 [17:31<03:49,  2.46it/s] 83%|████████▎ | 2598/3143 [17:37<03:32,  2.56it/s] 81%|████████  | 2548/3145 [17:31<04:11,  2.37it/s] 83%|████████▎ | 2609/3145 [17:31<03:33,  2.51it/s] 82%|████████▏ | 2580/3145 [17:31<03:42,  2.54it/s] 83%|████████▎ | 2599/3143 [17:37<03:33,  2.54it/s] 82%|████████▏ | 2581/3145 [17:31<03:31,  2.67it/s] 83%|████████▎ | 2610/3145 [17:31<03:28,  2.57it/s] 81%|████████  | 2549/3145 [17:31<04:09,  2.39it/s] 81%|████████  | 2550/3145 [17:32<03:41,  2.69it/s] 83%|████████▎ | 2600/3143 [17:37<03:33,  2.54it/s] 82%|████████▏ | 2582/3145 [17:32<03:33,  2.64it/s] 83%|████████▎ | 2611/3145 [17:32<03:32,  2.52it/s] 81%|████████  | 2551/3145 [17:32<03:54,  2.54it/s] 83%|████████▎ | 2601/3143 [17:38<03:42,  2.44it/s] 83%|████████▎ | 2612/3145 [17:32<03:26,  2.58it/s] 82%|████████▏ | 2583/3145 [17:32<03:43,  2.52it/s] 81%|████████  | 2552/3145 [17:32<04:03,  2.43it/s] 83%|████████▎ | 2602/3143 [17:38<03:48,  2.37it/s] 83%|████████▎ | 2613/3145 [17:32<03:36,  2.46it/s] 82%|████████▏ | 2584/3145 [17:33<03:51,  2.43it/s] 83%|████████▎ | 2603/3143 [17:39<03:39,  2.46it/s] 83%|████████▎ | 2614/3145 [17:33<03:31,  2.51it/s] 81%|████████  | 2553/3145 [17:33<04:12,  2.34it/s] 82%|████████▏ | 2585/3145 [17:33<03:52,  2.41it/s] 83%|████████▎ | 2604/3143 [17:39<03:43,  2.41it/s] 83%|████████▎ | 2615/3145 [17:33<03:34,  2.47it/s] 82%|████████▏ | 2586/3145 [17:33<03:46,  2.47it/s] 81%|████████  | 2554/3145 [17:33<04:24,  2.24it/s] 83%|████████▎ | 2605/3143 [17:40<03:43,  2.41it/s] 83%|████████▎ | 2616/3145 [17:34<03:33,  2.48it/s] 82%|████████▏ | 2587/3145 [17:34<03:44,  2.48it/s] 81%|████████  | 2555/3145 [17:34<04:24,  2.23it/s] 83%|████████▎ | 2606/3143 [17:40<03:34,  2.51it/s] 83%|████████▎ | 2617/3145 [17:34<03:26,  2.56it/s] 82%|████████▏ | 2588/3145 [17:34<03:53,  2.39it/s] 81%|████████▏ | 2556/3145 [17:34<04:21,  2.25it/s] 83%|████████▎ | 2607/3143 [17:40<03:27,  2.58it/s] 83%|████████▎ | 2618/3145 [17:34<03:24,  2.58it/s] 82%|████████▏ | 2589/3145 [17:35<03:49,  2.43it/s] 83%|████████▎ | 2608/3143 [17:41<03:21,  2.66it/s] 81%|████████▏ | 2557/3145 [17:35<04:25,  2.22it/s] 83%|████████▎ | 2619/3145 [17:35<03:25,  2.55it/s] 82%|████████▏ | 2590/3145 [17:35<04:07,  2.24it/s] 83%|████████▎ | 2609/3143 [17:41<03:27,  2.58it/s] 81%|████████▏ | 2558/3145 [17:35<04:16,  2.29it/s] 83%|████████▎ | 2620/3145 [17:35<03:37,  2.41it/s] 83%|████████▎ | 2610/3143 [17:41<03:39,  2.43it/s] 81%|████████▏ | 2559/3145 [17:36<04:28,  2.18it/s] 82%|████████▏ | 2591/3145 [17:36<04:28,  2.07it/s] 83%|████████▎ | 2621/3145 [17:36<03:40,  2.38it/s] 83%|████████▎ | 2611/3143 [17:42<03:37,  2.44it/s] 81%|████████▏ | 2560/3145 [17:36<04:19,  2.26it/s] 82%|████████▏ | 2592/3145 [17:36<04:20,  2.12it/s] 83%|████████▎ | 2622/3145 [17:36<03:41,  2.36it/s] 81%|████████▏ | 2561/3145 [17:36<04:12,  2.31it/s] 83%|████████▎ | 2612/3143 [17:42<03:46,  2.35it/s] 82%|████████▏ | 2593/3145 [17:37<04:27,  2.07it/s] 83%|████████▎ | 2623/3145 [17:37<03:54,  2.23it/s] 81%|████████▏ | 2562/3145 [17:37<04:07,  2.35it/s] 83%|████████▎ | 2613/3143 [17:43<03:43,  2.37it/s] 82%|████████▏ | 2594/3145 [17:37<04:05,  2.24it/s] 83%|████████▎ | 2624/3145 [17:37<03:57,  2.20it/s] 83%|████████▎ | 2595/3145 [17:37<03:31,  2.60it/s] 81%|████████▏ | 2563/3145 [17:37<04:03,  2.40it/s] 83%|████████▎ | 2614/3143 [17:43<03:49,  2.30it/s] 83%|████████▎ | 2625/3145 [17:38<04:00,  2.16it/s] 83%|████████▎ | 2596/3145 [17:38<03:38,  2.51it/s] 82%|████████▏ | 2564/3145 [17:38<03:58,  2.43it/s] 83%|████████▎ | 2615/3143 [17:44<03:50,  2.29it/s] 83%|████████▎ | 2626/3145 [17:38<03:53,  2.22it/s] 83%|████████▎ | 2597/3145 [17:38<03:45,  2.44it/s] 82%|████████▏ | 2565/3145 [17:38<04:09,  2.32it/s] 83%|████████▎ | 2616/3143 [17:44<03:46,  2.32it/s] 83%|████████▎ | 2598/3145 [17:39<03:45,  2.42it/s] 82%|████████▏ | 2566/3145 [17:39<04:10,  2.31it/s] 84%|████████▎ | 2627/3145 [17:39<04:15,  2.02it/s] 83%|████████▎ | 2617/3143 [17:45<03:45,  2.34it/s] 83%|████████▎ | 2599/3145 [17:39<03:42,  2.45it/s] 84%|████████▎ | 2628/3145 [17:39<04:01,  2.14it/s] 82%|████████▏ | 2567/3145 [17:39<04:14,  2.28it/s] 83%|████████▎ | 2618/3143 [17:45<03:50,  2.28it/s] 83%|████████▎ | 2600/3145 [17:39<03:50,  2.37it/s] 83%|████████▎ | 2619/3143 [17:45<03:44,  2.34it/s] 82%|████████▏ | 2568/3145 [17:39<04:18,  2.24it/s] 84%|████████▎ | 2629/3145 [17:40<04:22,  1.97it/s] 83%|████████▎ | 2620/3143 [17:46<03:34,  2.43it/s] 83%|████████▎ | 2601/3145 [17:40<04:14,  2.14it/s] 82%|████████▏ | 2569/3145 [17:40<04:15,  2.26it/s] 84%|████████▎ | 2630/3145 [17:40<04:13,  2.03it/s] 83%|████████▎ | 2621/3143 [17:46<03:43,  2.33it/s] 82%|████████▏ | 2570/3145 [17:40<04:09,  2.30it/s] 83%|████████▎ | 2602/3145 [17:40<04:08,  2.18it/s] 84%|████████▎ | 2631/3145 [17:40<03:59,  2.15it/s] 83%|████████▎ | 2622/3143 [17:47<03:34,  2.43it/s] 83%|████████▎ | 2603/3145 [17:41<04:06,  2.20it/s] 82%|████████▏ | 2571/3145 [17:41<04:21,  2.19it/s] 84%|████████▎ | 2632/3145 [17:41<03:46,  2.26it/s] 83%|████████▎ | 2623/3143 [17:47<03:33,  2.43it/s] 82%|████████▏ | 2572/3145 [17:41<04:05,  2.33it/s] 83%|████████▎ | 2604/3145 [17:41<04:06,  2.19it/s] 84%|████████▎ | 2633/3145 [17:41<03:40,  2.33it/s] 83%|████████▎ | 2624/3143 [17:47<03:32,  2.44it/s] 82%|████████▏ | 2573/3145 [17:42<04:09,  2.29it/s] 83%|████████▎ | 2605/3145 [17:42<03:59,  2.25it/s] 84%|████████▍ | 2634/3145 [17:42<03:37,  2.35it/s] 84%|████████▎ | 2625/3143 [17:48<03:23,  2.54it/s] 83%|████████▎ | 2606/3145 [17:42<03:53,  2.31it/s] 84%|████████▍ | 2635/3145 [17:42<03:38,  2.34it/s] 82%|████████▏ | 2574/3145 [17:42<04:23,  2.17it/s] 84%|████████▎ | 2626/3143 [17:48<03:33,  2.43it/s] 84%|████████▍ | 2636/3145 [17:42<03:21,  2.53it/s] 83%|████████▎ | 2607/3145 [17:42<03:47,  2.37it/s] 82%|████████▏ | 2575/3145 [17:43<04:27,  2.13it/s] 84%|████████▎ | 2627/3143 [17:49<03:25,  2.51it/s] 83%|████████▎ | 2608/3145 [17:43<03:40,  2.44it/s] 84%|████████▍ | 2637/3145 [17:43<03:25,  2.47it/s] 84%|████████▎ | 2628/3143 [17:49<03:20,  2.57it/s] 82%|████████▏ | 2576/3145 [17:43<04:19,  2.20it/s] 84%|████████▍ | 2638/3145 [17:43<03:18,  2.55it/s] 83%|████████▎ | 2609/3145 [17:43<03:45,  2.38it/s] 82%|████████▏ | 2577/3145 [17:43<04:05,  2.32it/s] 84%|████████▎ | 2629/3143 [17:49<03:38,  2.35it/s] 84%|████████▍ | 2639/3145 [17:44<03:13,  2.61it/s] 83%|████████▎ | 2610/3145 [17:44<03:38,  2.45it/s] 82%|████████▏ | 2578/3145 [17:44<03:53,  2.43it/s] 84%|████████▍ | 2640/3145 [17:44<03:20,  2.51it/s] 84%|████████▎ | 2630/3143 [17:50<03:44,  2.29it/s] 83%|████████▎ | 2611/3145 [17:44<03:43,  2.39it/s] 82%|████████▏ | 2579/3145 [17:44<03:51,  2.44it/s] 84%|████████▍ | 2641/3145 [17:44<03:20,  2.52it/s] 84%|████████▎ | 2631/3143 [17:50<03:39,  2.34it/s] 83%|████████▎ | 2612/3145 [17:45<03:45,  2.36it/s] 82%|████████▏ | 2580/3145 [17:45<03:54,  2.41it/s] 84%|████████▎ | 2632/3143 [17:51<03:11,  2.67it/s] 84%|████████▍ | 2642/3145 [17:45<03:14,  2.59it/s] 83%|████████▎ | 2613/3145 [17:45<03:46,  2.35it/s] 82%|████████▏ | 2581/3145 [17:45<04:01,  2.34it/s] 84%|████████▍ | 2633/3143 [17:51<03:31,  2.41it/s] 84%|████████▍ | 2643/3145 [17:45<03:28,  2.41it/s] 83%|████████▎ | 2614/3145 [17:45<03:50,  2.31it/s] 82%|████████▏ | 2582/3145 [17:45<03:48,  2.46it/s] 84%|████████▍ | 2634/3143 [17:51<03:28,  2.44it/s] 84%|████████▍ | 2644/3145 [17:46<03:33,  2.35it/s] 83%|████████▎ | 2615/3145 [17:46<03:59,  2.21it/s] 82%|████████▏ | 2583/3145 [17:46<03:57,  2.36it/s] 84%|████████▍ | 2635/3143 [17:52<03:36,  2.35it/s] 84%|████████▍ | 2645/3145 [17:46<03:33,  2.34it/s] 82%|████████▏ | 2584/3145 [17:46<03:52,  2.42it/s] 83%|████████▎ | 2616/3145 [17:46<03:55,  2.24it/s] 84%|████████▍ | 2646/3145 [17:46<03:08,  2.65it/s] 84%|████████▍ | 2636/3143 [17:52<03:37,  2.33it/s] 82%|████████▏ | 2585/3145 [17:47<03:40,  2.54it/s] 84%|████████▍ | 2647/3145 [17:47<03:10,  2.61it/s] 83%|████████▎ | 2617/3145 [17:47<03:52,  2.27it/s] 84%|████████▍ | 2637/3143 [17:53<03:32,  2.38it/s] 82%|████████▏ | 2586/3145 [17:47<03:34,  2.61it/s] 84%|████████▍ | 2638/3143 [17:53<03:06,  2.70it/s] 83%|████████▎ | 2618/3145 [17:47<03:40,  2.39it/s] 84%|████████▍ | 2648/3145 [17:47<03:12,  2.58it/s] 82%|████████▏ | 2587/3145 [17:47<03:35,  2.59it/s] 83%|████████▎ | 2619/3145 [17:47<03:24,  2.57it/s] 84%|████████▍ | 2639/3143 [17:53<03:10,  2.64it/s] 84%|████████▍ | 2649/3145 [17:48<03:18,  2.49it/s] 82%|████████▏ | 2588/3145 [17:48<03:33,  2.61it/s] 84%|████████▍ | 2640/3143 [17:54<03:10,  2.64it/s] 83%|████████▎ | 2620/3145 [17:48<03:46,  2.32it/s] 84%|████████▍ | 2650/3145 [17:48<03:22,  2.44it/s] 82%|████████▏ | 2589/3145 [17:48<03:21,  2.76it/s] 84%|████████▍ | 2651/3145 [17:48<02:56,  2.81it/s] 84%|████████▍ | 2641/3143 [17:54<03:17,  2.54it/s] 82%|████████▏ | 2590/3145 [17:48<03:03,  3.03it/s] 83%|████████▎ | 2621/3145 [17:48<03:46,  2.32it/s] 84%|████████▍ | 2652/3145 [17:49<03:10,  2.59it/s] 84%|████████▍ | 2642/3143 [17:55<03:24,  2.45it/s] 83%|████████▎ | 2622/3145 [17:49<03:45,  2.32it/s] 82%|████████▏ | 2591/3145 [17:49<03:44,  2.47it/s] 84%|████████▍ | 2643/3143 [17:55<03:16,  2.54it/s] 84%|████████▍ | 2653/3145 [17:49<03:18,  2.48it/s] 83%|████████▎ | 2623/3145 [17:49<03:27,  2.52it/s] 82%|████████▏ | 2592/3145 [17:49<03:37,  2.55it/s] 84%|████████▍ | 2644/3143 [17:55<02:51,  2.91it/s] 84%|████████▍ | 2654/3145 [17:50<03:16,  2.50it/s] 83%|████████▎ | 2624/3145 [17:50<03:28,  2.50it/s] 82%|████████▏ | 2593/3145 [17:50<03:22,  2.73it/s] 84%|████████▍ | 2645/3143 [17:56<02:56,  2.81it/s] 84%|████████▍ | 2655/3145 [17:50<03:13,  2.53it/s] 83%|████████▎ | 2625/3145 [17:50<03:28,  2.50it/s] 82%|████████▏ | 2594/3145 [17:50<03:32,  2.59it/s] 84%|████████▍ | 2646/3143 [17:56<03:01,  2.74it/s] 84%|████████▍ | 2656/3145 [17:50<03:07,  2.61it/s] 83%|████████▎ | 2626/3145 [17:50<03:30,  2.47it/s] 83%|████████▎ | 2595/3145 [17:50<03:40,  2.50it/s] 84%|████████▍ | 2647/3143 [17:57<03:12,  2.57it/s] 84%|████████▍ | 2657/3145 [17:51<03:15,  2.50it/s] 84%|████████▎ | 2627/3145 [17:51<03:32,  2.44it/s] 84%|████████▍ | 2648/3143 [17:57<02:55,  2.82it/s] 83%|████████▎ | 2596/3145 [17:51<03:46,  2.43it/s] 85%|████████▍ | 2658/3145 [17:51<03:18,  2.45it/s] 84%|████████▎ | 2628/3145 [17:51<03:34,  2.41it/s] 84%|████████▍ | 2649/3143 [17:57<03:07,  2.63it/s] 83%|████████▎ | 2597/3145 [17:51<03:43,  2.46it/s] 85%|████████▍ | 2659/3145 [17:51<02:51,  2.83it/s] 84%|████████▎ | 2629/3145 [17:52<03:26,  2.50it/s] 83%|████████▎ | 2598/3145 [17:52<03:35,  2.53it/s] 85%|████████▍ | 2660/3145 [17:52<03:01,  2.68it/s] 84%|████████▍ | 2650/3143 [17:58<03:35,  2.29it/s] 83%|████████▎ | 2599/3145 [17:52<03:28,  2.62it/s] 84%|████████▎ | 2630/3145 [17:52<03:30,  2.45it/s] 85%|████████▍ | 2661/3145 [17:52<03:02,  2.65it/s] 84%|████████▍ | 2651/3143 [17:58<03:24,  2.40it/s] 83%|████████▎ | 2600/3145 [17:52<03:32,  2.56it/s] 84%|████████▎ | 2631/3145 [17:53<03:33,  2.41it/s] 85%|████████▍ | 2662/3145 [17:53<03:04,  2.61it/s] 84%|████████▍ | 2652/3143 [17:59<03:26,  2.38it/s] 83%|████████▎ | 2601/3145 [17:53<03:29,  2.60it/s] 84%|████████▎ | 2632/3145 [17:53<03:36,  2.37it/s] 85%|████████▍ | 2663/3145 [17:53<03:07,  2.57it/s] 84%|████████▍ | 2653/3143 [17:59<03:29,  2.34it/s] 83%|████████▎ | 2602/3145 [17:53<03:31,  2.57it/s] 84%|████████▎ | 2633/3145 [17:53<03:22,  2.52it/s] 85%|████████▍ | 2664/3145 [17:53<03:08,  2.56it/s] 83%|████████▎ | 2603/3145 [17:54<03:24,  2.65it/s] 84%|████████▍ | 2654/3143 [17:59<03:34,  2.28it/s] 84%|████████▍ | 2634/3145 [17:54<03:20,  2.54it/s] 83%|████████▎ | 2604/3145 [17:54<03:03,  2.95it/s] 85%|████████▍ | 2665/3145 [17:54<03:14,  2.46it/s] 84%|████████▍ | 2655/3143 [18:00<03:28,  2.34it/s] 84%|████████▍ | 2635/3145 [17:54<03:28,  2.44it/s] 83%|████████▎ | 2605/3145 [17:54<02:57,  3.04it/s] 85%|████████▍ | 2666/3145 [17:54<03:17,  2.43it/s] 85%|████████▍ | 2656/3143 [18:00<03:22,  2.41it/s] 84%|████████▍ | 2636/3145 [17:54<03:22,  2.51it/s] 83%|████████▎ | 2606/3145 [17:54<03:02,  2.95it/s] 85%|████████▍ | 2667/3145 [17:55<03:16,  2.43it/s] 85%|████████▍ | 2657/3143 [18:01<03:16,  2.47it/s] 83%|████████▎ | 2607/3145 [17:55<03:07,  2.86it/s] 84%|████████▍ | 2637/3145 [17:55<03:28,  2.43it/s] 85%|████████▍ | 2668/3145 [17:55<03:09,  2.52it/s] 85%|████████▍ | 2658/3143 [18:01<03:20,  2.42it/s] 83%|████████▎ | 2608/3145 [17:55<03:10,  2.82it/s] 84%|████████▍ | 2638/3145 [17:55<03:35,  2.36it/s] 85%|████████▍ | 2669/3145 [17:55<03:05,  2.56it/s] 83%|████████▎ | 2609/3145 [17:55<02:50,  3.15it/s] 85%|████████▍ | 2659/3143 [18:02<03:21,  2.41it/s] 84%|████████▍ | 2639/3145 [17:56<03:24,  2.47it/s] 85%|████████▍ | 2670/3145 [17:56<03:08,  2.52it/s] 83%|████████▎ | 2610/3145 [17:56<03:09,  2.83it/s] 85%|████████▍ | 2660/3143 [18:02<03:13,  2.50it/s] 84%|████████▍ | 2640/3145 [17:56<03:15,  2.59it/s] 85%|████████▍ | 2671/3145 [17:56<03:06,  2.54it/s] 83%|████████▎ | 2611/3145 [17:56<03:21,  2.65it/s] 85%|████████▍ | 2661/3143 [18:02<03:13,  2.49it/s] 84%|████████▍ | 2641/3145 [17:56<03:12,  2.62it/s] 85%|████████▍ | 2672/3145 [17:57<03:06,  2.53it/s] 83%|████████▎ | 2612/3145 [17:57<03:25,  2.59it/s] 85%|████████▍ | 2662/3143 [18:03<03:13,  2.49it/s] 84%|████████▍ | 2642/3145 [17:57<03:13,  2.60it/s] 85%|████████▍ | 2673/3145 [17:57<03:05,  2.55it/s] 84%|████████▍ | 2643/3145 [17:57<03:08,  2.66it/s] 85%|████████▍ | 2663/3143 [18:03<03:17,  2.43it/s] 83%|████████▎ | 2613/3145 [17:57<03:56,  2.25it/s] 85%|████████▌ | 2674/3145 [17:57<03:10,  2.48it/s] 84%|████████▍ | 2644/3145 [17:57<02:50,  2.94it/s] 85%|████████▍ | 2664/3143 [18:03<03:12,  2.48it/s] 83%|████████▎ | 2614/3145 [17:58<03:52,  2.29it/s] 85%|████████▍ | 2665/3143 [18:04<02:48,  2.84it/s] 85%|████████▌ | 2675/3145 [17:58<03:13,  2.43it/s] 84%|████████▍ | 2645/3145 [17:58<03:04,  2.72it/s] 83%|████████▎ | 2615/3145 [17:58<03:40,  2.41it/s] 85%|████████▍ | 2666/3143 [18:04<02:55,  2.72it/s] 85%|████████▌ | 2676/3145 [17:58<03:10,  2.46it/s] 84%|████████▍ | 2646/3145 [17:58<03:17,  2.53it/s] 85%|████████▍ | 2667/3143 [18:04<02:35,  3.06it/s] 83%|████████▎ | 2616/3145 [17:59<03:38,  2.42it/s] 85%|████████▌ | 2677/3145 [17:59<03:14,  2.41it/s] 85%|████████▍ | 2668/3143 [18:05<02:39,  2.97it/s] 84%|████████▍ | 2647/3145 [17:59<03:41,  2.25it/s] 83%|████████▎ | 2617/3145 [17:59<03:34,  2.46it/s] 85%|████████▌ | 2678/3145 [17:59<03:19,  2.35it/s] 85%|████████▍ | 2669/3143 [18:05<02:49,  2.80it/s] 84%|████████▍ | 2648/3145 [17:59<03:26,  2.41it/s] 83%|████████▎ | 2618/3145 [17:59<03:23,  2.60it/s] 85%|████████▌ | 2679/3145 [18:00<03:09,  2.46it/s] 84%|████████▍ | 2649/3145 [18:00<03:19,  2.49it/s] 85%|████████▍ | 2670/3143 [18:06<03:02,  2.60it/s] 83%|████████▎ | 2619/3145 [18:00<03:32,  2.47it/s] 85%|████████▌ | 2680/3145 [18:00<03:11,  2.43it/s] 84%|████████▍ | 2650/3145 [18:00<03:13,  2.56it/s] 83%|████████▎ | 2620/3145 [18:00<03:23,  2.58it/s] 85%|████████▍ | 2671/3143 [18:06<03:11,  2.47it/s] 85%|████████▌ | 2681/3145 [18:00<03:08,  2.46it/s] 84%|████████▍ | 2651/3145 [18:00<03:12,  2.57it/s] 83%|████████▎ | 2621/3145 [18:00<03:22,  2.59it/s] 85%|████████▌ | 2672/3143 [18:06<03:06,  2.53it/s] 84%|████████▍ | 2652/3145 [18:01<03:05,  2.66it/s] 83%|████████▎ | 2622/3145 [18:01<03:16,  2.66it/s] 85%|████████▌ | 2682/3145 [18:01<03:16,  2.35it/s] 85%|████████▌ | 2673/3143 [18:07<03:11,  2.46it/s] 84%|████████▍ | 2653/3145 [18:01<03:01,  2.71it/s] 85%|████████▌ | 2683/3145 [18:01<03:17,  2.34it/s] 83%|████████▎ | 2623/3145 [18:01<03:43,  2.34it/s] 85%|████████▌ | 2674/3143 [18:07<03:15,  2.40it/s] 84%|████████▍ | 2654/3145 [18:02<03:11,  2.57it/s] 85%|████████▌ | 2684/3145 [18:02<03:21,  2.29it/s] 85%|████████▌ | 2675/3143 [18:08<03:10,  2.45it/s] 83%|████████▎ | 2624/3145 [18:02<04:10,  2.08it/s] 84%|████████▍ | 2655/3145 [18:02<03:18,  2.47it/s] 85%|████████▌ | 2685/3145 [18:02<03:10,  2.41it/s] 85%|████████▌ | 2676/3143 [18:08<03:04,  2.53it/s] 84%|████████▍ | 2656/3145 [18:02<03:01,  2.69it/s] 83%|████████▎ | 2625/3145 [18:02<04:03,  2.14it/s] 84%|████████▍ | 2657/3145 [18:02<02:43,  2.99it/s] 85%|████████▌ | 2686/3145 [18:03<03:13,  2.37it/s] 85%|████████▌ | 2677/3143 [18:08<03:08,  2.48it/s] 83%|████████▎ | 2626/3145 [18:03<04:01,  2.15it/s] 85%|████████▍ | 2658/3145 [18:03<02:53,  2.81it/s] 85%|████████▌ | 2687/3145 [18:03<03:14,  2.35it/s] 85%|████████▌ | 2678/3143 [18:09<03:04,  2.52it/s] 84%|████████▎ | 2627/3145 [18:03<03:52,  2.23it/s] 85%|████████▍ | 2659/3145 [18:03<03:03,  2.64it/s] 85%|████████▌ | 2679/3143 [18:09<03:01,  2.56it/s] 85%|████████▌ | 2688/3145 [18:03<03:16,  2.33it/s] 84%|████████▎ | 2628/3145 [18:04<03:50,  2.25it/s] 85%|████████▍ | 2660/3145 [18:04<03:10,  2.54it/s] 86%|████████▌ | 2689/3145 [18:04<03:11,  2.39it/s] 85%|████████▌ | 2680/3143 [18:10<03:07,  2.46it/s] 85%|████████▍ | 2661/3145 [18:04<03:03,  2.63it/s] 86%|████████▌ | 2690/3145 [18:04<03:03,  2.47it/s] 84%|████████▎ | 2629/3145 [18:04<03:51,  2.23it/s] 85%|████████▌ | 2681/3143 [18:10<03:17,  2.34it/s] 85%|████████▍ | 2662/3145 [18:04<03:03,  2.64it/s] 86%|████████▌ | 2691/3145 [18:05<03:08,  2.41it/s] 84%|████████▎ | 2630/3145 [18:05<03:59,  2.15it/s] 85%|████████▌ | 2682/3143 [18:11<03:19,  2.32it/s] 85%|████████▍ | 2663/3145 [18:05<02:59,  2.69it/s] 86%|████████▌ | 2692/3145 [18:05<03:06,  2.43it/s] 85%|████████▍ | 2664/3145 [18:05<02:36,  3.07it/s] 85%|████████▌ | 2683/3143 [18:11<03:09,  2.43it/s] 84%|████████▎ | 2631/3145 [18:05<03:56,  2.17it/s] 86%|████████▌ | 2693/3145 [18:05<02:58,  2.53it/s] 85%|████████▍ | 2665/3145 [18:05<02:46,  2.89it/s] 85%|████████▌ | 2684/3143 [18:11<03:08,  2.43it/s] 84%|████████▎ | 2632/3145 [18:05<03:49,  2.24it/s] 86%|████████▌ | 2694/3145 [18:06<02:51,  2.63it/s] 85%|████████▍ | 2666/3145 [18:06<02:53,  2.77it/s] 85%|████████▌ | 2685/3143 [18:12<03:08,  2.43it/s] 84%|████████▎ | 2633/3145 [18:06<03:42,  2.30it/s] 86%|████████▌ | 2695/3145 [18:06<02:32,  2.94it/s] 85%|████████▍ | 2667/3145 [18:06<02:50,  2.81it/s] 85%|████████▌ | 2686/3143 [18:12<03:00,  2.54it/s] 84%|████████▍ | 2634/3145 [18:06<03:38,  2.34it/s] 86%|████████▌ | 2696/3145 [18:06<02:40,  2.79it/s] 85%|████████▌ | 2687/3143 [18:12<02:57,  2.57it/s] 85%|████████▍ | 2668/3145 [18:07<03:07,  2.54it/s] 84%|████████▍ | 2635/3145 [18:07<03:31,  2.41it/s] 86%|████████▌ | 2697/3145 [18:07<02:48,  2.65it/s] 86%|████████▌ | 2688/3143 [18:13<02:56,  2.58it/s] 84%|████████▍ | 2636/3145 [18:07<03:26,  2.47it/s] 85%|████████▍ | 2669/3145 [18:07<03:15,  2.43it/s] 86%|████████▌ | 2698/3145 [18:07<02:52,  2.59it/s] 86%|████████▌ | 2689/3143 [18:13<02:56,  2.57it/s] 85%|████████▍ | 2670/3145 [18:07<03:07,  2.53it/s] 84%|████████▍ | 2637/3145 [18:07<03:27,  2.45it/s] 86%|████████▌ | 2699/3145 [18:08<02:49,  2.63it/s] 86%|████████▌ | 2690/3143 [18:14<02:58,  2.54it/s] 85%|████████▍ | 2671/3145 [18:08<03:04,  2.57it/s] 84%|████████▍ | 2638/3145 [18:08<03:24,  2.48it/s] 86%|████████▌ | 2700/3145 [18:08<02:53,  2.56it/s] 86%|████████▌ | 2691/3143 [18:14<02:54,  2.59it/s] 85%|████████▍ | 2672/3145 [18:08<02:58,  2.65it/s] 84%|████████▍ | 2639/3145 [18:08<03:29,  2.41it/s] 86%|████████▌ | 2701/3145 [18:08<02:56,  2.52it/s] 85%|████████▍ | 2673/3145 [18:09<02:56,  2.67it/s] 86%|████████▌ | 2692/3143 [18:14<03:01,  2.48it/s] 84%|████████▍ | 2640/3145 [18:09<03:22,  2.50it/s] 86%|████████▌ | 2702/3145 [18:09<03:01,  2.44it/s] 85%|████████▌ | 2674/3145 [18:09<03:07,  2.52it/s] 86%|████████▌ | 2693/3143 [18:15<03:06,  2.41it/s] 84%|████████▍ | 2641/3145 [18:09<03:32,  2.37it/s] 86%|████████▌ | 2703/3145 [18:09<03:05,  2.38it/s] 85%|████████▌ | 2675/3145 [18:09<03:11,  2.45it/s] 86%|████████▌ | 2694/3143 [18:15<03:09,  2.37it/s] 84%|████████▍ | 2642/3145 [18:10<03:22,  2.48it/s] 86%|████████▌ | 2704/3145 [18:10<03:06,  2.37it/s] 85%|████████▌ | 2676/3145 [18:10<02:48,  2.79it/s] 86%|████████▌ | 2695/3143 [18:16<03:11,  2.34it/s] 85%|████████▌ | 2677/3145 [18:10<02:51,  2.73it/s] 84%|████████▍ | 2643/3145 [18:10<03:47,  2.21it/s] 86%|████████▌ | 2705/3145 [18:10<03:11,  2.29it/s] 86%|████████▌ | 2696/3143 [18:16<03:03,  2.43it/s] 85%|████████▌ | 2678/3145 [18:10<02:50,  2.74it/s] 84%|████████▍ | 2644/3145 [18:11<03:52,  2.15it/s] 86%|████████▌ | 2706/3145 [18:11<03:21,  2.18it/s] 86%|████████▌ | 2697/3143 [18:17<03:00,  2.47it/s] 85%|████████▌ | 2679/3145 [18:11<03:02,  2.56it/s] 86%|████████▌ | 2698/3143 [18:17<02:39,  2.78it/s] 84%|████████▍ | 2645/3145 [18:11<03:38,  2.29it/s] 86%|████████▌ | 2707/3145 [18:11<03:12,  2.28it/s] 85%|████████▌ | 2680/3145 [18:11<03:06,  2.49it/s] 86%|████████▌ | 2699/3143 [18:17<02:49,  2.62it/s] 84%|████████▍ | 2646/3145 [18:11<03:41,  2.26it/s] 86%|████████▌ | 2708/3145 [18:11<03:09,  2.31it/s] 85%|████████▌ | 2681/3145 [18:12<03:09,  2.45it/s] 86%|████████▌ | 2700/3143 [18:18<03:02,  2.43it/s] 86%|████████▌ | 2709/3145 [18:12<03:04,  2.37it/s] 84%|████████▍ | 2647/3145 [18:12<03:43,  2.23it/s] 85%|████████▌ | 2682/3145 [18:12<02:59,  2.57it/s] 86%|████████▌ | 2710/3145 [18:12<03:01,  2.39it/s] 86%|████████▌ | 2701/3143 [18:18<03:06,  2.37it/s] 84%|████████▍ | 2648/3145 [18:12<03:35,  2.31it/s] 85%|████████▌ | 2683/3145 [18:12<03:01,  2.54it/s] 86%|████████▌ | 2711/3145 [18:13<03:03,  2.37it/s] 86%|████████▌ | 2702/3143 [18:19<03:05,  2.37it/s] 84%|████████▍ | 2649/3145 [18:13<03:37,  2.28it/s] 85%|████████▌ | 2684/3145 [18:13<03:09,  2.44it/s] 86%|████████▌ | 2712/3145 [18:13<03:03,  2.36it/s] 86%|████████▌ | 2703/3143 [18:19<03:05,  2.37it/s] 84%|████████▍ | 2650/3145 [18:13<03:44,  2.21it/s] 86%|████████▋ | 2713/3145 [18:14<02:59,  2.41it/s] 85%|████████▌ | 2685/3145 [18:14<03:35,  2.14it/s] 86%|████████▌ | 2704/3143 [18:20<03:17,  2.22it/s] 84%|████████▍ | 2651/3145 [18:14<03:48,  2.16it/s] 86%|████████▋ | 2714/3145 [18:14<03:04,  2.33it/s] 85%|████████▌ | 2686/3145 [18:14<03:33,  2.15it/s] 86%|████████▌ | 2705/3143 [18:20<03:06,  2.35it/s] 84%|████████▍ | 2652/3145 [18:14<03:44,  2.20it/s] 86%|████████▋ | 2715/3145 [18:14<02:57,  2.42it/s] 86%|████████▌ | 2706/3143 [18:20<03:01,  2.41it/s] 85%|████████▌ | 2687/3145 [18:15<03:47,  2.02it/s] 84%|████████▍ | 2653/3145 [18:15<03:43,  2.21it/s] 86%|████████▋ | 2716/3145 [18:15<02:55,  2.45it/s] 86%|████████▌ | 2707/3143 [18:21<02:58,  2.44it/s] 85%|████████▌ | 2688/3145 [18:15<03:26,  2.22it/s] 84%|████████▍ | 2654/3145 [18:15<03:49,  2.14it/s] 86%|████████▌ | 2708/3143 [18:21<02:53,  2.51it/s] 86%|████████▋ | 2717/3145 [18:15<02:58,  2.39it/s] 86%|████████▌ | 2689/3145 [18:15<03:18,  2.30it/s] 86%|████████▌ | 2690/3145 [18:16<02:53,  2.62it/s] 86%|████████▋ | 2718/3145 [18:16<02:54,  2.45it/s] 84%|████████▍ | 2655/3145 [18:16<03:56,  2.07it/s] 86%|████████▌ | 2709/3143 [18:22<03:00,  2.40it/s] 86%|████████▌ | 2691/3145 [18:16<02:53,  2.61it/s] 86%|████████▋ | 2719/3145 [18:16<02:57,  2.40it/s] 86%|████████▌ | 2710/3143 [18:22<03:03,  2.36it/s] 84%|████████▍ | 2656/3145 [18:16<03:56,  2.07it/s] 86%|████████▋ | 2711/3143 [18:22<02:42,  2.66it/s] 86%|████████▌ | 2692/3145 [18:16<02:55,  2.58it/s] 86%|████████▋ | 2720/3145 [18:16<02:54,  2.43it/s] 84%|████████▍ | 2657/3145 [18:17<04:02,  2.02it/s] 86%|████████▌ | 2693/3145 [18:17<03:01,  2.49it/s] 86%|████████▋ | 2712/3143 [18:23<02:50,  2.53it/s] 87%|████████▋ | 2721/3145 [18:17<02:51,  2.47it/s] 85%|████████▍ | 2658/3145 [18:17<04:04,  1.99it/s] 87%|████████▋ | 2722/3145 [18:17<02:48,  2.51it/s] 86%|████████▌ | 2694/3145 [18:17<03:02,  2.47it/s] 86%|████████▋ | 2713/3143 [18:23<02:53,  2.47it/s] 86%|████████▋ | 2714/3143 [18:23<02:47,  2.56it/s] 87%|████████▋ | 2723/3145 [18:18<02:47,  2.51it/s] 86%|████████▌ | 2695/3145 [18:18<03:01,  2.48it/s] 85%|████████▍ | 2659/3145 [18:18<04:07,  1.96it/s] 86%|████████▌ | 2696/3145 [18:18<02:54,  2.57it/s] 86%|████████▋ | 2715/3143 [18:24<02:49,  2.52it/s] 87%|████████▋ | 2724/3145 [18:18<02:52,  2.44it/s] 85%|████████▍ | 2660/3145 [18:18<03:50,  2.10it/s] 86%|████████▌ | 2697/3145 [18:18<02:52,  2.60it/s] 86%|████████▋ | 2716/3143 [18:24<02:52,  2.47it/s] 87%|████████▋ | 2725/3145 [18:18<02:56,  2.38it/s] 85%|████████▍ | 2661/3145 [18:18<03:40,  2.19it/s] 86%|████████▌ | 2698/3145 [18:19<02:52,  2.59it/s] 87%|████████▋ | 2726/3145 [18:19<02:46,  2.52it/s] 86%|████████▋ | 2717/3143 [18:25<02:55,  2.42it/s] 85%|████████▍ | 2662/3145 [18:19<03:28,  2.31it/s] 86%|████████▋ | 2718/3143 [18:25<02:36,  2.71it/s] 87%|████████▋ | 2727/3145 [18:19<02:38,  2.63it/s] 86%|████████▌ | 2699/3145 [18:19<02:59,  2.49it/s] 85%|████████▍ | 2663/3145 [18:19<03:18,  2.43it/s] 87%|████████▋ | 2719/3143 [18:25<02:21,  2.99it/s] 87%|████████▋ | 2720/3143 [18:25<02:10,  3.23it/s] 86%|████████▌ | 2700/3145 [18:20<03:05,  2.40it/s] 87%|████████▋ | 2728/3145 [18:20<02:55,  2.38it/s] 85%|████████▍ | 2664/3145 [18:20<03:22,  2.37it/s] 87%|████████▋ | 2721/3143 [18:26<02:01,  3.48it/s] 86%|████████▌ | 2701/3145 [18:20<03:03,  2.43it/s] 87%|████████▋ | 2729/3145 [18:20<02:49,  2.46it/s] 85%|████████▍ | 2665/3145 [18:20<03:15,  2.45it/s] 87%|████████▋ | 2722/3143 [18:26<02:12,  3.19it/s] 86%|████████▌ | 2702/3145 [18:20<03:05,  2.39it/s] 85%|████████▍ | 2666/3145 [18:20<03:14,  2.46it/s] 87%|████████▋ | 2730/3145 [18:21<03:00,  2.30it/s] 87%|████████▋ | 2723/3143 [18:27<02:22,  2.95it/s] 86%|████████▌ | 2703/3145 [18:21<03:07,  2.36it/s] 87%|████████▋ | 2731/3145 [18:21<02:51,  2.42it/s] 85%|████████▍ | 2667/3145 [18:21<03:24,  2.34it/s] 87%|████████▋ | 2724/3143 [18:27<02:31,  2.77it/s] 86%|████████▌ | 2704/3145 [18:21<02:57,  2.48it/s] 87%|████████▋ | 2732/3145 [18:21<02:48,  2.45it/s] 85%|████████▍ | 2668/3145 [18:21<03:25,  2.32it/s] 87%|████████▋ | 2725/3143 [18:27<02:30,  2.77it/s] 86%|████████▌ | 2705/3145 [18:22<02:51,  2.57it/s] 87%|████████▋ | 2733/3145 [18:22<02:52,  2.39it/s] 85%|████████▍ | 2669/3145 [18:22<03:27,  2.29it/s] 87%|████████▋ | 2726/3143 [18:28<02:44,  2.53it/s] 86%|████████▌ | 2706/3145 [18:22<02:50,  2.58it/s] 87%|████████▋ | 2734/3145 [18:22<02:58,  2.30it/s] 85%|████████▍ | 2670/3145 [18:22<03:23,  2.34it/s] 87%|████████▋ | 2727/3143 [18:28<02:49,  2.46it/s] 86%|████████▌ | 2707/3145 [18:22<02:54,  2.51it/s] 85%|████████▍ | 2671/3145 [18:22<02:55,  2.70it/s] 87%|████████▋ | 2735/3145 [18:23<02:53,  2.37it/s] 87%|████████▋ | 2728/3143 [18:29<02:44,  2.52it/s] 86%|████████▌ | 2708/3145 [18:23<02:52,  2.53it/s] 85%|████████▍ | 2672/3145 [18:23<03:00,  2.62it/s] 87%|████████▋ | 2736/3145 [18:23<02:54,  2.35it/s] 87%|████████▋ | 2729/3143 [18:29<02:44,  2.52it/s] 86%|████████▌ | 2709/3145 [18:23<02:56,  2.47it/s] 85%|████████▍ | 2673/3145 [18:23<03:09,  2.50it/s] 87%|████████▋ | 2737/3145 [18:23<02:49,  2.41it/s] 87%|████████▋ | 2730/3143 [18:29<02:40,  2.58it/s] 86%|████████▌ | 2710/3145 [18:24<02:52,  2.52it/s] 85%|████████▌ | 2674/3145 [18:24<03:20,  2.35it/s] 87%|████████▋ | 2738/3145 [18:24<02:49,  2.40it/s] 86%|████████▌ | 2711/3145 [18:24<02:56,  2.45it/s] 87%|████████▋ | 2731/3143 [18:30<03:07,  2.20it/s] 85%|████████▌ | 2675/3145 [18:24<03:17,  2.39it/s] 87%|████████▋ | 2739/3145 [18:24<02:50,  2.38it/s] 86%|████████▌ | 2712/3145 [18:24<02:57,  2.44it/s] 87%|████████▋ | 2732/3143 [18:30<03:10,  2.16it/s] 85%|████████▌ | 2676/3145 [18:25<03:21,  2.32it/s] 87%|████████▋ | 2740/3145 [18:25<02:48,  2.40it/s] 86%|████████▋ | 2713/3145 [18:25<02:55,  2.46it/s] 87%|████████▋ | 2733/3143 [18:31<03:07,  2.18it/s] 85%|████████▌ | 2677/3145 [18:25<03:17,  2.37it/s] 87%|████████▋ | 2741/3145 [18:25<02:46,  2.43it/s] 86%|████████▋ | 2714/3145 [18:25<02:49,  2.54it/s] 87%|████████▋ | 2734/3143 [18:31<03:04,  2.22it/s] 87%|████████▋ | 2742/3145 [18:25<02:39,  2.52it/s] 85%|████████▌ | 2678/3145 [18:25<03:14,  2.40it/s] 86%|████████▋ | 2715/3145 [18:26<02:50,  2.52it/s] 85%|████████▌ | 2679/3145 [18:26<03:05,  2.51it/s] 87%|████████▋ | 2743/3145 [18:26<02:44,  2.45it/s] 87%|████████▋ | 2735/3143 [18:32<03:12,  2.12it/s] 86%|████████▋ | 2716/3145 [18:26<02:50,  2.51it/s] 85%|████████▌ | 2680/3145 [18:26<02:50,  2.73it/s] 87%|████████▋ | 2744/3145 [18:26<02:31,  2.64it/s] 86%|████████▋ | 2717/3145 [18:26<02:43,  2.61it/s] 87%|████████▋ | 2736/3143 [18:32<03:17,  2.06it/s] 85%|████████▌ | 2681/3145 [18:27<02:59,  2.59it/s] 87%|████████▋ | 2745/3145 [18:27<02:35,  2.57it/s] 86%|████████▋ | 2718/3145 [18:27<02:39,  2.68it/s] 87%|████████▋ | 2737/3143 [18:33<03:10,  2.13it/s] 85%|████████▌ | 2682/3145 [18:27<02:59,  2.58it/s] 87%|████████▋ | 2746/3145 [18:27<02:31,  2.63it/s] 86%|████████▋ | 2719/3145 [18:27<02:46,  2.56it/s] 87%|████████▋ | 2738/3143 [18:33<03:00,  2.24it/s] 85%|████████▌ | 2683/3145 [18:27<02:57,  2.61it/s] 87%|████████▋ | 2747/3145 [18:27<02:34,  2.58it/s] 86%|████████▋ | 2720/3145 [18:28<02:45,  2.56it/s] 87%|████████▋ | 2739/3143 [18:34<02:54,  2.31it/s] 87%|████████▋ | 2748/3145 [18:28<02:30,  2.64it/s] 85%|████████▌ | 2684/3145 [18:28<03:05,  2.48it/s] 87%|████████▋ | 2721/3145 [18:28<02:41,  2.62it/s] 87%|████████▋ | 2740/3143 [18:34<02:49,  2.37it/s] 87%|████████▋ | 2749/3145 [18:28<02:36,  2.53it/s] 85%|████████▌ | 2685/3145 [18:28<03:09,  2.42it/s] 87%|████████▋ | 2722/3145 [18:28<02:42,  2.60it/s] 87%|████████▋ | 2741/3143 [18:34<02:53,  2.31it/s] 87%|████████▋ | 2750/3145 [18:29<02:36,  2.53it/s] 85%|████████▌ | 2686/3145 [18:29<03:04,  2.49it/s] 87%|████████▋ | 2723/3145 [18:29<02:48,  2.51it/s] 87%|████████▋ | 2751/3145 [18:29<02:30,  2.62it/s] 87%|████████▋ | 2742/3143 [18:35<02:56,  2.28it/s] 85%|████████▌ | 2687/3145 [18:29<03:10,  2.40it/s] 87%|████████▋ | 2724/3145 [18:29<02:46,  2.53it/s] 88%|████████▊ | 2752/3145 [18:29<02:34,  2.55it/s] 87%|████████▋ | 2743/3143 [18:35<02:46,  2.40it/s] 85%|████████▌ | 2688/3145 [18:29<03:14,  2.35it/s] 87%|████████▋ | 2725/3145 [18:30<02:46,  2.52it/s] 87%|████████▋ | 2744/3143 [18:35<02:23,  2.77it/s] 88%|████████▊ | 2753/3145 [18:30<02:39,  2.46it/s] 86%|████████▌ | 2689/3145 [18:30<03:09,  2.40it/s] 87%|████████▋ | 2726/3145 [18:30<02:51,  2.45it/s] 87%|████████▋ | 2745/3143 [18:36<02:25,  2.74it/s] 88%|████████▊ | 2754/3145 [18:30<02:34,  2.52it/s] 86%|████████▌ | 2690/3145 [18:30<03:01,  2.50it/s] 87%|████████▋ | 2727/3145 [18:30<02:44,  2.54it/s] 87%|████████▋ | 2746/3143 [18:36<02:26,  2.71it/s] 87%|████████▋ | 2747/3143 [18:36<02:10,  3.04it/s] 88%|████████▊ | 2755/3145 [18:31<02:42,  2.40it/s] 86%|████████▌ | 2691/3145 [18:31<03:05,  2.45it/s] 87%|████████▋ | 2728/3145 [18:31<02:45,  2.53it/s] 86%|████████▌ | 2692/3145 [18:31<02:43,  2.77it/s] 88%|████████▊ | 2756/3145 [18:31<02:38,  2.46it/s] 87%|████████▋ | 2748/3143 [18:37<02:22,  2.77it/s] 87%|████████▋ | 2729/3145 [18:31<02:49,  2.46it/s] 88%|████████▊ | 2757/3145 [18:31<02:31,  2.56it/s] 87%|████████▋ | 2749/3143 [18:37<02:23,  2.74it/s] 86%|████████▌ | 2693/3145 [18:31<03:16,  2.30it/s] 87%|████████▋ | 2730/3145 [18:32<02:47,  2.48it/s] 87%|████████▋ | 2750/3143 [18:38<02:30,  2.62it/s] 88%|████████▊ | 2758/3145 [18:32<02:43,  2.37it/s] 86%|████████▌ | 2694/3145 [18:32<03:16,  2.30it/s] 87%|████████▋ | 2731/3145 [18:32<02:50,  2.42it/s] 88%|████████▊ | 2751/3143 [18:38<02:30,  2.60it/s] 88%|████████▊ | 2759/3145 [18:32<02:40,  2.40it/s] 87%|████████▋ | 2732/3145 [18:32<02:52,  2.39it/s] 86%|████████▌ | 2695/3145 [18:32<03:28,  2.16it/s] 88%|████████▊ | 2752/3143 [18:38<02:32,  2.57it/s] 88%|████████▊ | 2760/3145 [18:33<02:50,  2.26it/s] 87%|████████▋ | 2733/3145 [18:33<02:59,  2.30it/s] 86%|████████▌ | 2696/3145 [18:33<03:20,  2.24it/s] 88%|████████▊ | 2753/3143 [18:39<02:37,  2.48it/s] 88%|████████▊ | 2761/3145 [18:33<02:53,  2.21it/s] 87%|████████▋ | 2734/3145 [18:33<02:59,  2.30it/s] 86%|████████▌ | 2697/3145 [18:33<03:24,  2.19it/s] 88%|████████▊ | 2754/3143 [18:39<02:36,  2.49it/s] 87%|████████▋ | 2735/3145 [18:34<02:37,  2.61it/s] 88%|████████▊ | 2762/3145 [18:34<03:08,  2.04it/s] 88%|████████▊ | 2755/3143 [18:40<02:36,  2.48it/s] 86%|████████▌ | 2698/3145 [18:34<03:43,  2.00it/s] 87%|████████▋ | 2736/3145 [18:34<02:41,  2.53it/s] 88%|████████▊ | 2763/3145 [18:34<02:56,  2.16it/s] 88%|████████▊ | 2756/3143 [18:40<02:34,  2.51it/s] 87%|████████▋ | 2737/3145 [18:34<02:37,  2.59it/s] 86%|████████▌ | 2699/3145 [18:34<03:35,  2.07it/s] 88%|████████▊ | 2757/3143 [18:40<02:24,  2.67it/s] 88%|████████▊ | 2764/3145 [18:35<02:57,  2.15it/s] 87%|████████▋ | 2738/3145 [18:35<02:42,  2.51it/s] 86%|████████▌ | 2700/3145 [18:35<03:32,  2.09it/s] 88%|████████▊ | 2758/3143 [18:41<02:23,  2.68it/s] 88%|████████▊ | 2765/3145 [18:35<02:57,  2.14it/s] 87%|████████▋ | 2739/3145 [18:35<02:46,  2.44it/s] 86%|████████▌ | 2701/3145 [18:35<03:22,  2.19it/s] 88%|████████▊ | 2759/3143 [18:41<02:22,  2.69it/s] 88%|████████▊ | 2766/3145 [18:36<02:48,  2.25it/s] 87%|████████▋ | 2740/3145 [18:36<02:43,  2.48it/s] 86%|████████▌ | 2702/3145 [18:36<03:15,  2.27it/s] 88%|████████▊ | 2760/3143 [18:42<02:33,  2.49it/s] 88%|████████▊ | 2767/3145 [18:36<02:46,  2.27it/s] 87%|████████▋ | 2741/3145 [18:36<02:40,  2.51it/s] 86%|████████▌ | 2703/3145 [18:36<03:15,  2.27it/s] 88%|████████▊ | 2761/3143 [18:42<02:42,  2.35it/s] 87%|████████▋ | 2742/3145 [18:36<02:27,  2.74it/s] 88%|████████▊ | 2768/3145 [18:36<02:40,  2.35it/s] 86%|████████▌ | 2704/3145 [18:37<03:09,  2.32it/s] 87%|████████▋ | 2743/3145 [18:37<02:17,  2.91it/s] 88%|████████▊ | 2762/3143 [18:42<02:35,  2.45it/s] 88%|████████▊ | 2769/3145 [18:37<02:38,  2.37it/s] 86%|████████▌ | 2705/3145 [18:37<03:06,  2.35it/s] 87%|████████▋ | 2744/3145 [18:37<02:24,  2.78it/s] 88%|████████▊ | 2763/3143 [18:43<02:38,  2.39it/s] 88%|████████▊ | 2770/3145 [18:37<02:37,  2.39it/s] 86%|████████▌ | 2706/3145 [18:37<03:02,  2.40it/s] 87%|████████▋ | 2745/3145 [18:37<02:27,  2.71it/s] 88%|████████▊ | 2764/3143 [18:43<02:40,  2.36it/s] 88%|████████▊ | 2771/3145 [18:38<02:32,  2.46it/s] 86%|████████▌ | 2707/3145 [18:38<02:56,  2.49it/s] 87%|████████▋ | 2746/3145 [18:38<02:34,  2.58it/s] 88%|████████▊ | 2765/3143 [18:44<02:35,  2.43it/s] 88%|████████▊ | 2772/3145 [18:38<02:37,  2.37it/s] 86%|████████▌ | 2708/3145 [18:38<02:55,  2.49it/s] 87%|████████▋ | 2747/3145 [18:38<02:35,  2.56it/s] 88%|████████▊ | 2766/3143 [18:44<02:37,  2.39it/s] 88%|████████▊ | 2773/3145 [18:38<02:34,  2.41it/s] 86%|████████▌ | 2709/3145 [18:39<03:00,  2.41it/s] 87%|████████▋ | 2748/3145 [18:39<02:35,  2.55it/s] 88%|████████▊ | 2774/3145 [18:39<02:21,  2.63it/s] 88%|████████▊ | 2767/3143 [18:45<02:40,  2.34it/s] 87%|████████▋ | 2749/3145 [18:39<02:35,  2.54it/s] 86%|████████▌ | 2710/3145 [18:39<03:16,  2.21it/s] 88%|████████▊ | 2775/3145 [18:39<02:22,  2.60it/s] 88%|████████▊ | 2768/3143 [18:45<02:40,  2.33it/s] 87%|████████▋ | 2750/3145 [18:39<02:29,  2.65it/s] 88%|████████▊ | 2776/3145 [18:39<02:07,  2.90it/s] 88%|████████▊ | 2769/3143 [18:45<02:42,  2.31it/s] 86%|████████▌ | 2711/3145 [18:40<03:32,  2.04it/s] 88%|████████▊ | 2777/3145 [18:40<02:08,  2.85it/s] 87%|████████▋ | 2751/3145 [18:40<02:33,  2.56it/s] 88%|████████▊ | 2770/3143 [18:46<02:33,  2.43it/s] 86%|████████▌ | 2712/3145 [18:40<03:20,  2.16it/s] 88%|████████▊ | 2778/3145 [18:40<02:13,  2.74it/s] 88%|████████▊ | 2752/3145 [18:40<02:32,  2.57it/s] 88%|████████▊ | 2771/3143 [18:46<02:28,  2.50it/s] 86%|████████▋ | 2713/3145 [18:40<03:17,  2.19it/s] 88%|████████▊ | 2753/3145 [18:41<02:41,  2.43it/s] 88%|████████▊ | 2779/3145 [18:41<02:26,  2.50it/s] 88%|████████▊ | 2772/3143 [18:47<02:28,  2.50it/s] 86%|████████▋ | 2714/3145 [18:41<03:10,  2.26it/s] 88%|████████▊ | 2780/3145 [18:41<02:25,  2.50it/s] 88%|████████▊ | 2754/3145 [18:41<02:43,  2.39it/s] 88%|████████▊ | 2773/3143 [18:47<02:27,  2.51it/s] 86%|████████▋ | 2715/3145 [18:41<03:06,  2.31it/s] 88%|████████▊ | 2781/3145 [18:41<02:27,  2.46it/s] 88%|████████▊ | 2774/3143 [18:47<02:14,  2.74it/s] 88%|████████▊ | 2755/3145 [18:41<02:41,  2.41it/s] 88%|████████▊ | 2775/3143 [18:48<02:05,  2.93it/s] 86%|████████▋ | 2716/3145 [18:42<03:09,  2.26it/s] 88%|████████▊ | 2782/3145 [18:42<02:25,  2.50it/s] 88%|████████▊ | 2756/3145 [18:42<02:41,  2.41it/s] 88%|████████▊ | 2776/3143 [18:48<02:11,  2.79it/s] 88%|████████▊ | 2783/3145 [18:42<02:21,  2.56it/s] 88%|████████▊ | 2757/3145 [18:42<02:38,  2.44it/s] 86%|████████▋ | 2717/3145 [18:42<03:13,  2.21it/s] 88%|████████▊ | 2777/3143 [18:48<01:58,  3.08it/s] 89%|████████▊ | 2784/3145 [18:43<02:27,  2.45it/s] 88%|████████▊ | 2758/3145 [18:43<02:38,  2.44it/s] 86%|████████▋ | 2718/3145 [18:43<03:06,  2.29it/s] 88%|████████▊ | 2778/3143 [18:49<01:58,  3.08it/s] 88%|████████▊ | 2779/3143 [18:49<01:58,  3.07it/s] 89%|████████▊ | 2785/3145 [18:43<02:26,  2.45it/s] 88%|████████▊ | 2759/3145 [18:43<02:38,  2.44it/s] 86%|████████▋ | 2719/3145 [18:43<03:01,  2.35it/s] 88%|████████▊ | 2780/3143 [18:49<02:06,  2.86it/s] 89%|████████▊ | 2786/3145 [18:43<02:26,  2.44it/s] 86%|████████▋ | 2720/3145 [18:43<02:58,  2.38it/s] 88%|████████▊ | 2760/3145 [18:43<02:39,  2.41it/s] 88%|████████▊ | 2781/3143 [18:50<02:18,  2.62it/s] 89%|████████▊ | 2787/3145 [18:44<02:29,  2.40it/s] 88%|████████▊ | 2761/3145 [18:44<02:39,  2.40it/s] 87%|████████▋ | 2721/3145 [18:44<02:59,  2.36it/s] 89%|████████▊ | 2782/3143 [18:50<02:19,  2.59it/s] 88%|████████▊ | 2762/3145 [18:44<02:34,  2.48it/s] 89%|████████▊ | 2788/3145 [18:44<02:32,  2.35it/s] 87%|████████▋ | 2722/3145 [18:44<03:05,  2.28it/s] 88%|████████▊ | 2763/3145 [18:45<02:35,  2.46it/s] 89%|████████▊ | 2783/3143 [18:51<02:25,  2.47it/s] 89%|████████▊ | 2789/3145 [18:45<02:28,  2.40it/s] 87%|████████▋ | 2723/3145 [18:45<03:01,  2.33it/s] 89%|████████▊ | 2790/3145 [18:45<02:18,  2.57it/s] 88%|████████▊ | 2764/3145 [18:45<02:30,  2.54it/s] 89%|████████▊ | 2784/3143 [18:51<02:29,  2.40it/s] 87%|████████▋ | 2724/3145 [18:45<03:01,  2.32it/s] 88%|████████▊ | 2765/3145 [18:45<02:27,  2.58it/s] 89%|████████▊ | 2791/3145 [18:45<02:22,  2.49it/s] 89%|████████▊ | 2785/3143 [18:51<02:31,  2.37it/s] 87%|████████▋ | 2725/3145 [18:46<02:54,  2.41it/s] 88%|████████▊ | 2766/3145 [18:46<02:23,  2.63it/s] 89%|████████▉ | 2792/3145 [18:46<02:17,  2.57it/s] 89%|████████▊ | 2786/3143 [18:52<02:29,  2.40it/s] 87%|████████▋ | 2726/3145 [18:46<03:01,  2.31it/s] 89%|████████▉ | 2793/3145 [18:46<02:16,  2.58it/s] 88%|████████▊ | 2767/3145 [18:46<02:33,  2.45it/s] 89%|████████▊ | 2787/3143 [18:52<02:30,  2.36it/s] 89%|████████▉ | 2794/3145 [18:46<02:01,  2.88it/s] 87%|████████▋ | 2727/3145 [18:46<02:56,  2.37it/s] 88%|████████▊ | 2768/3145 [18:47<02:26,  2.58it/s] 89%|████████▊ | 2788/3143 [18:53<02:27,  2.41it/s] 89%|████████▉ | 2795/3145 [18:47<02:03,  2.82it/s] 87%|████████▋ | 2728/3145 [18:47<02:55,  2.38it/s] 88%|████████▊ | 2769/3145 [18:47<02:23,  2.62it/s] 89%|████████▊ | 2789/3143 [18:53<02:25,  2.44it/s] 89%|████████▉ | 2796/3145 [18:47<02:12,  2.62it/s] 87%|████████▋ | 2729/3145 [18:47<02:56,  2.36it/s] 88%|████████▊ | 2770/3145 [18:47<02:25,  2.58it/s] 89%|████████▉ | 2797/3145 [18:48<02:11,  2.65it/s] 89%|████████▉ | 2790/3143 [18:54<02:27,  2.39it/s] 87%|████████▋ | 2730/3145 [18:48<02:56,  2.35it/s] 88%|████████▊ | 2771/3145 [18:48<02:24,  2.59it/s] 88%|████████▊ | 2772/3145 [18:48<02:09,  2.87it/s] 89%|████████▉ | 2791/3143 [18:54<02:24,  2.43it/s] 89%|████████▉ | 2798/3145 [18:48<02:16,  2.54it/s] 87%|████████▋ | 2731/3145 [18:48<02:55,  2.36it/s] 88%|████████▊ | 2773/3145 [18:48<02:14,  2.76it/s] 89%|████████▉ | 2799/3145 [18:48<02:15,  2.55it/s] 89%|████████▉ | 2792/3143 [18:54<02:24,  2.43it/s] 87%|████████▋ | 2732/3145 [18:49<02:52,  2.39it/s] 89%|████████▉ | 2800/3145 [18:49<02:11,  2.62it/s] 89%|████████▉ | 2793/3143 [18:55<02:25,  2.41it/s] 88%|████████▊ | 2774/3145 [18:49<02:37,  2.35it/s] 87%|████████▋ | 2733/3145 [18:49<02:54,  2.37it/s] 89%|████████▉ | 2801/3145 [18:49<02:13,  2.58it/s] 89%|████████▉ | 2794/3143 [18:55<02:29,  2.33it/s] 87%|████████▋ | 2734/3145 [18:49<02:55,  2.34it/s] 88%|████████▊ | 2775/3145 [18:50<02:54,  2.12it/s] 89%|████████▉ | 2802/3145 [18:50<02:13,  2.57it/s] 89%|████████▉ | 2795/3143 [18:56<02:23,  2.43it/s] 87%|████████▋ | 2735/3145 [18:50<02:55,  2.34it/s] 88%|████████▊ | 2776/3145 [18:50<02:50,  2.17it/s] 89%|████████▉ | 2803/3145 [18:50<02:18,  2.46it/s] 89%|████████▉ | 2796/3143 [18:56<02:24,  2.40it/s] 87%|████████▋ | 2736/3145 [18:50<02:49,  2.42it/s] 88%|████████▊ | 2777/3145 [18:50<02:46,  2.21it/s] 89%|████████▉ | 2804/3145 [18:50<02:15,  2.52it/s] 89%|████████▉ | 2797/3143 [18:56<02:26,  2.36it/s] 87%|████████▋ | 2737/3145 [18:51<02:46,  2.45it/s] 88%|████████▊ | 2778/3145 [18:51<02:36,  2.34it/s] 89%|████████▉ | 2805/3145 [18:51<02:11,  2.59it/s] 89%|████████▉ | 2798/3143 [18:57<02:25,  2.37it/s] 87%|████████▋ | 2738/3145 [18:51<02:45,  2.46it/s] 89%|████████▉ | 2806/3145 [18:51<02:11,  2.57it/s] 88%|████████▊ | 2779/3145 [18:51<02:35,  2.36it/s] 87%|████████▋ | 2739/3145 [18:51<02:38,  2.57it/s] 89%|████████▉ | 2799/3143 [18:57<02:22,  2.41it/s] 89%|████████▉ | 2807/3145 [18:52<02:16,  2.47it/s] 88%|████████▊ | 2780/3145 [18:52<02:35,  2.34it/s] 87%|████████▋ | 2740/3145 [18:52<02:44,  2.46it/s] 89%|████████▉ | 2800/3143 [18:58<02:24,  2.37it/s] 89%|████████▉ | 2808/3145 [18:52<02:21,  2.39it/s] 88%|████████▊ | 2781/3145 [18:52<02:37,  2.32it/s] 87%|████████▋ | 2741/3145 [18:52<02:52,  2.35it/s] 89%|████████▉ | 2801/3143 [18:58<02:40,  2.13it/s] 88%|████████▊ | 2782/3145 [18:52<02:30,  2.41it/s] 89%|████████▉ | 2809/3145 [18:53<02:23,  2.35it/s] 87%|████████▋ | 2742/3145 [18:53<02:48,  2.39it/s] 88%|████████▊ | 2783/3145 [18:53<02:22,  2.54it/s] 89%|████████▉ | 2802/3143 [18:59<02:36,  2.18it/s] 89%|████████▉ | 2810/3145 [18:53<02:19,  2.40it/s] 87%|████████▋ | 2743/3145 [18:53<02:47,  2.40it/s] 89%|████████▊ | 2784/3145 [18:53<02:17,  2.62it/s] 89%|████████▉ | 2811/3145 [18:53<02:20,  2.37it/s] 89%|████████▉ | 2803/3143 [18:59<02:40,  2.11it/s] 87%|████████▋ | 2744/3145 [18:53<02:42,  2.47it/s] 89%|████████▊ | 2785/3145 [18:54<02:19,  2.59it/s] 89%|████████▉ | 2812/3145 [18:54<02:21,  2.35it/s] 89%|████████▉ | 2804/3143 [19:00<02:47,  2.03it/s] 89%|████████▊ | 2786/3145 [18:54<02:15,  2.65it/s] 87%|████████▋ | 2745/3145 [18:54<02:48,  2.38it/s] 89%|████████▉ | 2813/3145 [18:54<02:21,  2.34it/s] 89%|████████▉ | 2805/3143 [19:00<02:35,  2.17it/s] 89%|████████▊ | 2787/3145 [18:54<02:19,  2.57it/s] 87%|████████▋ | 2746/3145 [18:54<02:45,  2.41it/s] 89%|████████▉ | 2814/3145 [18:55<02:19,  2.38it/s] 89%|████████▉ | 2806/3143 [19:01<02:28,  2.27it/s] 89%|████████▊ | 2788/3145 [18:55<02:19,  2.55it/s] 87%|████████▋ | 2747/3145 [18:55<02:51,  2.32it/s] 90%|████████▉ | 2815/3145 [18:55<02:12,  2.48it/s] 89%|████████▉ | 2807/3143 [19:01<02:22,  2.35it/s] 89%|████████▊ | 2789/3145 [18:55<02:15,  2.63it/s] 87%|████████▋ | 2748/3145 [18:55<02:47,  2.36it/s] 90%|████████▉ | 2816/3145 [18:55<02:13,  2.46it/s] 89%|████████▊ | 2790/3145 [18:56<02:21,  2.51it/s] 89%|████████▉ | 2808/3143 [19:01<02:27,  2.27it/s] 87%|████████▋ | 2749/3145 [18:56<02:46,  2.38it/s] 89%|████████▊ | 2791/3145 [18:56<02:03,  2.88it/s] 90%|████████▉ | 2817/3145 [18:56<02:11,  2.50it/s] 89%|████████▉ | 2809/3143 [19:02<02:23,  2.34it/s] 87%|████████▋ | 2750/3145 [18:56<02:45,  2.39it/s] 89%|████████▉ | 2792/3145 [18:56<02:12,  2.67it/s] 90%|████████▉ | 2818/3145 [18:56<02:14,  2.43it/s] 89%|████████▉ | 2810/3143 [19:02<02:15,  2.45it/s] 87%|████████▋ | 2751/3145 [18:56<02:38,  2.49it/s] 89%|████████▉ | 2793/3145 [18:57<02:07,  2.75it/s] 90%|████████▉ | 2819/3145 [18:57<02:14,  2.43it/s] 89%|████████▉ | 2811/3143 [19:03<02:17,  2.41it/s] 88%|████████▊ | 2752/3145 [18:57<02:33,  2.55it/s] 89%|████████▉ | 2794/3145 [18:57<02:11,  2.67it/s] 90%|████████▉ | 2820/3145 [18:57<02:10,  2.50it/s] 89%|████████▉ | 2812/3143 [19:03<02:19,  2.37it/s] 88%|████████▊ | 2753/3145 [18:57<02:34,  2.53it/s] 90%|████████▉ | 2821/3145 [18:57<02:09,  2.50it/s] 89%|████████▉ | 2795/3145 [18:57<02:24,  2.42it/s] 90%|████████▉ | 2813/3143 [19:03<02:15,  2.43it/s] 88%|████████▊ | 2754/3145 [18:58<02:38,  2.47it/s] 90%|████████▉ | 2822/3145 [18:58<02:11,  2.46it/s] 89%|████████▉ | 2796/3145 [18:58<02:27,  2.37it/s] 90%|████████▉ | 2814/3143 [19:04<02:11,  2.51it/s] 88%|████████▊ | 2755/3145 [18:58<02:38,  2.46it/s] 90%|████████▉ | 2823/3145 [18:58<02:10,  2.47it/s] 89%|████████▉ | 2797/3145 [18:58<02:26,  2.38it/s] 90%|████████▉ | 2815/3143 [19:04<02:07,  2.58it/s] 88%|████████▊ | 2756/3145 [18:58<02:45,  2.35it/s] 90%|████████▉ | 2824/3145 [18:59<02:05,  2.55it/s] 90%|████████▉ | 2816/3143 [19:05<02:08,  2.54it/s] 89%|████████▉ | 2798/3145 [18:59<02:27,  2.36it/s] 88%|████████▊ | 2757/3145 [18:59<02:44,  2.36it/s] 89%|████████▉ | 2799/3145 [18:59<02:04,  2.77it/s] 90%|████████▉ | 2825/3145 [18:59<02:02,  2.60it/s] 90%|████████▉ | 2817/3143 [19:05<02:15,  2.41it/s] 89%|████████▉ | 2800/3145 [18:59<01:55,  2.99it/s] 90%|████████▉ | 2826/3145 [18:59<02:03,  2.57it/s] 88%|████████▊ | 2758/3145 [18:59<02:48,  2.29it/s] 89%|████████▉ | 2801/3145 [18:59<01:48,  3.17it/s] 90%|████████▉ | 2818/3143 [19:05<02:09,  2.50it/s] 89%|████████▉ | 2802/3145 [19:00<01:43,  3.32it/s] 90%|████████▉ | 2827/3145 [19:00<02:11,  2.42it/s] 88%|████████▊ | 2759/3145 [19:00<02:50,  2.26it/s] 90%|████████▉ | 2819/3143 [19:06<02:04,  2.59it/s] 89%|████████▉ | 2803/3145 [19:00<01:47,  3.17it/s] 90%|████████▉ | 2828/3145 [19:00<02:08,  2.46it/s] 90%|████████▉ | 2820/3143 [19:06<02:03,  2.61it/s] 88%|████████▊ | 2760/3145 [19:00<02:48,  2.28it/s] 89%|████████▉ | 2804/3145 [19:00<01:50,  3.10it/s] 90%|████████▉ | 2829/3145 [19:01<02:06,  2.49it/s] 90%|████████▉ | 2821/3143 [19:07<01:59,  2.69it/s] 88%|████████▊ | 2761/3145 [19:01<02:49,  2.27it/s] 89%|████████▉ | 2805/3145 [19:01<01:58,  2.88it/s] 90%|████████▉ | 2822/3143 [19:07<01:57,  2.72it/s] 90%|████████▉ | 2830/3145 [19:01<02:04,  2.54it/s] 88%|████████▊ | 2762/3145 [19:01<02:39,  2.40it/s] 89%|████████▉ | 2806/3145 [19:01<02:00,  2.82it/s] 90%|████████▉ | 2823/3143 [19:07<02:06,  2.53it/s] 90%|█████████ | 2831/3145 [19:01<02:10,  2.41it/s] 88%|████████▊ | 2763/3145 [19:02<02:44,  2.32it/s] 89%|████████▉ | 2807/3145 [19:02<02:08,  2.64it/s] 90%|████████▉ | 2824/3143 [19:08<02:06,  2.52it/s] 90%|█████████ | 2832/3145 [19:02<02:12,  2.36it/s] 88%|████████▊ | 2764/3145 [19:02<02:45,  2.30it/s] 89%|████████▉ | 2808/3145 [19:02<02:13,  2.52it/s] 90%|████████▉ | 2825/3143 [19:08<02:06,  2.51it/s] 90%|█████████ | 2833/3145 [19:02<02:11,  2.37it/s] 88%|████████▊ | 2765/3145 [19:02<02:40,  2.36it/s] 89%|████████▉ | 2809/3145 [19:02<02:11,  2.55it/s] 90%|████████▉ | 2826/3143 [19:09<02:14,  2.36it/s] 90%|█████████ | 2834/3145 [19:03<02:11,  2.37it/s] 89%|████████▉ | 2810/3145 [19:03<02:07,  2.62it/s] 88%|████████▊ | 2766/3145 [19:03<02:42,  2.33it/s] 90%|████████▉ | 2827/3143 [19:09<02:11,  2.41it/s] 90%|█████████ | 2835/3145 [19:03<02:15,  2.29it/s] 89%|████████▉ | 2811/3145 [19:03<02:09,  2.59it/s] 88%|████████▊ | 2767/3145 [19:03<02:44,  2.29it/s] 89%|████████▉ | 2812/3145 [19:04<02:04,  2.68it/s] 90%|████████▉ | 2828/3143 [19:09<02:14,  2.34it/s] 90%|█████████ | 2836/3145 [19:04<02:08,  2.40it/s] 88%|████████▊ | 2768/3145 [19:04<02:44,  2.30it/s] 90%|█████████ | 2829/3143 [19:10<02:12,  2.37it/s] 89%|████████▉ | 2813/3145 [19:04<02:09,  2.56it/s] 90%|█████████ | 2837/3145 [19:04<02:11,  2.34it/s] 88%|████████▊ | 2769/3145 [19:04<02:44,  2.29it/s] 90%|█████████ | 2830/3143 [19:10<02:10,  2.40it/s] 90%|█████████ | 2838/3145 [19:04<02:07,  2.40it/s] 89%|████████▉ | 2814/3145 [19:04<02:13,  2.48it/s] 88%|████████▊ | 2770/3145 [19:05<02:43,  2.29it/s] 90%|█████████ | 2831/3143 [19:11<02:12,  2.35it/s] 90%|█████████ | 2839/3145 [19:05<02:09,  2.36it/s] 90%|████████▉ | 2815/3145 [19:05<02:16,  2.42it/s] 88%|████████▊ | 2771/3145 [19:05<02:40,  2.33it/s] 90%|████████▉ | 2816/3145 [19:05<02:02,  2.69it/s] 90%|█████████ | 2832/3143 [19:11<02:09,  2.39it/s] 90%|█████████ | 2840/3145 [19:05<02:11,  2.32it/s] 88%|████████▊ | 2772/3145 [19:05<02:41,  2.31it/s] 90%|████████▉ | 2817/3145 [19:06<02:07,  2.57it/s] 90%|█████████ | 2833/3143 [19:12<02:14,  2.31it/s] 90%|█████████ | 2841/3145 [19:06<02:08,  2.37it/s] 88%|████████▊ | 2773/3145 [19:06<02:37,  2.37it/s] 90%|█████████ | 2842/3145 [19:06<01:48,  2.78it/s] 90%|████████▉ | 2818/3145 [19:06<02:07,  2.57it/s] 90%|█████████ | 2834/3143 [19:12<02:10,  2.37it/s] 88%|████████▊ | 2774/3145 [19:06<02:24,  2.57it/s] 90%|████████▉ | 2819/3145 [19:06<01:50,  2.96it/s] 90%|█████████ | 2843/3145 [19:06<01:55,  2.61it/s] 90%|█████████ | 2835/3143 [19:12<02:04,  2.47it/s] 90%|████████▉ | 2820/3145 [19:07<01:52,  2.90it/s] 88%|████████▊ | 2775/3145 [19:07<02:25,  2.55it/s] 90%|█████████ | 2836/3143 [19:13<01:47,  2.84it/s] 90%|█████████ | 2844/3145 [19:07<01:59,  2.51it/s] 90%|████████▉ | 2821/3145 [19:07<02:03,  2.62it/s] 88%|████████▊ | 2776/3145 [19:07<02:33,  2.41it/s] 90%|█████████ | 2837/3143 [19:13<01:51,  2.74it/s] 90%|█████████ | 2845/3145 [19:07<01:57,  2.54it/s] 90%|████████▉ | 2822/3145 [19:07<02:05,  2.57it/s] 88%|████████▊ | 2777/3145 [19:07<02:32,  2.41it/s] 90%|█████████ | 2838/3143 [19:13<01:52,  2.72it/s] 90%|█████████ | 2846/3145 [19:08<01:59,  2.50it/s] 90%|████████▉ | 2823/3145 [19:08<02:04,  2.58it/s] 88%|████████▊ | 2778/3145 [19:08<02:26,  2.50it/s] 90%|█████████ | 2839/3143 [19:14<01:54,  2.66it/s] 91%|█████████ | 2847/3145 [19:08<02:02,  2.44it/s] 90%|████████▉ | 2824/3145 [19:08<02:06,  2.55it/s] 90%|█████████ | 2840/3143 [19:14<01:52,  2.70it/s] 88%|████████▊ | 2779/3145 [19:08<02:31,  2.41it/s] 91%|█████████ | 2848/3145 [19:08<02:01,  2.45it/s] 90%|█████████ | 2841/3143 [19:14<01:50,  2.72it/s] 90%|████████▉ | 2825/3145 [19:09<02:07,  2.50it/s] 91%|█████████ | 2849/3145 [19:09<01:55,  2.57it/s] 88%|████████▊ | 2780/3145 [19:09<02:45,  2.21it/s] 90%|█████████ | 2842/3143 [19:15<01:55,  2.60it/s] 90%|████████▉ | 2826/3145 [19:09<02:13,  2.38it/s] 91%|█████████ | 2850/3145 [19:09<02:00,  2.45it/s] 88%|████████▊ | 2781/3145 [19:09<02:45,  2.21it/s] 90%|█████████ | 2843/3143 [19:15<02:00,  2.50it/s] 90%|████████▉ | 2827/3145 [19:09<02:11,  2.42it/s] 91%|█████████ | 2851/3145 [19:10<01:58,  2.47it/s] 88%|████████▊ | 2782/3145 [19:10<02:44,  2.20it/s] 90%|█████████ | 2844/3143 [19:16<01:57,  2.55it/s] 90%|████████▉ | 2828/3145 [19:10<02:06,  2.50it/s] 91%|█████████ | 2852/3145 [19:10<01:57,  2.50it/s] 88%|████████▊ | 2783/3145 [19:10<02:40,  2.25it/s] 91%|█████████ | 2845/3143 [19:16<01:57,  2.54it/s] 90%|████████▉ | 2829/3145 [19:10<02:09,  2.44it/s] 91%|█████████ | 2853/3145 [19:10<02:00,  2.42it/s] 89%|████████▊ | 2784/3145 [19:11<02:38,  2.27it/s] 91%|█████████ | 2846/3143 [19:16<01:54,  2.60it/s] 90%|████████▉ | 2830/3145 [19:11<02:06,  2.49it/s] 91%|█████████ | 2854/3145 [19:11<01:59,  2.44it/s] 89%|████████▊ | 2785/3145 [19:11<02:35,  2.32it/s] 91%|█████████ | 2847/3143 [19:17<01:59,  2.48it/s] 90%|█████████ | 2831/3145 [19:11<02:08,  2.44it/s] 91%|█████████ | 2855/3145 [19:11<01:54,  2.53it/s] 91%|█████████ | 2848/3143 [19:17<01:53,  2.59it/s] 89%|████████▊ | 2786/3145 [19:11<02:38,  2.26it/s] 90%|█████████ | 2832/3145 [19:12<02:07,  2.46it/s] 91%|█████████ | 2856/3145 [19:12<01:53,  2.54it/s] 91%|█████████ | 2849/3143 [19:18<01:55,  2.55it/s] 89%|████████▊ | 2787/3145 [19:12<02:33,  2.34it/s] 90%|█████████ | 2833/3145 [19:12<02:09,  2.41it/s] 91%|█████████ | 2857/3145 [19:12<01:53,  2.53it/s] 91%|█████████ | 2850/3143 [19:18<01:58,  2.47it/s] 91%|█████████ | 2858/3145 [19:12<01:38,  2.91it/s] 90%|█████████ | 2834/3145 [19:12<02:07,  2.44it/s] 89%|████████▊ | 2788/3145 [19:12<02:44,  2.17it/s] 91%|█████████ | 2851/3143 [19:18<01:57,  2.49it/s] 91%|█████████ | 2859/3145 [19:13<01:42,  2.78it/s] 90%|█████████ | 2835/3145 [19:13<01:59,  2.59it/s] 89%|████████▊ | 2789/3145 [19:13<02:40,  2.21it/s] 90%|█████████ | 2836/3145 [19:13<01:55,  2.68it/s] 91%|█████████ | 2852/3143 [19:19<02:00,  2.41it/s] 91%|█████████ | 2860/3145 [19:13<01:49,  2.61it/s] 89%|████████▊ | 2790/3145 [19:13<02:35,  2.29it/s] 90%|█████████ | 2837/3145 [19:13<01:57,  2.62it/s] 91%|█████████ | 2861/3145 [19:13<01:50,  2.57it/s] 91%|█████████ | 2853/3143 [19:19<02:02,  2.36it/s] 89%|████████▊ | 2791/3145 [19:14<02:34,  2.30it/s] 91%|█████████ | 2862/3145 [19:14<01:50,  2.56it/s] 91%|█████████ | 2854/3143 [19:20<02:00,  2.41it/s] 90%|█████████ | 2838/3145 [19:14<02:12,  2.31it/s] 89%|████████▉ | 2792/3145 [19:14<02:39,  2.21it/s] 91%|█████████ | 2863/3145 [19:14<01:47,  2.63it/s] 91%|█████████ | 2855/3143 [19:20<01:55,  2.48it/s] 90%|█████████ | 2839/3145 [19:14<02:08,  2.37it/s] 89%|████████▉ | 2793/3145 [19:14<02:32,  2.31it/s] 91%|█████████ | 2864/3145 [19:15<01:44,  2.70it/s] 91%|█████████ | 2856/3143 [19:21<01:58,  2.43it/s] 90%|█████████ | 2840/3145 [19:15<02:02,  2.49it/s] 89%|████████▉ | 2794/3145 [19:15<02:31,  2.31it/s] 91%|█████████ | 2865/3145 [19:15<01:50,  2.53it/s] 91%|█████████ | 2857/3143 [19:21<01:56,  2.45it/s] 90%|█████████ | 2841/3145 [19:15<02:05,  2.42it/s] 91%|█████████ | 2866/3145 [19:15<01:36,  2.89it/s] 89%|████████▉ | 2795/3145 [19:15<02:27,  2.37it/s] 91%|█████████ | 2858/3143 [19:21<01:55,  2.46it/s] 90%|█████████ | 2842/3145 [19:16<02:04,  2.44it/s] 91%|█████████ | 2867/3145 [19:16<01:47,  2.59it/s] 89%|████████▉ | 2796/3145 [19:16<02:25,  2.40it/s] 91%|█████████ | 2859/3143 [19:22<01:54,  2.48it/s] 90%|█████████ | 2843/3145 [19:16<02:03,  2.45it/s] 91%|█████████ | 2868/3145 [19:16<01:47,  2.58it/s] 89%|████████▉ | 2797/3145 [19:16<02:25,  2.39it/s] 91%|█████████ | 2860/3143 [19:22<01:55,  2.45it/s] 90%|█████████ | 2844/3145 [19:16<02:09,  2.32it/s] 91%|█████████ | 2869/3145 [19:16<01:43,  2.66it/s] 89%|████████▉ | 2798/3145 [19:17<02:23,  2.41it/s] 91%|█████████ | 2861/3143 [19:23<01:56,  2.42it/s] 90%|█████████ | 2845/3145 [19:17<02:07,  2.34it/s] 91%|█████████▏| 2870/3145 [19:17<01:45,  2.59it/s] 89%|████████▉ | 2799/3145 [19:17<02:16,  2.54it/s] 91%|█████████ | 2862/3143 [19:23<01:54,  2.45it/s] 90%|█████████ | 2846/3145 [19:17<02:01,  2.46it/s] 91%|█████████▏| 2871/3145 [19:17<01:41,  2.69it/s] 89%|████████▉ | 2800/3145 [19:17<02:13,  2.58it/s] 91%|█████████ | 2863/3143 [19:23<01:43,  2.70it/s] 91%|█████████ | 2847/3145 [19:18<02:03,  2.41it/s] 89%|████████▉ | 2801/3145 [19:18<02:15,  2.54it/s] 91%|█████████▏| 2872/3145 [19:18<01:53,  2.40it/s] 91%|█████████ | 2864/3143 [19:24<01:49,  2.55it/s] 89%|████████▉ | 2802/3145 [19:18<02:05,  2.73it/s] 91%|█████████ | 2848/3145 [19:18<02:06,  2.34it/s] 91%|█████████▏| 2873/3145 [19:18<01:51,  2.44it/s] 91%|█████████ | 2865/3143 [19:24<01:50,  2.51it/s] 89%|████████▉ | 2803/3145 [19:18<02:12,  2.57it/s] 91%|█████████▏| 2874/3145 [19:19<01:49,  2.48it/s] 91%|█████████ | 2849/3145 [19:19<02:11,  2.26it/s] 91%|█████████ | 2866/3143 [19:25<01:50,  2.50it/s] 91%|█████████▏| 2875/3145 [19:19<01:44,  2.59it/s] 89%|████████▉ | 2804/3145 [19:19<02:17,  2.48it/s] 91%|█████████ | 2867/3143 [19:25<01:53,  2.44it/s] 91%|█████████ | 2850/3145 [19:19<02:21,  2.08it/s] 91%|█████████▏| 2876/3145 [19:19<01:41,  2.66it/s] 89%|████████▉ | 2805/3145 [19:19<02:18,  2.46it/s] 91%|█████████▏| 2868/3143 [19:25<01:51,  2.47it/s] 91%|█████████ | 2851/3145 [19:20<02:10,  2.24it/s] 91%|█████████▏| 2877/3145 [19:20<01:42,  2.63it/s] 89%|████████▉ | 2806/3145 [19:20<02:24,  2.35it/s] 91%|█████████ | 2852/3145 [19:20<02:01,  2.41it/s] 91%|█████████▏| 2869/3143 [19:26<01:53,  2.42it/s] 92%|█████████▏| 2878/3145 [19:20<01:42,  2.60it/s] 89%|████████▉ | 2807/3145 [19:20<02:22,  2.38it/s] 91%|█████████ | 2853/3145 [19:20<02:05,  2.33it/s] 91%|█████████▏| 2870/3143 [19:26<01:51,  2.45it/s] 92%|█████████▏| 2879/3145 [19:20<01:44,  2.56it/s] 89%|████████▉ | 2808/3145 [19:21<02:21,  2.38it/s] 92%|█████████▏| 2880/3145 [19:21<01:32,  2.87it/s] 91%|█████████▏| 2871/3143 [19:27<01:50,  2.45it/s] 91%|█████████ | 2854/3145 [19:21<02:08,  2.27it/s] 89%|████████▉ | 2809/3145 [19:21<02:19,  2.41it/s] 92%|█████████▏| 2881/3145 [19:21<01:40,  2.62it/s] 91%|█████████▏| 2872/3143 [19:27<01:50,  2.45it/s] 91%|█████████ | 2855/3145 [19:21<02:04,  2.33it/s] 89%|████████▉ | 2810/3145 [19:21<02:26,  2.29it/s] 92%|█████████▏| 2882/3145 [19:22<01:40,  2.60it/s] 91%|█████████ | 2856/3145 [19:22<02:09,  2.24it/s] 91%|█████████▏| 2873/3143 [19:28<02:08,  2.11it/s] 89%|████████▉ | 2811/3145 [19:22<02:22,  2.34it/s] 92%|█████████▏| 2883/3145 [19:22<01:45,  2.49it/s] 91%|█████████ | 2857/3145 [19:22<02:12,  2.18it/s] 91%|█████████▏| 2874/3143 [19:28<02:06,  2.12it/s] 89%|████████▉ | 2812/3145 [19:22<02:15,  2.46it/s] 92%|█████████▏| 2884/3145 [19:22<01:42,  2.54it/s] 91%|█████████ | 2858/3145 [19:23<02:02,  2.34it/s] 89%|████████▉ | 2813/3145 [19:23<02:09,  2.56it/s] 91%|█████████▏| 2875/3143 [19:29<02:04,  2.16it/s] 92%|█████████▏| 2885/3145 [19:23<01:45,  2.47it/s] 91%|█████████ | 2859/3145 [19:23<02:03,  2.31it/s] 89%|████████▉ | 2814/3145 [19:23<02:07,  2.61it/s] 92%|█████████▏| 2876/3143 [19:29<02:01,  2.20it/s] 92%|█████████▏| 2886/3145 [19:23<01:43,  2.50it/s] 91%|█████████ | 2860/3145 [19:23<02:01,  2.34it/s] 92%|█████████▏| 2877/3143 [19:29<01:46,  2.49it/s] 90%|████████▉ | 2815/3145 [19:23<02:14,  2.45it/s] 92%|█████████▏| 2887/3145 [19:24<01:45,  2.44it/s] 91%|█████████ | 2861/3145 [19:24<02:02,  2.32it/s] 90%|████████▉ | 2816/3145 [19:24<02:14,  2.44it/s] 92%|█████████▏| 2878/3143 [19:30<01:53,  2.34it/s] 92%|█████████▏| 2888/3145 [19:24<01:44,  2.46it/s] 91%|█████████ | 2862/3145 [19:24<01:45,  2.67it/s] 90%|████████▉ | 2817/3145 [19:24<02:09,  2.53it/s] 92%|█████████▏| 2889/3145 [19:24<01:47,  2.37it/s] 91%|█████████ | 2863/3145 [19:25<01:50,  2.55it/s] 92%|█████████▏| 2879/3143 [19:30<02:10,  2.02it/s] 90%|████████▉ | 2818/3145 [19:25<02:13,  2.45it/s] 92%|█████████▏| 2890/3145 [19:25<01:34,  2.71it/s] 91%|█████████ | 2864/3145 [19:25<01:53,  2.48it/s] 92%|█████████▏| 2880/3143 [19:31<02:05,  2.09it/s] 90%|████████▉ | 2819/3145 [19:25<02:10,  2.50it/s] 92%|█████████▏| 2891/3145 [19:25<01:41,  2.50it/s] 91%|█████████ | 2865/3145 [19:25<01:43,  2.71it/s] 90%|████████▉ | 2820/3145 [19:25<02:10,  2.49it/s] 91%|█████████ | 2866/3145 [19:25<01:31,  3.04it/s] 92%|█████████▏| 2881/3143 [19:31<02:08,  2.04it/s] 92%|█████████▏| 2892/3145 [19:26<01:38,  2.57it/s] 91%|█████████ | 2867/3145 [19:26<01:35,  2.90it/s] 90%|████████▉ | 2821/3145 [19:26<02:09,  2.51it/s] 92%|█████████▏| 2882/3143 [19:32<02:00,  2.16it/s] 92%|█████████▏| 2893/3145 [19:26<01:38,  2.55it/s] 91%|█████████ | 2868/3145 [19:26<01:41,  2.73it/s] 92%|█████████▏| 2883/3143 [19:32<01:55,  2.25it/s] 92%|█████████▏| 2894/3145 [19:26<01:41,  2.47it/s] 90%|████████▉ | 2822/3145 [19:26<02:27,  2.20it/s] 92%|█████████▏| 2884/3143 [19:33<01:48,  2.39it/s] 91%|█████████ | 2869/3145 [19:27<01:43,  2.66it/s] 92%|█████████▏| 2895/3145 [19:27<01:44,  2.40it/s] 90%|████████▉ | 2823/3145 [19:27<02:22,  2.26it/s] 91%|█████████▏| 2870/3145 [19:27<01:46,  2.59it/s] 92%|█████████▏| 2885/3143 [19:33<01:47,  2.39it/s] 92%|█████████▏| 2896/3145 [19:27<01:41,  2.44it/s] 90%|████████▉ | 2824/3145 [19:27<02:18,  2.32it/s] 92%|█████████▏| 2886/3143 [19:33<01:43,  2.48it/s] 91%|█████████▏| 2871/3145 [19:27<01:47,  2.54it/s] 90%|████████▉ | 2825/3145 [19:28<02:11,  2.43it/s] 92%|█████████▏| 2897/3145 [19:28<01:43,  2.39it/s] 92%|█████████▏| 2887/3143 [19:34<01:42,  2.51it/s] 91%|█████████▏| 2872/3145 [19:28<01:45,  2.58it/s] 90%|████████▉ | 2826/3145 [19:28<02:10,  2.44it/s] 92%|█████████▏| 2898/3145 [19:28<01:44,  2.37it/s] 92%|█████████▏| 2888/3143 [19:34<01:44,  2.44it/s] 91%|█████████▏| 2873/3145 [19:28<01:49,  2.49it/s] 90%|████████▉ | 2827/3145 [19:28<02:06,  2.50it/s] 92%|█████████▏| 2899/3145 [19:28<01:40,  2.44it/s] 92%|█████████▏| 2889/3143 [19:35<01:43,  2.46it/s] 91%|█████████▏| 2874/3145 [19:29<01:53,  2.39it/s] 90%|████████▉ | 2828/3145 [19:29<02:07,  2.48it/s] 92%|█████████▏| 2900/3145 [19:29<01:39,  2.47it/s] 92%|█████████▏| 2890/3143 [19:35<01:46,  2.37it/s] 91%|█████████▏| 2875/3145 [19:29<01:54,  2.36it/s] 92%|█████████▏| 2901/3145 [19:29<01:37,  2.50it/s] 90%|████████▉ | 2829/3145 [19:29<02:12,  2.39it/s] 92%|█████████▏| 2891/3143 [19:35<01:47,  2.35it/s] 91%|█████████▏| 2876/3145 [19:30<01:54,  2.35it/s] 92%|█████████▏| 2902/3145 [19:30<01:40,  2.42it/s] 90%|████████▉ | 2830/3145 [19:30<02:13,  2.35it/s] 92%|█████████▏| 2892/3143 [19:36<01:42,  2.45it/s] 91%|█████████▏| 2877/3145 [19:30<01:54,  2.34it/s] 92%|█████████▏| 2903/3145 [19:30<01:40,  2.41it/s] 90%|█████████ | 2831/3145 [19:30<02:12,  2.38it/s] 92%|█████████▏| 2893/3143 [19:36<01:40,  2.48it/s] 92%|█████████▏| 2878/3145 [19:30<01:51,  2.40it/s] 92%|█████████▏| 2904/3145 [19:30<01:36,  2.50it/s] 90%|█████████ | 2832/3145 [19:30<02:05,  2.49it/s] 92%|█████████▏| 2894/3143 [19:37<01:39,  2.51it/s] 92%|█████████▏| 2879/3145 [19:31<01:46,  2.51it/s] 92%|█████████▏| 2905/3145 [19:31<01:36,  2.48it/s] 90%|█████████ | 2833/3145 [19:31<02:16,  2.28it/s] 92%|█████████▏| 2895/3143 [19:37<01:43,  2.39it/s] 92%|█████████▏| 2880/3145 [19:31<01:48,  2.44it/s] 92%|█████████▏| 2906/3145 [19:31<01:36,  2.49it/s] 90%|█████████ | 2834/3145 [19:31<02:17,  2.27it/s] 92%|█████████▏| 2907/3145 [19:32<01:25,  2.78it/s] 92%|█████████▏| 2896/3143 [19:37<01:44,  2.37it/s] 92%|█████████▏| 2881/3145 [19:32<01:49,  2.40it/s] 90%|█████████ | 2835/3145 [19:32<02:13,  2.32it/s] 92%|█████████▏| 2908/3145 [19:32<01:27,  2.70it/s] 92%|█████████▏| 2882/3145 [19:32<01:45,  2.50it/s] 92%|█████████▏| 2897/3143 [19:38<01:43,  2.37it/s] 90%|█████████ | 2836/3145 [19:32<02:09,  2.39it/s] 92%|█████████▏| 2909/3145 [19:32<01:33,  2.52it/s] 92%|█████████▏| 2898/3143 [19:38<01:40,  2.45it/s] 92%|█████████▏| 2883/3145 [19:32<01:50,  2.38it/s] 90%|█████████ | 2837/3145 [19:33<02:05,  2.45it/s] 93%|█████████▎| 2910/3145 [19:33<01:34,  2.48it/s] 92%|█████████▏| 2899/3143 [19:39<01:47,  2.27it/s] 92%|█████████▏| 2884/3145 [19:33<01:52,  2.32it/s] 90%|█████████ | 2838/3145 [19:33<02:10,  2.36it/s] 93%|█████████▎| 2911/3145 [19:33<01:39,  2.36it/s] 92%|█████████▏| 2900/3143 [19:39<01:43,  2.34it/s] 92%|█████████▏| 2885/3145 [19:33<01:52,  2.32it/s] 90%|█████████ | 2839/3145 [19:33<02:03,  2.47it/s] 93%|█████████▎| 2912/3145 [19:34<01:35,  2.43it/s] 92%|█████████▏| 2901/3143 [19:40<01:44,  2.32it/s] 92%|█████████▏| 2886/3145 [19:34<01:51,  2.32it/s] 90%|█████████ | 2840/3145 [19:34<02:02,  2.49it/s] 93%|█████████▎| 2913/3145 [19:34<01:36,  2.39it/s] 90%|█████████ | 2841/3145 [19:34<01:50,  2.74it/s] 92%|█████████▏| 2902/3143 [19:40<01:44,  2.31it/s] 92%|█████████▏| 2887/3145 [19:34<01:47,  2.39it/s] 93%|█████████▎| 2914/3145 [19:34<01:34,  2.43it/s] 90%|█████████ | 2842/3145 [19:35<01:56,  2.60it/s] 92%|█████████▏| 2888/3145 [19:35<01:45,  2.45it/s] 92%|█████████▏| 2903/3143 [19:40<01:41,  2.35it/s] 90%|█████████ | 2843/3145 [19:35<01:56,  2.60it/s] 93%|█████████▎| 2915/3145 [19:35<01:38,  2.34it/s] 92%|█████████▏| 2889/3145 [19:35<01:44,  2.46it/s] 92%|█████████▏| 2904/3143 [19:41<01:39,  2.39it/s] 90%|█████████ | 2844/3145 [19:35<01:53,  2.64it/s] 92%|█████████▏| 2890/3145 [19:35<01:42,  2.48it/s] 92%|█████████▏| 2905/3143 [19:41<01:37,  2.43it/s] 93%|█████████▎| 2916/3145 [19:35<01:42,  2.24it/s] 90%|█████████ | 2845/3145 [19:36<01:42,  2.92it/s] 92%|█████████▏| 2891/3145 [19:36<01:38,  2.57it/s] 93%|█████████▎| 2917/3145 [19:36<01:35,  2.38it/s] 92%|█████████▏| 2906/3143 [19:42<01:38,  2.40it/s] 90%|█████████ | 2846/3145 [19:36<01:59,  2.51it/s] 92%|█████████▏| 2892/3145 [19:36<01:38,  2.57it/s] 93%|█████████▎| 2918/3145 [19:36<01:33,  2.42it/s] 92%|█████████▏| 2907/3143 [19:42<01:36,  2.44it/s] 91%|█████████ | 2847/3145 [19:36<01:59,  2.50it/s] 92%|█████████▏| 2893/3145 [19:37<01:41,  2.49it/s] 93%|█████████▎| 2919/3145 [19:37<01:31,  2.46it/s] 93%|█████████▎| 2908/3143 [19:42<01:35,  2.46it/s] 91%|█████████ | 2848/3145 [19:37<02:02,  2.43it/s] 93%|█████████▎| 2920/3145 [19:37<01:30,  2.48it/s] 93%|█████████▎| 2909/3143 [19:43<01:34,  2.49it/s] 92%|█████████▏| 2894/3145 [19:37<01:50,  2.27it/s] 91%|█████████ | 2849/3145 [19:37<02:04,  2.38it/s] 93%|█████████▎| 2921/3145 [19:37<01:30,  2.46it/s] 93%|█████████▎| 2910/3143 [19:43<01:36,  2.42it/s] 92%|█████████▏| 2895/3145 [19:37<01:47,  2.32it/s] 93%|█████████▎| 2922/3145 [19:38<01:26,  2.57it/s] 93%|█████████▎| 2911/3143 [19:44<01:28,  2.62it/s] 91%|█████████ | 2850/3145 [19:38<02:10,  2.26it/s] 92%|█████████▏| 2896/3145 [19:38<01:45,  2.36it/s] 93%|█████████▎| 2923/3145 [19:38<01:28,  2.52it/s] 93%|█████████▎| 2912/3143 [19:44<01:32,  2.48it/s] 91%|█████████ | 2851/3145 [19:38<02:10,  2.25it/s] 92%|█████████▏| 2897/3145 [19:38<01:46,  2.34it/s] 93%|█████████▎| 2924/3145 [19:39<01:28,  2.51it/s] 93%|█████████▎| 2913/3143 [19:45<01:34,  2.45it/s] 92%|█████████▏| 2898/3145 [19:39<01:43,  2.40it/s] 91%|█████████ | 2852/3145 [19:39<02:22,  2.06it/s] 93%|█████████▎| 2925/3145 [19:39<01:27,  2.52it/s] 92%|█████████▏| 2899/3145 [19:39<01:29,  2.74it/s] 93%|█████████▎| 2914/3143 [19:45<01:30,  2.53it/s] 92%|█████████▏| 2900/3145 [19:39<01:22,  2.97it/s] 91%|█████████ | 2853/3145 [19:39<02:12,  2.20it/s] 93%|█████████▎| 2926/3145 [19:39<01:23,  2.63it/s] 93%|█████████▎| 2915/3143 [19:45<01:33,  2.45it/s] 92%|█████████▏| 2901/3145 [19:40<01:26,  2.81it/s] 91%|█████████ | 2854/3145 [19:40<02:07,  2.29it/s] 93%|█████████▎| 2927/3145 [19:40<01:26,  2.52it/s] 93%|█████████▎| 2916/3143 [19:46<01:33,  2.42it/s] 92%|█████████▏| 2902/3145 [19:40<01:28,  2.74it/s] 91%|█████████ | 2855/3145 [19:40<02:06,  2.30it/s] 93%|█████████▎| 2928/3145 [19:40<01:28,  2.45it/s] 93%|█████████▎| 2917/3143 [19:46<01:35,  2.37it/s] 92%|█████████▏| 2903/3145 [19:40<01:30,  2.67it/s] 91%|█████████ | 2856/3145 [19:40<02:02,  2.36it/s] 93%|█████████▎| 2918/3143 [19:47<01:31,  2.46it/s] 93%|█████████▎| 2929/3145 [19:41<01:34,  2.28it/s] 92%|█████████▏| 2904/3145 [19:41<01:34,  2.56it/s] 91%|█████████ | 2857/3145 [19:41<02:00,  2.39it/s] 93%|█████████▎| 2919/3143 [19:47<01:27,  2.57it/s] 93%|█████████▎| 2930/3145 [19:41<01:34,  2.29it/s] 92%|█████████▏| 2905/3145 [19:41<01:31,  2.62it/s] 91%|█████████ | 2858/3145 [19:41<01:58,  2.42it/s] 93%|█████████▎| 2920/3143 [19:47<01:28,  2.53it/s] 93%|█████████▎| 2931/3145 [19:42<01:31,  2.35it/s] 91%|█████████ | 2859/3145 [19:42<01:43,  2.75it/s] 92%|█████████▏| 2906/3145 [19:42<01:32,  2.59it/s] 93%|█████████▎| 2932/3145 [19:42<01:18,  2.71it/s] 93%|█████████▎| 2921/3143 [19:48<01:31,  2.44it/s] 91%|█████████ | 2860/3145 [19:42<01:47,  2.66it/s] 92%|█████████▏| 2907/3145 [19:42<01:32,  2.57it/s] 93%|█████████▎| 2933/3145 [19:42<01:22,  2.56it/s] 93%|█████████▎| 2922/3143 [19:48<01:29,  2.48it/s] 92%|█████████▏| 2908/3145 [19:42<01:31,  2.58it/s] 91%|█████████ | 2861/3145 [19:42<01:52,  2.53it/s] 93%|█████████▎| 2934/3145 [19:43<01:24,  2.48it/s] 93%|█████████▎| 2923/3143 [19:49<01:32,  2.38it/s] 91%|█████████ | 2862/3145 [19:43<02:02,  2.30it/s] 92%|█████████▏| 2909/3145 [19:43<01:45,  2.23it/s] 93%|█████████▎| 2935/3145 [19:43<01:23,  2.51it/s] 93%|█████████▎| 2924/3143 [19:49<01:32,  2.36it/s] 93%|█████████▎| 2910/3145 [19:43<01:38,  2.38it/s] 91%|█████████ | 2863/3145 [19:43<02:03,  2.29it/s] 93%|█████████▎| 2936/3145 [19:43<01:23,  2.51it/s] 93%|█████████▎| 2925/3143 [19:49<01:29,  2.44it/s] 93%|█████████▎| 2911/3145 [19:44<01:38,  2.36it/s] 91%|█████████ | 2864/3145 [19:44<02:01,  2.31it/s] 93%|█████████▎| 2937/3145 [19:44<01:25,  2.44it/s] 93%|█████████▎| 2926/3143 [19:50<01:28,  2.46it/s] 93%|█████████▎| 2912/3145 [19:44<01:37,  2.39it/s] 91%|█████████ | 2865/3145 [19:44<02:01,  2.31it/s] 93%|█████████▎| 2927/3143 [19:50<01:25,  2.51it/s] 93%|█████████▎| 2938/3145 [19:44<01:29,  2.32it/s] 93%|█████████▎| 2913/3145 [19:45<01:32,  2.50it/s] 91%|█████████ | 2866/3145 [19:45<02:03,  2.25it/s] 93%|█████████▎| 2939/3145 [19:45<01:26,  2.37it/s] 93%|█████████▎| 2928/3143 [19:51<01:28,  2.42it/s] 93%|█████████▎| 2914/3145 [19:45<01:31,  2.52it/s] 93%|█████████▎| 2940/3145 [19:45<01:22,  2.48it/s] 91%|█████████ | 2867/3145 [19:45<02:00,  2.32it/s] 93%|█████████▎| 2929/3143 [19:51<01:26,  2.46it/s] 93%|█████████▎| 2915/3145 [19:45<01:30,  2.53it/s] 91%|█████████ | 2868/3145 [19:45<01:55,  2.40it/s] 94%|█████████▎| 2941/3145 [19:46<01:24,  2.40it/s] 93%|█████████▎| 2930/3143 [19:51<01:26,  2.47it/s] 93%|█████████▎| 2916/3145 [19:46<01:32,  2.47it/s] 91%|█████████ | 2869/3145 [19:46<01:54,  2.42it/s] 93%|█████████▎| 2931/3143 [19:52<01:25,  2.48it/s] 94%|█████████▎| 2942/3145 [19:46<01:27,  2.33it/s] 93%|█████████▎| 2917/3145 [19:46<01:34,  2.42it/s] 91%|█████████▏| 2870/3145 [19:46<01:52,  2.45it/s] 93%|█████████▎| 2932/3143 [19:52<01:26,  2.44it/s] 94%|█████████▎| 2943/3145 [19:46<01:29,  2.26it/s] 93%|█████████▎| 2918/3145 [19:47<01:37,  2.34it/s] 91%|█████████▏| 2871/3145 [19:47<01:54,  2.39it/s] 93%|█████████▎| 2933/3143 [19:53<01:23,  2.52it/s] 94%|█████████▎| 2944/3145 [19:47<01:28,  2.28it/s] 93%|█████████▎| 2919/3145 [19:47<01:34,  2.38it/s] 93%|█████████▎| 2934/3143 [19:53<01:20,  2.60it/s] 91%|█████████▏| 2872/3145 [19:47<01:52,  2.42it/s] 94%|█████████▎| 2945/3145 [19:47<01:30,  2.22it/s] 93%|█████████▎| 2920/3145 [19:47<01:32,  2.44it/s] 93%|█████████▎| 2935/3143 [19:53<01:18,  2.64it/s] 91%|█████████▏| 2873/3145 [19:48<01:56,  2.33it/s] 94%|█████████▎| 2946/3145 [19:48<01:26,  2.31it/s] 93%|█████████▎| 2921/3145 [19:48<01:28,  2.54it/s] 93%|█████████▎| 2936/3143 [19:54<01:19,  2.60it/s] 91%|█████████▏| 2874/3145 [19:48<01:53,  2.39it/s] 93%|█████████▎| 2937/3143 [19:54<01:14,  2.76it/s] 94%|█████████▎| 2947/3145 [19:48<01:24,  2.35it/s] 93%|█████████▎| 2922/3145 [19:48<01:30,  2.46it/s] 91%|█████████▏| 2875/3145 [19:48<01:53,  2.37it/s] 93%|█████████▎| 2938/3143 [19:54<01:14,  2.77it/s] 94%|█████████▎| 2948/3145 [19:49<01:21,  2.42it/s] 93%|█████████▎| 2923/3145 [19:49<01:32,  2.41it/s] 91%|█████████▏| 2876/3145 [19:49<01:57,  2.30it/s] 94%|█████████▎| 2939/3143 [19:55<01:16,  2.68it/s] 94%|█████████▍| 2949/3145 [19:49<01:22,  2.38it/s] 93%|█████████▎| 2924/3145 [19:49<01:30,  2.45it/s] 91%|█████████▏| 2877/3145 [19:49<01:54,  2.35it/s] 94%|█████████▎| 2940/3143 [19:55<01:16,  2.64it/s] 93%|█████████▎| 2925/3145 [19:49<01:26,  2.55it/s] 94%|█████████▍| 2950/3145 [19:49<01:21,  2.40it/s] 94%|█████████▎| 2941/3143 [19:56<01:17,  2.60it/s] 92%|█████████▏| 2878/3145 [19:50<01:55,  2.32it/s] 93%|█████████▎| 2926/3145 [19:50<01:26,  2.53it/s] 94%|█████████▍| 2951/3145 [19:50<01:21,  2.37it/s] 94%|█████████▎| 2942/3143 [19:56<01:11,  2.83it/s] 92%|█████████▏| 2879/3145 [19:50<01:53,  2.35it/s] 93%|█████████▎| 2927/3145 [19:50<01:26,  2.53it/s] 94%|█████████▍| 2952/3145 [19:50<01:23,  2.31it/s] 94%|█████████▎| 2943/3143 [19:56<01:15,  2.66it/s] 92%|█████████▏| 2880/3145 [19:50<01:43,  2.55it/s] 94%|█████████▍| 2953/3145 [19:51<01:19,  2.42it/s] 93%|█████████▎| 2928/3145 [19:51<01:31,  2.38it/s] 94%|█████████▎| 2944/3143 [19:57<01:16,  2.61it/s] 92%|█████████▏| 2881/3145 [19:51<01:45,  2.51it/s] 94%|█████████▍| 2954/3145 [19:51<01:18,  2.44it/s] 93%|█████████▎| 2929/3145 [19:51<01:31,  2.36it/s] 92%|█████████▏| 2882/3145 [19:51<01:50,  2.39it/s] 94%|█████████▎| 2945/3143 [19:57<01:28,  2.23it/s] 94%|█████████▍| 2955/3145 [19:51<01:16,  2.49it/s] 93%|█████████▎| 2930/3145 [19:51<01:26,  2.47it/s] 92%|█████████▏| 2883/3145 [19:52<01:55,  2.27it/s] 94%|█████████▍| 2956/3145 [19:52<01:15,  2.49it/s] 94%|█████████▎| 2946/3143 [19:58<01:27,  2.25it/s] 93%|█████████▎| 2931/3145 [19:52<01:26,  2.47it/s] 94%|█████████▍| 2957/3145 [19:52<01:06,  2.81it/s] 93%|█████████▎| 2932/3145 [19:52<01:25,  2.50it/s] 92%|█████████▏| 2884/3145 [19:52<01:54,  2.27it/s] 94%|█████████▍| 2947/3143 [19:58<01:25,  2.30it/s] 94%|█████████▍| 2958/3145 [19:53<01:10,  2.64it/s] 92%|█████████▏| 2885/3145 [19:53<01:49,  2.38it/s] 93%|█████████▎| 2933/3145 [19:53<01:24,  2.52it/s] 94%|█████████▍| 2948/3143 [19:59<01:28,  2.21it/s] 94%|█████████▍| 2959/3145 [19:53<01:11,  2.61it/s] 93%|█████████▎| 2934/3145 [19:53<01:23,  2.52it/s] 92%|█████████▏| 2886/3145 [19:53<01:53,  2.28it/s] 94%|█████████▍| 2949/3143 [19:59<01:22,  2.35it/s] 94%|█████████▍| 2960/3145 [19:53<01:14,  2.48it/s] 93%|█████████▎| 2935/3145 [19:53<01:21,  2.58it/s] 92%|█████████▏| 2887/3145 [19:53<01:47,  2.39it/s] 94%|█████████▍| 2950/3143 [19:59<01:21,  2.37it/s] 94%|█████████▍| 2961/3145 [19:54<01:13,  2.51it/s] 92%|█████████▏| 2888/3145 [19:54<01:48,  2.36it/s] 94%|█████████▍| 2951/3143 [20:00<01:19,  2.42it/s] 93%|█████████▎| 2936/3145 [19:54<01:34,  2.22it/s] 94%|█████████▍| 2962/3145 [19:54<01:10,  2.59it/s] 92%|█████████▏| 2889/3145 [19:54<01:46,  2.40it/s] 94%|█████████▍| 2952/3143 [20:00<01:18,  2.44it/s] 93%|█████████▎| 2937/3145 [19:54<01:29,  2.34it/s] 94%|█████████▍| 2963/3145 [19:55<01:10,  2.57it/s] 94%|█████████▍| 2953/3143 [20:01<01:18,  2.42it/s] 92%|█████████▏| 2890/3145 [19:55<01:48,  2.35it/s] 93%|█████████▎| 2938/3145 [19:55<01:26,  2.38it/s] 94%|█████████▍| 2964/3145 [19:55<01:10,  2.56it/s] 94%|█████████▍| 2954/3143 [20:01<01:17,  2.44it/s] 92%|█████████▏| 2891/3145 [19:55<01:46,  2.39it/s] 93%|█████████▎| 2939/3145 [19:55<01:26,  2.39it/s] 94%|█████████▍| 2965/3145 [19:55<01:13,  2.44it/s] 94%|█████████▍| 2955/3143 [20:01<01:16,  2.45it/s] 93%|█████████▎| 2940/3145 [19:56<01:23,  2.45it/s] 92%|█████████▏| 2892/3145 [19:56<01:48,  2.33it/s] 94%|█████████▍| 2966/3145 [19:56<01:12,  2.48it/s] 94%|█████████▍| 2956/3143 [20:02<01:15,  2.47it/s] 94%|█████████▎| 2941/3145 [19:56<01:22,  2.47it/s] 92%|█████████▏| 2893/3145 [19:56<01:46,  2.37it/s] 94%|█████████▍| 2967/3145 [19:56<01:11,  2.51it/s] 94%|█████████▍| 2957/3143 [20:02<01:14,  2.50it/s] 94%|█████████▎| 2942/3145 [19:56<01:21,  2.49it/s] 92%|█████████▏| 2894/3145 [19:56<01:47,  2.34it/s] 94%|█████████▍| 2968/3145 [19:57<01:12,  2.45it/s] 94%|█████████▍| 2958/3143 [20:03<01:12,  2.57it/s] 94%|█████████▎| 2943/3145 [19:57<01:22,  2.44it/s] 92%|█████████▏| 2895/3145 [19:57<01:47,  2.32it/s] 94%|█████████▍| 2969/3145 [19:57<01:14,  2.38it/s] 94%|█████████▍| 2959/3143 [20:03<01:09,  2.66it/s] 94%|█████████▎| 2944/3145 [19:57<01:19,  2.52it/s] 92%|█████████▏| 2896/3145 [19:57<01:50,  2.26it/s] 94%|█████████▍| 2960/3143 [20:03<01:07,  2.73it/s] 94%|█████████▍| 2970/3145 [19:57<01:12,  2.42it/s] 94%|█████████▎| 2945/3145 [19:58<01:22,  2.44it/s] 92%|█████████▏| 2897/3145 [19:58<01:45,  2.34it/s] 94%|█████████▍| 2961/3143 [20:04<01:08,  2.66it/s] 94%|█████████▍| 2971/3145 [19:58<01:13,  2.37it/s] 94%|█████████▎| 2946/3145 [19:58<01:21,  2.45it/s] 92%|█████████▏| 2898/3145 [19:58<01:46,  2.31it/s] 94%|█████████▍| 2962/3143 [20:04<01:12,  2.48it/s] 94%|█████████▍| 2972/3145 [19:58<01:13,  2.34it/s] 94%|█████████▎| 2947/3145 [19:58<01:13,  2.69it/s] 92%|█████████▏| 2899/3145 [19:59<01:44,  2.36it/s] 94%|█████████▎| 2948/3145 [19:59<01:12,  2.72it/s] 94%|█████████▍| 2963/3143 [20:05<01:12,  2.50it/s] 95%|█████████▍| 2973/3145 [19:59<01:14,  2.31it/s] 94%|█████████▍| 2964/3143 [20:05<01:12,  2.47it/s] 94%|█████████▍| 2949/3145 [19:59<01:15,  2.59it/s] 92%|█████████▏| 2900/3145 [19:59<01:48,  2.26it/s] 95%|█████████▍| 2974/3145 [19:59<01:11,  2.38it/s] 94%|█████████▍| 2950/3145 [19:59<01:13,  2.64it/s] 95%|█████████▍| 2975/3145 [20:00<01:09,  2.43it/s] 94%|█████████▍| 2965/3143 [20:05<01:15,  2.36it/s] 92%|█████████▏| 2901/3145 [20:00<01:49,  2.24it/s] 94%|█████████▍| 2951/3145 [20:00<01:12,  2.69it/s] 95%|█████████▍| 2976/3145 [20:00<01:09,  2.44it/s] 94%|█████████▍| 2966/3143 [20:06<01:14,  2.38it/s] 92%|█████████▏| 2902/3145 [20:00<01:50,  2.20it/s] 94%|█████████▍| 2952/3145 [20:00<01:12,  2.66it/s] 95%|█████████▍| 2977/3145 [20:00<01:08,  2.46it/s] 94%|█████████▍| 2967/3143 [20:06<01:12,  2.42it/s] 92%|█████████▏| 2903/3145 [20:00<01:44,  2.32it/s] 94%|█████████▍| 2953/3145 [20:01<01:14,  2.58it/s] 95%|█████████▍| 2978/3145 [20:01<01:07,  2.46it/s] 94%|█████████▍| 2968/3143 [20:07<01:16,  2.29it/s] 92%|█████████▏| 2904/3145 [20:01<01:46,  2.26it/s] 94%|█████████▍| 2954/3145 [20:01<01:13,  2.59it/s] 94%|█████████▍| 2969/3143 [20:07<01:12,  2.40it/s] 95%|█████████▍| 2979/3145 [20:01<01:11,  2.31it/s] 92%|█████████▏| 2905/3145 [20:01<01:48,  2.22it/s] 94%|█████████▍| 2955/3145 [20:01<01:15,  2.52it/s] 94%|█████████▍| 2970/3143 [20:07<01:02,  2.76it/s] 95%|█████████▍| 2980/3145 [20:02<01:08,  2.42it/s] 92%|█████████▏| 2906/3145 [20:02<01:44,  2.28it/s] 95%|█████████▍| 2971/3143 [20:08<01:01,  2.79it/s] 94%|█████████▍| 2956/3145 [20:02<01:16,  2.46it/s] 95%|█████████▍| 2981/3145 [20:02<01:10,  2.34it/s] 94%|█████████▍| 2957/3145 [20:02<01:15,  2.50it/s] 95%|█████████▍| 2972/3143 [20:08<01:05,  2.60it/s] 92%|█████████▏| 2907/3145 [20:02<01:48,  2.19it/s] 95%|█████████▍| 2982/3145 [20:02<01:07,  2.40it/s] 95%|█████████▍| 2973/3143 [20:08<00:59,  2.85it/s] 94%|█████████▍| 2958/3145 [20:03<01:17,  2.41it/s] 92%|█████████▏| 2908/3145 [20:03<01:47,  2.20it/s] 95%|█████████▍| 2983/3145 [20:03<01:03,  2.56it/s] 95%|█████████▍| 2974/3143 [20:09<00:59,  2.83it/s] 94%|█████████▍| 2959/3145 [20:03<01:16,  2.43it/s] 95%|█████████▍| 2984/3145 [20:03<00:58,  2.75it/s] 92%|█████████▏| 2909/3145 [20:03<01:46,  2.21it/s] 95%|█████████▍| 2975/3143 [20:09<01:01,  2.73it/s] 94%|█████████▍| 2960/3145 [20:03<01:14,  2.48it/s] 95%|█████████▍| 2985/3145 [20:03<01:00,  2.65it/s] 93%|█████████▎| 2910/3145 [20:04<01:42,  2.30it/s] 95%|█████████▍| 2976/3143 [20:10<01:02,  2.68it/s] 95%|█████████▍| 2986/3145 [20:04<01:00,  2.63it/s] 94%|█████████▍| 2961/3145 [20:04<01:17,  2.38it/s] 93%|█████████▎| 2911/3145 [20:04<01:39,  2.36it/s] 95%|█████████▍| 2977/3143 [20:10<01:04,  2.56it/s] 95%|█████████▍| 2987/3145 [20:04<01:00,  2.62it/s] 94%|█████████▍| 2962/3145 [20:04<01:18,  2.32it/s] 93%|█████████▎| 2912/3145 [20:04<01:39,  2.33it/s] 95%|█████████▍| 2978/3143 [20:10<01:06,  2.46it/s] 95%|█████████▌| 2988/3145 [20:05<01:02,  2.52it/s] 94%|█████████▍| 2963/3145 [20:05<01:18,  2.32it/s] 93%|█████████▎| 2913/3145 [20:05<01:43,  2.23it/s] 95%|█████████▍| 2979/3143 [20:11<01:04,  2.52it/s] 95%|█████████▌| 2989/3145 [20:05<01:03,  2.46it/s] 94%|█████████▍| 2964/3145 [20:05<01:16,  2.36it/s] 93%|█████████▎| 2914/3145 [20:05<01:37,  2.38it/s] 95%|█████████▍| 2980/3143 [20:11<01:04,  2.52it/s] 95%|█████████▌| 2990/3145 [20:06<01:02,  2.50it/s] 94%|█████████▍| 2965/3145 [20:06<01:14,  2.41it/s] 93%|█████████▎| 2915/3145 [20:06<01:35,  2.41it/s] 95%|█████████▍| 2981/3143 [20:12<01:07,  2.39it/s] 95%|█████████▌| 2991/3145 [20:06<01:01,  2.50it/s] 93%|█████████▎| 2916/3145 [20:06<01:30,  2.52it/s] 94%|█████████▍| 2966/3145 [20:06<01:13,  2.43it/s] 95%|█████████▍| 2982/3143 [20:12<01:05,  2.45it/s] 95%|█████████▌| 2992/3145 [20:06<00:59,  2.56it/s] 93%|█████████▎| 2917/3145 [20:06<01:31,  2.49it/s] 94%|█████████▍| 2967/3145 [20:06<01:12,  2.45it/s] 95%|█████████▍| 2983/3143 [20:12<01:04,  2.47it/s] 95%|█████████▌| 2993/3145 [20:07<01:02,  2.44it/s] 93%|█████████▎| 2918/3145 [20:07<01:28,  2.56it/s] 95%|█████████▍| 2984/3143 [20:13<00:58,  2.71it/s] 94%|█████████▍| 2968/3145 [20:07<01:21,  2.18it/s] 95%|█████████▌| 2994/3145 [20:07<01:01,  2.46it/s] 93%|█████████▎| 2919/3145 [20:07<01:33,  2.41it/s] 95%|█████████▍| 2985/3143 [20:13<00:59,  2.64it/s] 94%|█████████▍| 2969/3145 [20:07<01:17,  2.27it/s] 95%|█████████▌| 2995/3145 [20:08<01:00,  2.49it/s] 93%|█████████▎| 2920/3145 [20:08<01:34,  2.39it/s] 95%|█████████▌| 2986/3143 [20:14<01:02,  2.52it/s] 94%|█████████▍| 2970/3145 [20:08<01:12,  2.41it/s] 95%|█████████▌| 2996/3145 [20:08<00:58,  2.56it/s] 93%|█████████▎| 2921/3145 [20:08<01:32,  2.43it/s] 95%|█████████▌| 2987/3143 [20:14<01:03,  2.44it/s] 94%|█████████▍| 2971/3145 [20:08<01:20,  2.16it/s] 95%|█████████▌| 2997/3145 [20:08<00:59,  2.47it/s] 93%|█████████▎| 2922/3145 [20:08<01:31,  2.45it/s] 95%|█████████▌| 2988/3143 [20:14<01:05,  2.38it/s] 95%|█████████▌| 2998/3145 [20:09<01:01,  2.37it/s] 94%|█████████▍| 2972/3145 [20:09<01:21,  2.13it/s] 93%|█████████▎| 2923/3145 [20:09<01:28,  2.50it/s] 95%|█████████▌| 2989/3143 [20:15<01:03,  2.42it/s] 95%|█████████▌| 2999/3145 [20:09<00:58,  2.48it/s] 95%|█████████▍| 2973/3145 [20:09<01:17,  2.23it/s] 93%|█████████▎| 2924/3145 [20:09<01:28,  2.51it/s] 95%|█████████▌| 2990/3143 [20:15<01:03,  2.41it/s] 95%|█████████▌| 3000/3145 [20:10<00:59,  2.43it/s] 93%|█████████▎| 2925/3145 [20:10<01:28,  2.49it/s] 95%|█████████▍| 2974/3145 [20:10<01:16,  2.23it/s] 95%|█████████▌| 2991/3143 [20:16<01:02,  2.45it/s] 95%|█████████▌| 3001/3145 [20:10<00:58,  2.48it/s] 93%|█████████▎| 2926/3145 [20:10<01:29,  2.43it/s] 95%|█████████▍| 2975/3145 [20:10<01:18,  2.16it/s] 95%|█████████▌| 2992/3143 [20:16<01:00,  2.49it/s] 95%|█████████▌| 3002/3145 [20:10<00:55,  2.58it/s] 93%|█████████▎| 2927/3145 [20:10<01:29,  2.43it/s] 95%|█████████▌| 2993/3143 [20:17<01:04,  2.34it/s] 95%|█████████▍| 2976/3145 [20:11<01:23,  2.02it/s] 95%|█████████▌| 3003/3145 [20:11<00:57,  2.48it/s] 93%|█████████▎| 2928/3145 [20:11<01:33,  2.33it/s] 95%|█████████▌| 2994/3143 [20:17<01:00,  2.46it/s] 96%|█████████▌| 3004/3145 [20:11<00:54,  2.56it/s] 95%|█████████▍| 2977/3145 [20:11<01:21,  2.07it/s] 95%|█████████▌| 2995/3143 [20:17<00:59,  2.48it/s] 93%|█████████▎| 2929/3145 [20:11<01:35,  2.26it/s] 96%|█████████▌| 3005/3145 [20:12<00:55,  2.50it/s] 95%|█████████▍| 2978/3145 [20:12<01:17,  2.15it/s] 95%|█████████▌| 2996/3143 [20:18<00:51,  2.88it/s] 93%|█████████▎| 2930/3145 [20:12<01:34,  2.27it/s] 96%|█████████▌| 3006/3145 [20:12<00:56,  2.46it/s] 95%|█████████▍| 2979/3145 [20:12<01:13,  2.26it/s] 95%|█████████▌| 2997/3143 [20:18<00:53,  2.75it/s] 93%|█████████▎| 2931/3145 [20:12<01:30,  2.36it/s] 95%|█████████▍| 2980/3145 [20:12<01:10,  2.33it/s] 96%|█████████▌| 3007/3145 [20:12<00:57,  2.40it/s] 95%|█████████▌| 2998/3143 [20:18<00:54,  2.66it/s] 93%|█████████▎| 2932/3145 [20:13<01:32,  2.30it/s] 95%|█████████▍| 2981/3145 [20:13<01:07,  2.44it/s] 96%|█████████▌| 3008/3145 [20:13<00:56,  2.44it/s] 95%|█████████▌| 2999/3143 [20:19<01:03,  2.28it/s] 93%|█████████▎| 2933/3145 [20:13<01:27,  2.43it/s] 95%|█████████▍| 2982/3145 [20:13<01:07,  2.40it/s] 96%|█████████▌| 3009/3145 [20:13<00:55,  2.47it/s] 95%|█████████▌| 3000/3143 [20:19<00:56,  2.51it/s] 93%|█████████▎| 2934/3145 [20:13<01:24,  2.51it/s] 96%|█████████▌| 3010/3145 [20:14<00:53,  2.52it/s] 95%|█████████▍| 2983/3145 [20:14<01:06,  2.43it/s] 95%|█████████▌| 3001/3143 [20:20<00:58,  2.45it/s] 93%|█████████▎| 2935/3145 [20:14<01:26,  2.43it/s] 95%|█████████▍| 2984/3145 [20:14<01:04,  2.51it/s] 96%|█████████▌| 3011/3145 [20:14<00:54,  2.45it/s] 96%|█████████▌| 3002/3143 [20:20<00:56,  2.48it/s] 95%|█████████▍| 2985/3145 [20:14<01:02,  2.57it/s] 93%|█████████▎| 2936/3145 [20:14<01:29,  2.34it/s] 96%|█████████▌| 3012/3145 [20:14<00:53,  2.47it/s] 96%|█████████▌| 3003/3143 [20:20<00:56,  2.49it/s] 95%|█████████▍| 2986/3145 [20:15<01:03,  2.50it/s] 93%|█████████▎| 2937/3145 [20:15<01:29,  2.32it/s] 96%|█████████▌| 3013/3145 [20:15<00:54,  2.42it/s] 96%|█████████▌| 3004/3143 [20:21<00:54,  2.54it/s] 95%|█████████▍| 2987/3145 [20:15<01:03,  2.49it/s] 93%|█████████▎| 2938/3145 [20:15<01:27,  2.35it/s] 96%|█████████▌| 3014/3145 [20:15<00:53,  2.44it/s] 96%|█████████▌| 3005/3143 [20:21<00:56,  2.44it/s] 95%|█████████▌| 2988/3145 [20:16<01:02,  2.50it/s] 93%|█████████▎| 2939/3145 [20:16<01:24,  2.44it/s] 96%|█████████▌| 3015/3145 [20:16<00:54,  2.40it/s] 96%|█████████▌| 3006/3143 [20:22<00:53,  2.57it/s] 95%|█████████▌| 2989/3145 [20:16<01:01,  2.53it/s] 93%|█████████▎| 2940/3145 [20:16<01:25,  2.38it/s] 96%|█████████▌| 3016/3145 [20:16<00:53,  2.43it/s] 96%|█████████▌| 3007/3143 [20:22<00:54,  2.49it/s] 95%|█████████▌| 2990/3145 [20:16<00:55,  2.79it/s] 96%|█████████▌| 3017/3145 [20:16<00:51,  2.50it/s] 94%|█████████▎| 2941/3145 [20:16<01:27,  2.33it/s] 96%|█████████▌| 3008/3143 [20:22<00:54,  2.47it/s] 95%|█████████▌| 2991/3145 [20:17<00:55,  2.76it/s] 96%|█████████▌| 3018/3145 [20:17<00:50,  2.51it/s] 96%|█████████▌| 3009/3143 [20:23<00:49,  2.69it/s] 94%|█████████▎| 2942/3145 [20:17<01:26,  2.35it/s] 95%|█████████▌| 2992/3145 [20:17<00:59,  2.57it/s] 96%|█████████▌| 3019/3145 [20:17<00:49,  2.53it/s] 96%|█████████▌| 3010/3143 [20:23<00:50,  2.62it/s] 94%|█████████▎| 2943/3145 [20:17<01:28,  2.28it/s] 95%|█████████▌| 2993/3145 [20:17<00:59,  2.53it/s] 96%|█████████▌| 3020/3145 [20:18<00:48,  2.60it/s] 96%|█████████▌| 3011/3143 [20:24<00:50,  2.61it/s] 94%|█████████▎| 2944/3145 [20:18<01:27,  2.30it/s] 95%|█████████▌| 2994/3145 [20:18<01:03,  2.40it/s] 96%|█████████▌| 3021/3145 [20:18<00:49,  2.51it/s] 96%|█████████▌| 3012/3143 [20:24<00:52,  2.48it/s] 95%|█████████▌| 2995/3145 [20:18<01:01,  2.44it/s] 94%|█████████▎| 2945/3145 [20:18<01:36,  2.08it/s] 96%|█████████▌| 3022/3145 [20:18<00:49,  2.46it/s] 96%|█████████▌| 3013/3143 [20:24<00:52,  2.47it/s] 95%|█████████▌| 2996/3145 [20:19<00:59,  2.52it/s] 96%|█████████▌| 3023/3145 [20:19<00:46,  2.61it/s] 94%|█████████▎| 2946/3145 [20:19<01:30,  2.20it/s] 96%|█████████▌| 3014/3143 [20:25<00:52,  2.48it/s] 95%|█████████▌| 2997/3145 [20:19<00:52,  2.85it/s] 94%|█████████▎| 2947/3145 [20:19<01:24,  2.35it/s] 96%|█████████▌| 3024/3145 [20:19<00:48,  2.50it/s] 96%|█████████▌| 3015/3143 [20:25<00:51,  2.49it/s] 95%|█████████▌| 2998/3145 [20:19<00:53,  2.76it/s] 94%|█████████▎| 2948/3145 [20:19<01:22,  2.40it/s] 96%|█████████▌| 3025/3145 [20:20<00:47,  2.51it/s] 96%|█████████▌| 3016/3143 [20:26<00:52,  2.42it/s] 95%|█████████▌| 2999/3145 [20:20<00:56,  2.59it/s] 94%|█████████▍| 2949/3145 [20:20<01:23,  2.36it/s] 96%|█████████▌| 3026/3145 [20:20<00:48,  2.45it/s] 96%|█████████▌| 3017/3143 [20:26<00:51,  2.46it/s] 95%|█████████▌| 3000/3145 [20:20<00:58,  2.47it/s] 94%|█████████▍| 2950/3145 [20:20<01:21,  2.40it/s] 96%|█████████▌| 3027/3145 [20:20<00:49,  2.36it/s] 96%|█████████▌| 3018/3143 [20:26<00:51,  2.42it/s] 95%|█████████▌| 3001/3145 [20:21<00:57,  2.49it/s] 94%|█████████▍| 2951/3145 [20:21<01:22,  2.36it/s] 96%|█████████▌| 3019/3143 [20:27<00:50,  2.47it/s] 96%|█████████▋| 3028/3145 [20:21<00:50,  2.31it/s] 95%|█████████▌| 3002/3145 [20:21<00:56,  2.51it/s] 94%|█████████▍| 2952/3145 [20:21<01:22,  2.33it/s] 96%|█████████▌| 3020/3143 [20:27<00:49,  2.48it/s] 96%|█████████▋| 3029/3145 [20:21<00:52,  2.21it/s] 95%|█████████▌| 3003/3145 [20:22<01:02,  2.29it/s] 94%|█████████▍| 2953/3145 [20:22<01:24,  2.28it/s] 96%|█████████▌| 3021/3143 [20:28<00:52,  2.34it/s] 96%|█████████▋| 3030/3145 [20:22<00:49,  2.31it/s] 96%|█████████▌| 3004/3145 [20:22<00:59,  2.35it/s] 94%|█████████▍| 2954/3145 [20:22<01:21,  2.35it/s] 96%|█████████▌| 3022/3143 [20:28<00:50,  2.40it/s] 96%|█████████▋| 3031/3145 [20:22<00:48,  2.36it/s] 96%|█████████▌| 3005/3145 [20:22<00:58,  2.40it/s] 94%|█████████▍| 2955/3145 [20:22<01:18,  2.41it/s] 96%|█████████▋| 3032/3145 [20:23<00:45,  2.50it/s] 96%|█████████▌| 3023/3143 [20:28<00:48,  2.47it/s] 96%|█████████▌| 3006/3145 [20:23<00:57,  2.41it/s] 94%|█████████▍| 2956/3145 [20:23<01:16,  2.48it/s] 96%|█████████▋| 3033/3145 [20:23<00:42,  2.63it/s] 96%|█████████▌| 3024/3143 [20:29<00:48,  2.43it/s] 96%|█████████▌| 3007/3145 [20:23<00:58,  2.37it/s] 94%|█████████▍| 2957/3145 [20:23<01:15,  2.49it/s] 96%|█████████▋| 3034/3145 [20:23<00:42,  2.61it/s] 96%|█████████▌| 3025/3143 [20:29<00:49,  2.41it/s] 96%|█████████▌| 3008/3145 [20:24<00:58,  2.34it/s] 94%|█████████▍| 2958/3145 [20:24<01:18,  2.38it/s] 97%|█████████▋| 3035/3145 [20:24<00:45,  2.44it/s] 96%|█████████▋| 3026/3143 [20:30<00:50,  2.32it/s] 96%|█████████▌| 3009/3145 [20:24<00:55,  2.47it/s] 94%|█████████▍| 2959/3145 [20:24<01:19,  2.33it/s] 97%|█████████▋| 3036/3145 [20:24<00:45,  2.40it/s] 96%|█████████▋| 3027/3143 [20:30<00:47,  2.42it/s] 96%|█████████▌| 3010/3145 [20:24<00:54,  2.47it/s] 97%|█████████▋| 3037/3145 [20:25<00:43,  2.46it/s] 94%|█████████▍| 2960/3145 [20:25<01:20,  2.31it/s] 96%|█████████▋| 3028/3143 [20:31<00:46,  2.45it/s] 96%|█████████▌| 3011/3145 [20:25<00:54,  2.44it/s] 97%|█████████▋| 3038/3145 [20:25<00:43,  2.48it/s] 94%|█████████▍| 2961/3145 [20:25<01:17,  2.38it/s] 96%|█████████▋| 3029/3143 [20:31<00:46,  2.48it/s] 96%|█████████▌| 3012/3145 [20:25<00:53,  2.47it/s] 96%|█████████▋| 3030/3143 [20:31<00:40,  2.81it/s] 94%|█████████▍| 2962/3145 [20:25<01:16,  2.40it/s] 97%|█████████▋| 3039/3145 [20:25<00:44,  2.38it/s] 96%|█████████▌| 3013/3145 [20:26<00:59,  2.23it/s] 96%|█████████▋| 3031/3143 [20:32<00:43,  2.58it/s] 97%|█████████▋| 3040/3145 [20:26<00:43,  2.43it/s] 94%|█████████▍| 2963/3145 [20:26<01:17,  2.35it/s] 96%|█████████▌| 3014/3145 [20:26<00:55,  2.34it/s] 97%|█████████▋| 3041/3145 [20:26<00:41,  2.53it/s] 94%|█████████▍| 2964/3145 [20:26<01:13,  2.46it/s] 96%|█████████▋| 3032/3143 [20:32<00:45,  2.45it/s] 96%|█████████▌| 3015/3145 [20:26<00:49,  2.61it/s] 97%|█████████▋| 3042/3145 [20:27<00:39,  2.61it/s] 97%|█████████▋| 3033/3143 [20:33<00:45,  2.40it/s] 94%|█████████▍| 2965/3145 [20:27<01:15,  2.39it/s] 96%|█████████▌| 3016/3145 [20:27<00:47,  2.69it/s] 97%|█████████▋| 3043/3145 [20:27<00:39,  2.57it/s] 97%|█████████▋| 3034/3143 [20:33<00:43,  2.49it/s] 94%|█████████▍| 2966/3145 [20:27<01:16,  2.33it/s] 96%|█████████▌| 3017/3145 [20:27<00:50,  2.52it/s] 97%|█████████▋| 3035/3143 [20:33<00:43,  2.49it/s] 94%|█████████▍| 2967/3145 [20:27<01:12,  2.44it/s] 97%|█████████▋| 3044/3145 [20:28<00:45,  2.22it/s] 96%|█████████▌| 3018/3145 [20:28<00:54,  2.34it/s] 97%|█████████▋| 3036/3143 [20:34<00:42,  2.49it/s] 94%|█████████▍| 2968/3145 [20:28<01:10,  2.51it/s] 97%|█████████▋| 3045/3145 [20:28<00:42,  2.35it/s] 96%|█████████▌| 3019/3145 [20:28<00:54,  2.29it/s] 97%|█████████▋| 3037/3143 [20:34<00:41,  2.55it/s] 94%|█████████▍| 2969/3145 [20:28<01:10,  2.49it/s] 97%|█████████▋| 3046/3145 [20:28<00:45,  2.20it/s] 96%|█████████▌| 3020/3145 [20:29<00:52,  2.36it/s] 97%|█████████▋| 3038/3143 [20:35<00:42,  2.47it/s] 94%|█████████▍| 2970/3145 [20:29<01:09,  2.52it/s] 96%|█████████▌| 3021/3145 [20:29<00:51,  2.40it/s] 97%|█████████▋| 3047/3145 [20:29<00:46,  2.09it/s] 97%|█████████▋| 3039/3143 [20:35<00:41,  2.50it/s] 94%|█████████▍| 2971/3145 [20:29<01:09,  2.49it/s] 97%|█████████▋| 3048/3145 [20:29<00:44,  2.16it/s] 97%|█████████▋| 3040/3143 [20:35<00:40,  2.51it/s] 96%|█████████▌| 3022/3145 [20:29<00:54,  2.25it/s] 94%|█████████▍| 2972/3145 [20:29<01:09,  2.48it/s] 97%|█████████▋| 3049/3145 [20:30<00:41,  2.34it/s] 97%|█████████▋| 3041/3143 [20:36<00:40,  2.49it/s] 96%|█████████▌| 3023/3145 [20:30<00:52,  2.31it/s] 95%|█████████▍| 2973/3145 [20:30<01:09,  2.46it/s] 97%|█████████▋| 3050/3145 [20:30<00:40,  2.33it/s] 97%|█████████▋| 3042/3143 [20:36<00:39,  2.57it/s] 96%|█████████▌| 3024/3145 [20:30<00:51,  2.37it/s] 95%|█████████▍| 2974/3145 [20:30<01:07,  2.53it/s] 97%|█████████▋| 3051/3145 [20:31<00:38,  2.42it/s] 97%|█████████▋| 3043/3143 [20:36<00:39,  2.56it/s] 96%|█████████▌| 3025/3145 [20:31<00:50,  2.36it/s] 95%|█████████▍| 2975/3145 [20:31<01:09,  2.44it/s] 97%|█████████▋| 3044/3143 [20:37<00:38,  2.55it/s] 97%|█████████▋| 3052/3145 [20:31<00:40,  2.30it/s] 95%|█████████▍| 2976/3145 [20:31<01:06,  2.53it/s] 96%|█████████▌| 3026/3145 [20:31<00:52,  2.26it/s] 97%|█████████▋| 3045/3143 [20:37<00:37,  2.60it/s] 95%|█████████▍| 2977/3145 [20:31<01:05,  2.58it/s] 97%|█████████▋| 3053/3145 [20:31<00:39,  2.30it/s] 96%|█████████▌| 3027/3145 [20:32<00:52,  2.24it/s] 95%|█████████▍| 2978/3145 [20:32<01:08,  2.44it/s] 97%|█████████▋| 3054/3145 [20:32<00:39,  2.30it/s] 97%|█████████▋| 3046/3143 [20:38<00:43,  2.25it/s] 96%|█████████▋| 3028/3145 [20:32<00:50,  2.31it/s] 97%|█████████▋| 3055/3145 [20:32<00:37,  2.39it/s] 95%|█████████▍| 2979/3145 [20:32<01:10,  2.37it/s] 96%|█████████▋| 3029/3145 [20:32<00:47,  2.43it/s] 97%|█████████▋| 3047/3143 [20:38<00:42,  2.25it/s] 97%|█████████▋| 3056/3145 [20:33<00:36,  2.43it/s] 96%|█████████▋| 3030/3145 [20:33<00:46,  2.48it/s] 95%|█████████▍| 2980/3145 [20:33<01:08,  2.39it/s] 97%|█████████▋| 3048/3143 [20:39<00:42,  2.24it/s] 97%|█████████▋| 3057/3145 [20:33<00:36,  2.44it/s] 96%|█████████▋| 3031/3145 [20:33<00:44,  2.55it/s] 97%|█████████▋| 3049/3143 [20:39<00:40,  2.31it/s] 95%|█████████▍| 2981/3145 [20:33<01:11,  2.29it/s] 97%|█████████▋| 3058/3145 [20:33<00:34,  2.49it/s] 97%|█████████▋| 3050/3143 [20:40<00:39,  2.34it/s] 96%|█████████▋| 3032/3145 [20:34<00:50,  2.24it/s] 95%|█████████▍| 2982/3145 [20:34<01:13,  2.23it/s] 97%|█████████▋| 3059/3145 [20:34<00:34,  2.52it/s] 97%|█████████▋| 3051/3143 [20:40<00:37,  2.44it/s] 96%|█████████▋| 3033/3145 [20:34<00:47,  2.38it/s] 95%|█████████▍| 2983/3145 [20:34<01:11,  2.28it/s] 97%|█████████▋| 3060/3145 [20:34<00:34,  2.46it/s] 97%|█████████▋| 3052/3143 [20:40<00:36,  2.46it/s] 96%|█████████▋| 3034/3145 [20:34<00:47,  2.35it/s] 95%|█████████▍| 2984/3145 [20:34<01:08,  2.34it/s] 97%|█████████▋| 3061/3145 [20:35<00:32,  2.55it/s] 97%|█████████▋| 3053/3143 [20:41<00:34,  2.58it/s] 97%|█████████▋| 3035/3145 [20:35<00:45,  2.40it/s] 95%|█████████▍| 2985/3145 [20:35<01:07,  2.37it/s] 97%|█████████▋| 3062/3145 [20:35<00:31,  2.62it/s] 97%|█████████▋| 3054/3143 [20:41<00:36,  2.46it/s] 97%|█████████▋| 3036/3145 [20:35<00:43,  2.51it/s] 95%|█████████▍| 2986/3145 [20:35<01:05,  2.41it/s] 97%|█████████▋| 3063/3145 [20:35<00:33,  2.48it/s] 97%|█████████▋| 3055/3143 [20:41<00:32,  2.72it/s] 97%|█████████▋| 3037/3145 [20:36<00:41,  2.58it/s] 95%|█████████▍| 2987/3145 [20:36<01:08,  2.31it/s] 97%|█████████▋| 3064/3145 [20:36<00:32,  2.51it/s] 97%|█████████▋| 3056/3143 [20:42<00:33,  2.57it/s] 97%|█████████▋| 3038/3145 [20:36<00:42,  2.53it/s] 97%|█████████▋| 3065/3145 [20:36<00:31,  2.55it/s] 97%|█████████▋| 3057/3143 [20:42<00:32,  2.62it/s] 95%|█████████▌| 2988/3145 [20:36<01:14,  2.10it/s] 97%|█████████▋| 3039/3145 [20:36<00:42,  2.51it/s] 97%|█████████▋| 3066/3145 [20:37<00:31,  2.55it/s] 97%|█████████▋| 3058/3143 [20:43<00:32,  2.61it/s] 95%|█████████▌| 2989/3145 [20:37<01:10,  2.23it/s] 97%|█████████▋| 3040/3145 [20:37<00:41,  2.52it/s] 98%|█████████▊| 3067/3145 [20:37<00:30,  2.54it/s] 97%|█████████▋| 3059/3143 [20:43<00:32,  2.60it/s] 95%|█████████▌| 2990/3145 [20:37<01:07,  2.29it/s] 97%|█████████▋| 3041/3145 [20:37<00:42,  2.42it/s] 98%|█████████▊| 3068/3145 [20:37<00:30,  2.51it/s] 97%|█████████▋| 3060/3143 [20:43<00:32,  2.52it/s] 95%|█████████▌| 2991/3145 [20:38<01:08,  2.25it/s] 97%|█████████▋| 3042/3145 [20:38<00:41,  2.47it/s] 98%|█████████▊| 3069/3145 [20:38<00:29,  2.61it/s] 97%|█████████▋| 3061/3143 [20:44<00:32,  2.51it/s] 97%|█████████▋| 3043/3145 [20:38<00:40,  2.55it/s] 95%|█████████▌| 2992/3145 [20:38<01:07,  2.26it/s] 98%|█████████▊| 3070/3145 [20:38<00:28,  2.59it/s] 97%|█████████▋| 3062/3143 [20:44<00:32,  2.51it/s] 97%|█████████▋| 3044/3145 [20:38<00:39,  2.58it/s] 98%|█████████▊| 3071/3145 [20:38<00:25,  2.91it/s] 95%|█████████▌| 2993/3145 [20:38<01:07,  2.26it/s] 97%|█████████▋| 3063/3143 [20:45<00:32,  2.49it/s] 97%|█████████▋| 3045/3145 [20:39<00:38,  2.59it/s] 98%|█████████▊| 3072/3145 [20:39<00:26,  2.79it/s] 95%|█████████▌| 2994/3145 [20:39<01:06,  2.27it/s] 97%|█████████▋| 3064/3143 [20:45<00:32,  2.43it/s] 98%|█████████▊| 3073/3145 [20:39<00:25,  2.78it/s] 97%|█████████▋| 3046/3145 [20:39<00:43,  2.30it/s] 95%|█████████▌| 2995/3145 [20:39<01:03,  2.38it/s] 98%|█████████▊| 3065/3143 [20:45<00:31,  2.45it/s] 98%|█████████▊| 3074/3145 [20:40<00:26,  2.69it/s] 97%|█████████▋| 3047/3145 [20:40<00:41,  2.35it/s] 95%|█████████▌| 2996/3145 [20:40<01:03,  2.34it/s] 98%|█████████▊| 3066/3143 [20:46<00:31,  2.47it/s] 98%|█████████▊| 3075/3145 [20:40<00:27,  2.56it/s] 97%|█████████▋| 3048/3145 [20:40<00:40,  2.39it/s] 95%|█████████▌| 2997/3145 [20:40<01:02,  2.38it/s] 98%|█████████▊| 3076/3145 [20:40<00:26,  2.63it/s] 98%|█████████▊| 3067/3143 [20:46<00:31,  2.43it/s] 97%|█████████▋| 3049/3145 [20:41<00:39,  2.43it/s] 98%|█████████▊| 3077/3145 [20:41<00:22,  2.98it/s] 95%|█████████▌| 2998/3145 [20:41<01:03,  2.32it/s] 98%|█████████▊| 3068/3143 [20:47<00:31,  2.39it/s] 98%|█████████▊| 3078/3145 [20:41<00:23,  2.83it/s] 97%|█████████▋| 3050/3145 [20:41<00:40,  2.35it/s] 95%|█████████▌| 2999/3145 [20:41<01:03,  2.30it/s] 98%|█████████▊| 3069/3143 [20:47<00:29,  2.47it/s] 98%|█████████▊| 3079/3145 [20:41<00:23,  2.80it/s] 97%|█████████▋| 3051/3145 [20:41<00:39,  2.39it/s] 95%|█████████▌| 3000/3145 [20:41<01:00,  2.41it/s] 98%|█████████▊| 3070/3143 [20:47<00:29,  2.50it/s] 98%|█████████▊| 3071/3143 [20:48<00:25,  2.87it/s] 97%|█████████▋| 3052/3145 [20:42<00:38,  2.39it/s] 98%|█████████▊| 3080/3145 [20:42<00:26,  2.48it/s] 95%|█████████▌| 3001/3145 [20:42<01:03,  2.27it/s] 98%|█████████▊| 3072/3143 [20:48<00:25,  2.78it/s] 97%|█████████▋| 3053/3145 [20:42<00:38,  2.37it/s] 98%|█████████▊| 3081/3145 [20:42<00:25,  2.48it/s] 95%|█████████▌| 3002/3145 [20:42<01:02,  2.29it/s] 97%|█████████▋| 3054/3145 [20:42<00:32,  2.76it/s] 98%|█████████▊| 3073/3143 [20:48<00:25,  2.78it/s] 98%|█████████▊| 3082/3145 [20:43<00:24,  2.53it/s] 97%|█████████▋| 3055/3145 [20:43<00:33,  2.65it/s] 98%|█████████▊| 3074/3143 [20:49<00:25,  2.71it/s] 95%|█████████▌| 3003/3145 [20:43<01:08,  2.08it/s] 98%|█████████▊| 3083/3145 [20:43<00:24,  2.55it/s] 97%|█████████▋| 3056/3145 [20:43<00:33,  2.65it/s] 98%|█████████▊| 3075/3143 [20:49<00:26,  2.56it/s] 98%|█████████▊| 3084/3145 [20:43<00:23,  2.62it/s] 96%|█████████▌| 3004/3145 [20:43<01:06,  2.14it/s] 97%|█████████▋| 3057/3145 [20:44<00:33,  2.63it/s] 98%|█████████▊| 3076/3143 [20:50<00:25,  2.62it/s] 98%|█████████▊| 3085/3145 [20:44<00:23,  2.51it/s] 96%|█████████▌| 3005/3145 [20:44<01:07,  2.06it/s] 97%|█████████▋| 3058/3145 [20:44<00:32,  2.70it/s] 98%|█████████▊| 3077/3143 [20:50<00:25,  2.58it/s] 98%|█████████▊| 3086/3145 [20:44<00:22,  2.59it/s] 97%|█████████▋| 3059/3145 [20:44<00:32,  2.65it/s] 96%|█████████▌| 3006/3145 [20:44<01:05,  2.11it/s] 98%|█████████▊| 3078/3143 [20:50<00:26,  2.50it/s] 98%|█████████▊| 3087/3145 [20:45<00:22,  2.56it/s] 97%|█████████▋| 3060/3145 [20:45<00:34,  2.48it/s] 96%|█████████▌| 3007/3145 [20:45<01:05,  2.09it/s] 98%|█████████▊| 3079/3143 [20:51<00:24,  2.58it/s] 98%|█████████▊| 3088/3145 [20:45<00:22,  2.58it/s] 96%|█████████▌| 3008/3145 [20:45<00:58,  2.36it/s] 97%|█████████▋| 3061/3145 [20:45<00:33,  2.48it/s] 98%|█████████▊| 3089/3145 [20:45<00:22,  2.53it/s] 98%|█████████▊| 3080/3143 [20:51<00:28,  2.22it/s] 96%|█████████▌| 3009/3145 [20:46<00:57,  2.35it/s] 97%|█████████▋| 3062/3145 [20:46<00:34,  2.43it/s] 98%|█████████▊| 3090/3145 [20:46<00:21,  2.53it/s] 98%|█████████▊| 3081/3143 [20:52<00:27,  2.29it/s] 97%|█████████▋| 3063/3145 [20:46<00:34,  2.40it/s] 98%|█████████▊| 3091/3145 [20:46<00:21,  2.54it/s] 96%|█████████▌| 3010/3145 [20:46<01:04,  2.09it/s] 98%|█████████▊| 3082/3143 [20:52<00:25,  2.37it/s] 97%|█████████▋| 3064/3145 [20:46<00:33,  2.44it/s] 98%|█████████▊| 3092/3145 [20:47<00:20,  2.54it/s] 96%|█████████▌| 3011/3145 [20:47<01:02,  2.14it/s] 98%|█████████▊| 3083/3143 [20:53<00:26,  2.30it/s] 98%|█████████▊| 3093/3145 [20:47<00:20,  2.53it/s] 97%|█████████▋| 3065/3145 [20:47<00:34,  2.33it/s] 96%|█████████▌| 3012/3145 [20:47<00:58,  2.27it/s] 98%|█████████▊| 3084/3143 [20:53<00:26,  2.26it/s] 97%|█████████▋| 3066/3145 [20:47<00:32,  2.40it/s] 98%|█████████▊| 3094/3145 [20:47<00:20,  2.46it/s] 96%|█████████▌| 3013/3145 [20:47<00:58,  2.24it/s] 98%|█████████▊| 3085/3143 [20:54<00:25,  2.28it/s] 98%|█████████▊| 3067/3145 [20:48<00:31,  2.46it/s] 98%|█████████▊| 3095/3145 [20:48<00:20,  2.41it/s] 96%|█████████▌| 3014/3145 [20:48<01:00,  2.16it/s] 98%|█████████▊| 3086/3143 [20:54<00:24,  2.28it/s] 98%|█████████▊| 3068/3145 [20:48<00:30,  2.49it/s] 98%|█████████▊| 3096/3145 [20:48<00:20,  2.38it/s] 96%|█████████▌| 3015/3145 [20:48<00:59,  2.18it/s] 98%|█████████▊| 3087/3143 [20:54<00:23,  2.43it/s] 98%|█████████▊| 3069/3145 [20:49<00:30,  2.50it/s] 98%|█████████▊| 3097/3145 [20:49<00:20,  2.31it/s] 98%|█████████▊| 3088/3143 [20:55<00:22,  2.41it/s] 96%|█████████▌| 3016/3145 [20:49<00:58,  2.19it/s] 98%|█████████▊| 3070/3145 [20:49<00:29,  2.50it/s] 99%|█████████▊| 3098/3145 [20:49<00:20,  2.31it/s] 98%|█████████▊| 3089/3143 [20:55<00:22,  2.44it/s] 96%|█████████▌| 3017/3145 [20:49<00:58,  2.20it/s] 98%|█████████▊| 3071/3145 [20:49<00:32,  2.30it/s] 99%|█████████▊| 3099/3145 [20:50<00:20,  2.28it/s] 96%|█████████▌| 3018/3145 [20:50<00:55,  2.29it/s] 98%|█████████▊| 3090/3143 [20:56<00:22,  2.31it/s] 98%|█████████▊| 3072/3145 [20:50<00:30,  2.41it/s] 99%|█████████▊| 3100/3145 [20:50<00:19,  2.26it/s] 96%|█████████▌| 3019/3145 [20:50<00:52,  2.39it/s] 98%|█████████▊| 3091/3143 [20:56<00:22,  2.34it/s] 99%|█████████▊| 3101/3145 [20:50<00:16,  2.65it/s] 98%|█████████▊| 3073/3145 [20:50<00:30,  2.34it/s] 96%|█████████▌| 3020/3145 [20:50<00:50,  2.48it/s] 98%|█████████▊| 3092/3143 [20:56<00:20,  2.45it/s] 99%|█████████▊| 3102/3145 [20:51<00:16,  2.63it/s] 98%|█████████▊| 3074/3145 [20:51<00:29,  2.38it/s] 96%|█████████▌| 3021/3145 [20:51<00:49,  2.50it/s] 98%|█████████▊| 3093/3143 [20:57<00:20,  2.42it/s] 99%|█████████▊| 3103/3145 [20:51<00:16,  2.52it/s] 98%|█████████▊| 3075/3145 [20:51<00:31,  2.21it/s] 96%|█████████▌| 3022/3145 [20:51<00:49,  2.49it/s] 98%|█████████▊| 3094/3143 [20:57<00:19,  2.47it/s] 99%|█████████▊| 3104/3145 [20:51<00:16,  2.52it/s] 98%|█████████▊| 3076/3145 [20:52<00:30,  2.30it/s] 96%|█████████▌| 3023/3145 [20:52<00:49,  2.49it/s] 98%|█████████▊| 3095/3143 [20:58<00:18,  2.53it/s] 99%|█████████▊| 3105/3145 [20:52<00:15,  2.51it/s] 98%|█████████▊| 3077/3145 [20:52<00:28,  2.40it/s] 96%|█████████▌| 3024/3145 [20:52<00:51,  2.34it/s] 99%|█████████▊| 3096/3143 [20:58<00:20,  2.31it/s] 98%|█████████▊| 3078/3145 [20:52<00:29,  2.29it/s] 99%|█████████▉| 3106/3145 [20:52<00:17,  2.17it/s] 96%|█████████▌| 3025/3145 [20:53<00:51,  2.34it/s] 99%|█████████▊| 3097/3143 [20:58<00:19,  2.41it/s] 98%|█████████▊| 3079/3145 [20:53<00:27,  2.42it/s] 99%|█████████▉| 3107/3145 [20:53<00:16,  2.32it/s] 99%|█████████▊| 3098/3143 [20:59<00:18,  2.42it/s] 96%|█████████▌| 3026/3145 [20:53<00:52,  2.27it/s] 98%|█████████▊| 3080/3145 [20:53<00:26,  2.47it/s] 99%|█████████▉| 3108/3145 [20:53<00:15,  2.42it/s] 99%|█████████▊| 3099/3143 [20:59<00:18,  2.40it/s] 96%|█████████▌| 3027/3145 [20:53<00:51,  2.31it/s] 99%|█████████▉| 3109/3145 [20:53<00:13,  2.74it/s] 98%|█████████▊| 3081/3145 [20:54<00:25,  2.53it/s] 99%|█████████▊| 3100/3143 [21:00<00:17,  2.42it/s] 96%|█████████▋| 3028/3145 [20:54<00:49,  2.39it/s] 99%|█████████▉| 3110/3145 [20:54<00:13,  2.60it/s] 98%|█████████▊| 3082/3145 [20:54<00:24,  2.53it/s] 96%|█████████▋| 3029/3145 [20:54<00:48,  2.39it/s] 99%|█████████▉| 3111/3145 [20:54<00:13,  2.59it/s] 99%|█████████▊| 3101/3143 [21:00<00:18,  2.27it/s] 98%|█████████▊| 3083/3145 [20:54<00:25,  2.45it/s] 96%|█████████▋| 3030/3145 [20:55<00:47,  2.42it/s] 99%|█████████▉| 3112/3145 [20:55<00:13,  2.50it/s] 99%|█████████▊| 3102/3143 [21:01<00:17,  2.30it/s] 98%|█████████▊| 3084/3145 [20:55<00:27,  2.18it/s] 96%|█████████▋| 3031/3145 [20:55<00:46,  2.44it/s] 99%|█████████▉| 3113/3145 [20:55<00:12,  2.52it/s] 99%|█████████▊| 3103/3143 [21:01<00:16,  2.38it/s] 98%|█████████▊| 3085/3145 [20:55<00:26,  2.29it/s] 96%|█████████▋| 3032/3145 [20:55<00:45,  2.47it/s] 99%|█████████▉| 3114/3145 [20:55<00:12,  2.53it/s] 99%|█████████▉| 3104/3143 [21:01<00:16,  2.40it/s] 98%|█████████▊| 3086/3145 [20:56<00:26,  2.26it/s] 99%|█████████▉| 3105/3143 [21:02<00:15,  2.49it/s] 99%|█████████▉| 3115/3145 [20:56<00:12,  2.46it/s] 96%|█████████▋| 3033/3145 [20:56<00:48,  2.29it/s] 99%|█████████▉| 3106/3143 [21:02<00:13,  2.79it/s] 98%|█████████▊| 3087/3145 [20:56<00:24,  2.34it/s] 96%|█████████▋| 3034/3145 [20:56<00:46,  2.38it/s] 99%|█████████▉| 3116/3145 [20:56<00:12,  2.39it/s] 99%|█████████▉| 3107/3143 [21:02<00:13,  2.70it/s] 98%|█████████▊| 3088/3145 [20:57<00:24,  2.30it/s] 99%|█████████▉| 3117/3145 [20:57<00:11,  2.46it/s] 97%|█████████▋| 3035/3145 [20:57<00:47,  2.33it/s] 99%|█████████▉| 3108/3143 [21:03<00:13,  2.66it/s] 98%|█████████▊| 3089/3145 [20:57<00:23,  2.38it/s] 99%|█████████▉| 3118/3145 [20:57<00:10,  2.48it/s] 97%|█████████▋| 3036/3145 [20:57<00:45,  2.38it/s] 99%|█████████▉| 3109/3143 [21:03<00:12,  2.68it/s] 98%|█████████▊| 3090/3145 [20:57<00:22,  2.49it/s] 99%|█████████▉| 3119/3145 [20:58<00:10,  2.48it/s] 97%|█████████▋| 3037/3145 [20:58<00:45,  2.38it/s] 99%|█████████▉| 3110/3143 [21:04<00:13,  2.54it/s] 98%|█████████▊| 3091/3145 [20:58<00:22,  2.43it/s] 99%|█████████▉| 3120/3145 [20:58<00:10,  2.39it/s] 97%|█████████▋| 3038/3145 [20:58<00:45,  2.35it/s] 99%|█████████▉| 3111/3143 [21:04<00:12,  2.54it/s] 98%|█████████▊| 3092/3145 [20:58<00:22,  2.36it/s] 97%|█████████▋| 3039/3145 [20:58<00:40,  2.62it/s] 99%|█████████▉| 3121/3145 [20:58<00:09,  2.49it/s] 99%|█████████▉| 3112/3143 [21:04<00:11,  2.61it/s] 97%|█████████▋| 3040/3145 [20:59<00:36,  2.87it/s] 98%|█████████▊| 3093/3145 [20:59<00:21,  2.43it/s] 99%|█████████▉| 3122/3145 [20:59<00:09,  2.52it/s] 99%|█████████▉| 3113/3143 [21:05<00:12,  2.50it/s] 97%|█████████▋| 3041/3145 [20:59<00:39,  2.66it/s] 98%|█████████▊| 3094/3145 [20:59<00:20,  2.45it/s] 99%|█████████▉| 3123/3145 [20:59<00:08,  2.59it/s] 99%|█████████▉| 3114/3143 [21:05<00:11,  2.49it/s] 98%|█████████▊| 3095/3145 [20:59<00:19,  2.57it/s] 99%|█████████▉| 3124/3145 [20:59<00:07,  2.65it/s] 97%|█████████▋| 3042/3145 [20:59<00:40,  2.53it/s] 98%|█████████▊| 3096/3145 [21:00<00:19,  2.56it/s] 97%|█████████▋| 3043/3145 [21:00<00:39,  2.60it/s] 99%|█████████▉| 3125/3145 [21:00<00:07,  2.62it/s] 99%|█████████▉| 3115/3143 [21:06<00:12,  2.29it/s] 97%|█████████▋| 3044/3145 [21:00<00:37,  2.73it/s] 98%|█████████▊| 3097/3145 [21:00<00:19,  2.50it/s] 99%|█████████▉| 3116/3143 [21:06<00:11,  2.41it/s] 99%|█████████▉| 3126/3145 [21:00<00:07,  2.52it/s] 97%|█████████▋| 3045/3145 [21:01<00:37,  2.66it/s] 99%|█████████▉| 3117/3143 [21:06<00:10,  2.53it/s] 99%|█████████▊| 3098/3145 [21:01<00:18,  2.50it/s] 99%|█████████▉| 3127/3145 [21:01<00:07,  2.49it/s] 97%|█████████▋| 3046/3145 [21:01<00:34,  2.88it/s] 99%|█████████▊| 3099/3145 [21:01<00:18,  2.53it/s] 99%|█████████▉| 3118/3143 [21:07<00:10,  2.44it/s] 99%|█████████▉| 3128/3145 [21:01<00:06,  2.57it/s] 97%|█████████▋| 3047/3145 [21:01<00:34,  2.83it/s] 99%|█████████▊| 3100/3145 [21:01<00:17,  2.55it/s] 99%|█████████▉| 3119/3143 [21:07<00:09,  2.46it/s] 99%|█████████▉| 3129/3145 [21:01<00:06,  2.57it/s] 97%|█████████▋| 3048/3145 [21:02<00:36,  2.63it/s] 99%|█████████▉| 3120/3143 [21:08<00:09,  2.54it/s] 99%|█████████▊| 3101/3145 [21:02<00:17,  2.47it/s]100%|█████████▉| 3130/3145 [21:02<00:06,  2.37it/s] 97%|█████████▋| 3049/3145 [21:02<00:36,  2.66it/s] 99%|█████████▉| 3121/3143 [21:08<00:08,  2.45it/s] 99%|█████████▊| 3102/3145 [21:02<00:17,  2.40it/s]100%|█████████▉| 3131/3145 [21:02<00:05,  2.48it/s] 97%|█████████▋| 3050/3145 [21:02<00:35,  2.68it/s]100%|█████████▉| 3132/3145 [21:03<00:04,  2.84it/s] 99%|█████████▊| 3103/3145 [21:03<00:17,  2.47it/s] 97%|█████████▋| 3051/3145 [21:03<00:32,  2.91it/s] 99%|█████████▉| 3122/3143 [21:09<00:08,  2.36it/s]100%|█████████▉| 3133/3145 [21:03<00:03,  3.17it/s]100%|█████████▉| 3134/3145 [21:03<00:03,  3.46it/s] 97%|█████████▋| 3052/3145 [21:03<00:34,  2.68it/s] 99%|█████████▊| 3104/3145 [21:03<00:17,  2.38it/s] 99%|█████████▉| 3123/3143 [21:09<00:08,  2.34it/s]100%|█████████▉| 3135/3145 [21:03<00:03,  3.23it/s] 97%|█████████▋| 3053/3145 [21:03<00:34,  2.69it/s] 99%|█████████▊| 3105/3145 [21:04<00:17,  2.33it/s] 99%|█████████▉| 3124/3143 [21:09<00:08,  2.36it/s]100%|█████████▉| 3136/3145 [21:04<00:02,  3.08it/s] 97%|█████████▋| 3054/3145 [21:04<00:33,  2.70it/s] 99%|█████████▉| 3125/3143 [21:10<00:07,  2.38it/s] 99%|█████████▉| 3106/3145 [21:04<00:16,  2.32it/s] 97%|█████████▋| 3055/3145 [21:04<00:33,  2.71it/s]100%|█████████▉| 3137/3145 [21:04<00:03,  2.51it/s] 99%|█████████▉| 3126/3143 [21:10<00:07,  2.42it/s] 99%|█████████▉| 3107/3145 [21:04<00:16,  2.35it/s] 97%|█████████▋| 3056/3145 [21:05<00:33,  2.63it/s]100%|█████████▉| 3138/3145 [21:05<00:02,  2.45it/s] 99%|█████████▉| 3127/3143 [21:11<00:06,  2.29it/s] 99%|█████████▉| 3108/3145 [21:05<00:16,  2.29it/s] 97%|█████████▋| 3057/3145 [21:05<00:33,  2.65it/s]100%|█████████▉| 3139/3145 [21:05<00:02,  2.49it/s]100%|█████████▉| 3128/3143 [21:11<00:06,  2.37it/s] 99%|█████████▉| 3109/3145 [21:05<00:15,  2.29it/s] 97%|█████████▋| 3058/3145 [21:05<00:33,  2.59it/s]100%|█████████▉| 3140/3145 [21:05<00:01,  2.58it/s] 97%|█████████▋| 3059/3145 [21:06<00:29,  2.94it/s]100%|█████████▉| 3129/3143 [21:12<00:05,  2.34it/s] 99%|█████████▉| 3110/3145 [21:06<00:14,  2.35it/s]100%|█████████▉| 3141/3145 [21:06<00:01,  2.60it/s] 97%|█████████▋| 3060/3145 [21:06<00:29,  2.91it/s] 99%|█████████▉| 3111/3145 [21:06<00:14,  2.42it/s]100%|█████████▉| 3130/3143 [21:12<00:05,  2.28it/s]100%|█████████▉| 3142/3145 [21:06<00:01,  2.50it/s] 97%|█████████▋| 3061/3145 [21:06<00:31,  2.63it/s] 99%|█████████▉| 3112/3145 [21:06<00:13,  2.48it/s]100%|█████████▉| 3131/3143 [21:12<00:05,  2.28it/s]100%|█████████▉| 3143/3145 [21:07<00:00,  2.45it/s] 99%|█████████▉| 3113/3145 [21:07<00:13,  2.45it/s] 97%|█████████▋| 3062/3145 [21:07<00:37,  2.24it/s]100%|█████████▉| 3144/3145 [21:07<00:00,  2.45it/s]100%|█████████▉| 3132/3143 [21:13<00:05,  2.07it/s] 99%|█████████▉| 3114/3145 [21:07<00:12,  2.47it/s]100%|██████████| 3145/3145 [21:07<00:00,  2.51it/s]100%|██████████| 3145/3145 [21:08<00:00,  2.48it/s]
100%|█████████▉| 3133/3143 [21:13<00:04,  2.20it/s] 97%|█████████▋| 3063/3145 [21:08<00:39,  2.10it/s] 99%|█████████▉| 3115/3145 [21:08<00:12,  2.47it/s]100%|█████████▉| 3134/3143 [21:14<00:03,  2.59it/s] 97%|█████████▋| 3064/3145 [21:08<00:36,  2.21it/s] 99%|█████████▉| 3116/3145 [21:08<00:11,  2.49it/s]100%|█████████▉| 3135/3143 [21:14<00:03,  2.59it/s] 97%|█████████▋| 3065/3145 [21:08<00:35,  2.23it/s]100%|█████████▉| 3136/3143 [21:14<00:02,  2.64it/s] 99%|█████████▉| 3117/3145 [21:09<00:11,  2.35it/s] 97%|█████████▋| 3066/3145 [21:09<00:33,  2.39it/s]100%|█████████▉| 3137/3143 [21:15<00:02,  2.66it/s] 99%|█████████▉| 3118/3145 [21:09<00:11,  2.40it/s] 98%|█████████▊| 3067/3145 [21:09<00:31,  2.47it/s]100%|█████████▉| 3138/3143 [21:15<00:01,  2.60it/s] 99%|█████████▉| 3119/3145 [21:09<00:11,  2.28it/s] 98%|█████████▊| 3068/3145 [21:10<00:31,  2.44it/s]100%|█████████▉| 3139/3143 [21:16<00:01,  2.57it/s] 99%|█████████▉| 3120/3145 [21:10<00:09,  2.59it/s] 98%|█████████▊| 3069/3145 [21:10<00:30,  2.46it/s]100%|█████████▉| 3140/3143 [21:16<00:01,  2.56it/s] 99%|█████████▉| 3121/3145 [21:10<00:09,  2.56it/s] 99%|█████████▉| 3122/3145 [21:10<00:08,  2.82it/s] 98%|█████████▊| 3070/3145 [21:10<00:31,  2.39it/s]100%|█████████▉| 3141/3143 [21:16<00:00,  2.61it/s] 99%|█████████▉| 3123/3145 [21:11<00:08,  2.74it/s] 98%|█████████▊| 3071/3145 [21:11<00:29,  2.47it/s]100%|█████████▉| 3142/3143 [21:17<00:00,  2.69it/s] 99%|█████████▉| 3124/3145 [21:11<00:06,  3.02it/s] 98%|█████████▊| 3072/3145 [21:11<00:26,  2.77it/s] 99%|█████████▉| 3125/3145 [21:11<00:06,  3.26it/s]100%|██████████| 3143/3143 [21:17<00:00,  2.47it/s]100%|██████████| 3143/3143 [21:17<00:00,  2.46it/s]
 98%|█████████▊| 3073/3145 [21:11<00:26,  2.68it/s] 99%|█████████▉| 3126/3145 [21:12<00:06,  3.07it/s] 98%|█████████▊| 3074/3145 [21:12<00:27,  2.56it/s] 99%|█████████▉| 3127/3145 [21:12<00:06,  2.88it/s] 98%|█████████▊| 3075/3145 [21:12<00:27,  2.53it/s] 99%|█████████▉| 3128/3145 [21:12<00:06,  2.77it/s] 98%|█████████▊| 3076/3145 [21:13<00:26,  2.62it/s] 99%|█████████▉| 3129/3145 [21:13<00:05,  2.74it/s] 98%|█████████▊| 3077/3145 [21:13<00:30,  2.22it/s]100%|█████████▉| 3130/3145 [21:13<00:05,  2.60it/s]100%|█████████▉| 3131/3145 [21:14<00:05,  2.60it/s] 98%|█████████▊| 3078/3145 [21:14<00:29,  2.31it/s] 98%|█████████▊| 3079/3145 [21:14<00:27,  2.38it/s]100%|█████████▉| 3132/3145 [21:14<00:05,  2.50it/s] 98%|█████████▊| 3080/3145 [21:14<00:26,  2.43it/s]100%|█████████▉| 3133/3145 [21:15<00:04,  2.43it/s] 98%|█████████▊| 3081/3145 [21:15<00:26,  2.44it/s]100%|█████████▉| 3134/3145 [21:15<00:04,  2.36it/s] 98%|█████████▊| 3082/3145 [21:15<00:25,  2.47it/s]100%|█████████▉| 3135/3145 [21:15<00:04,  2.35it/s] 98%|█████████▊| 3083/3145 [21:16<00:24,  2.55it/s]100%|█████████▉| 3136/3145 [21:16<00:03,  2.41it/s] 98%|█████████▊| 3084/3145 [21:16<00:23,  2.58it/s]100%|█████████▉| 3137/3145 [21:16<00:03,  2.45it/s] 98%|█████████▊| 3085/3145 [21:16<00:24,  2.45it/s]100%|█████████▉| 3138/3145 [21:17<00:02,  2.55it/s] 98%|█████████▊| 3086/3145 [21:17<00:23,  2.49it/s]100%|█████████▉| 3139/3145 [21:17<00:02,  2.52it/s] 98%|█████████▊| 3087/3145 [21:17<00:23,  2.44it/s]100%|█████████▉| 3140/3145 [21:17<00:01,  2.57it/s] 98%|█████████▊| 3088/3145 [21:18<00:23,  2.40it/s]100%|█████████▉| 3141/3145 [21:18<00:01,  2.60it/s] 98%|█████████▊| 3089/3145 [21:18<00:23,  2.42it/s]100%|█████████▉| 3142/3145 [21:18<00:01,  2.54it/s] 98%|█████████▊| 3090/3145 [21:18<00:23,  2.36it/s]100%|█████████▉| 3143/3145 [21:19<00:00,  2.47it/s] 98%|█████████▊| 3091/3145 [21:19<00:22,  2.37it/s]100%|█████████▉| 3144/3145 [21:19<00:00,  2.46it/s]100%|██████████| 3145/3145 [21:19<00:00,  2.37it/s] 98%|█████████▊| 3092/3145 [21:19<00:23,  2.29it/s]100%|██████████| 3145/3145 [21:19<00:00,  2.46it/s]
 98%|█████████▊| 3093/3145 [21:20<00:20,  2.50it/s] 98%|█████████▊| 3094/3145 [21:20<00:20,  2.49it/s] 98%|█████████▊| 3095/3145 [21:21<00:20,  2.42it/s] 98%|█████████▊| 3096/3145 [21:21<00:19,  2.46it/s] 98%|█████████▊| 3097/3145 [21:21<00:19,  2.41it/s] 99%|█████████▊| 3098/3145 [21:22<00:19,  2.45it/s] 99%|█████████▊| 3099/3145 [21:22<00:20,  2.30it/s] 99%|█████████▊| 3100/3145 [21:23<00:20,  2.25it/s] 99%|█████████▊| 3101/3145 [21:23<00:21,  2.03it/s] 99%|█████████▊| 3102/3145 [21:24<00:19,  2.18it/s] 99%|█████████▊| 3103/3145 [21:24<00:18,  2.28it/s] 99%|█████████▊| 3104/3145 [21:25<00:18,  2.27it/s] 99%|█████████▊| 3105/3145 [21:25<00:16,  2.36it/s] 99%|█████████▉| 3106/3145 [21:25<00:16,  2.30it/s] 99%|█████████▉| 3107/3145 [21:26<00:16,  2.37it/s] 99%|█████████▉| 3108/3145 [21:26<00:15,  2.35it/s] 99%|█████████▉| 3109/3145 [21:27<00:16,  2.18it/s] 99%|█████████▉| 3110/3145 [21:27<00:15,  2.28it/s] 99%|█████████▉| 3111/3145 [21:28<00:15,  2.17it/s] 99%|█████████▉| 3112/3145 [21:28<00:14,  2.27it/s] 99%|█████████▉| 3113/3145 [21:28<00:13,  2.30it/s] 99%|█████████▉| 3114/3145 [21:29<00:13,  2.37it/s] 99%|█████████▉| 3115/3145 [21:29<00:12,  2.48it/s] 99%|█████████▉| 3116/3145 [21:30<00:11,  2.56it/s] 99%|█████████▉| 3117/3145 [21:30<00:10,  2.58it/s] 99%|█████████▉| 3118/3145 [21:30<00:10,  2.57it/s] 99%|█████████▉| 3119/3145 [21:31<00:10,  2.57it/s] 99%|█████████▉| 3120/3145 [21:31<00:10,  2.48it/s] 99%|█████████▉| 3121/3145 [21:32<00:09,  2.51it/s] 99%|█████████▉| 3122/3145 [21:32<00:09,  2.41it/s] 99%|█████████▉| 3123/3145 [21:32<00:08,  2.58it/s] 99%|█████████▉| 3124/3145 [21:33<00:08,  2.60it/s] 99%|█████████▉| 3125/3145 [21:33<00:07,  2.62it/s] 99%|█████████▉| 3126/3145 [21:33<00:07,  2.60it/s] 99%|█████████▉| 3127/3145 [21:34<00:07,  2.47it/s] 99%|█████████▉| 3128/3145 [21:34<00:06,  2.53it/s] 99%|█████████▉| 3129/3145 [21:35<00:06,  2.44it/s]100%|█████████▉| 3130/3145 [21:35<00:06,  2.41it/s]100%|█████████▉| 3131/3145 [21:36<00:05,  2.38it/s]100%|█████████▉| 3132/3145 [21:36<00:05,  2.37it/s]100%|█████████▉| 3133/3145 [21:36<00:04,  2.43it/s]100%|█████████▉| 3134/3145 [21:37<00:04,  2.43it/s]100%|█████████▉| 3135/3145 [21:37<00:04,  2.46it/s]100%|█████████▉| 3136/3145 [21:38<00:03,  2.48it/s]100%|█████████▉| 3137/3145 [21:38<00:03,  2.49it/s]100%|█████████▉| 3138/3145 [21:38<00:02,  2.44it/s]100%|█████████▉| 3139/3145 [21:39<00:02,  2.40it/s]100%|█████████▉| 3140/3145 [21:39<00:02,  2.33it/s]100%|█████████▉| 3141/3145 [21:40<00:01,  2.31it/s]100%|█████████▉| 3142/3145 [21:40<00:01,  2.38it/s]100%|█████████▉| 3143/3145 [21:41<00:00,  2.49it/s]100%|█████████▉| 3144/3145 [21:41<00:00,  2.41it/s]100%|██████████| 3145/3145 [21:41<00:00,  2.45it/s]100%|██████████| 3145/3145 [21:41<00:00,  2.42it/s]
Please make sure to use our provided visual features as gqadataset.org for better comparability. We provide both spatial and object-based features trained on GQA train set.
In particular please avoid using features from https://github.com/peteanderson80/bottom-up-attention since they were trained on images contained in the GQA validation set and thus may give false scores improvement.

Please consider using --consistency to compute consistency scores for entailed questions.
If you do so, please provide answers to all questions in val_all_questions.json.

Please consider using --grounding to compute attention scores.
If you do so, please provide attention maps through --attentions.

Loading questions...
Loading predictions...
  0%|          | 0/12578 [00:00<?, ?it/s]100%|██████████| 12578/12578 [00:00<00:00, 149599.90it/s]

Binary: 80.62%
Open: 50.35%
Accuracy: 64.24%
Validity: 0.00%
Plausibility: 0.00%
Distribution: 1.42 (lower is better)

Accuracy / structural type:
  choose: 85.56% (1129 questions)
  compare: 65.87% (589 questions)
  logical: 79.15% (1803 questions)
  query: 50.35% (6805 questions)
  verify: 83.17% (2252 questions)

Accuracy / semantic type:
  attr: 70.98% (5186 questions)
  cat: 53.52% (1149 questions)
  global: 59.24% (157 questions)
  obj: 87.79% (778 questions)
  rel: 56.67% (5308 questions)

Accuracy / steps number:
  1: 80.59% (237 questions)
  2: 58.86% (6395 questions)
  3: 66.99% (4266 questions)
  4: 71.25% (793 questions)
  5: 78.10% (822 questions)
  6: 90.24% (41 questions)
  7: 95.00% (20 questions)
  8: 100.00% (3 questions)
  9: 100.00% (1 questions)

Accuracy / words number:
  3: 42.38% (151 questions)
  4: 59.21% (630 questions)
  5: 50.62% (1290 questions)
  6: 60.17% (2074 questions)
  7: 63.34% (1642 questions)
  8: 66.41% (1185 questions)
  9: 69.95% (1281 questions)
  10: 71.66% (1249 questions)
  11: 65.90% (994 questions)
  12: 69.28% (638 questions)
  13: 65.80% (462 questions)
  14: 73.62% (345 questions)
  15: 71.31% (237 questions)
  16: 76.92% (117 questions)
  17: 64.89% (94 questions)
  18: 80.26% (76 questions)
  19: 83.72% (43 questions)
  20: 75.00% (32 questions)
  21: 73.68% (19 questions)
  22: 83.33% (12 questions)
  23: 25.00% (4 questions)
  24: 100.00% (2 questions)
  25: 100.00% (1 questions)
