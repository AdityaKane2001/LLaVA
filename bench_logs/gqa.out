config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.68k/1.68k [00:00<00:00, 4.58MB/s]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
model.safetensors.index.json:   0%|          | 0.00/73.2k [00:00<?, ?B/s]model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73.2k/73.2k [00:00<00:00, 6.90MB/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s][A
model-00001-of-00003.safetensors:   1%|          | 31.5M/4.94G [00:00<00:20, 236MB/s][A
model-00001-of-00003.safetensors:   1%|â–         | 73.4M/4.94G [00:00<00:16, 301MB/s][A
model-00001-of-00003.safetensors:   2%|â–         | 115M/4.94G [00:00<00:15, 315MB/s] [A
model-00001-of-00003.safetensors:   3%|â–Ž         | 157M/4.94G [00:00<00:15, 303MB/s][A
model-00001-of-00003.safetensors:   4%|â–         | 199M/4.94G [00:00<00:15, 315MB/s][A
model-00001-of-00003.safetensors:   5%|â–         | 241M/4.94G [00:00<00:14, 321MB/s][A
model-00001-of-00003.safetensors:   6%|â–Œ         | 283M/4.94G [00:00<00:14, 315MB/s][A
model-00001-of-00003.safetensors:   7%|â–‹         | 325M/4.94G [00:01<00:14, 326MB/s][A
model-00001-of-00003.safetensors:   7%|â–‹         | 367M/4.94G [00:01<00:15, 290MB/s][A
model-00001-of-00003.safetensors:   8%|â–Š         | 398M/4.94G [00:01<00:16, 276MB/s][A
model-00001-of-00003.safetensors:   9%|â–‰         | 440M/4.94G [00:01<00:15, 293MB/s][A
model-00001-of-00003.safetensors:  10%|â–‰         | 482M/4.94G [00:01<00:14, 310MB/s][A
model-00001-of-00003.safetensors:  11%|â–ˆ         | 524M/4.94G [00:01<00:14, 315MB/s][A
model-00001-of-00003.safetensors:  11%|â–ˆâ–        | 566M/4.94G [00:01<00:15, 283MB/s][A
model-00001-of-00003.safetensors:  12%|â–ˆâ–        | 598M/4.94G [00:02<00:16, 260MB/s][A
model-00001-of-00003.safetensors:  13%|â–ˆâ–Ž        | 629M/4.94G [00:02<00:16, 260MB/s][A
model-00001-of-00003.safetensors:  13%|â–ˆâ–Ž        | 661M/4.94G [00:02<00:16, 262MB/s][A
model-00001-of-00003.safetensors:  14%|â–ˆâ–        | 692M/4.94G [00:02<00:16, 253MB/s][A
model-00001-of-00003.safetensors:  15%|â–ˆâ–        | 724M/4.94G [00:02<00:16, 254MB/s][A
model-00001-of-00003.safetensors:  15%|â–ˆâ–Œ        | 755M/4.94G [00:02<00:16, 259MB/s][A
model-00001-of-00003.safetensors:  16%|â–ˆâ–Œ        | 786M/4.94G [00:02<00:16, 257MB/s][A
model-00001-of-00003.safetensors:  17%|â–ˆâ–‹        | 818M/4.94G [00:02<00:15, 265MB/s][A
model-00001-of-00003.safetensors:  17%|â–ˆâ–‹        | 849M/4.94G [00:03<00:15, 268MB/s][A
model-00001-of-00003.safetensors:  18%|â–ˆâ–Š        | 881M/4.94G [00:03<00:14, 273MB/s][A
model-00001-of-00003.safetensors:  18%|â–ˆâ–Š        | 912M/4.94G [00:03<00:15, 262MB/s][A
model-00001-of-00003.safetensors:  19%|â–ˆâ–‰        | 954M/4.94G [00:03<00:14, 284MB/s][A
model-00001-of-00003.safetensors:  20%|â–ˆâ–ˆ        | 996M/4.94G [00:03<00:13, 301MB/s][A
model-00001-of-00003.safetensors:  21%|â–ˆâ–ˆ        | 1.03G/4.94G [00:03<00:13, 300MB/s][A
model-00001-of-00003.safetensors:  22%|â–ˆâ–ˆâ–       | 1.07G/4.94G [00:03<00:12, 304MB/s][A
model-00001-of-00003.safetensors:  22%|â–ˆâ–ˆâ–       | 1.10G/4.94G [00:03<00:13, 291MB/s][A
model-00001-of-00003.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 1.13G/4.94G [00:03<00:13, 279MB/s][A
model-00001-of-00003.safetensors:  24%|â–ˆâ–ˆâ–Ž       | 1.16G/4.94G [00:04<00:13, 272MB/s][A
model-00001-of-00003.safetensors:  24%|â–ˆâ–ˆâ–       | 1.20G/4.94G [00:04<00:14, 253MB/s][A
model-00001-of-00003.safetensors:  25%|â–ˆâ–ˆâ–       | 1.23G/4.94G [00:04<00:14, 263MB/s][A
model-00001-of-00003.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 1.27G/4.94G [00:04<00:12, 285MB/s][A
model-00001-of-00003.safetensors:  26%|â–ˆâ–ˆâ–‹       | 1.30G/4.94G [00:04<00:12, 286MB/s][A
model-00001-of-00003.safetensors:  27%|â–ˆâ–ˆâ–‹       | 1.34G/4.94G [00:04<00:11, 313MB/s][A
model-00001-of-00003.safetensors:  28%|â–ˆâ–ˆâ–Š       | 1.38G/4.94G [00:04<00:11, 299MB/s][A
model-00001-of-00003.safetensors:  29%|â–ˆâ–ˆâ–‰       | 1.43G/4.94G [00:04<00:11, 316MB/s][A
model-00001-of-00003.safetensors:  30%|â–ˆâ–ˆâ–‰       | 1.48G/4.94G [00:05<00:09, 364MB/s][A
model-00001-of-00003.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.52G/4.94G [00:05<00:09, 370MB/s][A
model-00001-of-00003.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.56G/4.94G [00:05<00:09, 351MB/s][A
model-00001-of-00003.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.60G/4.94G [00:05<00:09, 357MB/s][A
model-00001-of-00003.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.65G/4.94G [00:05<00:08, 372MB/s][A
model-00001-of-00003.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.70G/4.94G [00:05<00:08, 390MB/s][A
model-00001-of-00003.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.76G/4.94G [00:05<00:07, 424MB/s][A
model-00001-of-00003.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.81G/4.94G [00:05<00:07, 408MB/s][A
model-00001-of-00003.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.86G/4.94G [00:06<00:07, 387MB/s][A
model-00001-of-00003.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.90G/4.94G [00:06<00:07, 387MB/s][A
model-00001-of-00003.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.95G/4.94G [00:06<00:07, 412MB/s][A
model-00001-of-00003.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.99G/4.94G [00:06<00:07, 402MB/s][A
model-00001-of-00003.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2.03G/4.94G [00:06<00:07, 405MB/s][A
model-00001-of-00003.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.10G/4.94G [00:06<00:06, 432MB/s][A
model-00001-of-00003.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2.15G/4.94G [00:06<00:06, 417MB/s][A
model-00001-of-00003.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.19G/4.94G [00:06<00:06, 413MB/s][A
model-00001-of-00003.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2.23G/4.94G [00:06<00:06, 409MB/s][A
model-00001-of-00003.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2.28G/4.94G [00:07<00:06, 398MB/s][A
model-00001-of-00003.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2.32G/4.94G [00:07<00:07, 346MB/s][A
model-00001-of-00003.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.36G/4.94G [00:07<00:07, 327MB/s][A
model-00001-of-00003.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.40G/4.94G [00:07<00:07, 342MB/s][A
model-00001-of-00003.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2.44G/4.94G [00:07<00:07, 328MB/s][A
model-00001-of-00003.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.49G/4.94G [00:07<00:07, 315MB/s][A
model-00001-of-00003.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.53G/4.94G [00:07<00:08, 276MB/s][A
model-00001-of-00003.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.56G/4.94G [00:08<00:09, 253MB/s][A
model-00001-of-00003.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.59G/4.94G [00:08<00:09, 259MB/s][A
model-00001-of-00003.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2.63G/4.94G [00:08<00:08, 276MB/s][A
model-00001-of-00003.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.66G/4.94G [00:08<00:08, 279MB/s][A
model-00001-of-00003.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.69G/4.94G [00:08<00:08, 272MB/s][A
model-00001-of-00003.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.74G/4.94G [00:08<00:07, 290MB/s][A
model-00001-of-00003.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.77G/4.94G [00:08<00:07, 289MB/s][A
model-00001-of-00003.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.81G/4.94G [00:08<00:07, 303MB/s][A
model-00001-of-00003.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.85G/4.94G [00:09<00:06, 323MB/s][A
model-00001-of-00003.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.89G/4.94G [00:09<00:06, 325MB/s][A
model-00001-of-00003.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.94G/4.94G [00:09<00:05, 336MB/s][A
model-00001-of-00003.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.98G/4.94G [00:09<00:05, 339MB/s][A
model-00001-of-00003.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3.02G/4.94G [00:09<00:05, 346MB/s][A
model-00001-of-00003.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3.06G/4.94G [00:09<00:05, 345MB/s][A
model-00001-of-00003.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3.10G/4.94G [00:09<00:05, 337MB/s][A
model-00001-of-00003.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3.15G/4.94G [00:09<00:05, 339MB/s][A
model-00001-of-00003.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3.19G/4.94G [00:10<00:05, 328MB/s][A
model-00001-of-00003.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3.23G/4.94G [00:10<00:05, 327MB/s][A
model-00001-of-00003.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3.27G/4.94G [00:10<00:04, 335MB/s][A
model-00001-of-00003.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3.31G/4.94G [00:10<00:05, 317MB/s][A
model-00001-of-00003.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3.37G/4.94G [00:10<00:04, 346MB/s][A
model-00001-of-00003.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3.41G/4.94G [00:10<00:04, 346MB/s][A
model-00001-of-00003.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3.45G/4.94G [00:10<00:04, 317MB/s][A
model-00001-of-00003.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3.49G/4.94G [00:11<00:05, 288MB/s][A
model-00001-of-00003.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.52G/4.94G [00:11<00:04, 292MB/s][A
model-00001-of-00003.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.55G/4.94G [00:11<00:05, 272MB/s][A
model-00001-of-00003.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3.60G/4.94G [00:11<00:04, 293MB/s][A
model-00001-of-00003.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3.64G/4.94G [00:11<00:04, 304MB/s][A
model-00001-of-00003.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.68G/4.94G [00:11<00:03, 315MB/s][A
model-00001-of-00003.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.72G/4.94G [00:11<00:03, 317MB/s][A
model-00001-of-00003.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.76G/4.94G [00:11<00:03, 307MB/s][A
model-00001-of-00003.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3.81G/4.94G [00:12<00:03, 321MB/s][A
model-00001-of-00003.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3.85G/4.94G [00:12<00:03, 343MB/s][A
model-00001-of-00003.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.89G/4.94G [00:12<00:03, 333MB/s][A
model-00001-of-00003.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.93G/4.94G [00:12<00:03, 334MB/s][A
model-00001-of-00003.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3.97G/4.94G [00:12<00:02, 339MB/s][A
model-00001-of-00003.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.02G/4.94G [00:12<00:02, 328MB/s][A
model-00001-of-00003.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.06G/4.94G [00:12<00:02, 319MB/s][A
model-00001-of-00003.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4.10G/4.94G [00:13<00:04, 198MB/s][A
model-00001-of-00003.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.14G/4.94G [00:13<00:03, 234MB/s][A
model-00001-of-00003.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.18G/4.94G [00:13<00:02, 264MB/s][A
model-00001-of-00003.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4.23G/4.94G [00:13<00:02, 295MB/s][A
model-00001-of-00003.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4.27G/4.94G [00:13<00:02, 317MB/s][A
model-00001-of-00003.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4.31G/4.94G [00:13<00:01, 339MB/s][A
model-00001-of-00003.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4.35G/4.94G [00:13<00:01, 352MB/s][A
model-00001-of-00003.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4.39G/4.94G [00:13<00:01, 362MB/s][A
model-00001-of-00003.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4.46G/4.94G [00:14<00:01, 407MB/s][A
model-00001-of-00003.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4.50G/4.94G [00:14<00:01, 403MB/s][A
model-00001-of-00003.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.54G/4.94G [00:14<00:01, 391MB/s][A
model-00001-of-00003.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4.59G/4.94G [00:14<00:00, 408MB/s][A
model-00001-of-00003.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.65G/4.94G [00:14<00:00, 413MB/s][A
model-00001-of-00003.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.69G/4.94G [00:14<00:00, 408MB/s][A
model-00001-of-00003.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4.73G/4.94G [00:14<00:00, 403MB/s][A
model-00001-of-00003.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.77G/4.94G [00:14<00:00, 401MB/s][A
model-00001-of-00003.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.81G/4.94G [00:14<00:00, 402MB/s][A
model-00001-of-00003.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4.85G/4.94G [00:15<00:00, 397MB/s][A
model-00001-of-00003.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.90G/4.94G [00:15<00:00, 392MB/s][A
model-00001-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.94G/4.94G [00:15<00:00, 387MB/s][Amodel-00001-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.94G/4.94G [00:15<00:00, 323MB/s]
Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:15<00:31, 15.52s/it]Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:15<00:31, 15.51s/it]Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:15<00:31, 15.53s/it]Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:15<00:31, 15.52s/it]
model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s][A
model-00002-of-00003.safetensors:   1%|          | 31.5M/4.95G [00:00<00:17, 285MB/s][A
model-00002-of-00003.safetensors:   1%|â–         | 73.4M/4.95G [00:00<00:14, 330MB/s][A
model-00002-of-00003.safetensors:   2%|â–         | 115M/4.95G [00:00<00:14, 342MB/s] [A
model-00002-of-00003.safetensors:   3%|â–Ž         | 157M/4.95G [00:00<00:13, 366MB/s][A
model-00002-of-00003.safetensors:   4%|â–         | 199M/4.95G [00:00<00:12, 379MB/s][A
model-00002-of-00003.safetensors:   5%|â–         | 241M/4.95G [00:00<00:12, 380MB/s][A
model-00002-of-00003.safetensors:   6%|â–Œ         | 294M/4.95G [00:00<00:11, 392MB/s][A
model-00002-of-00003.safetensors:   7%|â–‹         | 346M/4.95G [00:00<00:11, 417MB/s][A
model-00002-of-00003.safetensors:   8%|â–Š         | 388M/4.95G [00:01<00:11, 407MB/s][A
model-00002-of-00003.safetensors:   9%|â–Š         | 430M/4.95G [00:01<00:11, 380MB/s][A
model-00002-of-00003.safetensors:  10%|â–‰         | 472M/4.95G [00:01<00:11, 381MB/s][A
model-00002-of-00003.safetensors:  11%|â–ˆ         | 524M/4.95G [00:01<00:10, 416MB/s][A
model-00002-of-00003.safetensors:  12%|â–ˆâ–        | 577M/4.95G [00:01<00:10, 426MB/s][A
model-00002-of-00003.safetensors:  13%|â–ˆâ–Ž        | 629M/4.95G [00:01<00:10, 407MB/s][A
model-00002-of-00003.safetensors:  14%|â–ˆâ–Ž        | 671M/4.95G [00:01<00:10, 401MB/s][A
model-00002-of-00003.safetensors:  14%|â–ˆâ–        | 713M/4.95G [00:01<00:11, 373MB/s][A
model-00002-of-00003.safetensors:  15%|â–ˆâ–Œ        | 755M/4.95G [00:01<00:10, 385MB/s][A
model-00002-of-00003.safetensors:  16%|â–ˆâ–Œ        | 797M/4.95G [00:02<00:11, 366MB/s][A
model-00002-of-00003.safetensors:  17%|â–ˆâ–‹        | 849M/4.95G [00:02<00:15, 266MB/s][A
model-00002-of-00003.safetensors:  18%|â–ˆâ–Š        | 881M/4.95G [00:02<00:15, 266MB/s][A
model-00002-of-00003.safetensors:  19%|â–ˆâ–Š        | 923M/4.95G [00:02<00:13, 295MB/s][A
model-00002-of-00003.safetensors:  20%|â–ˆâ–‰        | 975M/4.95G [00:02<00:11, 335MB/s][A
model-00002-of-00003.safetensors:  21%|â–ˆâ–ˆ        | 1.03G/4.95G [00:02<00:10, 373MB/s][A
model-00002-of-00003.safetensors:  22%|â–ˆâ–ˆâ–       | 1.07G/4.95G [00:02<00:11, 352MB/s][A
model-00002-of-00003.safetensors:  22%|â–ˆâ–ˆâ–       | 1.11G/4.95G [00:03<00:10, 358MB/s][A
model-00002-of-00003.safetensors:  24%|â–ˆâ–ˆâ–Ž       | 1.16G/4.95G [00:03<00:10, 372MB/s][A
model-00002-of-00003.safetensors:  25%|â–ˆâ–ˆâ–       | 1.22G/4.95G [00:03<00:09, 390MB/s][A
model-00002-of-00003.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 1.26G/4.95G [00:03<00:09, 397MB/s][A
model-00002-of-00003.safetensors:  26%|â–ˆâ–ˆâ–‹       | 1.31G/4.95G [00:03<00:08, 405MB/s][A
model-00002-of-00003.safetensors:  28%|â–ˆâ–ˆâ–Š       | 1.36G/4.95G [00:03<00:08, 423MB/s][A
model-00002-of-00003.safetensors:  29%|â–ˆâ–ˆâ–Š       | 1.42G/4.95G [00:03<00:08, 415MB/s][A
model-00002-of-00003.safetensors:  30%|â–ˆâ–ˆâ–‰       | 1.47G/4.95G [00:03<00:08, 422MB/s][A
model-00002-of-00003.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.53G/4.95G [00:04<00:07, 441MB/s][A
model-00002-of-00003.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.58G/4.95G [00:04<00:07, 450MB/s][A
model-00002-of-00003.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.64G/4.95G [00:04<00:07, 431MB/s][A
model-00002-of-00003.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.69G/4.95G [00:04<00:07, 452MB/s][A
model-00002-of-00003.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.74G/4.95G [00:04<00:07, 423MB/s][A
model-00002-of-00003.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.79G/4.95G [00:04<00:07, 408MB/s][A
model-00002-of-00003.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.85G/4.95G [00:04<00:07, 430MB/s][A
model-00002-of-00003.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.90G/4.95G [00:04<00:07, 432MB/s][A
model-00002-of-00003.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.95G/4.95G [00:05<00:07, 425MB/s][A
model-00002-of-00003.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2.00G/4.95G [00:05<00:06, 427MB/s][A
model-00002-of-00003.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.06G/4.95G [00:05<00:06, 424MB/s][A
model-00002-of-00003.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2.11G/4.95G [00:05<00:06, 425MB/s][A
model-00002-of-00003.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2.16G/4.95G [00:05<00:06, 441MB/s][A
model-00002-of-00003.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.21G/4.95G [00:05<00:06, 428MB/s][A
model-00002-of-00003.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2.26G/4.95G [00:05<00:06, 435MB/s][A
model-00002-of-00003.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2.32G/4.95G [00:05<00:05, 452MB/s][A
model-00002-of-00003.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.37G/4.95G [00:05<00:05, 437MB/s][A
model-00002-of-00003.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2.42G/4.95G [00:06<00:05, 447MB/s][A
model-00002-of-00003.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.47G/4.95G [00:06<00:05, 428MB/s][A
model-00002-of-00003.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.53G/4.95G [00:06<00:05, 424MB/s][A
model-00002-of-00003.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.58G/4.95G [00:06<00:05, 418MB/s][A
model-00002-of-00003.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2.63G/4.95G [00:06<00:05, 430MB/s][A
model-00002-of-00003.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.68G/4.95G [00:06<00:05, 407MB/s][A
model-00002-of-00003.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.73G/4.95G [00:06<00:05, 388MB/s][A
model-00002-of-00003.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.77G/4.95G [00:06<00:05, 374MB/s][A
model-00002-of-00003.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.81G/4.95G [00:07<00:05, 368MB/s][A
model-00002-of-00003.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.85G/4.95G [00:07<00:05, 381MB/s][A
model-00002-of-00003.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.89G/4.95G [00:07<00:05, 385MB/s][A
model-00002-of-00003.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.94G/4.95G [00:07<00:05, 371MB/s][A
model-00002-of-00003.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.98G/4.95G [00:07<00:05, 369MB/s][A
model-00002-of-00003.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3.02G/4.95G [00:07<00:05, 343MB/s][A
model-00002-of-00003.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3.07G/4.95G [00:07<00:04, 382MB/s][A
model-00002-of-00003.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3.11G/4.95G [00:07<00:04, 379MB/s][A
model-00002-of-00003.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3.17G/4.95G [00:08<00:04, 386MB/s][A
model-00002-of-00003.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3.22G/4.95G [00:08<00:04, 409MB/s][A
model-00002-of-00003.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3.26G/4.95G [00:08<00:04, 396MB/s][A
model-00002-of-00003.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3.31G/4.95G [00:08<00:03, 421MB/s][A
model-00002-of-00003.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3.37G/4.95G [00:08<00:03, 422MB/s][A
model-00002-of-00003.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3.42G/4.95G [00:08<00:03, 404MB/s][A
model-00002-of-00003.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3.46G/4.95G [00:08<00:03, 383MB/s][A
model-00002-of-00003.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3.50G/4.95G [00:08<00:04, 359MB/s][A
model-00002-of-00003.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.54G/4.95G [00:09<00:04, 337MB/s][A
model-00002-of-00003.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.59G/4.95G [00:09<00:04, 329MB/s][A
model-00002-of-00003.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3.63G/4.95G [00:09<00:03, 335MB/s][A
model-00002-of-00003.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.67G/4.95G [00:09<00:03, 332MB/s][A
model-00002-of-00003.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.71G/4.95G [00:09<00:03, 332MB/s][A
model-00002-of-00003.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.75G/4.95G [00:09<00:03, 323MB/s][A
model-00002-of-00003.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3.80G/4.95G [00:09<00:03, 335MB/s][A
model-00002-of-00003.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3.85G/4.95G [00:09<00:03, 356MB/s][A
model-00002-of-00003.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3.89G/4.95G [00:10<00:02, 353MB/s][A
model-00002-of-00003.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.93G/4.95G [00:10<00:02, 353MB/s][A
model-00002-of-00003.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3.97G/4.95G [00:10<00:02, 337MB/s][A
model-00002-of-00003.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4.02G/4.95G [00:10<00:02, 344MB/s][A
model-00002-of-00003.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.06G/4.95G [00:10<00:02, 350MB/s][A
model-00002-of-00003.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4.10G/4.95G [00:10<00:02, 337MB/s][A
model-00002-of-00003.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4.14G/4.95G [00:10<00:02, 348MB/s][A
model-00002-of-00003.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4.18G/4.95G [00:10<00:02, 352MB/s][A
model-00002-of-00003.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4.23G/4.95G [00:11<00:03, 229MB/s][A
model-00002-of-00003.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4.27G/4.95G [00:11<00:02, 258MB/s][A
model-00002-of-00003.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4.31G/4.95G [00:11<00:02, 287MB/s][A
model-00002-of-00003.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4.35G/4.95G [00:11<00:01, 305MB/s][A
model-00002-of-00003.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4.39G/4.95G [00:11<00:01, 315MB/s][A
model-00002-of-00003.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4.44G/4.95G [00:11<00:01, 330MB/s][A
model-00002-of-00003.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4.48G/4.95G [00:11<00:01, 344MB/s][A
model-00002-of-00003.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.52G/4.95G [00:12<00:01, 352MB/s][A
model-00002-of-00003.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.56G/4.95G [00:12<00:01, 357MB/s][A
model-00002-of-00003.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4.61G/4.95G [00:12<00:00, 385MB/s][A
model-00002-of-00003.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.66G/4.95G [00:12<00:01, 253MB/s][A
model-00002-of-00003.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.69G/4.95G [00:12<00:01, 250MB/s][A
model-00002-of-00003.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4.73G/4.95G [00:12<00:00, 270MB/s][A
model-00002-of-00003.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4.76G/4.95G [00:12<00:00, 276MB/s][A
model-00002-of-00003.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.80G/4.95G [00:13<00:00, 291MB/s][A
model-00002-of-00003.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4.85G/4.95G [00:13<00:00, 330MB/s][A
model-00002-of-00003.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.90G/4.95G [00:13<00:00, 342MB/s][A
model-00002-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.95G/4.95G [00:13<00:00, 368MB/s][Amodel-00002-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.95G/4.95G [00:13<00:00, 368MB/s]
Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:29<00:14, 14.40s/it]Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:29<00:14, 14.39s/it]Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:29<00:14, 14.39s/it]Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:29<00:14, 14.38s/it]
model-00003-of-00003.safetensors:   0%|          | 0.00/4.24G [00:00<?, ?B/s][A
model-00003-of-00003.safetensors:   1%|          | 31.5M/4.24G [00:00<00:14, 292MB/s][A
model-00003-of-00003.safetensors:   2%|â–         | 83.9M/4.24G [00:00<00:11, 374MB/s][A
model-00003-of-00003.safetensors:   3%|â–Ž         | 126M/4.24G [00:00<00:11, 362MB/s] [A
model-00003-of-00003.safetensors:   4%|â–         | 178M/4.24G [00:00<00:10, 373MB/s][A
model-00003-of-00003.safetensors:   5%|â–Œ         | 220M/4.24G [00:00<00:11, 340MB/s][A
model-00003-of-00003.safetensors:   6%|â–Œ         | 262M/4.24G [00:00<00:12, 319MB/s][A
model-00003-of-00003.safetensors:   7%|â–‹         | 315M/4.24G [00:00<00:11, 349MB/s][A
model-00003-of-00003.safetensors:   8%|â–Š         | 357M/4.24G [00:01<00:11, 352MB/s][A
model-00003-of-00003.safetensors:  10%|â–‰         | 409M/4.24G [00:01<00:09, 385MB/s][A
model-00003-of-00003.safetensors:  11%|â–ˆ         | 461M/4.24G [00:01<00:09, 410MB/s][A
model-00003-of-00003.safetensors:  12%|â–ˆâ–        | 503M/4.24G [00:01<00:09, 387MB/s][A
model-00003-of-00003.safetensors:  13%|â–ˆâ–Ž        | 556M/4.24G [00:01<00:09, 403MB/s][A
model-00003-of-00003.safetensors:  14%|â–ˆâ–        | 608M/4.24G [00:01<00:08, 408MB/s][A
model-00003-of-00003.safetensors:  15%|â–ˆâ–Œ        | 650M/4.24G [00:01<00:09, 379MB/s][A
model-00003-of-00003.safetensors:  16%|â–ˆâ–‹        | 692M/4.24G [00:01<00:09, 369MB/s][A
model-00003-of-00003.safetensors:  17%|â–ˆâ–‹        | 734M/4.24G [00:01<00:09, 381MB/s][A
model-00003-of-00003.safetensors:  18%|â–ˆâ–Š        | 776M/4.24G [00:02<00:09, 370MB/s][A
model-00003-of-00003.safetensors:  20%|â–ˆâ–‰        | 828M/4.24G [00:02<00:08, 396MB/s][A
model-00003-of-00003.safetensors:  21%|â–ˆâ–ˆ        | 870M/4.24G [00:02<00:08, 385MB/s][A
model-00003-of-00003.safetensors:  22%|â–ˆâ–ˆâ–       | 912M/4.24G [00:02<00:09, 343MB/s][A
model-00003-of-00003.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 954M/4.24G [00:02<00:09, 335MB/s][A
model-00003-of-00003.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 996M/4.24G [00:02<00:09, 342MB/s][A
model-00003-of-00003.safetensors:  24%|â–ˆâ–ˆâ–       | 1.04G/4.24G [00:02<00:09, 346MB/s][A
model-00003-of-00003.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 1.08G/4.24G [00:02<00:09, 350MB/s][A
model-00003-of-00003.safetensors:  26%|â–ˆâ–ˆâ–‹       | 1.12G/4.24G [00:03<00:08, 362MB/s][A
model-00003-of-00003.safetensors:  27%|â–ˆâ–ˆâ–‹       | 1.16G/4.24G [00:03<00:08, 355MB/s][A
model-00003-of-00003.safetensors:  28%|â–ˆâ–ˆâ–Š       | 1.21G/4.24G [00:03<00:08, 363MB/s][A
model-00003-of-00003.safetensors:  29%|â–ˆâ–ˆâ–‰       | 1.25G/4.24G [00:03<00:08, 359MB/s][A
model-00003-of-00003.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 1.29G/4.24G [00:03<00:08, 344MB/s][A
model-00003-of-00003.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 1.33G/4.24G [00:03<00:08, 326MB/s][A
model-00003-of-00003.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.37G/4.24G [00:03<00:09, 313MB/s][A
model-00003-of-00003.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.42G/4.24G [00:03<00:08, 330MB/s][A
model-00003-of-00003.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 1.47G/4.24G [00:04<00:07, 368MB/s][A
model-00003-of-00003.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.52G/4.24G [00:04<00:06, 392MB/s][A
model-00003-of-00003.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.56G/4.24G [00:04<00:07, 373MB/s][A
model-00003-of-00003.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.60G/4.24G [00:04<00:06, 384MB/s][A
model-00003-of-00003.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.65G/4.24G [00:04<00:06, 384MB/s][A
model-00003-of-00003.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.69G/4.24G [00:04<00:06, 385MB/s][A
model-00003-of-00003.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.73G/4.24G [00:04<00:06, 392MB/s][A
model-00003-of-00003.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.77G/4.24G [00:04<00:06, 387MB/s][A
model-00003-of-00003.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.81G/4.24G [00:04<00:06, 382MB/s][A
model-00003-of-00003.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.86G/4.24G [00:05<00:06, 381MB/s][A
model-00003-of-00003.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.90G/4.24G [00:05<00:06, 371MB/s][A
model-00003-of-00003.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.94G/4.24G [00:05<00:06, 363MB/s][A
model-00003-of-00003.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.98G/4.24G [00:05<00:06, 364MB/s][A
model-00003-of-00003.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.02G/4.24G [00:05<00:06, 366MB/s][A
model-00003-of-00003.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.07G/4.24G [00:05<00:05, 375MB/s][A
model-00003-of-00003.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2.11G/4.24G [00:05<00:05, 366MB/s][A
model-00003-of-00003.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.15G/4.24G [00:05<00:05, 369MB/s][A
model-00003-of-00003.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.19G/4.24G [00:05<00:05, 367MB/s][A
model-00003-of-00003.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2.23G/4.24G [00:06<00:05, 346MB/s][A
model-00003-of-00003.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2.28G/4.24G [00:06<00:05, 343MB/s][A
model-00003-of-00003.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.32G/4.24G [00:06<00:05, 347MB/s][A
model-00003-of-00003.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.36G/4.24G [00:06<00:05, 334MB/s][A
model-00003-of-00003.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.40G/4.24G [00:06<00:05, 349MB/s][A
model-00003-of-00003.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.44G/4.24G [00:06<00:05, 346MB/s][A
model-00003-of-00003.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.49G/4.24G [00:06<00:05, 348MB/s][A
model-00003-of-00003.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.53G/4.24G [00:06<00:04, 360MB/s][A
model-00003-of-00003.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.57G/4.24G [00:07<00:04, 366MB/s][A
model-00003-of-00003.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.61G/4.24G [00:07<00:04, 364MB/s][A
model-00003-of-00003.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.65G/4.24G [00:07<00:04, 374MB/s][A
model-00003-of-00003.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.69G/4.24G [00:07<00:04, 371MB/s][A
model-00003-of-00003.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.74G/4.24G [00:07<00:04, 375MB/s][A
model-00003-of-00003.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.78G/4.24G [00:07<00:03, 369MB/s][A
model-00003-of-00003.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.82G/4.24G [00:07<00:03, 360MB/s][A
model-00003-of-00003.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.86G/4.24G [00:07<00:03, 372MB/s][A
model-00003-of-00003.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.90G/4.24G [00:07<00:03, 377MB/s][A
model-00003-of-00003.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.95G/4.24G [00:08<00:03, 378MB/s][A
model-00003-of-00003.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.99G/4.24G [00:08<00:03, 371MB/s][A
model-00003-of-00003.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.03G/4.24G [00:08<00:03, 379MB/s][A
model-00003-of-00003.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.07G/4.24G [00:08<00:03, 378MB/s][A
model-00003-of-00003.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3.11G/4.24G [00:08<00:02, 386MB/s][A
model-00003-of-00003.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.16G/4.24G [00:08<00:02, 389MB/s][A
model-00003-of-00003.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.20G/4.24G [00:08<00:02, 380MB/s][A
model-00003-of-00003.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3.24G/4.24G [00:08<00:02, 388MB/s][A
model-00003-of-00003.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3.28G/4.24G [00:08<00:02, 389MB/s][A
model-00003-of-00003.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3.32G/4.24G [00:09<00:03, 250MB/s][A
model-00003-of-00003.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.37G/4.24G [00:09<00:03, 271MB/s][A
model-00003-of-00003.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3.41G/4.24G [00:09<00:02, 300MB/s][A
model-00003-of-00003.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3.45G/4.24G [00:09<00:02, 323MB/s][A
model-00003-of-00003.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3.49G/4.24G [00:09<00:02, 333MB/s][A
model-00003-of-00003.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3.53G/4.24G [00:09<00:02, 352MB/s][A
model-00003-of-00003.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3.58G/4.24G [00:09<00:01, 365MB/s][A
model-00003-of-00003.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3.62G/4.24G [00:10<00:01, 378MB/s][A
model-00003-of-00003.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.66G/4.24G [00:10<00:01, 376MB/s][A
model-00003-of-00003.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.70G/4.24G [00:10<00:01, 381MB/s][A
model-00003-of-00003.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.74G/4.24G [00:10<00:01, 387MB/s][A
model-00003-of-00003.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.79G/4.24G [00:10<00:01, 389MB/s][A
model-00003-of-00003.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.83G/4.24G [00:10<00:01, 394MB/s][A
model-00003-of-00003.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.87G/4.24G [00:10<00:00, 398MB/s][A
model-00003-of-00003.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.92G/4.24G [00:10<00:00, 416MB/s][A
model-00003-of-00003.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.96G/4.24G [00:10<00:00, 409MB/s][A
model-00003-of-00003.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.01G/4.24G [00:11<00:00, 377MB/s][A
model-00003-of-00003.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4.05G/4.24G [00:11<00:00, 381MB/s][A
model-00003-of-00003.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.10G/4.24G [00:11<00:00, 397MB/s][A
model-00003-of-00003.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4.14G/4.24G [00:11<00:00, 398MB/s][A
model-00003-of-00003.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.19G/4.24G [00:11<00:00, 404MB/s][A
model-00003-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.24G/4.24G [00:11<00:00, 413MB/s][Amodel-00003-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.24G/4.24G [00:11<00:00, 366MB/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.16s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.60s/it]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.15s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.60s/it]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.16s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.60s/it]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.16s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.61s/it]
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:16<00:33, 16.76s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:22<00:44, 22.43s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:24<00:49, 24.81s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:24<00:49, 24.74s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:39<00:20, 20.29s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:44<00:22, 22.05s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:46<00:23, 23.03s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:47<00:23, 23.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:06<00:00, 23.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:06<00:00, 22.17s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [00:00<00:00, 1.38MB/s]
  0%|          | 0/3145 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:14<00:00, 25.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:14<00:00, 24.79s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/3145 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.31s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 25.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 25.40s/it]
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.6-vicuna-7b and are newly initialized: ['model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 1/3145 [00:12<11:00:58, 12.61s/it]  0%|          | 0/3143 [00:00<?, ?it/s]  0%|          | 0/3145 [00:00<?, ?it/s]  0%|          | 2/3145 [00:12<4:41:33,  5.38s/it]   0%|          | 3/3145 [00:13<2:42:16,  3.10s/it]  0%|          | 4/3145 [00:13<1:45:00,  2.01s/it]  0%|          | 5/3145 [00:14<1:13:54,  1.41s/it]  0%|          | 6/3145 [00:14<55:30,  1.06s/it]    0%|          | 7/3145 [00:14<43:27,  1.20it/s]  0%|          | 8/3145 [00:15<34:59,  1.49it/s]  0%|          | 9/3145 [00:15<31:20,  1.67it/s]  0%|          | 10/3145 [00:15<27:11,  1.92it/s]  0%|          | 11/3145 [00:16<24:32,  2.13it/s]  0%|          | 12/3145 [00:16<23:10,  2.25it/s]  0%|          | 13/3145 [00:16<21:24,  2.44it/s]  0%|          | 14/3145 [00:17<21:53,  2.38it/s]  0%|          | 15/3145 [00:17<20:53,  2.50it/s]  0%|          | 1/3145 [00:07<6:41:30,  7.66s/it]  1%|          | 16/3145 [00:18<19:46,  2.64it/s]  0%|          | 2/3145 [00:08<2:56:40,  3.37s/it]  1%|          | 17/3145 [00:18<20:32,  2.54it/s]  0%|          | 1/3143 [00:05<5:05:30,  5.83s/it]  0%|          | 3/3145 [00:08<1:44:07,  1.99s/it]  0%|          | 1/3145 [00:06<5:14:24,  6.00s/it]  1%|          | 18/3145 [00:18<20:02,  2.60it/s]  0%|          | 2/3143 [00:06<2:17:52,  2.63s/it]  0%|          | 4/3145 [00:08<1:10:59,  1.36s/it]  1%|          | 19/3145 [00:19<18:36,  2.80it/s]  0%|          | 2/3145 [00:06<2:20:58,  2.69s/it]  0%|          | 3/3143 [00:06<1:24:44,  1.62s/it]  0%|          | 5/3145 [00:09<53:15,  1.02s/it]    1%|          | 20/3145 [00:19<18:11,  2.86it/s]  0%|          | 3/3145 [00:06<1:25:30,  1.63s/it]  0%|          | 4/3143 [00:06<58:16,  1.11s/it]    0%|          | 4/3145 [00:06<56:39,  1.08s/it]    0%|          | 6/3145 [00:09<41:56,  1.25it/s]  1%|          | 21/3145 [00:19<18:57,  2.75it/s]  0%|          | 5/3143 [00:07<43:57,  1.19it/s]  0%|          | 5/3145 [00:07<43:14,  1.21it/s]  0%|          | 7/3145 [00:09<34:33,  1.51it/s]  1%|          | 22/3145 [00:20<19:19,  2.69it/s]  0%|          | 6/3143 [00:07<34:51,  1.50it/s]  0%|          | 6/3145 [00:07<34:23,  1.52it/s]  0%|          | 8/3145 [00:10<29:14,  1.79it/s]  1%|          | 23/3145 [00:20<19:09,  2.71it/s]  0%|          | 7/3143 [00:08<29:29,  1.77it/s]  0%|          | 7/3145 [00:08<28:49,  1.81it/s]  0%|          | 9/3145 [00:10<25:19,  2.06it/s]  1%|          | 24/3145 [00:21<19:26,  2.67it/s]  0%|          | 10/3145 [00:10<21:27,  2.44it/s]  0%|          | 8/3143 [00:08<25:33,  2.04it/s]  0%|          | 8/3145 [00:08<25:08,  2.08it/s]  1%|          | 25/3145 [00:21<18:48,  2.77it/s]  0%|          | 11/3145 [00:11<18:51,  2.77it/s]  0%|          | 9/3143 [00:08<23:08,  2.26it/s]  0%|          | 9/3145 [00:08<22:17,  2.34it/s]  0%|          | 12/3145 [00:11<18:18,  2.85it/s]  1%|          | 26/3145 [00:21<19:18,  2.69it/s]  0%|          | 10/3145 [00:09<21:39,  2.41it/s]  0%|          | 10/3143 [00:09<22:21,  2.34it/s]  0%|          | 13/3145 [00:11<16:24,  3.18it/s]  1%|          | 27/3145 [00:22<20:02,  2.59it/s]  0%|          | 11/3143 [00:09<20:56,  2.49it/s]  0%|          | 11/3145 [00:09<21:22,  2.44it/s]  0%|          | 14/3145 [00:12<17:05,  3.05it/s]  1%|          | 28/3145 [00:22<20:08,  2.58it/s]  0%|          | 12/3145 [00:09<20:05,  2.60it/s]  0%|          | 12/3143 [00:09<20:47,  2.51it/s]  0%|          | 15/3145 [00:12<17:09,  3.04it/s]  1%|          | 29/3145 [00:22<19:36,  2.65it/s]  0%|          | 13/3145 [00:10<20:03,  2.60it/s]  1%|          | 16/3145 [00:12<18:02,  2.89it/s]  0%|          | 13/3143 [00:10<21:42,  2.40it/s]  1%|          | 30/3145 [00:23<19:26,  2.67it/s]  0%|          | 14/3143 [00:10<20:10,  2.58it/s]  0%|          | 14/3145 [00:10<20:31,  2.54it/s]  1%|          | 17/3145 [00:13<19:19,  2.70it/s]  1%|          | 31/3145 [00:23<19:48,  2.62it/s]  0%|          | 15/3143 [00:10<19:18,  2.70it/s]  0%|          | 15/3145 [00:10<19:54,  2.62it/s]  1%|          | 18/3145 [00:13<18:44,  2.78it/s]  1%|          | 32/3145 [00:24<19:17,  2.69it/s]  1%|          | 16/3145 [00:11<19:25,  2.68it/s]  1%|          | 19/3145 [00:13<18:40,  2.79it/s]  1%|          | 16/3143 [00:11<20:48,  2.50it/s]  1%|          | 17/3145 [00:11<17:00,  3.06it/s]  1%|          | 33/3145 [00:24<20:32,  2.53it/s]  1%|          | 17/3143 [00:11<20:07,  2.59it/s]  1%|          | 18/3145 [00:11<15:51,  3.29it/s]  1%|          | 34/3145 [00:24<19:49,  2.62it/s]  1%|          | 20/3145 [00:14<24:39,  2.11it/s]  1%|          | 18/3143 [00:12<19:46,  2.63it/s]  1%|          | 19/3145 [00:12<17:03,  3.06it/s]  1%|          | 20/3145 [00:12<17:02,  3.06it/s]  1%|          | 19/3143 [00:12<19:50,  2.62it/s]  1%|          | 21/3145 [00:15<25:01,  2.08it/s]  1%|          | 35/3145 [00:25<22:46,  2.28it/s]  1%|          | 20/3143 [00:12<19:21,  2.69it/s]  1%|          | 21/3145 [00:12<17:33,  2.96it/s]  1%|          | 22/3145 [00:15<22:41,  2.29it/s]  1%|          | 36/3145 [00:25<21:24,  2.42it/s]  1%|          | 21/3143 [00:13<19:13,  2.71it/s]  1%|          | 22/3145 [00:13<18:00,  2.89it/s]  1%|          | 23/3145 [00:15<21:52,  2.38it/s]  1%|          | 37/3145 [00:26<21:55,  2.36it/s]  1%|          | 22/3143 [00:13<18:37,  2.79it/s]  1%|          | 23/3145 [00:13<17:59,  2.89it/s]  1%|          | 24/3145 [00:16<20:56,  2.48it/s]  1%|          | 38/3145 [00:26<20:49,  2.49it/s]  1%|          | 23/3143 [00:13<18:41,  2.78it/s]  1%|          | 24/3145 [00:13<19:15,  2.70it/s]  1%|          | 25/3145 [00:16<20:16,  2.56it/s]  1%|          | 39/3145 [00:26<21:32,  2.40it/s]  1%|          | 24/3143 [00:14<18:40,  2.78it/s]  1%|          | 25/3145 [00:14<19:01,  2.73it/s]  1%|          | 26/3145 [00:16<19:40,  2.64it/s]  1%|â–         | 40/3145 [00:27<21:33,  2.40it/s]  1%|          | 25/3143 [00:14<19:07,  2.72it/s]  1%|          | 26/3145 [00:14<18:52,  2.76it/s]  1%|          | 27/3145 [00:17<19:23,  2.68it/s]  1%|â–         | 41/3145 [00:27<20:35,  2.51it/s]  1%|          | 26/3143 [00:15<19:19,  2.69it/s]  1%|          | 27/3145 [00:15<19:02,  2.73it/s]  1%|          | 28/3145 [00:17<20:03,  2.59it/s]  1%|          | 27/3143 [00:15<18:43,  2.77it/s]  1%|â–         | 42/3145 [00:28<20:50,  2.48it/s]  1%|          | 28/3145 [00:15<18:28,  2.81it/s]  1%|          | 29/3145 [00:18<20:42,  2.51it/s]  1%|          | 28/3143 [00:15<19:01,  2.73it/s]  1%|â–         | 43/3145 [00:28<20:06,  2.57it/s]  1%|          | 29/3145 [00:15<18:43,  2.77it/s]  1%|          | 30/3145 [00:18<19:59,  2.60it/s]  1%|â–         | 44/3145 [00:28<19:42,  2.62it/s]  1%|          | 30/3145 [00:16<19:04,  2.72it/s]  1%|          | 29/3143 [00:16<19:59,  2.60it/s]  1%|          | 31/3145 [00:18<19:31,  2.66it/s]  1%|â–         | 45/3145 [00:29<19:45,  2.62it/s]  1%|          | 30/3143 [00:16<19:13,  2.70it/s]  1%|          | 32/3145 [00:19<19:09,  2.71it/s]  1%|â–         | 46/3145 [00:29<20:11,  2.56it/s]  1%|          | 31/3145 [00:16<25:29,  2.04it/s]  1%|          | 33/3145 [00:19<19:06,  2.72it/s]  1%|          | 31/3143 [00:17<22:10,  2.34it/s]  1%|â–         | 47/3145 [00:30<20:09,  2.56it/s]  1%|          | 32/3145 [00:17<24:38,  2.11it/s]  1%|          | 34/3145 [00:19<19:18,  2.69it/s]  1%|          | 32/3143 [00:17<21:29,  2.41it/s]  2%|â–         | 48/3145 [00:30<20:07,  2.57it/s]  1%|          | 35/3145 [00:20<19:06,  2.71it/s]  1%|          | 33/3145 [00:17<23:20,  2.22it/s]  1%|          | 33/3143 [00:17<21:30,  2.41it/s]  1%|          | 36/3145 [00:20<18:44,  2.76it/s]  2%|â–         | 49/3145 [00:30<20:42,  2.49it/s]  1%|          | 34/3145 [00:18<22:34,  2.30it/s]  1%|          | 34/3143 [00:18<21:27,  2.41it/s]  1%|          | 37/3145 [00:20<16:37,  3.12it/s]  1%|          | 35/3145 [00:18<20:51,  2.48it/s]  2%|â–         | 50/3145 [00:31<21:01,  2.45it/s]  1%|          | 35/3143 [00:18<20:38,  2.51it/s]  1%|          | 38/3145 [00:21<16:46,  3.09it/s]  1%|          | 36/3145 [00:18<20:06,  2.58it/s]  1%|          | 39/3145 [00:21<17:52,  2.90it/s]  1%|          | 36/3143 [00:19<20:42,  2.50it/s]  1%|          | 37/3145 [00:19<20:30,  2.53it/s]  1%|          | 37/3143 [00:19<20:12,  2.56it/s]  1%|â–         | 40/3145 [00:21<18:28,  2.80it/s]  1%|          | 38/3145 [00:19<21:08,  2.45it/s]  1%|          | 38/3143 [00:19<19:46,  2.62it/s]  1%|â–         | 41/3145 [00:22<18:40,  2.77it/s]  1%|          | 39/3145 [00:20<20:14,  2.56it/s]  1%|          | 39/3143 [00:20<19:57,  2.59it/s]  1%|â–         | 42/3145 [00:22<19:17,  2.68it/s]  1%|â–         | 40/3145 [00:20<19:32,  2.65it/s]  2%|â–         | 51/3145 [00:33<45:13,  1.14it/s]  1%|â–         | 40/3143 [00:20<20:27,  2.53it/s]  1%|â–         | 43/3145 [00:23<20:06,  2.57it/s]  1%|â–         | 41/3145 [00:20<18:40,  2.77it/s]  2%|â–         | 52/3145 [00:33<37:11,  1.39it/s]  1%|â–         | 44/3145 [00:23<19:34,  2.64it/s]  1%|â–         | 41/3143 [00:20<20:18,  2.55it/s]  1%|â–         | 42/3145 [00:21<19:01,  2.72it/s]  2%|â–         | 53/3145 [00:34<32:27,  1.59it/s]  1%|â–         | 45/3145 [00:23<19:23,  2.66it/s]  1%|â–         | 42/3143 [00:21<21:54,  2.36it/s]  1%|â–         | 43/3145 [00:21<19:09,  2.70it/s]  2%|â–         | 54/3145 [00:34<28:18,  1.82it/s]  1%|â–         | 46/3145 [00:24<18:56,  2.73it/s]  1%|â–         | 44/3145 [00:21<18:36,  2.78it/s]  1%|â–         | 43/3143 [00:21<22:27,  2.30it/s]  2%|â–         | 55/3145 [00:34<25:09,  2.05it/s]  1%|â–         | 47/3145 [00:24<19:12,  2.69it/s]  1%|â–         | 45/3145 [00:22<18:44,  2.76it/s]  1%|â–         | 44/3143 [00:22<21:04,  2.45it/s]  2%|â–         | 56/3145 [00:35<23:15,  2.21it/s]  1%|â–         | 46/3145 [00:22<18:34,  2.78it/s]  2%|â–         | 48/3145 [00:25<21:37,  2.39it/s]  1%|â–         | 45/3143 [00:22<20:14,  2.55it/s]  2%|â–         | 57/3145 [00:35<22:01,  2.34it/s]  1%|â–         | 47/3145 [00:22<19:00,  2.72it/s]  2%|â–         | 58/3145 [00:35<20:39,  2.49it/s]  1%|â–         | 46/3143 [00:23<21:47,  2.37it/s]  2%|â–         | 48/3145 [00:23<18:23,  2.81it/s]  2%|â–         | 49/3145 [00:25<25:39,  2.01it/s]  2%|â–         | 59/3145 [00:36<20:26,  2.52it/s]  1%|â–         | 47/3143 [00:23<21:16,  2.42it/s]  2%|â–         | 49/3145 [00:23<18:59,  2.72it/s]  2%|â–         | 50/3145 [00:26<24:23,  2.11it/s]  2%|â–         | 60/3145 [00:36<20:37,  2.49it/s]  2%|â–         | 48/3143 [00:23<20:42,  2.49it/s]  2%|â–         | 50/3145 [00:23<18:56,  2.72it/s]  2%|â–         | 49/3143 [00:24<17:56,  2.87it/s]  2%|â–         | 61/3145 [00:36<19:50,  2.59it/s]  2%|â–         | 51/3145 [00:26<26:49,  1.92it/s]  2%|â–         | 51/3145 [00:24<18:48,  2.74it/s]  2%|â–         | 50/3143 [00:24<18:27,  2.79it/s]  2%|â–         | 62/3145 [00:37<19:47,  2.60it/s]  2%|â–         | 52/3145 [00:24<18:41,  2.76it/s]  2%|â–         | 51/3143 [00:24<16:31,  3.12it/s]  2%|â–         | 52/3145 [00:27<25:29,  2.02it/s]  2%|â–         | 63/3145 [00:37<20:28,  2.51it/s]  2%|â–         | 53/3145 [00:25<19:06,  2.70it/s]  2%|â–         | 53/3145 [00:27<23:21,  2.21it/s]  2%|â–         | 52/3143 [00:25<17:45,  2.90it/s]  2%|â–         | 64/3145 [00:38<19:33,  2.63it/s]  2%|â–         | 54/3145 [00:27<22:05,  2.33it/s]  2%|â–         | 53/3143 [00:25<18:35,  2.77it/s]  2%|â–         | 54/3145 [00:25<20:02,  2.57it/s]  2%|â–         | 54/3143 [00:25<16:47,  3.07it/s]  2%|â–         | 65/3145 [00:38<20:10,  2.54it/s]  2%|â–         | 55/3145 [00:28<20:39,  2.49it/s]  2%|â–         | 55/3145 [00:25<19:34,  2.63it/s]  2%|â–         | 66/3145 [00:38<18:10,  2.82it/s]  2%|â–         | 56/3145 [00:28<19:55,  2.58it/s]  2%|â–         | 55/3143 [00:26<18:13,  2.82it/s]  2%|â–         | 56/3145 [00:26<18:59,  2.71it/s]  2%|â–         | 67/3145 [00:39<18:09,  2.82it/s]  2%|â–         | 56/3143 [00:26<18:14,  2.82it/s]  2%|â–         | 57/3145 [00:29<20:19,  2.53it/s]  2%|â–         | 57/3145 [00:26<19:24,  2.65it/s]  2%|â–         | 68/3145 [00:39<18:39,  2.75it/s]  2%|â–         | 57/3143 [00:26<18:15,  2.82it/s]  2%|â–         | 58/3145 [00:29<19:35,  2.63it/s]  2%|â–         | 58/3145 [00:27<19:30,  2.64it/s]  2%|â–         | 69/3145 [00:39<18:32,  2.76it/s]  2%|â–         | 59/3145 [00:29<19:09,  2.68it/s]  2%|â–         | 58/3143 [00:27<19:08,  2.69it/s]  2%|â–         | 60/3145 [00:30<16:56,  3.04it/s]  2%|â–         | 70/3145 [00:40<18:49,  2.72it/s]  2%|â–         | 59/3143 [00:27<18:32,  2.77it/s]  2%|â–         | 61/3145 [00:30<15:24,  3.34it/s]  2%|â–         | 60/3143 [00:27<16:38,  3.09it/s]  2%|â–         | 71/3145 [00:40<18:43,  2.74it/s]  2%|â–         | 62/3145 [00:30<16:40,  3.08it/s]  2%|â–         | 61/3143 [00:28<17:17,  2.97it/s]  2%|â–         | 59/3145 [00:28<32:45,  1.57it/s]  2%|â–         | 72/3145 [00:41<19:06,  2.68it/s]  2%|â–         | 63/3145 [00:30<15:15,  3.37it/s]  2%|â–         | 62/3143 [00:28<17:34,  2.92it/s]  2%|â–         | 73/3145 [00:41<19:26,  2.63it/s]  2%|â–         | 60/3145 [00:28<29:25,  1.75it/s]  2%|â–         | 64/3145 [00:31<16:11,  3.17it/s]  2%|â–         | 74/3145 [00:41<19:07,  2.68it/s]  2%|â–         | 63/3143 [00:29<19:18,  2.66it/s]  2%|â–         | 65/3145 [00:31<16:52,  3.04it/s]  2%|â–         | 61/3145 [00:29<27:15,  1.89it/s]  2%|â–         | 64/3143 [00:29<19:22,  2.65it/s]  2%|â–         | 75/3145 [00:42<20:16,  2.52it/s]  2%|â–         | 66/3145 [00:32<18:10,  2.82it/s]  2%|â–         | 62/3145 [00:29<25:10,  2.04it/s]  2%|â–         | 65/3143 [00:29<19:08,  2.68it/s]  2%|â–         | 76/3145 [00:42<19:45,  2.59it/s]  2%|â–         | 67/3145 [00:32<18:15,  2.81it/s]  2%|â–         | 63/3145 [00:29<23:04,  2.23it/s]  2%|â–         | 66/3143 [00:30<19:17,  2.66it/s]  2%|â–         | 68/3145 [00:32<18:36,  2.76it/s]  2%|â–         | 64/3145 [00:30<21:54,  2.34it/s]  2%|â–         | 77/3145 [00:43<19:58,  2.56it/s]  2%|â–         | 67/3143 [00:30<17:56,  2.86it/s]  2%|â–         | 78/3145 [00:43<19:23,  2.64it/s]  2%|â–         | 65/3145 [00:30<21:24,  2.40it/s]  2%|â–         | 69/3145 [00:33<19:11,  2.67it/s]  2%|â–         | 68/3143 [00:30<18:28,  2.77it/s]  3%|â–Ž         | 79/3145 [00:43<18:46,  2.72it/s]  2%|â–         | 66/3145 [00:31<21:00,  2.44it/s]  2%|â–         | 70/3145 [00:33<19:34,  2.62it/s]  3%|â–Ž         | 80/3145 [00:43<16:38,  3.07it/s]  2%|â–         | 69/3143 [00:31<19:02,  2.69it/s]  2%|â–         | 71/3145 [00:33<18:51,  2.72it/s]  2%|â–         | 67/3145 [00:31<20:42,  2.48it/s]  3%|â–Ž         | 81/3145 [00:44<18:01,  2.83it/s]  2%|â–         | 70/3143 [00:31<18:46,  2.73it/s]  2%|â–         | 72/3145 [00:34<18:16,  2.80it/s]  2%|â–         | 68/3145 [00:31<19:56,  2.57it/s]  3%|â–Ž         | 82/3145 [00:44<18:53,  2.70it/s]  2%|â–         | 71/3143 [00:32<19:33,  2.62it/s]  2%|â–         | 73/3145 [00:34<18:20,  2.79it/s]  2%|â–         | 69/3145 [00:32<19:23,  2.64it/s]  3%|â–Ž         | 83/3145 [00:44<16:34,  3.08it/s]  2%|â–         | 72/3143 [00:32<19:33,  2.62it/s]  2%|â–         | 74/3145 [00:34<18:16,  2.80it/s]  2%|â–         | 70/3145 [00:32<19:31,  2.62it/s]  3%|â–Ž         | 84/3145 [00:45<17:07,  2.98it/s]  2%|â–         | 73/3143 [00:32<19:18,  2.65it/s]  2%|â–         | 75/3145 [00:35<18:23,  2.78it/s]  2%|â–         | 71/3145 [00:32<19:10,  2.67it/s]  3%|â–Ž         | 85/3145 [00:45<17:49,  2.86it/s]  2%|â–         | 74/3143 [00:33<19:07,  2.67it/s]  2%|â–         | 76/3145 [00:35<18:27,  2.77it/s]  2%|â–         | 72/3145 [00:33<18:51,  2.71it/s]  3%|â–Ž         | 86/3145 [00:46<17:57,  2.84it/s]  2%|â–         | 75/3143 [00:33<18:38,  2.74it/s]  2%|â–         | 77/3145 [00:36<18:46,  2.72it/s]  2%|â–         | 73/3145 [00:33<19:12,  2.66it/s]  3%|â–Ž         | 87/3145 [00:46<18:00,  2.83it/s]  2%|â–         | 76/3143 [00:33<18:12,  2.81it/s]  2%|â–         | 78/3145 [00:36<19:06,  2.67it/s]  2%|â–         | 74/3145 [00:34<19:52,  2.58it/s]  3%|â–Ž         | 88/3145 [00:46<18:30,  2.75it/s]  2%|â–         | 77/3143 [00:34<18:37,  2.74it/s]  3%|â–Ž         | 79/3145 [00:36<18:32,  2.76it/s]  2%|â–         | 75/3145 [00:34<19:23,  2.64it/s]  3%|â–Ž         | 89/3145 [00:47<18:30,  2.75it/s]  2%|â–         | 78/3143 [00:34<18:18,  2.79it/s]  3%|â–Ž         | 80/3145 [00:37<19:21,  2.64it/s]  3%|â–Ž         | 90/3145 [00:47<18:08,  2.81it/s]  2%|â–         | 76/3145 [00:34<20:35,  2.48it/s]  3%|â–Ž         | 79/3143 [00:34<18:44,  2.72it/s]  3%|â–Ž         | 81/3145 [00:37<19:58,  2.56it/s]  3%|â–Ž         | 91/3145 [00:47<18:13,  2.79it/s]  2%|â–         | 77/3145 [00:35<19:57,  2.56it/s]  3%|â–Ž         | 80/3143 [00:35<18:35,  2.74it/s]  3%|â–Ž         | 92/3145 [00:48<18:10,  2.80it/s]  3%|â–Ž         | 82/3145 [00:37<19:54,  2.56it/s]  2%|â–         | 78/3145 [00:35<18:23,  2.78it/s]  3%|â–Ž         | 81/3143 [00:35<19:50,  2.57it/s]  3%|â–Ž         | 93/3145 [00:48<17:54,  2.84it/s]  3%|â–Ž         | 83/3145 [00:38<19:39,  2.60it/s]  3%|â–Ž         | 79/3145 [00:35<19:20,  2.64it/s]  3%|â–Ž         | 82/3143 [00:36<18:56,  2.69it/s]  3%|â–Ž         | 84/3145 [00:38<18:51,  2.71it/s]  3%|â–Ž         | 94/3145 [00:48<18:10,  2.80it/s]  3%|â–Ž         | 80/3145 [00:36<19:24,  2.63it/s]  3%|â–Ž         | 85/3145 [00:38<16:42,  3.05it/s]  3%|â–Ž         | 83/3143 [00:36<19:26,  2.62it/s]  3%|â–Ž         | 95/3145 [00:49<18:09,  2.80it/s]  3%|â–Ž         | 81/3145 [00:36<20:03,  2.55it/s]  3%|â–Ž         | 86/3145 [00:39<17:33,  2.90it/s]  3%|â–Ž         | 84/3143 [00:36<19:57,  2.55it/s]  3%|â–Ž         | 96/3145 [00:49<18:41,  2.72it/s]  3%|â–Ž         | 82/3145 [00:37<19:28,  2.62it/s]  3%|â–Ž         | 87/3145 [00:39<18:15,  2.79it/s]  3%|â–Ž         | 85/3143 [00:37<19:50,  2.57it/s]  3%|â–Ž         | 97/3145 [00:50<19:17,  2.63it/s]  3%|â–Ž         | 83/3145 [00:37<18:35,  2.75it/s]  3%|â–Ž         | 88/3145 [00:40<18:52,  2.70it/s]  3%|â–Ž         | 86/3143 [00:37<19:21,  2.63it/s]  3%|â–Ž         | 98/3145 [00:50<18:54,  2.68it/s]  3%|â–Ž         | 84/3145 [00:37<18:27,  2.76it/s]  3%|â–Ž         | 89/3145 [00:40<19:17,  2.64it/s]  3%|â–Ž         | 87/3143 [00:38<19:29,  2.61it/s]  3%|â–Ž         | 99/3145 [00:50<19:13,  2.64it/s]  3%|â–Ž         | 85/3145 [00:38<18:24,  2.77it/s]  3%|â–Ž         | 88/3143 [00:38<19:08,  2.66it/s]  3%|â–Ž         | 90/3145 [00:40<20:06,  2.53it/s]  3%|â–Ž         | 100/3145 [00:51<19:02,  2.67it/s]  3%|â–Ž         | 86/3145 [00:38<18:55,  2.69it/s]  3%|â–Ž         | 89/3143 [00:38<19:15,  2.64it/s]  3%|â–Ž         | 91/3145 [00:41<20:28,  2.49it/s]  3%|â–Ž         | 101/3145 [00:51<19:46,  2.56it/s]  3%|â–Ž         | 87/3145 [00:38<19:22,  2.63it/s]  3%|â–Ž         | 90/3143 [00:39<18:13,  2.79it/s]  3%|â–Ž         | 92/3145 [00:41<19:34,  2.60it/s]  3%|â–Ž         | 102/3145 [00:52<20:08,  2.52it/s]  3%|â–Ž         | 88/3145 [00:39<19:25,  2.62it/s]  3%|â–Ž         | 91/3143 [00:39<18:15,  2.79it/s]  3%|â–Ž         | 93/3145 [00:42<20:09,  2.52it/s]  3%|â–Ž         | 103/3145 [00:52<20:03,  2.53it/s]  3%|â–Ž         | 92/3143 [00:39<17:56,  2.84it/s]  3%|â–Ž         | 94/3145 [00:42<20:04,  2.53it/s]  3%|â–Ž         | 104/3145 [00:52<19:51,  2.55it/s]  3%|â–Ž         | 93/3143 [00:40<18:01,  2.82it/s]  3%|â–Ž         | 89/3145 [00:40<27:41,  1.84it/s]  3%|â–Ž         | 94/3143 [00:40<15:56,  3.19it/s]  3%|â–Ž         | 95/3145 [00:42<20:04,  2.53it/s]  3%|â–Ž         | 105/3145 [00:53<19:25,  2.61it/s]  3%|â–Ž         | 90/3145 [00:40<24:18,  2.10it/s]  3%|â–Ž         | 95/3143 [00:40<16:23,  3.10it/s]  3%|â–Ž         | 96/3145 [00:43<19:44,  2.57it/s]  3%|â–Ž         | 106/3145 [00:53<19:07,  2.65it/s]  3%|â–Ž         | 91/3145 [00:40<23:22,  2.18it/s]  3%|â–Ž         | 96/3143 [00:41<16:09,  3.14it/s]  3%|â–Ž         | 97/3145 [00:43<19:07,  2.66it/s]  3%|â–Ž         | 107/3145 [00:53<18:49,  2.69it/s]  3%|â–Ž         | 97/3143 [00:41<15:04,  3.37it/s]  3%|â–Ž         | 92/3145 [00:41<21:44,  2.34it/s]  3%|â–Ž         | 98/3145 [00:43<18:45,  2.71it/s]  3%|â–Ž         | 98/3143 [00:41<14:30,  3.50it/s]  3%|â–Ž         | 108/3145 [00:54<19:32,  2.59it/s]  3%|â–Ž         | 93/3145 [00:41<20:38,  2.46it/s]  3%|â–Ž         | 99/3143 [00:41<13:30,  3.75it/s]  3%|â–Ž         | 99/3145 [00:44<18:39,  2.72it/s]  3%|â–Ž         | 109/3145 [00:54<19:32,  2.59it/s]  3%|â–Ž         | 94/3145 [00:41<19:31,  2.60it/s]  3%|â–Ž         | 100/3143 [00:42<14:49,  3.42it/s]  3%|â–Ž         | 100/3145 [00:44<18:28,  2.75it/s]  3%|â–Ž         | 110/3145 [00:55<19:34,  2.58it/s]  3%|â–Ž         | 95/3145 [00:42<19:04,  2.66it/s]  3%|â–Ž         | 101/3143 [00:42<15:48,  3.21it/s]  3%|â–Ž         | 101/3145 [00:45<18:23,  2.76it/s]  3%|â–Ž         | 102/3143 [00:42<14:53,  3.40it/s]  4%|â–Ž         | 111/3145 [00:55<19:36,  2.58it/s]  3%|â–Ž         | 96/3145 [00:42<19:16,  2.64it/s]  3%|â–Ž         | 102/3145 [00:45<19:09,  2.65it/s]  3%|â–Ž         | 103/3143 [00:42<14:33,  3.48it/s]  4%|â–Ž         | 112/3145 [00:55<19:31,  2.59it/s]  3%|â–Ž         | 97/3145 [00:43<20:01,  2.54it/s]  3%|â–Ž         | 104/3143 [00:43<14:23,  3.52it/s]  3%|â–Ž         | 103/3145 [00:45<19:45,  2.57it/s]  4%|â–Ž         | 113/3145 [00:56<19:27,  2.60it/s]  3%|â–Ž         | 98/3145 [00:43<19:04,  2.66it/s]  3%|â–Ž         | 105/3143 [00:43<15:54,  3.18it/s]  3%|â–Ž         | 104/3145 [00:46<19:16,  2.63it/s]  3%|â–Ž         | 99/3145 [00:43<18:56,  2.68it/s]  4%|â–Ž         | 114/3145 [00:56<19:30,  2.59it/s]  3%|â–Ž         | 106/3143 [00:44<17:31,  2.89it/s]  3%|â–Ž         | 105/3145 [00:46<19:23,  2.61it/s]  3%|â–Ž         | 100/3145 [00:44<18:20,  2.77it/s]  3%|â–Ž         | 107/3143 [00:44<16:15,  3.11it/s]  4%|â–Ž         | 115/3145 [00:57<21:10,  2.39it/s]  3%|â–Ž         | 106/3145 [00:46<18:28,  2.74it/s]  3%|â–Ž         | 101/3145 [00:44<18:22,  2.76it/s]  3%|â–Ž         | 108/3143 [00:44<16:43,  3.02it/s]  4%|â–Ž         | 116/3145 [00:57<20:05,  2.51it/s]  3%|â–Ž         | 107/3145 [00:47<18:28,  2.74it/s]  3%|â–Ž         | 102/3145 [00:44<17:59,  2.82it/s]  3%|â–Ž         | 109/3143 [00:45<16:44,  3.02it/s]  4%|â–Ž         | 117/3145 [00:57<19:29,  2.59it/s]  3%|â–Ž         | 108/3145 [00:47<18:30,  2.73it/s]  3%|â–Ž         | 103/3145 [00:45<18:31,  2.74it/s]  4%|â–         | 118/3145 [00:58<19:30,  2.59it/s]  3%|â–Ž         | 110/3143 [00:45<19:09,  2.64it/s]  3%|â–Ž         | 109/3145 [00:48<18:31,  2.73it/s]  3%|â–Ž         | 104/3145 [00:45<18:50,  2.69it/s]  4%|â–Ž         | 111/3143 [00:45<17:33,  2.88it/s]  4%|â–         | 119/3145 [00:58<19:13,  2.62it/s]  3%|â–Ž         | 110/3145 [00:48<19:50,  2.55it/s]  4%|â–Ž         | 112/3143 [00:46<16:15,  3.11it/s]  3%|â–Ž         | 105/3145 [00:46<19:05,  2.65it/s]  4%|â–         | 120/3145 [00:58<18:37,  2.71it/s]  4%|â–Ž         | 111/3145 [00:48<19:43,  2.56it/s]  3%|â–Ž         | 106/3145 [00:46<18:23,  2.75it/s]  4%|â–         | 121/3145 [00:59<16:59,  2.97it/s]  4%|â–Ž         | 112/3145 [00:49<19:11,  2.63it/s]  3%|â–Ž         | 107/3145 [00:46<18:41,  2.71it/s]  4%|â–         | 122/3145 [00:59<18:09,  2.78it/s]  4%|â–Ž         | 113/3145 [00:49<18:51,  2.68it/s]  3%|â–Ž         | 108/3145 [00:47<18:09,  2.79it/s]  4%|â–         | 123/3145 [01:00<17:54,  2.81it/s]  4%|â–Ž         | 113/3143 [00:47<30:56,  1.63it/s]  4%|â–Ž         | 114/3145 [00:50<19:29,  2.59it/s]  3%|â–Ž         | 109/3145 [00:47<18:31,  2.73it/s]  4%|â–         | 124/3145 [01:00<17:33,  2.87it/s]  4%|â–Ž         | 114/3143 [00:47<27:57,  1.81it/s]  4%|â–Ž         | 115/3145 [00:50<19:34,  2.58it/s]  3%|â–Ž         | 110/3145 [00:47<18:54,  2.67it/s]  4%|â–         | 125/3145 [01:00<18:07,  2.78it/s]  4%|â–Ž         | 116/3145 [00:50<17:13,  2.93it/s]  4%|â–Ž         | 115/3143 [00:48<27:05,  1.86it/s]  4%|â–         | 126/3145 [01:01<17:32,  2.87it/s]  4%|â–Ž         | 111/3145 [00:48<20:15,  2.50it/s]  4%|â–Ž         | 117/3145 [00:51<17:29,  2.89it/s]  4%|â–Ž         | 116/3143 [00:48<24:20,  2.07it/s]  4%|â–         | 127/3145 [01:01<18:03,  2.78it/s]  4%|â–Ž         | 112/3145 [00:48<20:26,  2.47it/s]  4%|â–         | 118/3145 [00:51<18:11,  2.77it/s]  4%|â–Ž         | 117/3143 [00:48<22:04,  2.28it/s]  4%|â–         | 128/3145 [01:01<18:24,  2.73it/s]  4%|â–Ž         | 113/3145 [00:49<19:12,  2.63it/s]  4%|â–         | 119/3145 [00:51<19:06,  2.64it/s]  4%|â–         | 118/3143 [00:49<21:44,  2.32it/s]  4%|â–         | 129/3145 [01:02<17:43,  2.84it/s]  4%|â–Ž         | 114/3145 [00:49<18:36,  2.71it/s]  4%|â–         | 120/3145 [00:52<18:52,  2.67it/s]  4%|â–         | 130/3145 [01:02<17:48,  2.82it/s]  4%|â–         | 119/3143 [00:49<21:02,  2.40it/s]  4%|â–Ž         | 115/3145 [00:49<18:52,  2.68it/s]  4%|â–         | 120/3143 [00:50<19:49,  2.54it/s]  4%|â–         | 131/3145 [01:02<18:09,  2.77it/s]  4%|â–         | 121/3145 [00:52<20:29,  2.46it/s]  4%|â–Ž         | 116/3145 [00:50<18:37,  2.71it/s]  4%|â–         | 121/3143 [00:50<19:17,  2.61it/s]  4%|â–         | 132/3145 [01:03<17:52,  2.81it/s]  4%|â–Ž         | 117/3145 [00:50<19:45,  2.56it/s]  4%|â–         | 122/3145 [00:53<21:52,  2.30it/s]  4%|â–         | 122/3143 [00:50<18:17,  2.75it/s]  4%|â–         | 133/3145 [01:03<17:58,  2.79it/s]  4%|â–         | 123/3145 [00:53<21:09,  2.38it/s]  4%|â–         | 118/3145 [00:51<20:03,  2.52it/s]  4%|â–         | 123/3143 [00:51<18:42,  2.69it/s]  4%|â–         | 134/3145 [01:03<18:24,  2.72it/s]  4%|â–         | 124/3145 [00:53<20:08,  2.50it/s]  4%|â–         | 119/3145 [00:51<19:31,  2.58it/s]  4%|â–         | 124/3143 [00:51<19:20,  2.60it/s]  4%|â–         | 135/3145 [01:04<18:45,  2.67it/s]  4%|â–         | 120/3145 [00:51<19:30,  2.58it/s]  4%|â–         | 125/3145 [00:54<20:31,  2.45it/s]  4%|â–         | 136/3145 [01:04<18:20,  2.73it/s]  4%|â–         | 125/3143 [00:51<19:23,  2.59it/s]  4%|â–         | 121/3145 [00:52<19:29,  2.58it/s]  4%|â–         | 126/3145 [00:54<20:15,  2.48it/s]  4%|â–         | 137/3145 [01:05<18:03,  2.77it/s]  4%|â–         | 126/3143 [00:52<19:56,  2.52it/s]  4%|â–         | 122/3145 [00:52<18:59,  2.65it/s]  4%|â–         | 138/3145 [01:05<17:00,  2.95it/s]  4%|â–         | 127/3145 [00:55<20:04,  2.51it/s]  4%|â–         | 127/3143 [00:52<19:49,  2.53it/s]  4%|â–         | 128/3145 [00:55<17:55,  2.81it/s]  4%|â–         | 123/3145 [00:52<18:42,  2.69it/s]  4%|â–         | 139/3145 [01:05<18:11,  2.75it/s]  4%|â–         | 128/3143 [00:53<19:40,  2.55it/s]  4%|â–         | 129/3145 [00:55<17:43,  2.84it/s]  4%|â–         | 124/3145 [00:53<18:24,  2.74it/s]  4%|â–         | 140/3145 [01:06<17:54,  2.80it/s]  4%|â–         | 129/3143 [00:53<19:32,  2.57it/s]  4%|â–         | 130/3145 [00:56<18:16,  2.75it/s]  4%|â–         | 125/3145 [00:53<18:20,  2.74it/s]  4%|â–         | 141/3145 [01:06<18:01,  2.78it/s]  4%|â–         | 130/3143 [00:53<19:25,  2.58it/s]  4%|â–         | 126/3145 [00:53<17:52,  2.81it/s]  4%|â–         | 131/3145 [00:56<18:48,  2.67it/s]  5%|â–         | 142/3145 [01:06<18:24,  2.72it/s]  4%|â–         | 131/3143 [00:54<18:55,  2.65it/s]  4%|â–         | 127/3145 [00:54<17:52,  2.82it/s]  4%|â–         | 132/3145 [00:56<18:21,  2.74it/s]  5%|â–         | 143/3145 [01:07<18:40,  2.68it/s]  4%|â–         | 128/3145 [00:54<17:19,  2.90it/s]  4%|â–         | 133/3145 [00:57<18:07,  2.77it/s]  4%|â–         | 132/3143 [00:54<20:28,  2.45it/s]  5%|â–         | 144/3145 [01:07<18:48,  2.66it/s]  4%|â–         | 129/3145 [00:54<17:55,  2.80it/s]  4%|â–         | 134/3145 [00:57<17:39,  2.84it/s]  4%|â–         | 133/3143 [00:55<20:08,  2.49it/s]  5%|â–         | 145/3145 [01:08<18:58,  2.63it/s]  4%|â–         | 130/3145 [00:55<16:57,  2.96it/s]  4%|â–         | 135/3145 [00:57<17:23,  2.89it/s]  4%|â–         | 134/3143 [00:55<18:24,  2.72it/s]  5%|â–         | 146/3145 [01:08<18:32,  2.69it/s]  4%|â–         | 135/3143 [00:55<16:17,  3.08it/s]  4%|â–         | 131/3145 [00:55<17:13,  2.92it/s]  4%|â–         | 136/3145 [00:58<17:31,  2.86it/s]  5%|â–         | 147/3145 [01:08<18:06,  2.76it/s]  4%|â–         | 137/3145 [00:58<15:47,  3.17it/s]  4%|â–         | 132/3145 [00:55<17:04,  2.94it/s]  4%|â–         | 136/3143 [00:56<17:10,  2.92it/s]  4%|â–         | 137/3143 [00:56<15:28,  3.24it/s]  4%|â–         | 138/3145 [00:58<15:53,  3.15it/s]  5%|â–         | 148/3145 [01:09<18:30,  2.70it/s]  4%|â–         | 133/3145 [00:56<18:05,  2.78it/s]  4%|â–         | 139/3145 [00:59<16:09,  3.10it/s]  5%|â–         | 149/3145 [01:09<17:57,  2.78it/s]  4%|â–         | 138/3143 [00:56<17:05,  2.93it/s]  4%|â–         | 134/3145 [00:56<16:50,  2.98it/s]  4%|â–         | 140/3145 [00:59<16:08,  3.10it/s]  5%|â–         | 150/3145 [01:09<17:56,  2.78it/s]  4%|â–         | 135/3145 [00:57<17:15,  2.91it/s]  4%|â–         | 139/3143 [00:57<18:00,  2.78it/s]  4%|â–         | 141/3145 [00:59<16:37,  3.01it/s]  4%|â–         | 136/3145 [00:57<17:05,  2.93it/s]  5%|â–         | 151/3145 [01:10<18:30,  2.70it/s]  4%|â–         | 140/3143 [00:57<20:45,  2.41it/s]  5%|â–         | 142/3145 [01:00<17:55,  2.79it/s]  4%|â–         | 137/3145 [00:57<17:22,  2.89it/s]  5%|â–         | 152/3145 [01:10<18:41,  2.67it/s]  4%|â–         | 141/3143 [00:57<19:23,  2.58it/s]  5%|â–         | 143/3145 [01:00<18:01,  2.78it/s]  4%|â–         | 138/3145 [00:58<17:49,  2.81it/s]  5%|â–         | 153/3145 [01:10<18:55,  2.64it/s]  5%|â–         | 142/3143 [00:58<19:18,  2.59it/s]  5%|â–         | 144/3145 [01:00<17:26,  2.87it/s]  4%|â–         | 139/3145 [00:58<18:06,  2.77it/s]  5%|â–         | 154/3145 [01:11<18:32,  2.69it/s]  5%|â–         | 143/3143 [00:58<19:48,  2.52it/s]  5%|â–         | 145/3145 [01:01<17:38,  2.83it/s]  4%|â–         | 140/3145 [00:58<19:48,  2.53it/s]  5%|â–         | 155/3145 [01:11<19:27,  2.56it/s]  5%|â–         | 144/3143 [00:59<19:13,  2.60it/s]  5%|â–         | 146/3145 [01:01<18:06,  2.76it/s]  4%|â–         | 141/3145 [00:59<20:12,  2.48it/s]  5%|â–         | 145/3143 [00:59<18:49,  2.66it/s]  5%|â–         | 156/3145 [01:12<20:50,  2.39it/s]  5%|â–         | 147/3145 [01:01<18:05,  2.76it/s]  5%|â–         | 142/3145 [00:59<17:23,  2.88it/s]  5%|â–         | 157/3145 [01:12<20:03,  2.48it/s]  5%|â–         | 146/3143 [00:59<19:04,  2.62it/s]  5%|â–         | 148/3145 [01:02<18:06,  2.76it/s]  5%|â–         | 143/3145 [01:00<18:52,  2.65it/s]  5%|â–         | 147/3143 [01:00<18:22,  2.72it/s]  5%|â–Œ         | 158/3145 [01:12<19:47,  2.52it/s]  5%|â–         | 149/3145 [01:02<17:53,  2.79it/s]  5%|â–         | 144/3145 [01:00<18:52,  2.65it/s]  5%|â–         | 148/3143 [01:00<18:13,  2.74it/s]  5%|â–         | 150/3145 [01:03<17:44,  2.81it/s]  5%|â–Œ         | 159/3145 [01:13<19:10,  2.59it/s]  5%|â–         | 145/3145 [01:00<18:34,  2.69it/s]  5%|â–         | 149/3143 [01:00<17:44,  2.81it/s]  5%|â–         | 151/3145 [01:03<17:56,  2.78it/s]  5%|â–Œ         | 160/3145 [01:13<19:17,  2.58it/s]  5%|â–         | 146/3145 [01:01<18:19,  2.73it/s]  5%|â–         | 150/3143 [01:01<17:28,  2.85it/s]  5%|â–         | 152/3145 [01:03<17:55,  2.78it/s]  5%|â–Œ         | 161/3145 [01:14<20:38,  2.41it/s]  5%|â–         | 147/3145 [01:01<18:42,  2.67it/s]  5%|â–         | 151/3143 [01:01<17:39,  2.82it/s]  5%|â–         | 153/3145 [01:04<18:53,  2.64it/s]  5%|â–Œ         | 162/3145 [01:14<21:00,  2.37it/s]  5%|â–         | 148/3145 [01:01<18:52,  2.65it/s]  5%|â–         | 152/3143 [01:01<18:08,  2.75it/s]  5%|â–         | 154/3145 [01:04<19:27,  2.56it/s]  5%|â–Œ         | 163/3145 [01:15<20:37,  2.41it/s]  5%|â–         | 149/3145 [01:02<18:59,  2.63it/s]  5%|â–         | 153/3143 [01:02<19:56,  2.50it/s]  5%|â–         | 155/3145 [01:04<19:01,  2.62it/s]  5%|â–         | 150/3145 [01:02<18:08,  2.75it/s]  5%|â–Œ         | 164/3145 [01:15<20:23,  2.44it/s]  5%|â–         | 154/3143 [01:02<19:29,  2.56it/s]  5%|â–         | 156/3145 [01:05<19:05,  2.61it/s]  5%|â–         | 151/3145 [01:02<18:02,  2.77it/s]  5%|â–Œ         | 165/3145 [01:15<20:06,  2.47it/s]  5%|â–         | 155/3143 [01:03<18:58,  2.62it/s]  5%|â–         | 157/3145 [01:05<19:24,  2.57it/s]  5%|â–         | 152/3145 [01:03<18:20,  2.72it/s]  5%|â–         | 156/3143 [01:03<18:05,  2.75it/s]  5%|â–Œ         | 166/3145 [01:16<20:33,  2.42it/s]  5%|â–Œ         | 158/3145 [01:06<18:52,  2.64it/s]  5%|â–         | 153/3145 [01:03<17:16,  2.89it/s]  5%|â–Œ         | 167/3145 [01:16<19:52,  2.50it/s]  5%|â–         | 157/3143 [01:03<18:54,  2.63it/s]  5%|â–Œ         | 159/3145 [01:06<18:14,  2.73it/s]  5%|â–         | 154/3145 [01:03<17:23,  2.87it/s]  5%|â–Œ         | 168/3145 [01:17<20:02,  2.47it/s]  5%|â–Œ         | 158/3143 [01:04<19:28,  2.55it/s]  5%|â–Œ         | 160/3145 [01:06<18:44,  2.65it/s]  5%|â–         | 155/3145 [01:04<17:37,  2.83it/s]  5%|â–Œ         | 159/3143 [01:04<19:05,  2.60it/s]  5%|â–Œ         | 169/3145 [01:17<20:16,  2.45it/s]  5%|â–         | 156/3145 [01:04<17:45,  2.81it/s]  5%|â–Œ         | 161/3145 [01:07<19:37,  2.54it/s]  5%|â–Œ         | 160/3143 [01:04<16:54,  2.94it/s]  5%|â–Œ         | 170/3145 [01:17<19:19,  2.57it/s]  5%|â–         | 157/3145 [01:05<17:28,  2.85it/s]  5%|â–Œ         | 162/3145 [01:07<19:03,  2.61it/s]  5%|â–Œ         | 161/3143 [01:05<17:34,  2.83it/s]  5%|â–Œ         | 158/3145 [01:05<16:09,  3.08it/s]  5%|â–Œ         | 171/3145 [01:18<18:43,  2.65it/s]  5%|â–Œ         | 163/3145 [01:08<20:10,  2.46it/s]  5%|â–Œ         | 159/3145 [01:05<16:29,  3.02it/s]  5%|â–Œ         | 162/3143 [01:05<18:39,  2.66it/s]  5%|â–Œ         | 172/3145 [01:18<18:32,  2.67it/s]  5%|â–Œ         | 164/3145 [01:08<19:16,  2.58it/s]  5%|â–Œ         | 160/3145 [01:06<16:51,  2.95it/s]  5%|â–Œ         | 163/3143 [01:06<18:19,  2.71it/s]  6%|â–Œ         | 173/3145 [01:18<18:21,  2.70it/s]  5%|â–Œ         | 165/3145 [01:08<18:50,  2.64it/s]  6%|â–Œ         | 174/3145 [01:19<17:50,  2.77it/s]  5%|â–Œ         | 161/3145 [01:06<18:41,  2.66it/s]  5%|â–Œ         | 164/3143 [01:06<20:13,  2.45it/s]  5%|â–Œ         | 166/3145 [01:09<18:42,  2.65it/s]  6%|â–Œ         | 175/3145 [01:19<18:41,  2.65it/s]  5%|â–Œ         | 162/3145 [01:06<19:15,  2.58it/s]  5%|â–Œ         | 167/3145 [01:09<18:36,  2.67it/s]  5%|â–Œ         | 165/3143 [01:07<21:26,  2.31it/s]  5%|â–Œ         | 163/3145 [01:07<18:20,  2.71it/s]  6%|â–Œ         | 176/3145 [01:20<18:44,  2.64it/s]  5%|â–Œ         | 168/3145 [01:09<16:22,  3.03it/s]  5%|â–Œ         | 166/3143 [01:07<20:15,  2.45it/s]  5%|â–Œ         | 164/3145 [01:07<17:51,  2.78it/s]  5%|â–Œ         | 169/3145 [01:10<16:19,  3.04it/s]  6%|â–Œ         | 177/3145 [01:20<18:44,  2.64it/s]  5%|â–Œ         | 167/3143 [01:07<19:09,  2.59it/s]  5%|â–Œ         | 165/3145 [01:07<17:47,  2.79it/s]  5%|â–Œ         | 170/3145 [01:10<16:40,  2.97it/s]  6%|â–Œ         | 178/3145 [01:20<18:27,  2.68it/s]  5%|â–Œ         | 168/3143 [01:08<18:49,  2.63it/s]  5%|â–Œ         | 166/3145 [01:08<17:52,  2.78it/s]  5%|â–Œ         | 171/3145 [01:10<17:34,  2.82it/s]  6%|â–Œ         | 179/3145 [01:21<18:48,  2.63it/s]  5%|â–Œ         | 169/3143 [01:08<18:31,  2.68it/s]  5%|â–Œ         | 167/3145 [01:08<17:47,  2.79it/s]  5%|â–Œ         | 172/3145 [01:11<18:15,  2.71it/s]  6%|â–Œ         | 180/3145 [01:21<19:29,  2.54it/s]  5%|â–Œ         | 170/3143 [01:08<17:47,  2.78it/s]  5%|â–Œ         | 168/3145 [01:08<17:48,  2.79it/s]  6%|â–Œ         | 173/3145 [01:11<18:33,  2.67it/s]  6%|â–Œ         | 181/3145 [01:21<19:03,  2.59it/s]  5%|â–Œ         | 171/3143 [01:09<19:21,  2.56it/s]  5%|â–Œ         | 169/3145 [01:09<18:19,  2.71it/s]  6%|â–Œ         | 182/3145 [01:22<18:30,  2.67it/s]  6%|â–Œ         | 174/3145 [01:12<18:44,  2.64it/s]  5%|â–Œ         | 172/3143 [01:09<18:31,  2.67it/s]  5%|â–Œ         | 170/3145 [01:09<17:41,  2.80it/s]  6%|â–Œ         | 183/3145 [01:22<18:49,  2.62it/s]  6%|â–Œ         | 175/3145 [01:12<18:55,  2.62it/s]  6%|â–Œ         | 173/3143 [01:09<18:12,  2.72it/s]  5%|â–Œ         | 171/3145 [01:10<18:06,  2.74it/s]  6%|â–Œ         | 176/3145 [01:12<17:55,  2.76it/s]  6%|â–Œ         | 184/3145 [01:23<19:24,  2.54it/s]  6%|â–Œ         | 174/3143 [01:10<18:31,  2.67it/s]  5%|â–Œ         | 172/3145 [01:10<17:41,  2.80it/s]  6%|â–Œ         | 177/3145 [01:13<17:50,  2.77it/s]  6%|â–Œ         | 185/3145 [01:23<18:52,  2.61it/s]  6%|â–Œ         | 175/3143 [01:10<18:20,  2.70it/s]  6%|â–Œ         | 173/3145 [01:10<18:05,  2.74it/s]  6%|â–Œ         | 178/3145 [01:13<18:10,  2.72it/s]  6%|â–Œ         | 176/3143 [01:11<17:44,  2.79it/s]  6%|â–Œ         | 186/3145 [01:23<18:34,  2.65it/s]  6%|â–Œ         | 174/3145 [01:11<18:22,  2.69it/s]  6%|â–Œ         | 179/3145 [01:13<18:22,  2.69it/s]  6%|â–Œ         | 177/3143 [01:11<17:45,  2.78it/s]  6%|â–Œ         | 187/3145 [01:24<18:50,  2.62it/s]  6%|â–Œ         | 175/3145 [01:11<18:35,  2.66it/s]  6%|â–Œ         | 180/3145 [01:14<18:20,  2.69it/s]  6%|â–Œ         | 178/3143 [01:11<17:42,  2.79it/s]  6%|â–Œ         | 188/3145 [01:24<18:17,  2.69it/s]  6%|â–Œ         | 176/3145 [01:11<18:24,  2.69it/s]  6%|â–Œ         | 179/3143 [01:12<17:42,  2.79it/s]  6%|â–Œ         | 181/3145 [01:14<18:33,  2.66it/s]  6%|â–Œ         | 189/3145 [01:24<17:44,  2.78it/s]  6%|â–Œ         | 177/3145 [01:12<18:12,  2.72it/s]  6%|â–Œ         | 180/3143 [01:12<17:24,  2.84it/s]  6%|â–Œ         | 182/3145 [01:15<18:38,  2.65it/s]  6%|â–Œ         | 190/3145 [01:25<17:54,  2.75it/s]  6%|â–Œ         | 178/3145 [01:12<18:01,  2.74it/s]  6%|â–Œ         | 181/3143 [01:12<17:57,  2.75it/s]  6%|â–Œ         | 183/3145 [01:15<18:59,  2.60it/s]  6%|â–Œ         | 191/3145 [01:25<18:56,  2.60it/s]  6%|â–Œ         | 179/3145 [01:13<17:55,  2.76it/s]  6%|â–Œ         | 182/3143 [01:13<18:09,  2.72it/s]  6%|â–Œ         | 184/3145 [01:15<18:38,  2.65it/s]  6%|â–Œ         | 192/3145 [01:26<19:23,  2.54it/s]  6%|â–Œ         | 180/3145 [01:13<18:44,  2.64it/s]  6%|â–Œ         | 185/3145 [01:16<17:53,  2.76it/s]  6%|â–Œ         | 183/3143 [01:13<19:08,  2.58it/s]  6%|â–Œ         | 193/3145 [01:26<18:50,  2.61it/s]  6%|â–Œ         | 181/3145 [01:13<18:48,  2.63it/s]  6%|â–Œ         | 186/3145 [01:16<17:28,  2.82it/s]  6%|â–Œ         | 184/3143 [01:14<19:08,  2.58it/s]  6%|â–Œ         | 194/3145 [01:26<18:33,  2.65it/s]  6%|â–Œ         | 182/3145 [01:14<18:02,  2.74it/s]  6%|â–Œ         | 187/3145 [01:16<17:33,  2.81it/s]  6%|â–Œ         | 185/3143 [01:14<18:43,  2.63it/s]  6%|â–Œ         | 195/3145 [01:27<18:14,  2.69it/s]  6%|â–Œ         | 183/3145 [01:14<19:09,  2.58it/s]  6%|â–Œ         | 188/3145 [01:17<17:30,  2.81it/s]  6%|â–Œ         | 196/3145 [01:27<17:37,  2.79it/s]  6%|â–Œ         | 186/3143 [01:14<18:43,  2.63it/s]  6%|â–Œ         | 184/3145 [01:14<18:33,  2.66it/s]  6%|â–Œ         | 189/3145 [01:17<17:38,  2.79it/s]  6%|â–Œ         | 187/3143 [01:15<17:44,  2.78it/s]  6%|â–‹         | 197/3145 [01:27<17:43,  2.77it/s]  6%|â–Œ         | 185/3145 [01:15<18:10,  2.72it/s]  6%|â–Œ         | 190/3145 [01:17<17:18,  2.85it/s]  6%|â–Œ         | 188/3143 [01:15<17:20,  2.84it/s]  6%|â–‹         | 198/3145 [01:28<17:19,  2.84it/s]  6%|â–Œ         | 186/3145 [01:15<17:47,  2.77it/s]  6%|â–Œ         | 191/3145 [01:18<17:23,  2.83it/s]  6%|â–‹         | 199/3145 [01:28<17:32,  2.80it/s]  6%|â–Œ         | 189/3143 [01:15<18:25,  2.67it/s]  6%|â–Œ         | 187/3145 [01:16<17:59,  2.74it/s]  6%|â–Œ         | 192/3145 [01:18<17:26,  2.82it/s]  6%|â–‹         | 200/3145 [01:29<18:00,  2.72it/s]  6%|â–Œ         | 190/3143 [01:16<18:43,  2.63it/s]  6%|â–Œ         | 188/3145 [01:16<18:40,  2.64it/s]  6%|â–Œ         | 193/3145 [01:19<19:22,  2.54it/s]  6%|â–‹         | 201/3145 [01:29<17:25,  2.81it/s]  6%|â–Œ         | 191/3143 [01:16<19:20,  2.54it/s]  6%|â–Œ         | 189/3145 [01:16<19:13,  2.56it/s]  6%|â–‹         | 202/3145 [01:29<17:34,  2.79it/s]  6%|â–Œ         | 194/3145 [01:19<19:28,  2.53it/s]  6%|â–Œ         | 192/3143 [01:17<18:47,  2.62it/s]  6%|â–Œ         | 190/3145 [01:17<18:26,  2.67it/s]  6%|â–Œ         | 195/3145 [01:19<18:41,  2.63it/s]  6%|â–‹         | 203/3145 [01:30<18:12,  2.69it/s]  6%|â–Œ         | 193/3143 [01:17<18:19,  2.68it/s]  6%|â–Œ         | 194/3143 [01:17<16:12,  3.03it/s]  6%|â–‹         | 204/3145 [01:30<17:37,  2.78it/s]  6%|â–Œ         | 196/3145 [01:20<18:24,  2.67it/s]  6%|â–Œ         | 191/3145 [01:17<20:16,  2.43it/s]  6%|â–Œ         | 195/3143 [01:17<16:16,  3.02it/s]  7%|â–‹         | 205/3145 [01:30<18:02,  2.72it/s]  6%|â–‹         | 197/3145 [01:20<18:56,  2.60it/s]  6%|â–Œ         | 192/3145 [01:18<19:52,  2.48it/s]  6%|â–Œ         | 196/3143 [01:18<16:37,  2.95it/s]  6%|â–‹         | 198/3145 [01:20<18:37,  2.64it/s]  6%|â–Œ         | 193/3145 [01:18<19:12,  2.56it/s]  7%|â–‹         | 206/3145 [01:31<18:47,  2.61it/s]  6%|â–‹         | 197/3143 [01:18<15:03,  3.26it/s]  6%|â–‹         | 198/3143 [01:18<14:00,  3.50it/s]  6%|â–‹         | 199/3145 [01:21<18:17,  2.68it/s]  7%|â–‹         | 207/3145 [01:31<19:00,  2.58it/s]  6%|â–Œ         | 194/3145 [01:18<20:09,  2.44it/s]  6%|â–‹         | 200/3145 [01:21<18:05,  2.71it/s]  6%|â–‹         | 199/3143 [01:19<16:16,  3.02it/s]  7%|â–‹         | 208/3145 [01:32<18:58,  2.58it/s]  6%|â–Œ         | 195/3145 [01:19<19:55,  2.47it/s]  6%|â–‹         | 201/3145 [01:22<18:12,  2.70it/s]  7%|â–‹         | 209/3145 [01:32<18:28,  2.65it/s]  6%|â–‹         | 200/3143 [01:19<17:08,  2.86it/s]  6%|â–Œ         | 196/3145 [01:19<19:36,  2.51it/s]  6%|â–‹         | 202/3145 [01:22<17:57,  2.73it/s]  7%|â–‹         | 210/3145 [01:32<17:39,  2.77it/s]  6%|â–‹         | 201/3143 [01:20<17:46,  2.76it/s]  6%|â–‹         | 197/3145 [01:20<19:12,  2.56it/s]  6%|â–‹         | 203/3145 [01:22<19:14,  2.55it/s]  7%|â–‹         | 211/3145 [01:33<18:27,  2.65it/s]  6%|â–‹         | 202/3143 [01:20<17:53,  2.74it/s]  6%|â–‹         | 198/3145 [01:20<19:11,  2.56it/s]  7%|â–‹         | 212/3145 [01:33<18:20,  2.67it/s]  6%|â–‹         | 203/3143 [01:20<18:06,  2.70it/s]  6%|â–‹         | 204/3145 [01:23<19:45,  2.48it/s]  6%|â–‹         | 199/3145 [01:20<18:34,  2.64it/s]  7%|â–‹         | 213/3145 [01:33<17:33,  2.78it/s]  7%|â–‹         | 205/3145 [01:23<19:19,  2.54it/s]  6%|â–‹         | 204/3143 [01:21<18:26,  2.66it/s]  6%|â–‹         | 200/3145 [01:21<19:15,  2.55it/s]  7%|â–‹         | 214/3145 [01:34<18:00,  2.71it/s]  7%|â–‹         | 206/3145 [01:23<18:22,  2.66it/s]  7%|â–‹         | 205/3143 [01:21<18:13,  2.69it/s]  6%|â–‹         | 201/3145 [01:21<19:09,  2.56it/s]  7%|â–‹         | 215/3145 [01:34<17:22,  2.81it/s]  7%|â–‹         | 207/3145 [01:24<17:47,  2.75it/s]  7%|â–‹         | 206/3143 [01:21<17:58,  2.72it/s]  6%|â–‹         | 202/3145 [01:21<18:18,  2.68it/s]  7%|â–‹         | 216/3145 [01:34<17:46,  2.75it/s]  7%|â–‹         | 208/3145 [01:24<18:07,  2.70it/s]  7%|â–‹         | 207/3143 [01:22<18:19,  2.67it/s]  6%|â–‹         | 203/3145 [01:22<18:32,  2.64it/s]  7%|â–‹         | 217/3145 [01:35<17:43,  2.75it/s]  7%|â–‹         | 209/3145 [01:25<18:32,  2.64it/s]  7%|â–‹         | 208/3143 [01:22<18:31,  2.64it/s]  6%|â–‹         | 204/3145 [01:22<18:10,  2.70it/s]  7%|â–‹         | 218/3145 [01:35<18:28,  2.64it/s]  7%|â–‹         | 210/3145 [01:25<18:13,  2.69it/s]  7%|â–‹         | 209/3143 [01:23<18:38,  2.62it/s]  7%|â–‹         | 205/3145 [01:23<18:34,  2.64it/s]  7%|â–‹         | 219/3145 [01:36<17:46,  2.74it/s]  7%|â–‹         | 211/3145 [01:25<17:59,  2.72it/s]  7%|â–‹         | 210/3143 [01:23<18:37,  2.62it/s]  7%|â–‹         | 206/3145 [01:23<18:13,  2.69it/s]  7%|â–‹         | 212/3145 [01:26<18:38,  2.62it/s]  7%|â–‹         | 220/3145 [01:36<20:23,  2.39it/s]  7%|â–‹         | 211/3143 [01:23<19:06,  2.56it/s]  7%|â–‹         | 207/3145 [01:23<18:36,  2.63it/s]  7%|â–‹         | 213/3145 [01:26<18:29,  2.64it/s]  7%|â–‹         | 221/3145 [01:36<19:07,  2.55it/s]  7%|â–‹         | 208/3145 [01:24<18:17,  2.68it/s]  7%|â–‹         | 212/3143 [01:24<19:39,  2.48it/s]  7%|â–‹         | 214/3145 [01:26<18:09,  2.69it/s]  7%|â–‹         | 222/3145 [01:37<18:35,  2.62it/s]  7%|â–‹         | 209/3145 [01:24<17:47,  2.75it/s]  7%|â–‹         | 213/3143 [01:24<18:58,  2.57it/s]  7%|â–‹         | 215/3145 [01:27<18:23,  2.66it/s]  7%|â–‹         | 223/3145 [01:37<18:40,  2.61it/s]  7%|â–‹         | 210/3145 [01:24<18:09,  2.69it/s]  7%|â–‹         | 214/3143 [01:24<18:34,  2.63it/s]  7%|â–‹         | 216/3145 [01:27<18:25,  2.65it/s]  7%|â–‹         | 215/3143 [01:25<17:44,  2.75it/s]  7%|â–‹         | 224/3145 [01:38<19:37,  2.48it/s]  7%|â–‹         | 211/3145 [01:25<18:30,  2.64it/s]  7%|â–‹         | 217/3145 [01:27<16:52,  2.89it/s]  7%|â–‹         | 216/3143 [01:25<17:26,  2.80it/s]  7%|â–‹         | 225/3145 [01:38<18:51,  2.58it/s]  7%|â–‹         | 212/3145 [01:25<18:40,  2.62it/s]  7%|â–‹         | 218/3145 [01:28<16:54,  2.89it/s]  7%|â–‹         | 226/3145 [01:38<18:32,  2.62it/s]  7%|â–‹         | 217/3143 [01:26<17:44,  2.75it/s]  7%|â–‹         | 213/3145 [01:26<18:34,  2.63it/s]  7%|â–‹         | 219/3145 [01:28<17:04,  2.85it/s]  7%|â–‹         | 218/3143 [01:26<17:34,  2.77it/s]  7%|â–‹         | 227/3145 [01:39<18:20,  2.65it/s]  7%|â–‹         | 214/3145 [01:26<18:23,  2.66it/s]  7%|â–‹         | 220/3145 [01:29<16:58,  2.87it/s]  7%|â–‹         | 219/3143 [01:26<17:29,  2.79it/s]  7%|â–‹         | 228/3145 [01:39<18:58,  2.56it/s]  7%|â–‹         | 215/3145 [01:26<18:38,  2.62it/s]  7%|â–‹         | 221/3145 [01:29<17:22,  2.80it/s]  7%|â–‹         | 220/3143 [01:27<17:26,  2.79it/s]  7%|â–‹         | 222/3145 [01:29<17:19,  2.81it/s]  7%|â–‹         | 229/3145 [01:40<19:57,  2.43it/s]  7%|â–‹         | 216/3145 [01:27<20:10,  2.42it/s]  7%|â–‹         | 221/3143 [01:27<17:51,  2.73it/s]  7%|â–‹         | 223/3145 [01:30<17:19,  2.81it/s]  7%|â–‹         | 217/3145 [01:27<19:18,  2.53it/s]  7%|â–‹         | 230/3145 [01:40<21:14,  2.29it/s]  7%|â–‹         | 222/3143 [01:27<18:39,  2.61it/s]  7%|â–‹         | 218/3145 [01:27<16:49,  2.90it/s]  7%|â–‹         | 224/3145 [01:30<17:32,  2.78it/s]  7%|â–‹         | 231/3145 [01:40<20:34,  2.36it/s]  7%|â–‹         | 223/3143 [01:28<17:35,  2.77it/s]  7%|â–‹         | 219/3145 [01:28<17:23,  2.80it/s]  7%|â–‹         | 225/3145 [01:30<18:06,  2.69it/s]  7%|â–‹         | 232/3145 [01:41<19:37,  2.47it/s]  7%|â–‹         | 224/3143 [01:28<17:55,  2.71it/s]  7%|â–‹         | 226/3145 [01:31<15:53,  3.06it/s]  7%|â–‹         | 220/3145 [01:28<17:36,  2.77it/s]  7%|â–‹         | 233/3145 [01:41<18:53,  2.57it/s]  7%|â–‹         | 225/3143 [01:28<17:55,  2.71it/s]  7%|â–‹         | 227/3145 [01:31<16:23,  2.97it/s]  7%|â–‹         | 221/3145 [01:29<17:31,  2.78it/s]  7%|â–‹         | 234/3145 [01:42<18:08,  2.67it/s]  7%|â–‹         | 228/3145 [01:31<16:39,  2.92it/s]  7%|â–‹         | 226/3143 [01:29<19:37,  2.48it/s]  7%|â–‹         | 222/3145 [01:29<19:29,  2.50it/s]  7%|â–‹         | 235/3145 [01:42<17:53,  2.71it/s]  7%|â–‹         | 229/3145 [01:32<16:43,  2.91it/s]  7%|â–‹         | 227/3143 [01:29<19:22,  2.51it/s]  7%|â–‹         | 223/3145 [01:29<19:01,  2.56it/s]  8%|â–Š         | 236/3145 [01:42<18:04,  2.68it/s]  7%|â–‹         | 230/3145 [01:32<17:05,  2.84it/s]  7%|â–‹         | 228/3143 [01:30<18:49,  2.58it/s]  7%|â–‹         | 224/3145 [01:30<19:04,  2.55it/s]  8%|â–Š         | 237/3145 [01:43<17:51,  2.71it/s]  7%|â–‹         | 231/3145 [01:32<17:02,  2.85it/s]  7%|â–‹         | 229/3143 [01:30<16:37,  2.92it/s]  7%|â–‹         | 225/3145 [01:30<18:33,  2.62it/s]  8%|â–Š         | 238/3145 [01:43<18:09,  2.67it/s]  7%|â–‹         | 230/3143 [01:30<16:58,  2.86it/s]  7%|â–‹         | 232/3145 [01:33<18:02,  2.69it/s]  7%|â–‹         | 226/3145 [01:31<18:40,  2.60it/s]  8%|â–Š         | 239/3145 [01:43<18:16,  2.65it/s]  7%|â–‹         | 231/3143 [01:31<17:05,  2.84it/s]  7%|â–‹         | 233/3145 [01:33<18:31,  2.62it/s]  7%|â–‹         | 227/3145 [01:31<18:42,  2.60it/s]  8%|â–Š         | 240/3145 [01:44<18:19,  2.64it/s]  7%|â–‹         | 232/3143 [01:31<17:12,  2.82it/s]  7%|â–‹         | 234/3145 [01:34<17:50,  2.72it/s]  7%|â–‹         | 228/3145 [01:31<18:38,  2.61it/s]  8%|â–Š         | 241/3145 [01:44<18:27,  2.62it/s]  7%|â–‹         | 235/3145 [01:34<17:22,  2.79it/s]  7%|â–‹         | 233/3143 [01:31<18:29,  2.62it/s]  7%|â–‹         | 229/3145 [01:32<19:08,  2.54it/s]  8%|â–Š         | 236/3145 [01:34<17:06,  2.83it/s]  8%|â–Š         | 242/3145 [01:45<18:26,  2.62it/s]  7%|â–‹         | 234/3143 [01:32<18:04,  2.68it/s]  7%|â–‹         | 230/3145 [01:32<18:48,  2.58it/s]  8%|â–Š         | 237/3145 [01:35<17:17,  2.80it/s]  7%|â–‹         | 235/3143 [01:32<17:31,  2.76it/s]  8%|â–Š         | 243/3145 [01:45<21:14,  2.28it/s]  8%|â–Š         | 238/3145 [01:35<17:24,  2.78it/s]  7%|â–‹         | 231/3145 [01:32<18:45,  2.59it/s]  8%|â–Š         | 236/3143 [01:33<17:55,  2.70it/s]  8%|â–Š         | 244/3145 [01:45<20:03,  2.41it/s]  8%|â–Š         | 237/3143 [01:33<15:43,  3.08it/s]  7%|â–‹         | 232/3145 [01:33<18:28,  2.63it/s]  8%|â–Š         | 239/3145 [01:35<18:01,  2.69it/s]  8%|â–Š         | 245/3145 [01:46<20:31,  2.35it/s]  8%|â–Š         | 240/3145 [01:36<17:50,  2.71it/s]  8%|â–Š         | 238/3143 [01:33<17:56,  2.70it/s]  7%|â–‹         | 233/3145 [01:33<19:12,  2.53it/s]  8%|â–Š         | 239/3143 [01:34<17:24,  2.78it/s]  8%|â–Š         | 246/3145 [01:46<21:12,  2.28it/s]  8%|â–Š         | 241/3145 [01:36<18:09,  2.66it/s]  7%|â–‹         | 234/3145 [01:34<18:40,  2.60it/s]  8%|â–Š         | 240/3143 [01:34<17:43,  2.73it/s]  8%|â–Š         | 247/3145 [01:47<21:05,  2.29it/s]  8%|â–Š         | 242/3145 [01:37<19:21,  2.50it/s]  7%|â–‹         | 235/3145 [01:34<20:11,  2.40it/s]  8%|â–Š         | 241/3143 [01:34<18:10,  2.66it/s]  8%|â–Š         | 248/3145 [01:47<20:46,  2.32it/s]  8%|â–Š         | 243/3145 [01:37<19:49,  2.44it/s]  8%|â–Š         | 236/3145 [01:35<21:12,  2.29it/s]  8%|â–Š         | 242/3143 [01:35<18:54,  2.56it/s]  8%|â–Š         | 249/3145 [01:48<19:41,  2.45it/s]  8%|â–Š         | 244/3145 [01:37<19:11,  2.52it/s]  8%|â–Š         | 237/3145 [01:35<21:50,  2.22it/s]  8%|â–Š         | 250/3145 [01:48<19:56,  2.42it/s]  8%|â–Š         | 243/3143 [01:35<19:48,  2.44it/s]  8%|â–Š         | 245/3145 [01:38<19:36,  2.47it/s]  8%|â–Š         | 238/3145 [01:36<22:14,  2.18it/s]  8%|â–Š         | 251/3145 [01:48<19:36,  2.46it/s]  8%|â–Š         | 244/3143 [01:36<19:37,  2.46it/s]  8%|â–Š         | 246/3145 [01:38<19:23,  2.49it/s]  8%|â–Š         | 252/3145 [01:49<18:55,  2.55it/s]  8%|â–Š         | 245/3143 [01:36<19:23,  2.49it/s]  8%|â–Š         | 239/3145 [01:36<22:48,  2.12it/s]  8%|â–Š         | 247/3145 [01:39<19:08,  2.52it/s]  8%|â–Š         | 240/3145 [01:36<19:14,  2.52it/s]  8%|â–Š         | 253/3145 [01:49<18:52,  2.55it/s]  8%|â–Š         | 248/3145 [01:39<18:39,  2.59it/s]  8%|â–Š         | 246/3143 [01:36<20:14,  2.39it/s]  8%|â–Š         | 241/3145 [01:37<18:29,  2.62it/s]  8%|â–Š         | 254/3145 [01:49<17:57,  2.68it/s]  8%|â–Š         | 249/3145 [01:39<18:18,  2.64it/s]  8%|â–Š         | 247/3143 [01:37<18:51,  2.56it/s]  8%|â–Š         | 255/3145 [01:50<17:40,  2.72it/s]  8%|â–Š         | 242/3145 [01:37<18:56,  2.55it/s]  8%|â–Š         | 250/3145 [01:40<18:09,  2.66it/s]  8%|â–Š         | 248/3143 [01:37<18:55,  2.55it/s]  8%|â–Š         | 256/3145 [01:50<17:26,  2.76it/s]  8%|â–Š         | 243/3145 [01:37<18:35,  2.60it/s]  8%|â–Š         | 251/3145 [01:40<18:17,  2.64it/s]  8%|â–Š         | 249/3143 [01:38<18:26,  2.62it/s]  8%|â–Š         | 244/3145 [01:38<18:14,  2.65it/s]  8%|â–Š         | 257/3145 [01:51<18:18,  2.63it/s]  8%|â–Š         | 252/3145 [01:40<18:07,  2.66it/s]  8%|â–Š         | 250/3143 [01:38<18:41,  2.58it/s]  8%|â–Š         | 258/3145 [01:51<18:18,  2.63it/s]  8%|â–Š         | 245/3145 [01:38<18:55,  2.55it/s]  8%|â–Š         | 251/3143 [01:38<18:46,  2.57it/s]  8%|â–Š         | 253/3145 [01:41<19:31,  2.47it/s]  8%|â–Š         | 259/3145 [01:51<18:04,  2.66it/s]  8%|â–Š         | 246/3145 [01:39<18:33,  2.60it/s]  8%|â–Š         | 252/3143 [01:39<18:15,  2.64it/s]  8%|â–Š         | 254/3145 [01:41<18:56,  2.54it/s]  8%|â–Š         | 247/3145 [01:39<16:53,  2.86it/s]  8%|â–Š         | 260/3145 [01:52<18:44,  2.57it/s]  8%|â–Š         | 255/3145 [01:41<16:33,  2.91it/s]  8%|â–Š         | 253/3143 [01:39<18:27,  2.61it/s]  8%|â–Š         | 248/3145 [01:39<17:58,  2.69it/s]  8%|â–Š         | 256/3145 [01:42<16:57,  2.84it/s]  8%|â–Š         | 261/3145 [01:52<18:52,  2.55it/s]  8%|â–Š         | 254/3143 [01:39<18:10,  2.65it/s]  8%|â–Š         | 257/3145 [01:42<15:10,  3.17it/s]  8%|â–Š         | 249/3145 [01:40<17:53,  2.70it/s]  8%|â–Š         | 262/3145 [01:52<17:58,  2.67it/s]  8%|â–Š         | 255/3143 [01:40<18:18,  2.63it/s]  8%|â–Š         | 250/3145 [01:40<17:11,  2.81it/s]  8%|â–Š         | 263/3145 [01:53<16:20,  2.94it/s]  8%|â–Š         | 258/3145 [01:42<16:15,  2.96it/s]  8%|â–Š         | 256/3143 [01:40<18:23,  2.62it/s]  8%|â–Š         | 264/3145 [01:53<16:23,  2.93it/s]  8%|â–Š         | 251/3145 [01:40<17:26,  2.77it/s]  8%|â–Š         | 259/3145 [01:43<17:50,  2.69it/s]  8%|â–Š         | 257/3143 [01:41<18:26,  2.61it/s]  8%|â–Š         | 265/3145 [01:53<17:04,  2.81it/s]  8%|â–Š         | 252/3145 [01:41<17:54,  2.69it/s]  8%|â–Š         | 260/3145 [01:43<18:01,  2.67it/s]  8%|â–Š         | 258/3143 [01:41<18:26,  2.61it/s]  8%|â–Š         | 266/3145 [01:54<17:08,  2.80it/s]  8%|â–Š         | 253/3145 [01:41<17:56,  2.69it/s]  8%|â–Š         | 261/3145 [01:44<17:55,  2.68it/s]  8%|â–Š         | 259/3143 [01:41<18:19,  2.62it/s]  8%|â–Š         | 254/3145 [01:41<17:12,  2.80it/s]  8%|â–Š         | 267/3145 [01:54<17:12,  2.79it/s]  8%|â–Š         | 262/3145 [01:44<18:04,  2.66it/s]  9%|â–Š         | 268/3145 [01:54<15:43,  3.05it/s]  8%|â–Š         | 260/3143 [01:42<17:49,  2.70it/s]  8%|â–Š         | 255/3145 [01:42<18:06,  2.66it/s]  8%|â–Š         | 263/3145 [01:44<18:43,  2.57it/s]  8%|â–Š         | 261/3143 [01:42<17:35,  2.73it/s]  9%|â–Š         | 269/3145 [01:55<16:58,  2.82it/s]  8%|â–Š         | 256/3145 [01:42<17:53,  2.69it/s]  8%|â–Š         | 264/3145 [01:45<18:45,  2.56it/s]  8%|â–Š         | 262/3143 [01:42<17:31,  2.74it/s]  9%|â–Š         | 270/3145 [01:55<17:24,  2.75it/s]  8%|â–Š         | 257/3145 [01:42<16:47,  2.87it/s]  8%|â–Š         | 265/3145 [01:45<19:07,  2.51it/s]  8%|â–Š         | 263/3143 [01:43<17:25,  2.75it/s]  9%|â–Š         | 271/3145 [01:56<17:05,  2.80it/s]  8%|â–Š         | 258/3145 [01:43<17:31,  2.75it/s]  8%|â–Š         | 259/3145 [01:43<15:17,  3.14it/s]  8%|â–Š         | 266/3145 [01:46<18:55,  2.54it/s]  8%|â–Š         | 264/3143 [01:43<17:49,  2.69it/s]  9%|â–Š         | 272/3145 [01:56<17:41,  2.71it/s]  8%|â–Š         | 260/3145 [01:44<17:02,  2.82it/s]  8%|â–Š         | 267/3145 [01:46<18:28,  2.60it/s]  9%|â–Š         | 273/3145 [01:56<17:37,  2.72it/s]  8%|â–Š         | 265/3143 [01:44<18:13,  2.63it/s]  9%|â–Š         | 268/3145 [01:46<18:06,  2.65it/s]  9%|â–Š         | 274/3145 [01:57<17:28,  2.74it/s]  8%|â–Š         | 261/3145 [01:44<18:04,  2.66it/s]  8%|â–Š         | 266/3143 [01:44<18:39,  2.57it/s]  8%|â–Š         | 262/3145 [01:44<17:15,  2.78it/s]  9%|â–Š         | 275/3145 [01:57<17:23,  2.75it/s]  8%|â–Š         | 267/3143 [01:44<18:36,  2.58it/s]  9%|â–Š         | 269/3145 [01:47<19:47,  2.42it/s]  9%|â–Š         | 268/3143 [01:45<16:41,  2.87it/s]  9%|â–‰         | 276/3145 [01:57<17:15,  2.77it/s]  8%|â–Š         | 263/3145 [01:45<17:38,  2.72it/s]  9%|â–Š         | 270/3145 [01:47<18:38,  2.57it/s]  9%|â–‰         | 277/3145 [01:58<17:19,  2.76it/s]  8%|â–Š         | 264/3145 [01:45<17:33,  2.74it/s]  9%|â–Š         | 269/3143 [01:45<18:42,  2.56it/s]  9%|â–Š         | 271/3145 [01:48<20:42,  2.31it/s]  9%|â–‰         | 278/3145 [01:58<17:20,  2.75it/s]  8%|â–Š         | 265/3145 [01:45<17:24,  2.76it/s]  9%|â–Š         | 270/3143 [01:46<18:11,  2.63it/s]  9%|â–Š         | 272/3145 [01:48<20:01,  2.39it/s]  9%|â–‰         | 279/3145 [01:59<17:23,  2.75it/s]  8%|â–Š         | 266/3145 [01:46<17:30,  2.74it/s]  9%|â–Š         | 271/3143 [01:46<18:27,  2.59it/s]  9%|â–Š         | 273/3145 [01:49<19:23,  2.47it/s]  9%|â–‰         | 280/3145 [01:59<17:25,  2.74it/s]  8%|â–Š         | 267/3145 [01:46<17:57,  2.67it/s]  9%|â–Š         | 272/3143 [01:46<18:26,  2.60it/s]  9%|â–Š         | 274/3145 [01:49<19:22,  2.47it/s]  9%|â–Š         | 268/3145 [01:46<17:39,  2.71it/s]  9%|â–‰         | 281/3145 [01:59<17:48,  2.68it/s]  9%|â–Š         | 273/3143 [01:47<18:00,  2.66it/s]  9%|â–Š         | 275/3145 [01:49<19:09,  2.50it/s]  9%|â–Š         | 269/3145 [01:47<17:27,  2.75it/s]  9%|â–‰         | 282/3145 [02:00<18:14,  2.62it/s]  9%|â–Š         | 274/3143 [01:47<18:05,  2.64it/s]  9%|â–‰         | 276/3145 [01:50<18:32,  2.58it/s]  9%|â–Š         | 270/3145 [01:47<17:21,  2.76it/s]  9%|â–‰         | 283/3145 [02:00<17:29,  2.73it/s]  9%|â–Š         | 275/3143 [01:47<18:12,  2.62it/s]  9%|â–Š         | 271/3145 [01:47<15:47,  3.03it/s]  9%|â–‰         | 277/3145 [01:50<18:01,  2.65it/s]  9%|â–‰         | 284/3145 [02:00<17:47,  2.68it/s]  9%|â–Š         | 272/3145 [01:48<14:22,  3.33it/s]  9%|â–‰         | 276/3143 [01:48<18:17,  2.61it/s]  9%|â–‰         | 278/3145 [01:50<17:50,  2.68it/s]  9%|â–‰         | 285/3145 [02:01<17:31,  2.72it/s]  9%|â–‰         | 279/3145 [01:51<16:16,  2.93it/s]  9%|â–‰         | 277/3143 [01:48<18:21,  2.60it/s]  9%|â–Š         | 273/3145 [01:48<17:14,  2.78it/s]  9%|â–‰         | 286/3145 [02:01<17:53,  2.66it/s]  9%|â–Š         | 274/3145 [01:48<15:19,  3.12it/s]  9%|â–‰         | 280/3145 [01:51<16:04,  2.97it/s]  9%|â–‰         | 278/3143 [01:49<17:28,  2.73it/s]  9%|â–‰         | 287/3145 [02:02<17:13,  2.76it/s]  9%|â–Š         | 275/3145 [01:49<15:57,  3.00it/s]  9%|â–‰         | 281/3145 [01:51<16:56,  2.82it/s]  9%|â–‰         | 279/3143 [01:49<17:51,  2.67it/s]  9%|â–‰         | 288/3145 [02:02<18:02,  2.64it/s]  9%|â–‰         | 282/3145 [01:52<17:01,  2.80it/s]  9%|â–‰         | 280/3143 [01:49<17:33,  2.72it/s]  9%|â–‰         | 276/3145 [01:49<18:11,  2.63it/s]  9%|â–‰         | 289/3145 [02:02<17:43,  2.68it/s]  9%|â–‰         | 283/3145 [01:52<17:38,  2.70it/s]  9%|â–‰         | 277/3145 [01:50<18:25,  2.59it/s]  9%|â–‰         | 281/3143 [01:50<20:08,  2.37it/s]  9%|â–‰         | 290/3145 [02:03<17:28,  2.72it/s]  9%|â–‰         | 284/3145 [01:53<17:31,  2.72it/s]  9%|â–‰         | 278/3145 [01:50<17:40,  2.70it/s]  9%|â–‰         | 291/3145 [02:03<15:28,  3.07it/s]  9%|â–‰         | 282/3143 [01:50<19:41,  2.42it/s]  9%|â–‰         | 279/3145 [01:50<17:27,  2.74it/s]  9%|â–‰         | 285/3145 [01:53<17:47,  2.68it/s]  9%|â–‰         | 292/3145 [02:03<17:36,  2.70it/s]  9%|â–‰         | 283/3143 [01:51<19:21,  2.46it/s]  9%|â–‰         | 280/3145 [01:51<17:31,  2.72it/s]  9%|â–‰         | 286/3145 [01:53<18:28,  2.58it/s]  9%|â–‰         | 293/3145 [02:04<17:32,  2.71it/s]  9%|â–‰         | 284/3143 [01:51<19:28,  2.45it/s]  9%|â–‰         | 281/3145 [01:51<17:47,  2.68it/s]  9%|â–‰         | 287/3145 [01:54<17:57,  2.65it/s]  9%|â–‰         | 294/3145 [02:04<18:46,  2.53it/s]  9%|â–‰         | 285/3143 [01:51<19:05,  2.50it/s]  9%|â–‰         | 282/3145 [01:52<17:59,  2.65it/s]  9%|â–‰         | 288/3145 [01:54<17:39,  2.70it/s]  9%|â–‰         | 295/3145 [02:04<17:57,  2.65it/s]  9%|â–‰         | 286/3143 [01:52<19:24,  2.45it/s]  9%|â–‰         | 283/3145 [01:52<17:47,  2.68it/s]  9%|â–‰         | 289/3145 [01:54<17:45,  2.68it/s]  9%|â–‰         | 296/3145 [02:05<17:38,  2.69it/s]  9%|â–‰         | 287/3143 [01:52<18:42,  2.54it/s]  9%|â–‰         | 290/3145 [01:55<17:30,  2.72it/s]  9%|â–‰         | 284/3145 [01:52<18:45,  2.54it/s]  9%|â–‰         | 288/3143 [01:53<18:10,  2.62it/s]  9%|â–‰         | 297/3145 [02:05<18:50,  2.52it/s]  9%|â–‰         | 291/3145 [01:55<17:54,  2.66it/s]  9%|â–‰         | 285/3145 [01:53<19:04,  2.50it/s]  9%|â–‰         | 289/3143 [01:53<18:12,  2.61it/s]  9%|â–‰         | 298/3145 [02:06<19:41,  2.41it/s]  9%|â–‰         | 292/3145 [01:56<18:00,  2.64it/s]  9%|â–‰         | 286/3145 [01:53<18:30,  2.58it/s] 10%|â–‰         | 299/3145 [02:06<18:35,  2.55it/s]  9%|â–‰         | 290/3143 [01:53<20:18,  2.34it/s]  9%|â–‰         | 293/3145 [01:56<18:34,  2.56it/s]  9%|â–‰         | 287/3145 [01:53<18:06,  2.63it/s] 10%|â–‰         | 300/3145 [02:06<17:48,  2.66it/s]  9%|â–‰         | 288/3145 [01:54<16:06,  2.96it/s]  9%|â–‰         | 294/3145 [01:56<18:09,  2.62it/s]  9%|â–‰         | 289/3145 [01:54<14:38,  3.25it/s]  9%|â–‰         | 291/3143 [01:54<21:45,  2.18it/s] 10%|â–‰         | 301/3145 [02:07<18:25,  2.57it/s]  9%|â–‰         | 295/3145 [01:57<18:29,  2.57it/s]  9%|â–‰         | 292/3143 [01:54<20:23,  2.33it/s]  9%|â–‰         | 290/3145 [01:54<16:24,  2.90it/s] 10%|â–‰         | 302/3145 [02:07<18:00,  2.63it/s]  9%|â–‰         | 296/3145 [01:57<17:59,  2.64it/s] 10%|â–‰         | 303/3145 [02:07<16:15,  2.91it/s]  9%|â–‰         | 293/3143 [01:55<20:03,  2.37it/s]  9%|â–‰         | 291/3145 [01:55<17:06,  2.78it/s]  9%|â–‰         | 297/3145 [01:57<17:13,  2.76it/s]  9%|â–‰         | 294/3143 [01:55<19:05,  2.49it/s] 10%|â–‰         | 304/3145 [02:08<16:59,  2.79it/s]  9%|â–‰         | 292/3145 [01:55<17:42,  2.69it/s]  9%|â–‰         | 298/3145 [01:58<16:57,  2.80it/s]  9%|â–‰         | 295/3143 [01:55<18:15,  2.60it/s] 10%|â–‰         | 305/3145 [02:08<17:01,  2.78it/s] 10%|â–‰         | 299/3145 [01:58<15:33,  3.05it/s]  9%|â–‰         | 293/3145 [01:56<17:54,  2.65it/s]  9%|â–‰         | 296/3143 [01:56<18:13,  2.60it/s] 10%|â–‰         | 306/3145 [02:09<17:22,  2.72it/s] 10%|â–‰         | 300/3145 [01:58<15:59,  2.96it/s]  9%|â–‰         | 294/3145 [01:56<17:16,  2.75it/s]  9%|â–‰         | 297/3143 [01:56<18:13,  2.60it/s] 10%|â–‰         | 307/3145 [02:09<17:37,  2.68it/s] 10%|â–‰         | 301/3145 [01:59<17:10,  2.76it/s]  9%|â–‰         | 295/3145 [01:56<18:08,  2.62it/s]  9%|â–‰         | 298/3143 [01:57<17:50,  2.66it/s] 10%|â–‰         | 308/3145 [02:09<17:47,  2.66it/s] 10%|â–‰         | 302/3145 [01:59<17:23,  2.72it/s]  9%|â–‰         | 296/3145 [01:57<18:10,  2.61it/s] 10%|â–‰         | 299/3143 [01:57<17:56,  2.64it/s] 10%|â–‰         | 309/3145 [02:10<17:37,  2.68it/s]  9%|â–‰         | 297/3145 [01:57<18:04,  2.62it/s] 10%|â–‰         | 303/3145 [02:00<18:36,  2.54it/s] 10%|â–‰         | 310/3145 [02:10<16:09,  2.92it/s] 10%|â–‰         | 300/3143 [01:57<17:15,  2.75it/s]  9%|â–‰         | 298/3145 [01:57<17:23,  2.73it/s] 10%|â–‰         | 304/3145 [02:00<18:59,  2.49it/s] 10%|â–‰         | 301/3143 [01:58<16:42,  2.84it/s] 10%|â–‰         | 311/3145 [02:10<16:48,  2.81it/s] 10%|â–‰         | 299/3145 [01:58<16:56,  2.80it/s] 10%|â–‰         | 302/3143 [01:58<16:45,  2.83it/s] 10%|â–‰         | 305/3145 [02:00<19:27,  2.43it/s] 10%|â–‰         | 312/3145 [02:11<16:49,  2.81it/s] 10%|â–‰         | 300/3145 [01:58<16:33,  2.86it/s] 10%|â–‰         | 303/3143 [01:58<16:27,  2.88it/s] 10%|â–‰         | 313/3145 [02:11<16:32,  2.85it/s] 10%|â–‰         | 306/3145 [02:01<19:06,  2.48it/s] 10%|â–‰         | 301/3145 [01:58<16:21,  2.90it/s] 10%|â–‰         | 304/3143 [01:59<16:41,  2.84it/s] 10%|â–‰         | 314/3145 [02:11<16:40,  2.83it/s] 10%|â–‰         | 307/3145 [02:01<18:34,  2.55it/s] 10%|â–‰         | 302/3145 [01:59<18:12,  2.60it/s] 10%|â–ˆ         | 315/3145 [02:12<15:41,  3.00it/s] 10%|â–‰         | 305/3143 [01:59<17:10,  2.75it/s] 10%|â–‰         | 308/3145 [02:02<18:26,  2.56it/s] 10%|â–‰         | 303/3145 [01:59<18:12,  2.60it/s] 10%|â–ˆ         | 316/3145 [02:12<15:59,  2.95it/s] 10%|â–‰         | 306/3143 [01:59<17:30,  2.70it/s] 10%|â–‰         | 309/3145 [02:02<18:25,  2.56it/s] 10%|â–ˆ         | 317/3145 [02:12<16:16,  2.90it/s] 10%|â–‰         | 307/3143 [02:00<17:00,  2.78it/s] 10%|â–‰         | 304/3145 [02:00<20:16,  2.34it/s] 10%|â–‰         | 310/3145 [02:02<18:25,  2.56it/s] 10%|â–ˆ         | 318/3145 [02:13<16:30,  2.85it/s] 10%|â–‰         | 308/3143 [02:00<17:05,  2.76it/s] 10%|â–‰         | 311/3145 [02:03<17:56,  2.63it/s] 10%|â–‰         | 305/3145 [02:00<21:10,  2.23it/s] 10%|â–ˆ         | 319/3145 [02:13<16:33,  2.84it/s] 10%|â–‰         | 309/3143 [02:00<17:00,  2.78it/s] 10%|â–‰         | 312/3145 [02:03<18:03,  2.61it/s] 10%|â–ˆ         | 320/3145 [02:14<16:32,  2.85it/s] 10%|â–‰         | 306/3145 [02:01<22:20,  2.12it/s] 10%|â–‰         | 310/3143 [02:01<17:04,  2.77it/s] 10%|â–‰         | 313/3145 [02:04<18:11,  2.59it/s] 10%|â–ˆ         | 321/3145 [02:14<16:35,  2.84it/s] 10%|â–‰         | 311/3143 [02:01<17:10,  2.75it/s] 10%|â–‰         | 307/3145 [02:01<21:12,  2.23it/s] 10%|â–‰         | 314/3145 [02:04<17:06,  2.76it/s] 10%|â–ˆ         | 322/3145 [02:14<16:52,  2.79it/s] 10%|â–‰         | 312/3143 [02:02<16:36,  2.84it/s] 10%|â–‰         | 308/3145 [02:02<19:44,  2.40it/s] 10%|â–ˆ         | 315/3145 [02:04<16:42,  2.82it/s] 10%|â–ˆ         | 323/3145 [02:15<17:00,  2.77it/s] 10%|â–‰         | 313/3143 [02:02<16:49,  2.80it/s] 10%|â–‰         | 309/3145 [02:02<18:53,  2.50it/s] 10%|â–ˆ         | 316/3145 [02:05<16:43,  2.82it/s] 10%|â–ˆ         | 324/3145 [02:15<17:22,  2.71it/s] 10%|â–‰         | 314/3143 [02:02<17:10,  2.75it/s] 10%|â–‰         | 310/3145 [02:02<18:42,  2.52it/s] 10%|â–ˆ         | 317/3145 [02:05<16:09,  2.92it/s] 10%|â–ˆ         | 318/3145 [02:05<14:21,  3.28it/s] 10%|â–ˆ         | 325/3145 [02:15<17:10,  2.74it/s] 10%|â–ˆ         | 315/3143 [02:03<16:57,  2.78it/s] 10%|â–‰         | 311/3145 [02:03<18:01,  2.62it/s] 10%|â–‰         | 312/3145 [02:03<15:40,  3.01it/s] 10%|â–ˆ         | 319/3145 [02:05<15:33,  3.03it/s] 10%|â–ˆ         | 326/3145 [02:16<17:24,  2.70it/s] 10%|â–ˆ         | 316/3143 [02:03<16:59,  2.77it/s] 10%|â–‰         | 313/3145 [02:03<15:42,  3.00it/s] 10%|â–ˆ         | 320/3145 [02:06<16:19,  2.88it/s] 10%|â–ˆ         | 317/3143 [02:03<17:20,  2.71it/s] 10%|â–ˆ         | 327/3145 [02:16<18:04,  2.60it/s] 10%|â–‰         | 314/3145 [02:03<15:21,  3.07it/s] 10%|â–ˆ         | 321/3145 [02:06<14:33,  3.23it/s] 10%|â–ˆ         | 318/3143 [02:04<17:07,  2.75it/s] 10%|â–ˆ         | 328/3145 [02:17<17:46,  2.64it/s] 10%|â–ˆ         | 315/3145 [02:04<15:37,  3.02it/s] 10%|â–ˆ         | 322/3145 [02:06<16:06,  2.92it/s] 10%|â–ˆ         | 319/3143 [02:04<17:01,  2.76it/s] 10%|â–ˆ         | 329/3145 [02:17<17:37,  2.66it/s] 10%|â–ˆ         | 316/3145 [02:04<16:18,  2.89it/s] 10%|â–ˆ         | 323/3145 [02:07<16:18,  2.88it/s] 10%|â–ˆ         | 320/3143 [02:04<16:48,  2.80it/s] 10%|â–ˆ         | 330/3145 [02:17<17:41,  2.65it/s] 10%|â–ˆ         | 317/3145 [02:05<16:32,  2.85it/s] 10%|â–ˆ         | 324/3145 [02:07<16:13,  2.90it/s] 10%|â–ˆ         | 321/3143 [02:05<16:10,  2.91it/s] 11%|â–ˆ         | 331/3145 [02:18<16:56,  2.77it/s] 10%|â–ˆ         | 318/3145 [02:05<17:01,  2.77it/s] 10%|â–ˆ         | 325/3145 [02:08<16:53,  2.78it/s] 10%|â–ˆ         | 322/3143 [02:05<16:07,  2.92it/s] 11%|â–ˆ         | 332/3145 [02:18<16:53,  2.77it/s] 10%|â–ˆ         | 319/3145 [02:05<16:57,  2.78it/s] 10%|â–ˆ         | 326/3145 [02:08<17:19,  2.71it/s] 10%|â–ˆ         | 323/3143 [02:05<16:45,  2.81it/s] 11%|â–ˆ         | 333/3145 [02:18<16:59,  2.76it/s] 10%|â–ˆ         | 320/3145 [02:06<17:19,  2.72it/s] 10%|â–ˆ         | 327/3145 [02:08<17:14,  2.72it/s] 10%|â–ˆ         | 324/3143 [02:06<16:46,  2.80it/s] 11%|â–ˆ         | 334/3145 [02:19<16:48,  2.79it/s] 10%|â–ˆ         | 321/3145 [02:06<17:22,  2.71it/s] 10%|â–ˆ         | 328/3145 [02:09<17:29,  2.68it/s] 10%|â–ˆ         | 325/3143 [02:06<17:09,  2.74it/s] 11%|â–ˆ         | 335/3145 [02:19<17:21,  2.70it/s] 10%|â–ˆ         | 322/3145 [02:06<17:09,  2.74it/s] 10%|â–ˆ         | 329/3145 [02:09<17:15,  2.72it/s] 10%|â–ˆ         | 326/3143 [02:07<17:24,  2.70it/s] 11%|â–ˆ         | 336/3145 [02:19<17:30,  2.67it/s] 10%|â–ˆ         | 323/3145 [02:07<17:25,  2.70it/s] 10%|â–ˆ         | 327/3143 [02:07<16:07,  2.91it/s] 10%|â–ˆ         | 330/3145 [02:09<17:13,  2.72it/s] 11%|â–ˆ         | 337/3145 [02:20<16:55,  2.77it/s] 10%|â–ˆ         | 324/3145 [02:07<17:33,  2.68it/s] 10%|â–ˆ         | 328/3143 [02:07<16:51,  2.78it/s] 11%|â–ˆ         | 331/3145 [02:10<17:30,  2.68it/s] 11%|â–ˆ         | 338/3145 [02:20<17:01,  2.75it/s] 10%|â–ˆ         | 329/3143 [02:08<16:38,  2.82it/s] 11%|â–ˆ         | 332/3145 [02:10<17:42,  2.65it/s] 11%|â–ˆ         | 339/3145 [02:21<16:43,  2.80it/s] 10%|â–ˆ         | 325/3145 [02:08<19:27,  2.42it/s] 10%|â–ˆ         | 330/3143 [02:08<17:23,  2.70it/s] 10%|â–ˆ         | 326/3145 [02:08<19:06,  2.46it/s] 11%|â–ˆ         | 333/3145 [02:11<18:13,  2.57it/s] 11%|â–ˆ         | 340/3145 [02:21<18:01,  2.59it/s] 10%|â–ˆ         | 327/3145 [02:08<18:06,  2.59it/s] 11%|â–ˆ         | 331/3143 [02:08<18:31,  2.53it/s] 11%|â–ˆ         | 334/3145 [02:11<18:05,  2.59it/s] 11%|â–ˆ         | 341/3145 [02:21<18:29,  2.53it/s] 11%|â–ˆ         | 332/3143 [02:09<17:23,  2.69it/s] 11%|â–ˆ         | 335/3145 [02:11<17:47,  2.63it/s] 10%|â–ˆ         | 328/3145 [02:09<19:57,  2.35it/s] 11%|â–ˆ         | 342/3145 [02:22<18:59,  2.46it/s] 11%|â–ˆ         | 333/3143 [02:09<17:15,  2.71it/s] 11%|â–ˆ         | 336/3145 [02:12<18:30,  2.53it/s] 10%|â–ˆ         | 329/3145 [02:09<19:33,  2.40it/s] 11%|â–ˆ         | 343/3145 [02:22<19:15,  2.42it/s] 11%|â–ˆ         | 334/3143 [02:10<17:57,  2.61it/s] 11%|â–ˆ         | 337/3145 [02:12<18:51,  2.48it/s] 10%|â–ˆ         | 330/3145 [02:10<19:42,  2.38it/s] 11%|â–ˆ         | 344/3145 [02:23<18:43,  2.49it/s] 11%|â–ˆ         | 335/3143 [02:10<18:15,  2.56it/s] 11%|â–ˆ         | 331/3145 [02:10<18:07,  2.59it/s] 11%|â–ˆ         | 338/3145 [02:13<19:04,  2.45it/s] 11%|â–ˆ         | 345/3145 [02:23<19:00,  2.45it/s] 11%|â–ˆ         | 336/3143 [02:10<18:26,  2.54it/s] 11%|â–ˆ         | 339/3145 [02:13<19:00,  2.46it/s] 11%|â–ˆ         | 332/3145 [02:11<19:10,  2.44it/s] 11%|â–ˆ         | 346/3145 [02:23<19:06,  2.44it/s] 11%|â–ˆ         | 337/3143 [02:11<18:21,  2.55it/s] 11%|â–ˆ         | 340/3145 [02:13<18:37,  2.51it/s] 11%|â–ˆ         | 333/3145 [02:11<19:40,  2.38it/s] 11%|â–ˆ         | 347/3145 [02:24<20:16,  2.30it/s] 11%|â–ˆ         | 338/3143 [02:11<18:52,  2.48it/s] 11%|â–ˆ         | 334/3145 [02:11<17:49,  2.63it/s] 11%|â–ˆ         | 341/3145 [02:14<18:58,  2.46it/s] 11%|â–ˆ         | 348/3145 [02:24<19:34,  2.38it/s] 11%|â–ˆ         | 339/3143 [02:12<18:53,  2.47it/s] 11%|â–ˆ         | 342/3145 [02:14<18:21,  2.55it/s] 11%|â–ˆ         | 335/3145 [02:12<18:11,  2.58it/s] 11%|â–ˆ         | 349/3145 [02:25<19:07,  2.44it/s] 11%|â–ˆ         | 336/3145 [02:12<17:03,  2.74it/s] 11%|â–ˆ         | 340/3143 [02:12<18:57,  2.46it/s] 11%|â–ˆ         | 343/3145 [02:15<19:16,  2.42it/s] 11%|â–ˆ         | 350/3145 [02:25<18:32,  2.51it/s] 11%|â–ˆ         | 341/3143 [02:12<18:34,  2.51it/s] 11%|â–ˆ         | 337/3145 [02:12<18:47,  2.49it/s] 11%|â–ˆ         | 344/3145 [02:15<20:23,  2.29it/s] 11%|â–ˆ         | 351/3145 [02:25<18:26,  2.53it/s] 11%|â–ˆ         | 342/3143 [02:13<18:25,  2.53it/s] 11%|â–ˆ         | 338/3145 [02:13<17:58,  2.60it/s] 11%|â–ˆ         | 345/3145 [02:16<19:36,  2.38it/s] 11%|â–ˆ         | 352/3145 [02:26<18:14,  2.55it/s] 11%|â–ˆ         | 343/3143 [02:13<18:44,  2.49it/s] 11%|â–ˆ         | 339/3145 [02:13<18:11,  2.57it/s] 11%|â–ˆ         | 346/3145 [02:16<19:50,  2.35it/s] 11%|â–ˆ         | 353/3145 [02:26<19:14,  2.42it/s] 11%|â–ˆ         | 344/3143 [02:14<18:39,  2.50it/s] 11%|â–ˆ         | 340/3145 [02:14<20:02,  2.33it/s] 11%|â–ˆ         | 347/3145 [02:16<19:47,  2.36it/s] 11%|â–ˆâ–        | 354/3145 [02:27<18:59,  2.45it/s] 11%|â–ˆ         | 345/3143 [02:14<18:38,  2.50it/s] 11%|â–ˆ         | 341/3145 [02:14<19:12,  2.43it/s] 11%|â–ˆ         | 348/3145 [02:17<18:45,  2.49it/s] 11%|â–ˆâ–        | 355/3145 [02:27<18:06,  2.57it/s] 11%|â–ˆ         | 346/3143 [02:14<18:20,  2.54it/s] 11%|â–ˆ         | 342/3145 [02:14<18:38,  2.51it/s] 11%|â–ˆ         | 349/3145 [02:17<18:34,  2.51it/s] 11%|â–ˆâ–        | 356/3145 [02:27<18:04,  2.57it/s] 11%|â–ˆ         | 347/3143 [02:15<18:09,  2.57it/s] 11%|â–ˆ         | 343/3145 [02:15<18:46,  2.49it/s] 11%|â–ˆ         | 350/3145 [02:18<18:03,  2.58it/s] 11%|â–ˆâ–        | 357/3145 [02:28<18:04,  2.57it/s] 11%|â–ˆ         | 348/3143 [02:15<18:03,  2.58it/s] 11%|â–ˆ         | 344/3145 [02:15<16:39,  2.80it/s] 11%|â–ˆ         | 351/3145 [02:18<18:26,  2.53it/s] 11%|â–ˆâ–        | 358/3145 [02:28<18:03,  2.57it/s] 11%|â–ˆ         | 349/3143 [02:16<17:55,  2.60it/s] 11%|â–ˆ         | 345/3145 [02:16<18:59,  2.46it/s] 11%|â–ˆâ–        | 359/3145 [02:29<17:59,  2.58it/s] 11%|â–ˆ         | 352/3145 [02:18<19:01,  2.45it/s] 11%|â–ˆ         | 350/3143 [02:16<18:25,  2.53it/s] 11%|â–ˆ         | 346/3145 [02:16<19:35,  2.38it/s] 11%|â–ˆ         | 353/3145 [02:19<18:40,  2.49it/s] 11%|â–ˆâ–        | 360/3145 [02:29<18:44,  2.48it/s] 11%|â–ˆ         | 351/3143 [02:16<18:25,  2.53it/s] 11%|â–ˆ         | 347/3145 [02:17<19:17,  2.42it/s] 11%|â–ˆâ–        | 354/3145 [02:19<18:08,  2.56it/s] 11%|â–ˆâ–        | 361/3145 [02:29<18:54,  2.45it/s] 11%|â–ˆ         | 352/3143 [02:17<18:39,  2.49it/s] 11%|â–ˆ         | 348/3145 [02:17<18:43,  2.49it/s] 11%|â–ˆâ–        | 355/3145 [02:20<18:50,  2.47it/s] 12%|â–ˆâ–        | 362/3145 [02:30<19:01,  2.44it/s] 11%|â–ˆ         | 353/3143 [02:17<18:44,  2.48it/s] 11%|â–ˆ         | 349/3145 [02:17<18:25,  2.53it/s] 11%|â–ˆâ–        | 356/3145 [02:20<18:33,  2.51it/s] 12%|â–ˆâ–        | 363/3145 [02:30<18:44,  2.47it/s] 11%|â–ˆâ–        | 354/3143 [02:18<18:38,  2.49it/s] 11%|â–ˆ         | 350/3145 [02:18<18:21,  2.54it/s] 11%|â–ˆâ–        | 357/3145 [02:20<19:27,  2.39it/s] 12%|â–ˆâ–        | 364/3145 [02:31<18:39,  2.48it/s] 11%|â–ˆâ–        | 355/3143 [02:18<18:55,  2.45it/s] 11%|â–ˆ         | 351/3145 [02:18<18:56,  2.46it/s] 11%|â–ˆâ–        | 358/3145 [02:21<19:01,  2.44it/s] 12%|â–ˆâ–        | 365/3145 [02:31<18:54,  2.45it/s] 11%|â–ˆâ–        | 356/3143 [02:18<18:53,  2.46it/s] 11%|â–ˆ         | 352/3145 [02:19<18:53,  2.46it/s] 11%|â–ˆâ–        | 359/3145 [02:21<17:53,  2.60it/s] 12%|â–ˆâ–        | 366/3145 [02:31<18:13,  2.54it/s] 11%|â–ˆâ–        | 357/3143 [02:19<18:16,  2.54it/s] 11%|â–ˆ         | 353/3145 [02:19<17:49,  2.61it/s] 11%|â–ˆâ–        | 360/3145 [02:21<17:37,  2.63it/s] 12%|â–ˆâ–        | 367/3145 [02:32<18:29,  2.50it/s] 11%|â–ˆâ–        | 358/3143 [02:19<18:03,  2.57it/s] 11%|â–ˆâ–        | 354/3145 [02:19<17:33,  2.65it/s] 11%|â–ˆâ–        | 361/3145 [02:22<18:14,  2.54it/s] 12%|â–ˆâ–        | 368/3145 [02:32<18:00,  2.57it/s] 11%|â–ˆâ–        | 359/3143 [02:20<17:59,  2.58it/s] 11%|â–ˆâ–        | 355/3145 [02:20<18:36,  2.50it/s] 12%|â–ˆâ–        | 362/3145 [02:22<18:23,  2.52it/s] 11%|â–ˆâ–        | 360/3143 [02:20<18:31,  2.50it/s] 12%|â–ˆâ–        | 369/3145 [02:33<19:40,  2.35it/s] 11%|â–ˆâ–        | 356/3145 [02:20<19:51,  2.34it/s] 12%|â–ˆâ–        | 363/3145 [02:23<18:12,  2.55it/s] 11%|â–ˆâ–        | 361/3143 [02:20<16:47,  2.76it/s] 12%|â–ˆâ–        | 370/3145 [02:33<19:13,  2.41it/s] 11%|â–ˆâ–        | 357/3145 [02:21<19:27,  2.39it/s] 12%|â–ˆâ–        | 364/3145 [02:23<18:10,  2.55it/s] 12%|â–ˆâ–        | 362/3143 [02:21<17:25,  2.66it/s] 12%|â–ˆâ–        | 371/3145 [02:34<18:44,  2.47it/s] 11%|â–ˆâ–        | 358/3145 [02:21<19:18,  2.41it/s] 12%|â–ˆâ–        | 365/3145 [02:24<18:31,  2.50it/s] 12%|â–ˆâ–        | 372/3145 [02:34<18:09,  2.55it/s] 12%|â–ˆâ–        | 363/3143 [02:21<19:43,  2.35it/s] 11%|â–ˆâ–        | 359/3145 [02:21<18:57,  2.45it/s] 12%|â–ˆâ–        | 373/3145 [02:34<17:43,  2.61it/s] 12%|â–ˆâ–        | 366/3145 [02:24<19:26,  2.38it/s] 12%|â–ˆâ–        | 364/3143 [02:22<18:49,  2.46it/s] 11%|â–ˆâ–        | 360/3145 [02:22<18:11,  2.55it/s] 12%|â–ˆâ–        | 367/3145 [02:24<19:04,  2.43it/s] 12%|â–ˆâ–        | 374/3145 [02:35<18:34,  2.49it/s] 12%|â–ˆâ–        | 365/3143 [02:22<18:29,  2.50it/s] 11%|â–ˆâ–        | 361/3145 [02:22<18:27,  2.51it/s] 12%|â–ˆâ–        | 368/3145 [02:25<19:03,  2.43it/s] 12%|â–ˆâ–        | 366/3143 [02:22<17:56,  2.58it/s] 12%|â–ˆâ–        | 375/3145 [02:35<18:49,  2.45it/s] 12%|â–ˆâ–        | 362/3145 [02:22<18:18,  2.53it/s] 12%|â–ˆâ–        | 367/3143 [02:23<15:59,  2.89it/s] 12%|â–ˆâ–        | 369/3145 [02:25<18:24,  2.51it/s] 12%|â–ˆâ–        | 376/3145 [02:35<18:03,  2.55it/s] 12%|â–ˆâ–        | 363/3145 [02:23<17:31,  2.65it/s] 12%|â–ˆâ–        | 368/3143 [02:23<16:38,  2.78it/s] 12%|â–ˆâ–        | 370/3145 [02:26<17:57,  2.58it/s] 12%|â–ˆâ–        | 377/3145 [02:36<17:29,  2.64it/s] 12%|â–ˆâ–        | 369/3143 [02:23<14:48,  3.12it/s] 12%|â–ˆâ–        | 364/3145 [02:23<17:31,  2.64it/s] 12%|â–ˆâ–        | 371/3145 [02:26<17:48,  2.60it/s] 12%|â–ˆâ–        | 378/3145 [02:36<17:08,  2.69it/s] 12%|â–ˆâ–        | 365/3145 [02:23<15:32,  2.98it/s] 12%|â–ˆâ–        | 370/3143 [02:24<15:50,  2.92it/s] 12%|â–ˆâ–        | 372/3145 [02:26<17:48,  2.59it/s] 12%|â–ˆâ–        | 379/3145 [02:37<17:53,  2.58it/s] 12%|â–ˆâ–        | 366/3145 [02:24<15:48,  2.93it/s] 12%|â–ˆâ–        | 371/3143 [02:24<16:00,  2.89it/s] 12%|â–ˆâ–        | 373/3145 [02:27<17:29,  2.64it/s] 12%|â–ˆâ–        | 372/3143 [02:24<14:47,  3.12it/s] 12%|â–ˆâ–        | 367/3145 [02:24<15:46,  2.94it/s] 12%|â–ˆâ–        | 380/3145 [02:37<18:17,  2.52it/s] 12%|â–ˆâ–        | 368/3145 [02:25<16:05,  2.88it/s] 12%|â–ˆâ–        | 373/3143 [02:25<15:41,  2.94it/s] 12%|â–ˆâ–        | 374/3145 [02:27<18:03,  2.56it/s] 12%|â–ˆâ–        | 381/3145 [02:37<18:53,  2.44it/s] 12%|â–ˆâ–        | 369/3145 [02:25<16:08,  2.87it/s] 12%|â–ˆâ–        | 374/3143 [02:25<16:13,  2.85it/s] 12%|â–ˆâ–        | 375/3145 [02:27<18:16,  2.53it/s] 12%|â–ˆâ–        | 382/3145 [02:38<18:10,  2.53it/s] 12%|â–ˆâ–        | 375/3143 [02:25<16:20,  2.82it/s] 12%|â–ˆâ–        | 376/3145 [02:28<18:07,  2.55it/s] 12%|â–ˆâ–        | 370/3145 [02:25<17:52,  2.59it/s] 12%|â–ˆâ–        | 383/3145 [02:38<17:32,  2.62it/s] 12%|â–ˆâ–        | 376/3143 [02:26<16:44,  2.75it/s] 12%|â–ˆâ–        | 377/3145 [02:28<17:23,  2.65it/s] 12%|â–ˆâ–        | 371/3145 [02:26<17:07,  2.70it/s] 12%|â–ˆâ–        | 384/3145 [02:39<17:44,  2.59it/s] 12%|â–ˆâ–        | 372/3145 [02:26<16:44,  2.76it/s] 12%|â–ˆâ–        | 378/3145 [02:29<17:16,  2.67it/s] 12%|â–ˆâ–        | 377/3143 [02:26<17:34,  2.62it/s] 12%|â–ˆâ–        | 385/3145 [02:39<17:26,  2.64it/s] 12%|â–ˆâ–        | 386/3145 [02:39<15:29,  2.97it/s] 12%|â–ˆâ–        | 373/3145 [02:26<17:06,  2.70it/s] 12%|â–ˆâ–        | 378/3143 [02:26<17:18,  2.66it/s] 12%|â–ˆâ–        | 379/3145 [02:29<17:47,  2.59it/s] 12%|â–ˆâ–        | 380/3145 [02:29<16:47,  2.75it/s] 12%|â–ˆâ–        | 379/3143 [02:27<16:42,  2.76it/s] 12%|â–ˆâ–        | 374/3145 [02:27<16:58,  2.72it/s] 12%|â–ˆâ–        | 387/3145 [02:40<16:34,  2.77it/s] 12%|â–ˆâ–        | 375/3145 [02:27<16:06,  2.87it/s] 12%|â–ˆâ–        | 381/3145 [02:30<17:01,  2.71it/s] 12%|â–ˆâ–        | 388/3145 [02:40<17:15,  2.66it/s] 12%|â–ˆâ–        | 380/3143 [02:27<17:54,  2.57it/s] 12%|â–ˆâ–        | 376/3145 [02:27<16:35,  2.78it/s] 12%|â–ˆâ–        | 382/3145 [02:30<16:56,  2.72it/s] 12%|â–ˆâ–        | 381/3143 [02:28<16:38,  2.77it/s] 12%|â–ˆâ–        | 389/3145 [02:40<17:09,  2.68it/s] 12%|â–ˆâ–        | 377/3145 [02:28<15:47,  2.92it/s] 12%|â–ˆâ–        | 382/3143 [02:28<16:16,  2.83it/s] 12%|â–ˆâ–        | 383/3145 [02:30<17:00,  2.71it/s] 12%|â–ˆâ–        | 390/3145 [02:41<17:15,  2.66it/s] 12%|â–ˆâ–        | 378/3145 [02:28<15:49,  2.91it/s] 12%|â–ˆâ–        | 383/3143 [02:28<16:43,  2.75it/s] 12%|â–ˆâ–        | 384/3145 [02:31<16:57,  2.71it/s] 12%|â–ˆâ–        | 391/3145 [02:41<16:56,  2.71it/s] 12%|â–ˆâ–        | 379/3145 [02:28<15:58,  2.89it/s] 12%|â–ˆâ–        | 384/3143 [02:29<16:15,  2.83it/s] 12%|â–ˆâ–        | 385/3145 [02:31<16:34,  2.77it/s] 12%|â–ˆâ–        | 392/3145 [02:41<16:52,  2.72it/s] 12%|â–ˆâ–        | 380/3145 [02:29<16:01,  2.88it/s] 12%|â–ˆâ–        | 386/3145 [02:31<16:33,  2.78it/s] 12%|â–ˆâ–        | 385/3143 [02:29<16:46,  2.74it/s] 12%|â–ˆâ–        | 393/3145 [02:42<16:48,  2.73it/s] 12%|â–ˆâ–        | 381/3145 [02:29<16:02,  2.87it/s] 12%|â–ˆâ–        | 387/3145 [02:32<15:52,  2.89it/s] 12%|â–ˆâ–        | 386/3143 [02:29<17:44,  2.59it/s] 13%|â–ˆâ–Ž        | 394/3145 [02:42<17:31,  2.62it/s] 12%|â–ˆâ–        | 382/3145 [02:30<16:52,  2.73it/s] 12%|â–ˆâ–        | 388/3145 [02:32<15:45,  2.92it/s] 12%|â–ˆâ–        | 387/3143 [02:30<17:50,  2.57it/s] 13%|â–ˆâ–Ž        | 395/3145 [02:43<19:36,  2.34it/s] 12%|â–ˆâ–        | 383/3145 [02:30<18:03,  2.55it/s] 12%|â–ˆâ–        | 389/3145 [02:33<16:57,  2.71it/s] 12%|â–ˆâ–        | 388/3143 [02:30<17:25,  2.64it/s] 13%|â–ˆâ–Ž        | 396/3145 [02:43<18:59,  2.41it/s] 12%|â–ˆâ–        | 384/3145 [02:30<17:40,  2.60it/s] 12%|â–ˆâ–        | 390/3145 [02:33<17:20,  2.65it/s] 12%|â–ˆâ–        | 389/3143 [02:31<17:06,  2.68it/s] 13%|â–ˆâ–Ž        | 397/3145 [02:44<18:07,  2.53it/s] 12%|â–ˆâ–        | 385/3145 [02:31<17:23,  2.65it/s] 12%|â–ˆâ–        | 391/3145 [02:33<17:54,  2.56it/s] 12%|â–ˆâ–        | 390/3143 [02:31<17:22,  2.64it/s] 13%|â–ˆâ–Ž        | 398/3145 [02:44<17:38,  2.60it/s] 12%|â–ˆâ–        | 386/3145 [02:31<17:04,  2.69it/s] 12%|â–ˆâ–        | 391/3143 [02:31<17:02,  2.69it/s] 12%|â–ˆâ–        | 392/3145 [02:34<18:23,  2.49it/s] 12%|â–ˆâ–        | 387/3145 [02:31<17:15,  2.66it/s] 13%|â–ˆâ–Ž        | 399/3145 [02:44<18:34,  2.46it/s] 12%|â–ˆâ–        | 392/3143 [02:32<16:47,  2.73it/s] 12%|â–ˆâ–        | 393/3145 [02:34<18:21,  2.50it/s] 12%|â–ˆâ–        | 388/3145 [02:32<17:28,  2.63it/s] 13%|â–ˆâ–Ž        | 400/3145 [02:45<18:18,  2.50it/s] 13%|â–ˆâ–Ž        | 393/3143 [02:32<16:32,  2.77it/s] 13%|â–ˆâ–Ž        | 394/3145 [02:35<19:02,  2.41it/s] 12%|â–ˆâ–        | 389/3145 [02:32<17:35,  2.61it/s] 13%|â–ˆâ–Ž        | 401/3145 [02:45<18:16,  2.50it/s] 13%|â–ˆâ–Ž        | 394/3143 [02:32<17:21,  2.64it/s] 13%|â–ˆâ–Ž        | 395/3145 [02:35<18:43,  2.45it/s] 12%|â–ˆâ–        | 390/3145 [02:33<16:53,  2.72it/s] 13%|â–ˆâ–Ž        | 402/3145 [02:46<18:27,  2.48it/s] 13%|â–ˆâ–Ž        | 395/3143 [02:33<17:26,  2.63it/s] 13%|â–ˆâ–Ž        | 396/3145 [02:35<18:02,  2.54it/s] 12%|â–ˆâ–        | 391/3145 [02:33<16:45,  2.74it/s] 13%|â–ˆâ–Ž        | 403/3145 [02:46<17:25,  2.62it/s] 13%|â–ˆâ–Ž        | 396/3143 [02:33<17:13,  2.66it/s] 13%|â–ˆâ–Ž        | 397/3145 [02:36<17:52,  2.56it/s] 12%|â–ˆâ–        | 392/3145 [02:33<16:37,  2.76it/s] 13%|â–ˆâ–Ž        | 404/3145 [02:46<17:07,  2.67it/s] 13%|â–ˆâ–Ž        | 397/3143 [02:33<16:53,  2.71it/s] 13%|â–ˆâ–Ž        | 398/3145 [02:36<19:14,  2.38it/s] 13%|â–ˆâ–Ž        | 405/3145 [02:47<16:59,  2.69it/s] 12%|â–ˆâ–        | 393/3145 [02:34<18:00,  2.55it/s] 13%|â–ˆâ–Ž        | 398/3143 [02:34<16:17,  2.81it/s] 13%|â–ˆâ–Ž        | 406/3145 [02:47<16:53,  2.70it/s] 13%|â–ˆâ–Ž        | 394/3145 [02:34<17:45,  2.58it/s] 13%|â–ˆâ–Ž        | 399/3143 [02:34<16:46,  2.73it/s] 13%|â–ˆâ–Ž        | 399/3145 [02:37<20:49,  2.20it/s] 13%|â–ˆâ–Ž        | 395/3145 [02:35<17:32,  2.61it/s] 13%|â–ˆâ–Ž        | 400/3143 [02:35<16:39,  2.74it/s] 13%|â–ˆâ–Ž        | 407/3145 [02:47<17:54,  2.55it/s] 13%|â–ˆâ–Ž        | 400/3145 [02:37<19:45,  2.32it/s] 13%|â–ˆâ–Ž        | 396/3145 [02:35<16:54,  2.71it/s] 13%|â–ˆâ–Ž        | 408/3145 [02:48<17:06,  2.67it/s] 13%|â–ˆâ–Ž        | 401/3143 [02:35<17:04,  2.68it/s] 13%|â–ˆâ–Ž        | 401/3145 [02:38<19:08,  2.39it/s] 13%|â–ˆâ–Ž        | 397/3145 [02:35<17:14,  2.66it/s] 13%|â–ˆâ–Ž        | 409/3145 [02:48<17:18,  2.64it/s] 13%|â–ˆâ–Ž        | 402/3143 [02:35<17:16,  2.65it/s] 13%|â–ˆâ–Ž        | 402/3145 [02:38<18:24,  2.48it/s] 13%|â–ˆâ–Ž        | 398/3145 [02:36<16:52,  2.71it/s] 13%|â–ˆâ–Ž        | 410/3145 [02:48<16:40,  2.73it/s] 13%|â–ˆâ–Ž        | 403/3143 [02:36<17:23,  2.63it/s] 13%|â–ˆâ–Ž        | 403/3145 [02:38<17:47,  2.57it/s] 13%|â–ˆâ–Ž        | 399/3145 [02:36<16:50,  2.72it/s] 13%|â–ˆâ–Ž        | 411/3145 [02:49<16:36,  2.74it/s] 13%|â–ˆâ–Ž        | 404/3143 [02:36<16:49,  2.71it/s] 13%|â–ˆâ–Ž        | 404/3145 [02:39<17:44,  2.57it/s] 13%|â–ˆâ–Ž        | 400/3145 [02:36<16:40,  2.74it/s] 13%|â–ˆâ–Ž        | 412/3145 [02:49<16:29,  2.76it/s] 13%|â–ˆâ–Ž        | 405/3143 [02:36<16:55,  2.70it/s] 13%|â–ˆâ–Ž        | 405/3145 [02:39<17:41,  2.58it/s] 13%|â–ˆâ–Ž        | 413/3145 [02:50<16:28,  2.76it/s] 13%|â–ˆâ–Ž        | 401/3145 [02:37<17:24,  2.63it/s] 13%|â–ˆâ–Ž        | 406/3143 [02:37<16:24,  2.78it/s] 13%|â–ˆâ–Ž        | 406/3145 [02:39<16:58,  2.69it/s] 13%|â–ˆâ–Ž        | 414/3145 [02:50<16:25,  2.77it/s] 13%|â–ˆâ–Ž        | 402/3145 [02:37<17:06,  2.67it/s] 13%|â–ˆâ–Ž        | 407/3143 [02:37<16:46,  2.72it/s] 13%|â–ˆâ–Ž        | 407/3145 [02:40<17:06,  2.67it/s] 13%|â–ˆâ–Ž        | 403/3145 [02:37<17:15,  2.65it/s] 13%|â–ˆâ–Ž        | 408/3143 [02:38<17:04,  2.67it/s] 13%|â–ˆâ–Ž        | 415/3145 [02:50<18:26,  2.47it/s] 13%|â–ˆâ–Ž        | 408/3145 [02:40<16:53,  2.70it/s] 13%|â–ˆâ–Ž        | 404/3145 [02:38<16:54,  2.70it/s] 13%|â–ˆâ–Ž        | 409/3143 [02:38<16:59,  2.68it/s] 13%|â–ˆâ–Ž        | 416/3145 [02:51<17:44,  2.56it/s] 13%|â–ˆâ–Ž        | 409/3145 [02:40<16:25,  2.78it/s] 13%|â–ˆâ–Ž        | 410/3143 [02:38<16:43,  2.72it/s] 13%|â–ˆâ–Ž        | 405/3145 [02:38<17:54,  2.55it/s] 13%|â–ˆâ–Ž        | 410/3145 [02:41<16:43,  2.73it/s] 13%|â–ˆâ–Ž        | 417/3145 [02:51<19:45,  2.30it/s] 13%|â–ˆâ–Ž        | 411/3143 [02:39<16:19,  2.79it/s] 13%|â–ˆâ–Ž        | 406/3145 [02:39<18:45,  2.43it/s] 13%|â–ˆâ–Ž        | 411/3145 [02:41<17:21,  2.62it/s] 13%|â–ˆâ–Ž        | 418/3145 [02:52<19:41,  2.31it/s] 13%|â–ˆâ–Ž        | 412/3143 [02:39<16:22,  2.78it/s] 13%|â–ˆâ–Ž        | 412/3145 [02:42<17:06,  2.66it/s] 13%|â–ˆâ–Ž        | 407/3145 [02:39<20:03,  2.28it/s] 13%|â–ˆâ–Ž        | 419/3145 [02:52<18:50,  2.41it/s] 13%|â–ˆâ–Ž        | 413/3143 [02:39<16:43,  2.72it/s] 13%|â–ˆâ–Ž        | 413/3145 [02:42<17:18,  2.63it/s] 13%|â–ˆâ–Ž        | 420/3145 [02:52<18:02,  2.52it/s] 13%|â–ˆâ–Ž        | 408/3145 [02:40<19:33,  2.33it/s] 13%|â–ˆâ–Ž        | 414/3143 [02:40<16:22,  2.78it/s] 13%|â–ˆâ–Ž        | 414/3145 [02:42<16:09,  2.82it/s] 13%|â–ˆâ–Ž        | 409/3145 [02:40<18:59,  2.40it/s] 13%|â–ˆâ–Ž        | 415/3143 [02:40<16:21,  2.78it/s] 13%|â–ˆâ–Ž        | 421/3145 [02:53<18:23,  2.47it/s] 13%|â–ˆâ–Ž        | 415/3145 [02:43<16:34,  2.74it/s] 13%|â–ˆâ–Ž        | 422/3145 [02:53<18:09,  2.50it/s] 13%|â–ˆâ–Ž        | 410/3145 [02:40<19:01,  2.40it/s] 13%|â–ˆâ–Ž        | 416/3143 [02:41<17:41,  2.57it/s] 13%|â–ˆâ–Ž        | 416/3145 [02:43<16:22,  2.78it/s] 13%|â–ˆâ–Ž        | 417/3143 [02:41<15:56,  2.85it/s] 13%|â–ˆâ–Ž        | 423/3145 [02:54<17:18,  2.62it/s] 13%|â–ˆâ–Ž        | 411/3145 [02:41<18:39,  2.44it/s] 13%|â–ˆâ–Ž        | 417/3145 [02:43<16:25,  2.77it/s] 13%|â–ˆâ–Ž        | 424/3145 [02:54<16:35,  2.73it/s] 13%|â–ˆâ–Ž        | 418/3143 [02:41<16:05,  2.82it/s] 13%|â–ˆâ–Ž        | 418/3145 [02:44<14:50,  3.06it/s] 13%|â–ˆâ–Ž        | 412/3145 [02:41<18:06,  2.52it/s] 13%|â–ˆâ–Ž        | 419/3145 [02:44<14:34,  3.12it/s] 14%|â–ˆâ–Ž        | 425/3145 [02:54<16:19,  2.78it/s] 13%|â–ˆâ–Ž        | 419/3143 [02:42<16:02,  2.83it/s] 13%|â–ˆâ–Ž        | 413/3145 [02:42<19:45,  2.30it/s] 13%|â–ˆâ–Ž        | 420/3145 [02:44<15:23,  2.95it/s] 14%|â–ˆâ–Ž        | 426/3145 [02:55<16:49,  2.69it/s] 13%|â–ˆâ–Ž        | 420/3143 [02:42<17:58,  2.52it/s] 13%|â–ˆâ–Ž        | 414/3145 [02:42<18:41,  2.43it/s] 14%|â–ˆâ–Ž        | 427/3145 [02:55<16:21,  2.77it/s] 13%|â–ˆâ–Ž        | 421/3143 [02:42<15:34,  2.91it/s] 13%|â–ˆâ–Ž        | 421/3145 [02:45<15:59,  2.84it/s] 13%|â–ˆâ–Ž        | 415/3145 [02:42<17:50,  2.55it/s] 13%|â–ˆâ–Ž        | 422/3143 [02:43<16:00,  2.83it/s] 14%|â–ˆâ–Ž        | 428/3145 [02:55<16:37,  2.72it/s] 13%|â–ˆâ–Ž        | 422/3145 [02:45<16:37,  2.73it/s] 13%|â–ˆâ–Ž        | 416/3145 [02:43<15:29,  2.94it/s] 14%|â–ˆâ–Ž        | 429/3145 [02:56<16:52,  2.68it/s] 13%|â–ˆâ–Ž        | 423/3145 [02:46<16:58,  2.67it/s] 13%|â–ˆâ–Ž        | 423/3143 [02:43<17:03,  2.66it/s] 13%|â–ˆâ–Ž        | 417/3145 [02:43<15:43,  2.89it/s] 14%|â–ˆâ–Ž        | 430/3145 [02:56<16:54,  2.68it/s] 13%|â–ˆâ–Ž        | 418/3145 [02:43<15:39,  2.90it/s] 13%|â–ˆâ–Ž        | 424/3145 [02:46<17:18,  2.62it/s] 13%|â–ˆâ–Ž        | 424/3143 [02:43<18:07,  2.50it/s] 14%|â–ˆâ–Ž        | 431/3145 [02:57<16:53,  2.68it/s] 14%|â–ˆâ–Ž        | 425/3145 [02:46<16:42,  2.71it/s] 14%|â–ˆâ–Ž        | 425/3143 [02:44<16:40,  2.72it/s] 13%|â–ˆâ–Ž        | 419/3145 [02:44<16:25,  2.76it/s] 14%|â–ˆâ–Ž        | 432/3145 [02:57<17:06,  2.64it/s] 14%|â–ˆâ–Ž        | 426/3145 [02:47<16:43,  2.71it/s] 14%|â–ˆâ–Ž        | 426/3143 [02:44<17:03,  2.66it/s] 13%|â–ˆâ–Ž        | 420/3145 [02:44<17:35,  2.58it/s] 14%|â–ˆâ–        | 433/3145 [02:57<16:39,  2.71it/s] 14%|â–ˆâ–Ž        | 427/3143 [02:45<16:30,  2.74it/s] 14%|â–ˆâ–Ž        | 427/3145 [02:47<17:17,  2.62it/s] 13%|â–ˆâ–Ž        | 421/3145 [02:45<18:03,  2.51it/s] 14%|â–ˆâ–Ž        | 428/3143 [02:45<16:28,  2.75it/s] 14%|â–ˆâ–        | 434/3145 [02:58<17:29,  2.58it/s] 14%|â–ˆâ–Ž        | 428/3145 [02:47<17:28,  2.59it/s] 13%|â–ˆâ–Ž        | 422/3145 [02:45<17:29,  2.59it/s] 14%|â–ˆâ–Ž        | 429/3143 [02:45<16:14,  2.79it/s] 14%|â–ˆâ–Ž        | 429/3145 [02:48<17:35,  2.57it/s] 14%|â–ˆâ–        | 435/3145 [02:58<18:23,  2.46it/s] 13%|â–ˆâ–Ž        | 423/3145 [02:45<17:02,  2.66it/s] 14%|â–ˆâ–Ž        | 430/3143 [02:46<15:57,  2.83it/s] 14%|â–ˆâ–        | 436/3145 [02:58<17:31,  2.58it/s] 14%|â–ˆâ–Ž        | 430/3145 [02:48<17:20,  2.61it/s] 13%|â–ˆâ–Ž        | 424/3145 [02:46<17:13,  2.63it/s] 14%|â–ˆâ–Ž        | 431/3143 [02:46<16:01,  2.82it/s] 14%|â–ˆâ–        | 437/3145 [02:59<17:17,  2.61it/s] 14%|â–ˆâ–Ž        | 425/3145 [02:46<16:37,  2.73it/s] 14%|â–ˆâ–Ž        | 431/3145 [02:49<18:29,  2.45it/s] 14%|â–ˆâ–Ž        | 432/3143 [02:46<16:24,  2.75it/s] 14%|â–ˆâ–        | 438/3145 [02:59<16:45,  2.69it/s] 14%|â–ˆâ–Ž        | 426/3145 [02:46<16:33,  2.74it/s] 14%|â–ˆâ–Ž        | 432/3145 [02:49<18:14,  2.48it/s] 14%|â–ˆâ–        | 433/3143 [02:47<16:41,  2.71it/s] 14%|â–ˆâ–        | 439/3145 [03:00<16:52,  2.67it/s] 14%|â–ˆâ–Ž        | 427/3145 [02:47<17:42,  2.56it/s] 14%|â–ˆâ–        | 433/3145 [02:49<18:19,  2.47it/s] 14%|â–ˆâ–        | 434/3143 [02:47<16:23,  2.76it/s] 14%|â–ˆâ–        | 440/3145 [03:00<16:11,  2.78it/s] 14%|â–ˆâ–Ž        | 428/3145 [02:47<17:38,  2.57it/s] 14%|â–ˆâ–        | 434/3145 [02:50<17:19,  2.61it/s] 14%|â–ˆâ–        | 435/3143 [02:47<16:29,  2.74it/s] 14%|â–ˆâ–        | 441/3145 [03:00<16:18,  2.76it/s] 14%|â–ˆâ–Ž        | 429/3145 [02:48<17:10,  2.64it/s] 14%|â–ˆâ–        | 435/3145 [02:50<17:08,  2.63it/s] 14%|â–ˆâ–        | 436/3143 [02:48<16:19,  2.76it/s] 14%|â–ˆâ–        | 442/3145 [03:01<16:26,  2.74it/s] 14%|â–ˆâ–        | 436/3145 [02:51<16:31,  2.73it/s] 14%|â–ˆâ–Ž        | 430/3145 [02:48<17:17,  2.62it/s] 14%|â–ˆâ–        | 437/3143 [02:48<16:40,  2.70it/s] 14%|â–ˆâ–        | 443/3145 [03:01<17:11,  2.62it/s] 14%|â–ˆâ–        | 437/3145 [02:51<16:28,  2.74it/s] 14%|â–ˆâ–Ž        | 431/3145 [02:48<17:44,  2.55it/s] 14%|â–ˆâ–        | 438/3143 [02:49<16:53,  2.67it/s] 14%|â–ˆâ–        | 444/3145 [03:01<16:48,  2.68it/s] 14%|â–ˆâ–        | 438/3145 [02:51<16:21,  2.76it/s] 14%|â–ˆâ–Ž        | 432/3145 [02:49<16:55,  2.67it/s] 14%|â–ˆâ–        | 445/3145 [03:02<16:39,  2.70it/s] 14%|â–ˆâ–        | 439/3143 [02:49<18:22,  2.45it/s] 14%|â–ˆâ–        | 439/3145 [02:52<16:05,  2.80it/s] 14%|â–ˆâ–        | 433/3145 [02:49<17:07,  2.64it/s] 14%|â–ˆâ–        | 440/3143 [02:49<17:37,  2.56it/s] 14%|â–ˆâ–        | 440/3145 [02:52<15:59,  2.82it/s] 14%|â–ˆâ–        | 446/3145 [03:02<17:27,  2.58it/s] 14%|â–ˆâ–        | 434/3145 [02:49<16:31,  2.73it/s] 14%|â–ˆâ–        | 441/3143 [02:50<17:08,  2.63it/s] 14%|â–ˆâ–        | 441/3145 [02:52<16:08,  2.79it/s] 14%|â–ˆâ–        | 447/3145 [03:03<18:09,  2.48it/s] 14%|â–ˆâ–        | 435/3145 [02:50<16:48,  2.69it/s] 14%|â–ˆâ–        | 442/3143 [02:50<16:21,  2.75it/s] 14%|â–ˆâ–        | 442/3145 [02:53<16:35,  2.71it/s] 14%|â–ˆâ–        | 436/3145 [02:50<16:14,  2.78it/s] 14%|â–ˆâ–        | 448/3145 [03:03<17:53,  2.51it/s] 14%|â–ˆâ–        | 443/3143 [02:50<15:58,  2.82it/s] 14%|â–ˆâ–        | 443/3145 [02:53<16:47,  2.68it/s] 14%|â–ˆâ–        | 437/3145 [02:51<16:17,  2.77it/s] 14%|â–ˆâ–        | 449/3145 [03:03<17:48,  2.52it/s] 14%|â–ˆâ–        | 444/3143 [02:51<15:58,  2.81it/s] 14%|â–ˆâ–        | 444/3145 [02:53<16:33,  2.72it/s] 14%|â–ˆâ–        | 438/3145 [02:51<16:19,  2.76it/s] 14%|â–ˆâ–        | 450/3145 [03:04<17:28,  2.57it/s] 14%|â–ˆâ–        | 445/3143 [02:51<15:58,  2.82it/s] 14%|â–ˆâ–        | 445/3145 [02:54<16:24,  2.74it/s] 14%|â–ˆâ–        | 439/3145 [02:51<16:22,  2.75it/s] 14%|â–ˆâ–        | 451/3145 [03:04<17:08,  2.62it/s] 14%|â–ˆâ–        | 446/3143 [02:51<15:40,  2.87it/s] 14%|â–ˆâ–        | 440/3145 [02:52<16:16,  2.77it/s] 14%|â–ˆâ–        | 446/3145 [02:54<17:14,  2.61it/s] 14%|â–ˆâ–        | 447/3143 [02:52<15:09,  2.96it/s] 14%|â–ˆâ–        | 452/3145 [03:05<16:50,  2.66it/s] 14%|â–ˆâ–        | 441/3145 [02:52<16:25,  2.74it/s] 14%|â–ˆâ–        | 448/3143 [02:52<14:54,  3.01it/s] 14%|â–ˆâ–        | 453/3145 [03:05<16:43,  2.68it/s] 14%|â–ˆâ–        | 447/3145 [02:55<17:59,  2.50it/s] 14%|â–ˆâ–        | 449/3143 [02:52<14:18,  3.14it/s] 14%|â–ˆâ–        | 442/3145 [02:52<16:00,  2.81it/s] 14%|â–ˆâ–        | 454/3145 [03:05<16:29,  2.72it/s] 14%|â–ˆâ–        | 448/3145 [02:55<17:09,  2.62it/s] 14%|â–ˆâ–        | 450/3143 [02:53<14:10,  3.17it/s] 14%|â–ˆâ–        | 443/3145 [02:53<16:34,  2.72it/s] 14%|â–ˆâ–        | 455/3145 [03:06<16:34,  2.71it/s] 14%|â–ˆâ–        | 449/3145 [02:55<17:29,  2.57it/s] 14%|â–ˆâ–        | 451/3143 [02:53<14:05,  3.18it/s] 14%|â–ˆâ–        | 444/3145 [02:53<14:45,  3.05it/s] 14%|â–ˆâ–        | 456/3145 [03:06<16:21,  2.74it/s] 14%|â–ˆâ–        | 450/3145 [02:56<17:32,  2.56it/s] 14%|â–ˆâ–        | 452/3143 [02:53<15:08,  2.96it/s] 14%|â–ˆâ–        | 445/3145 [02:53<15:34,  2.89it/s] 15%|â–ˆâ–        | 457/3145 [03:06<16:18,  2.75it/s] 14%|â–ˆâ–        | 451/3145 [02:56<18:03,  2.49it/s] 14%|â–ˆâ–        | 453/3143 [02:54<15:20,  2.92it/s] 14%|â–ˆâ–        | 446/3145 [02:54<16:04,  2.80it/s] 15%|â–ˆâ–        | 458/3145 [03:07<16:43,  2.68it/s] 14%|â–ˆâ–        | 454/3143 [02:54<15:27,  2.90it/s] 14%|â–ˆâ–        | 452/3145 [02:57<18:57,  2.37it/s] 14%|â–ˆâ–        | 447/3145 [02:54<17:25,  2.58it/s] 15%|â–ˆâ–        | 459/3145 [03:07<16:37,  2.69it/s] 14%|â–ˆâ–        | 455/3143 [02:54<16:10,  2.77it/s] 14%|â–ˆâ–        | 453/3145 [02:57<18:15,  2.46it/s] 14%|â–ˆâ–        | 448/3145 [02:55<16:40,  2.70it/s] 15%|â–ˆâ–        | 460/3145 [03:07<16:56,  2.64it/s] 15%|â–ˆâ–        | 456/3143 [02:55<16:16,  2.75it/s] 14%|â–ˆâ–        | 449/3145 [02:55<17:05,  2.63it/s] 15%|â–ˆâ–        | 461/3145 [03:08<16:42,  2.68it/s] 14%|â–ˆâ–        | 454/3145 [02:58<19:54,  2.25it/s] 15%|â–ˆâ–        | 457/3143 [02:55<16:33,  2.70it/s] 14%|â–ˆâ–        | 450/3145 [02:55<17:07,  2.62it/s] 15%|â–ˆâ–        | 462/3145 [03:08<16:37,  2.69it/s] 14%|â–ˆâ–        | 455/3145 [02:58<19:17,  2.32it/s] 15%|â–ˆâ–        | 458/3143 [02:56<16:28,  2.72it/s] 14%|â–ˆâ–        | 451/3145 [02:56<15:56,  2.82it/s] 15%|â–ˆâ–        | 463/3145 [03:09<16:28,  2.71it/s] 14%|â–ˆâ–        | 456/3145 [02:58<18:35,  2.41it/s] 15%|â–ˆâ–        | 459/3143 [02:56<17:21,  2.58it/s] 14%|â–ˆâ–        | 452/3145 [02:56<15:59,  2.81it/s] 15%|â–ˆâ–        | 464/3145 [03:09<17:00,  2.63it/s] 15%|â–ˆâ–        | 457/3145 [02:59<18:36,  2.41it/s] 14%|â–ˆâ–        | 453/3145 [02:56<15:53,  2.82it/s] 15%|â–ˆâ–        | 460/3143 [02:56<17:16,  2.59it/s] 15%|â–ˆâ–        | 465/3145 [03:09<17:22,  2.57it/s] 15%|â–ˆâ–        | 458/3145 [02:59<17:41,  2.53it/s] 14%|â–ˆâ–        | 454/3145 [02:57<16:22,  2.74it/s] 15%|â–ˆâ–        | 461/3143 [02:57<17:25,  2.56it/s] 15%|â–ˆâ–        | 466/3145 [03:10<16:21,  2.73it/s] 14%|â–ˆâ–        | 455/3145 [02:57<16:05,  2.79it/s] 15%|â–ˆâ–        | 459/3145 [03:00<19:07,  2.34it/s] 15%|â–ˆâ–        | 462/3143 [02:57<16:57,  2.63it/s] 15%|â–ˆâ–        | 467/3145 [03:10<15:37,  2.86it/s] 14%|â–ˆâ–        | 456/3145 [02:57<15:47,  2.84it/s] 15%|â–ˆâ–        | 460/3145 [03:00<18:22,  2.44it/s] 15%|â–ˆâ–        | 463/3143 [02:58<17:03,  2.62it/s] 15%|â–ˆâ–        | 468/3145 [03:10<16:02,  2.78it/s] 15%|â–ˆâ–        | 457/3145 [02:58<16:11,  2.77it/s] 15%|â–ˆâ–        | 461/3145 [03:00<18:28,  2.42it/s] 15%|â–ˆâ–        | 464/3143 [02:58<17:04,  2.62it/s] 15%|â–ˆâ–        | 469/3145 [03:11<16:09,  2.76it/s] 15%|â–ˆâ–        | 458/3145 [02:58<15:52,  2.82it/s] 15%|â–ˆâ–        | 462/3145 [03:01<17:41,  2.53it/s] 15%|â–ˆâ–        | 470/3145 [03:11<15:41,  2.84it/s] 15%|â–ˆâ–        | 465/3143 [02:58<17:37,  2.53it/s] 15%|â–ˆâ–        | 459/3145 [02:59<16:11,  2.76it/s] 15%|â–ˆâ–        | 463/3145 [03:01<16:54,  2.64it/s] 15%|â–ˆâ–        | 471/3145 [03:11<15:44,  2.83it/s] 15%|â–ˆâ–        | 466/3143 [02:59<17:10,  2.60it/s] 15%|â–ˆâ–        | 460/3145 [02:59<16:28,  2.72it/s] 15%|â–ˆâ–        | 464/3145 [03:01<16:38,  2.68it/s] 15%|â–ˆâ–        | 467/3143 [02:59<16:28,  2.71it/s] 15%|â–ˆâ–Œ        | 472/3145 [03:12<16:43,  2.66it/s] 15%|â–ˆâ–        | 461/3145 [02:59<16:44,  2.67it/s] 15%|â–ˆâ–        | 465/3145 [03:02<16:25,  2.72it/s] 15%|â–ˆâ–        | 468/3143 [02:59<16:16,  2.74it/s] 15%|â–ˆâ–Œ        | 473/3145 [03:12<16:40,  2.67it/s] 15%|â–ˆâ–        | 466/3145 [03:02<16:16,  2.74it/s] 15%|â–ˆâ–        | 462/3145 [03:00<16:50,  2.66it/s] 15%|â–ˆâ–Œ        | 474/3145 [03:13<16:13,  2.74it/s] 15%|â–ˆâ–        | 469/3143 [03:00<17:07,  2.60it/s] 15%|â–ˆâ–        | 467/3145 [03:03<16:03,  2.78it/s] 15%|â–ˆâ–        | 463/3145 [03:00<17:14,  2.59it/s] 15%|â–ˆâ–Œ        | 475/3145 [03:13<16:18,  2.73it/s] 15%|â–ˆâ–        | 470/3143 [03:00<17:25,  2.56it/s] 15%|â–ˆâ–        | 464/3145 [03:00<16:58,  2.63it/s] 15%|â–ˆâ–        | 468/3145 [03:03<17:22,  2.57it/s] 15%|â–ˆâ–Œ        | 476/3145 [03:13<16:38,  2.67it/s] 15%|â–ˆâ–        | 471/3143 [03:01<17:29,  2.55it/s] 15%|â–ˆâ–        | 465/3145 [03:01<17:30,  2.55it/s] 15%|â–ˆâ–        | 469/3145 [03:03<17:35,  2.54it/s] 15%|â–ˆâ–Œ        | 477/3145 [03:14<16:27,  2.70it/s] 15%|â–ˆâ–Œ        | 472/3143 [03:01<17:42,  2.51it/s] 15%|â–ˆâ–Œ        | 478/3145 [03:14<14:34,  3.05it/s] 15%|â–ˆâ–        | 466/3145 [03:01<17:49,  2.51it/s] 15%|â–ˆâ–        | 470/3145 [03:04<17:59,  2.48it/s] 15%|â–ˆâ–Œ        | 473/3143 [03:01<18:16,  2.43it/s] 15%|â–ˆâ–Œ        | 479/3145 [03:14<15:40,  2.83it/s] 15%|â–ˆâ–        | 467/3145 [03:02<17:18,  2.58it/s] 15%|â–ˆâ–        | 471/3145 [03:04<18:41,  2.39it/s] 15%|â–ˆâ–Œ        | 474/3143 [03:02<17:57,  2.48it/s] 15%|â–ˆâ–Œ        | 480/3145 [03:15<16:09,  2.75it/s] 15%|â–ˆâ–        | 468/3145 [03:02<17:08,  2.60it/s] 15%|â–ˆâ–Œ        | 475/3143 [03:02<17:34,  2.53it/s] 15%|â–ˆâ–Œ        | 481/3145 [03:15<16:40,  2.66it/s] 15%|â–ˆâ–Œ        | 472/3145 [03:05<21:37,  2.06it/s] 15%|â–ˆâ–        | 469/3145 [03:02<17:51,  2.50it/s] 15%|â–ˆâ–Œ        | 476/3143 [03:03<18:07,  2.45it/s] 15%|â–ˆâ–Œ        | 482/3145 [03:16<16:32,  2.68it/s] 15%|â–ˆâ–Œ        | 473/3145 [03:05<20:06,  2.21it/s] 15%|â–ˆâ–        | 470/3145 [03:03<17:56,  2.49it/s] 15%|â–ˆâ–Œ        | 477/3143 [03:03<18:36,  2.39it/s] 15%|â–ˆâ–Œ        | 474/3145 [03:06<19:18,  2.31it/s] 15%|â–ˆâ–Œ        | 483/3145 [03:16<18:11,  2.44it/s] 15%|â–ˆâ–        | 471/3145 [03:03<18:08,  2.46it/s] 15%|â–ˆâ–Œ        | 472/3145 [03:04<15:50,  2.81it/s] 15%|â–ˆâ–Œ        | 475/3145 [03:06<18:40,  2.38it/s] 15%|â–ˆâ–Œ        | 478/3143 [03:04<18:56,  2.34it/s] 15%|â–ˆâ–Œ        | 484/3145 [03:16<17:48,  2.49it/s] 15%|â–ˆâ–Œ        | 476/3145 [03:06<17:59,  2.47it/s] 15%|â–ˆâ–Œ        | 473/3145 [03:04<16:51,  2.64it/s] 15%|â–ˆâ–Œ        | 479/3143 [03:04<18:44,  2.37it/s] 15%|â–ˆâ–Œ        | 485/3145 [03:17<17:26,  2.54it/s] 15%|â–ˆâ–Œ        | 477/3145 [03:07<17:28,  2.54it/s] 15%|â–ˆâ–Œ        | 474/3145 [03:04<16:17,  2.73it/s] 15%|â–ˆâ–Œ        | 480/3143 [03:04<18:20,  2.42it/s] 15%|â–ˆâ–Œ        | 486/3145 [03:17<18:04,  2.45it/s] 15%|â–ˆâ–Œ        | 475/3145 [03:05<15:00,  2.97it/s] 15%|â–ˆâ–Œ        | 478/3145 [03:07<17:06,  2.60it/s] 15%|â–ˆâ–Œ        | 481/3143 [03:05<18:17,  2.43it/s] 15%|â–ˆâ–Œ        | 487/3145 [03:18<18:10,  2.44it/s] 15%|â–ˆâ–Œ        | 476/3145 [03:05<14:57,  2.97it/s] 15%|â–ˆâ–Œ        | 479/3145 [03:08<17:31,  2.54it/s] 15%|â–ˆâ–Œ        | 477/3145 [03:05<13:59,  3.18it/s] 16%|â–ˆâ–Œ        | 488/3145 [03:18<18:09,  2.44it/s] 15%|â–ˆâ–Œ        | 482/3143 [03:05<20:30,  2.16it/s] 15%|â–ˆâ–Œ        | 478/3145 [03:05<14:18,  3.11it/s] 15%|â–ˆâ–Œ        | 480/3145 [03:08<18:19,  2.42it/s] 16%|â–ˆâ–Œ        | 489/3145 [03:18<17:58,  2.46it/s] 15%|â–ˆâ–Œ        | 483/3143 [03:06<19:36,  2.26it/s] 15%|â–ˆâ–Œ        | 481/3145 [03:08<17:42,  2.51it/s] 15%|â–ˆâ–Œ        | 479/3145 [03:06<15:21,  2.89it/s] 16%|â–ˆâ–Œ        | 490/3145 [03:19<17:06,  2.59it/s] 15%|â–ˆâ–Œ        | 484/3143 [03:06<19:30,  2.27it/s] 15%|â–ˆâ–Œ        | 480/3145 [03:06<15:56,  2.79it/s] 15%|â–ˆâ–Œ        | 482/3145 [03:09<18:03,  2.46it/s] 16%|â–ˆâ–Œ        | 491/3145 [03:19<17:26,  2.54it/s] 15%|â–ˆâ–Œ        | 485/3143 [03:07<17:40,  2.51it/s] 15%|â–ˆâ–Œ        | 483/3145 [03:09<17:45,  2.50it/s] 15%|â–ˆâ–Œ        | 481/3145 [03:07<16:49,  2.64it/s] 16%|â–ˆâ–Œ        | 492/3145 [03:20<17:17,  2.56it/s] 15%|â–ˆâ–Œ        | 486/3143 [03:07<17:56,  2.47it/s] 15%|â–ˆâ–Œ        | 482/3145 [03:07<17:47,  2.49it/s] 15%|â–ˆâ–Œ        | 484/3145 [03:10<18:48,  2.36it/s] 16%|â–ˆâ–Œ        | 493/3145 [03:20<17:39,  2.50it/s] 15%|â–ˆâ–Œ        | 487/3143 [03:07<17:53,  2.47it/s] 16%|â–ˆâ–Œ        | 494/3145 [03:20<17:34,  2.51it/s] 15%|â–ˆâ–Œ        | 485/3145 [03:10<18:49,  2.36it/s] 16%|â–ˆâ–Œ        | 488/3143 [03:08<18:15,  2.42it/s] 15%|â–ˆâ–Œ        | 483/3145 [03:08<20:30,  2.16it/s] 16%|â–ˆâ–Œ        | 489/3143 [03:08<16:07,  2.74it/s] 16%|â–ˆâ–Œ        | 495/3145 [03:21<17:29,  2.52it/s] 15%|â–ˆâ–Œ        | 486/3145 [03:11<19:10,  2.31it/s] 15%|â–ˆâ–Œ        | 484/3145 [03:08<19:16,  2.30it/s] 16%|â–ˆâ–Œ        | 496/3145 [03:21<17:46,  2.48it/s] 16%|â–ˆâ–Œ        | 490/3143 [03:08<16:56,  2.61it/s] 15%|â–ˆâ–Œ        | 487/3145 [03:11<19:07,  2.32it/s] 15%|â–ˆâ–Œ        | 485/3145 [03:09<18:31,  2.39it/s] 16%|â–ˆâ–Œ        | 497/3145 [03:22<17:39,  2.50it/s] 16%|â–ˆâ–Œ        | 491/3143 [03:09<17:43,  2.49it/s] 15%|â–ˆâ–Œ        | 486/3145 [03:09<18:07,  2.45it/s] 16%|â–ˆâ–Œ        | 488/3145 [03:11<19:17,  2.30it/s] 16%|â–ˆâ–Œ        | 498/3145 [03:22<17:30,  2.52it/s] 16%|â–ˆâ–Œ        | 489/3145 [03:12<18:51,  2.35it/s] 16%|â–ˆâ–Œ        | 492/3143 [03:09<18:56,  2.33it/s] 16%|â–ˆâ–Œ        | 499/3145 [03:22<15:20,  2.87it/s] 15%|â–ˆâ–Œ        | 487/3145 [03:10<21:33,  2.06it/s] 16%|â–ˆâ–Œ        | 500/3145 [03:23<15:21,  2.87it/s] 16%|â–ˆâ–Œ        | 493/3143 [03:10<19:29,  2.27it/s] 16%|â–ˆâ–Œ        | 490/3145 [03:12<19:59,  2.21it/s] 16%|â–ˆâ–Œ        | 488/3145 [03:10<20:04,  2.21it/s] 16%|â–ˆâ–Œ        | 494/3143 [03:10<18:16,  2.42it/s] 16%|â–ˆâ–Œ        | 501/3145 [03:23<16:34,  2.66it/s] 16%|â–ˆâ–Œ        | 491/3145 [03:13<19:10,  2.31it/s] 16%|â–ˆâ–Œ        | 489/3145 [03:10<19:16,  2.30it/s] 16%|â–ˆâ–Œ        | 495/3143 [03:11<18:08,  2.43it/s] 16%|â–ˆâ–Œ        | 502/3145 [03:23<16:43,  2.63it/s] 16%|â–ˆâ–Œ        | 492/3145 [03:13<18:38,  2.37it/s] 16%|â–ˆâ–Œ        | 490/3145 [03:11<19:29,  2.27it/s] 16%|â–ˆâ–Œ        | 496/3143 [03:11<17:47,  2.48it/s] 16%|â–ˆâ–Œ        | 503/3145 [03:24<16:37,  2.65it/s] 16%|â–ˆâ–Œ        | 493/3145 [03:14<18:22,  2.41it/s] 16%|â–ˆâ–Œ        | 491/3145 [03:11<18:25,  2.40it/s] 16%|â–ˆâ–Œ        | 497/3143 [03:11<16:18,  2.70it/s] 16%|â–ˆâ–Œ        | 504/3145 [03:24<16:46,  2.63it/s] 16%|â–ˆâ–Œ        | 494/3145 [03:14<17:52,  2.47it/s] 16%|â–ˆâ–Œ        | 492/3145 [03:12<17:36,  2.51it/s] 16%|â–ˆâ–Œ        | 498/3143 [03:12<15:22,  2.87it/s] 16%|â–ˆâ–Œ        | 505/3145 [03:25<16:24,  2.68it/s] 16%|â–ˆâ–Œ        | 495/3145 [03:14<18:05,  2.44it/s] 16%|â–ˆâ–Œ        | 493/3145 [03:12<17:50,  2.48it/s] 16%|â–ˆâ–Œ        | 499/3143 [03:12<17:38,  2.50it/s] 16%|â–ˆâ–Œ        | 506/3145 [03:25<16:58,  2.59it/s] 16%|â–ˆâ–Œ        | 496/3145 [03:15<17:40,  2.50it/s] 16%|â–ˆâ–Œ        | 494/3145 [03:12<17:56,  2.46it/s] 16%|â–ˆâ–Œ        | 500/3143 [03:12<17:34,  2.51it/s] 16%|â–ˆâ–Œ        | 507/3145 [03:25<16:21,  2.69it/s] 16%|â–ˆâ–Œ        | 497/3145 [03:15<17:25,  2.53it/s] 16%|â–ˆâ–Œ        | 495/3145 [03:13<17:36,  2.51it/s] 16%|â–ˆâ–Œ        | 508/3145 [03:26<16:30,  2.66it/s] 16%|â–ˆâ–Œ        | 501/3143 [03:13<18:17,  2.41it/s] 16%|â–ˆâ–Œ        | 498/3145 [03:16<17:25,  2.53it/s] 16%|â–ˆâ–Œ        | 496/3145 [03:13<17:54,  2.47it/s] 16%|â–ˆâ–Œ        | 502/3143 [03:13<17:47,  2.47it/s] 16%|â–ˆâ–Œ        | 509/3145 [03:26<17:56,  2.45it/s] 16%|â–ˆâ–Œ        | 499/3145 [03:16<17:31,  2.52it/s] 16%|â–ˆâ–Œ        | 497/3145 [03:13<17:13,  2.56it/s] 16%|â–ˆâ–Œ        | 503/3143 [03:14<17:33,  2.51it/s] 16%|â–ˆâ–Œ        | 510/3145 [03:27<18:24,  2.39it/s] 16%|â–ˆâ–Œ        | 500/3145 [03:16<17:27,  2.53it/s] 16%|â–ˆâ–Œ        | 498/3145 [03:14<17:34,  2.51it/s] 16%|â–ˆâ–Œ        | 504/3143 [03:14<18:28,  2.38it/s] 16%|â–ˆâ–Œ        | 511/3145 [03:27<18:24,  2.38it/s] 16%|â–ˆâ–Œ        | 501/3145 [03:17<18:01,  2.45it/s] 16%|â–ˆâ–Œ        | 499/3145 [03:14<17:32,  2.51it/s] 16%|â–ˆâ–Œ        | 505/3143 [03:15<18:01,  2.44it/s] 16%|â–ˆâ–‹        | 512/3145 [03:27<17:46,  2.47it/s] 16%|â–ˆâ–Œ        | 500/3145 [03:15<17:08,  2.57it/s] 16%|â–ˆâ–Œ        | 502/3145 [03:17<19:58,  2.21it/s] 16%|â–ˆâ–‹        | 513/3145 [03:28<17:23,  2.52it/s] 16%|â–ˆâ–Œ        | 506/3143 [03:15<19:09,  2.29it/s] 16%|â–ˆâ–Œ        | 503/3145 [03:18<18:32,  2.38it/s] 16%|â–ˆâ–Œ        | 501/3145 [03:15<18:49,  2.34it/s] 16%|â–ˆâ–‹        | 514/3145 [03:28<17:23,  2.52it/s] 16%|â–ˆâ–Œ        | 507/3143 [03:15<17:59,  2.44it/s] 16%|â–ˆâ–Œ        | 504/3145 [03:18<18:18,  2.40it/s] 16%|â–ˆâ–Œ        | 502/3145 [03:16<18:54,  2.33it/s] 16%|â–ˆâ–‹        | 515/3145 [03:29<17:31,  2.50it/s] 16%|â–ˆâ–Œ        | 508/3143 [03:16<17:34,  2.50it/s] 16%|â–ˆâ–Œ        | 505/3145 [03:18<18:11,  2.42it/s] 16%|â–ˆâ–Œ        | 503/3145 [03:16<18:18,  2.40it/s] 16%|â–ˆâ–Œ        | 509/3143 [03:16<17:32,  2.50it/s] 16%|â–ˆâ–‹        | 516/3145 [03:29<17:58,  2.44it/s] 16%|â–ˆâ–Œ        | 506/3145 [03:19<17:57,  2.45it/s] 16%|â–ˆâ–Œ        | 504/3145 [03:16<17:43,  2.48it/s] 16%|â–ˆâ–Œ        | 510/3143 [03:17<17:44,  2.47it/s] 16%|â–ˆâ–‹        | 517/3145 [03:29<17:37,  2.49it/s] 16%|â–ˆâ–Œ        | 507/3145 [03:19<18:19,  2.40it/s] 16%|â–ˆâ–Œ        | 505/3145 [03:17<18:26,  2.39it/s] 16%|â–ˆâ–‹        | 518/3145 [03:30<17:27,  2.51it/s] 16%|â–ˆâ–‹        | 511/3143 [03:17<18:19,  2.39it/s] 16%|â–ˆâ–Œ        | 508/3145 [03:20<18:00,  2.44it/s] 16%|â–ˆâ–Œ        | 506/3145 [03:17<18:22,  2.39it/s] 17%|â–ˆâ–‹        | 519/3145 [03:30<16:25,  2.66it/s] 16%|â–ˆâ–‹        | 512/3143 [03:17<17:52,  2.45it/s] 16%|â–ˆâ–Œ        | 507/3145 [03:18<16:47,  2.62it/s] 16%|â–ˆâ–Œ        | 509/3145 [03:20<18:05,  2.43it/s] 17%|â–ˆâ–‹        | 520/3145 [03:30<16:09,  2.71it/s] 16%|â–ˆâ–‹        | 513/3143 [03:18<19:54,  2.20it/s] 16%|â–ˆâ–Œ        | 508/3145 [03:18<17:22,  2.53it/s] 17%|â–ˆâ–‹        | 521/3145 [03:31<15:52,  2.75it/s] 16%|â–ˆâ–Œ        | 510/3145 [03:21<18:23,  2.39it/s] 16%|â–ˆâ–‹        | 514/3143 [03:18<19:14,  2.28it/s] 17%|â–ˆâ–‹        | 522/3145 [03:31<16:44,  2.61it/s] 16%|â–ˆâ–Œ        | 511/3145 [03:21<18:43,  2.35it/s] 16%|â–ˆâ–Œ        | 509/3145 [03:19<19:47,  2.22it/s] 17%|â–ˆâ–‹        | 523/3145 [03:31<14:55,  2.93it/s] 16%|â–ˆâ–‹        | 512/3145 [03:21<17:58,  2.44it/s] 16%|â–ˆâ–‹        | 515/3143 [03:19<20:08,  2.18it/s] 17%|â–ˆâ–‹        | 524/3145 [03:32<13:59,  3.12it/s] 16%|â–ˆâ–Œ        | 510/3145 [03:19<19:25,  2.26it/s] 16%|â–ˆâ–‹        | 516/3143 [03:19<17:22,  2.52it/s] 17%|â–ˆâ–‹        | 525/3145 [03:32<14:55,  2.93it/s] 16%|â–ˆâ–‹        | 513/3145 [03:22<20:11,  2.17it/s] 16%|â–ˆâ–Œ        | 511/3145 [03:19<19:11,  2.29it/s] 16%|â–ˆâ–‹        | 517/3143 [03:20<17:52,  2.45it/s] 17%|â–ˆâ–‹        | 526/3145 [03:33<15:22,  2.84it/s] 16%|â–ˆâ–‹        | 512/3145 [03:20<18:58,  2.31it/s] 16%|â–ˆâ–‹        | 514/3145 [03:22<19:44,  2.22it/s] 16%|â–ˆâ–‹        | 518/3143 [03:20<18:16,  2.39it/s] 16%|â–ˆâ–‹        | 515/3145 [03:23<17:01,  2.57it/s] 17%|â–ˆâ–‹        | 527/3145 [03:33<16:25,  2.66it/s] 16%|â–ˆâ–‹        | 513/3145 [03:20<18:02,  2.43it/s] 17%|â–ˆâ–‹        | 519/3143 [03:20<17:16,  2.53it/s] 16%|â–ˆâ–‹        | 516/3145 [03:23<17:08,  2.56it/s] 17%|â–ˆâ–‹        | 528/3145 [03:33<16:54,  2.58it/s] 16%|â–ˆâ–‹        | 514/3145 [03:21<17:20,  2.53it/s] 17%|â–ˆâ–‹        | 520/3143 [03:21<17:19,  2.52it/s] 16%|â–ˆâ–‹        | 517/3145 [03:23<16:07,  2.72it/s] 16%|â–ˆâ–‹        | 515/3145 [03:21<16:58,  2.58it/s] 17%|â–ˆâ–‹        | 529/3145 [03:34<17:27,  2.50it/s] 17%|â–ˆâ–‹        | 521/3143 [03:21<17:07,  2.55it/s] 16%|â–ˆâ–‹        | 518/3145 [03:24<16:20,  2.68it/s] 16%|â–ˆâ–‹        | 516/3145 [03:21<16:36,  2.64it/s] 17%|â–ˆâ–‹        | 530/3145 [03:34<17:26,  2.50it/s] 17%|â–ˆâ–‹        | 522/3143 [03:22<17:06,  2.55it/s] 17%|â–ˆâ–‹        | 519/3145 [03:24<16:44,  2.62it/s] 16%|â–ˆâ–‹        | 517/3145 [03:22<16:53,  2.59it/s] 17%|â–ˆâ–‹        | 523/3143 [03:22<16:42,  2.61it/s] 17%|â–ˆâ–‹        | 531/3145 [03:35<19:12,  2.27it/s] 16%|â–ˆâ–‹        | 518/3145 [03:22<16:35,  2.64it/s] 17%|â–ˆâ–‹        | 520/3145 [03:25<18:31,  2.36it/s] 17%|â–ˆâ–‹        | 524/3143 [03:22<17:17,  2.52it/s] 17%|â–ˆâ–‹        | 532/3145 [03:35<18:41,  2.33it/s] 17%|â–ˆâ–‹        | 519/3145 [03:22<16:53,  2.59it/s] 17%|â–ˆâ–‹        | 521/3145 [03:25<19:00,  2.30it/s] 17%|â–ˆâ–‹        | 525/3143 [03:23<16:53,  2.58it/s] 17%|â–ˆâ–‹        | 533/3145 [03:36<18:49,  2.31it/s] 17%|â–ˆâ–‹        | 522/3145 [03:25<18:20,  2.38it/s] 17%|â–ˆâ–‹        | 520/3145 [03:23<18:33,  2.36it/s] 17%|â–ˆâ–‹        | 526/3143 [03:23<16:30,  2.64it/s] 17%|â–ˆâ–‹        | 534/3145 [03:36<19:18,  2.25it/s] 17%|â–ˆâ–‹        | 523/3145 [03:26<18:10,  2.40it/s] 17%|â–ˆâ–‹        | 521/3145 [03:23<18:29,  2.37it/s] 17%|â–ˆâ–‹        | 527/3143 [03:23<16:40,  2.62it/s] 17%|â–ˆâ–‹        | 535/3145 [03:36<18:16,  2.38it/s] 17%|â–ˆâ–‹        | 524/3145 [03:26<18:12,  2.40it/s] 17%|â–ˆâ–‹        | 522/3145 [03:24<18:56,  2.31it/s] 17%|â–ˆâ–‹        | 536/3145 [03:37<17:29,  2.49it/s] 17%|â–ˆâ–‹        | 528/3143 [03:24<19:16,  2.26it/s] 17%|â–ˆâ–‹        | 525/3145 [03:27<17:29,  2.50it/s] 17%|â–ˆâ–‹        | 523/3145 [03:24<18:07,  2.41it/s] 17%|â–ˆâ–‹        | 537/3145 [03:37<17:28,  2.49it/s] 17%|â–ˆâ–‹        | 529/3143 [03:24<18:41,  2.33it/s] 17%|â–ˆâ–‹        | 526/3145 [03:27<17:12,  2.54it/s] 17%|â–ˆâ–‹        | 524/3145 [03:25<17:39,  2.47it/s] 17%|â–ˆâ–‹        | 530/3143 [03:25<18:07,  2.40it/s] 17%|â–ˆâ–‹        | 538/3145 [03:38<18:06,  2.40it/s] 17%|â–ˆâ–‹        | 527/3145 [03:27<17:05,  2.55it/s] 17%|â–ˆâ–‹        | 525/3145 [03:25<17:38,  2.48it/s] 17%|â–ˆâ–‹        | 531/3143 [03:25<17:23,  2.50it/s] 17%|â–ˆâ–‹        | 539/3145 [03:38<17:27,  2.49it/s] 17%|â–ˆâ–‹        | 528/3145 [03:28<17:04,  2.55it/s] 17%|â–ˆâ–‹        | 526/3145 [03:25<18:05,  2.41it/s] 17%|â–ˆâ–‹        | 540/3145 [03:38<16:49,  2.58it/s] 17%|â–ˆâ–‹        | 532/3143 [03:26<17:15,  2.52it/s] 17%|â–ˆâ–‹        | 529/3145 [03:28<17:00,  2.56it/s] 17%|â–ˆâ–‹        | 541/3145 [03:39<16:08,  2.69it/s] 17%|â–ˆâ–‹        | 527/3145 [03:26<18:31,  2.35it/s] 17%|â–ˆâ–‹        | 533/3143 [03:26<17:11,  2.53it/s] 17%|â–ˆâ–‹        | 530/3145 [03:29<16:56,  2.57it/s] 17%|â–ˆâ–‹        | 528/3145 [03:26<17:37,  2.47it/s] 17%|â–ˆâ–‹        | 542/3145 [03:39<16:17,  2.66it/s] 17%|â–ˆâ–‹        | 534/3143 [03:26<16:26,  2.65it/s] 17%|â–ˆâ–‹        | 531/3145 [03:29<17:18,  2.52it/s] 17%|â–ˆâ–‹        | 535/3143 [03:27<15:56,  2.73it/s] 17%|â–ˆâ–‹        | 529/3145 [03:27<17:22,  2.51it/s] 17%|â–ˆâ–‹        | 543/3145 [03:40<17:31,  2.48it/s] 17%|â–ˆâ–‹        | 536/3143 [03:27<14:05,  3.09it/s] 17%|â–ˆâ–‹        | 532/3145 [03:29<17:13,  2.53it/s] 17%|â–ˆâ–‹        | 530/3145 [03:27<17:13,  2.53it/s] 17%|â–ˆâ–‹        | 544/3145 [03:40<16:43,  2.59it/s] 17%|â–ˆâ–‹        | 537/3143 [03:27<14:17,  3.04it/s] 17%|â–ˆâ–‹        | 533/3145 [03:30<16:32,  2.63it/s] 17%|â–ˆâ–‹        | 545/3145 [03:40<17:04,  2.54it/s] 17%|â–ˆâ–‹        | 531/3145 [03:28<18:33,  2.35it/s] 17%|â–ˆâ–‹        | 538/3143 [03:28<14:37,  2.97it/s] 17%|â–ˆâ–‹        | 534/3145 [03:30<17:00,  2.56it/s] 17%|â–ˆâ–‹        | 546/3145 [03:41<16:06,  2.69it/s] 17%|â–ˆâ–‹        | 532/3145 [03:28<17:35,  2.48it/s] 17%|â–ˆâ–‹        | 539/3143 [03:28<14:40,  2.96it/s] 17%|â–ˆâ–‹        | 535/3145 [03:31<16:34,  2.62it/s] 17%|â–ˆâ–‹        | 540/3143 [03:28<13:08,  3.30it/s] 17%|â–ˆâ–‹        | 533/3145 [03:28<17:20,  2.51it/s] 17%|â–ˆâ–‹        | 547/3145 [03:41<17:21,  2.49it/s] 17%|â–ˆâ–‹        | 536/3145 [03:31<15:56,  2.73it/s] 17%|â–ˆâ–‹        | 541/3143 [03:29<14:13,  3.05it/s] 17%|â–ˆâ–‹        | 548/3145 [03:41<16:26,  2.63it/s] 17%|â–ˆâ–‹        | 534/3145 [03:29<16:47,  2.59it/s] 17%|â–ˆâ–‹        | 537/3145 [03:31<16:14,  2.68it/s] 17%|â–ˆâ–‹        | 542/3143 [03:29<14:17,  3.03it/s] 17%|â–ˆâ–‹        | 535/3145 [03:29<16:31,  2.63it/s] 17%|â–ˆâ–‹        | 549/3145 [03:42<16:40,  2.60it/s] 17%|â–ˆâ–‹        | 538/3145 [03:32<16:49,  2.58it/s] 17%|â–ˆâ–‹        | 543/3143 [03:29<14:33,  2.98it/s] 17%|â–ˆâ–‹        | 536/3145 [03:29<16:12,  2.68it/s] 17%|â–ˆâ–‹        | 550/3145 [03:42<16:22,  2.64it/s] 17%|â–ˆâ–‹        | 539/3145 [03:32<16:50,  2.58it/s] 17%|â–ˆâ–‹        | 544/3143 [03:30<14:48,  2.92it/s] 17%|â–ˆâ–‹        | 537/3145 [03:30<15:59,  2.72it/s] 18%|â–ˆâ–Š        | 551/3145 [03:43<16:57,  2.55it/s] 17%|â–ˆâ–‹        | 545/3143 [03:30<15:06,  2.86it/s] 17%|â–ˆâ–‹        | 540/3145 [03:32<17:01,  2.55it/s] 17%|â–ˆâ–‹        | 538/3145 [03:30<15:49,  2.74it/s] 18%|â–ˆâ–Š        | 552/3145 [03:43<16:54,  2.55it/s] 17%|â–ˆâ–‹        | 546/3143 [03:30<15:36,  2.77it/s] 17%|â–ˆâ–‹        | 541/3145 [03:33<16:34,  2.62it/s] 17%|â–ˆâ–‹        | 539/3145 [03:30<15:32,  2.79it/s] 17%|â–ˆâ–‹        | 542/3145 [03:33<14:36,  2.97it/s] 18%|â–ˆâ–Š        | 553/3145 [03:43<17:18,  2.50it/s] 17%|â–ˆâ–‹        | 540/3145 [03:31<15:08,  2.87it/s] 17%|â–ˆâ–‹        | 547/3143 [03:31<17:44,  2.44it/s] 17%|â–ˆâ–‹        | 543/3145 [03:33<14:54,  2.91it/s] 18%|â–ˆâ–Š        | 554/3145 [03:44<17:05,  2.53it/s] 17%|â–ˆâ–‹        | 541/3145 [03:31<14:42,  2.95it/s] 17%|â–ˆâ–‹        | 548/3143 [03:31<19:08,  2.26it/s] 17%|â–ˆâ–‹        | 544/3145 [03:34<16:18,  2.66it/s] 18%|â–ˆâ–Š        | 555/3145 [03:44<17:10,  2.51it/s] 17%|â–ˆâ–‹        | 542/3145 [03:31<15:18,  2.83it/s] 18%|â–ˆâ–Š        | 556/3145 [03:44<15:56,  2.71it/s] 17%|â–ˆâ–‹        | 545/3145 [03:34<16:38,  2.61it/s] 17%|â–ˆâ–‹        | 543/3145 [03:32<16:07,  2.69it/s] 17%|â–ˆâ–‹        | 549/3143 [03:32<20:50,  2.07it/s] 17%|â–ˆâ–‹        | 546/3145 [03:35<15:57,  2.72it/s] 18%|â–ˆâ–Š        | 557/3145 [03:45<16:14,  2.66it/s] 17%|â–ˆâ–‹        | 544/3145 [03:32<15:35,  2.78it/s] 17%|â–ˆâ–‹        | 550/3143 [03:32<20:08,  2.15it/s] 17%|â–ˆâ–‹        | 547/3145 [03:35<15:50,  2.73it/s] 18%|â–ˆâ–Š        | 558/3145 [03:45<16:30,  2.61it/s] 17%|â–ˆâ–‹        | 545/3145 [03:33<15:52,  2.73it/s] 18%|â–ˆâ–Š        | 551/3143 [03:33<18:38,  2.32it/s] 18%|â–ˆâ–Š        | 559/3145 [03:46<15:26,  2.79it/s] 17%|â–ˆâ–‹        | 548/3145 [03:35<15:51,  2.73it/s] 17%|â–ˆâ–‹        | 546/3145 [03:33<15:16,  2.84it/s] 18%|â–ˆâ–Š        | 560/3145 [03:46<15:35,  2.76it/s] 18%|â–ˆâ–Š        | 552/3143 [03:33<19:12,  2.25it/s] 17%|â–ˆâ–‹        | 549/3145 [03:36<16:10,  2.67it/s] 17%|â–ˆâ–‹        | 547/3145 [03:33<15:39,  2.77it/s] 18%|â–ˆâ–Š        | 553/3143 [03:34<18:23,  2.35it/s] 17%|â–ˆâ–‹        | 550/3145 [03:36<16:47,  2.58it/s] 17%|â–ˆâ–‹        | 548/3145 [03:34<15:39,  2.76it/s] 18%|â–ˆâ–Š        | 561/3145 [03:46<17:16,  2.49it/s] 18%|â–ˆâ–Š        | 554/3143 [03:34<17:30,  2.46it/s] 18%|â–ˆâ–Š        | 551/3145 [03:36<16:33,  2.61it/s] 17%|â–ˆâ–‹        | 549/3145 [03:34<16:00,  2.70it/s] 18%|â–ˆâ–Š        | 562/3145 [03:47<16:54,  2.55it/s] 18%|â–ˆâ–Š        | 555/3143 [03:34<16:55,  2.55it/s] 18%|â–ˆâ–Š        | 552/3145 [03:37<15:55,  2.71it/s] 17%|â–ˆâ–‹        | 550/3145 [03:34<16:10,  2.67it/s] 18%|â–ˆâ–Š        | 563/3145 [03:47<16:36,  2.59it/s] 18%|â–ˆâ–Š        | 556/3143 [03:35<16:31,  2.61it/s] 18%|â–ˆâ–Š        | 553/3145 [03:37<15:29,  2.79it/s] 18%|â–ˆâ–Š        | 551/3145 [03:35<16:05,  2.69it/s] 18%|â–ˆâ–Š        | 564/3145 [03:48<16:37,  2.59it/s] 18%|â–ˆâ–Š        | 557/3143 [03:35<14:24,  2.99it/s] 18%|â–ˆâ–Š        | 554/3145 [03:38<15:32,  2.78it/s] 18%|â–ˆâ–Š        | 552/3145 [03:35<15:44,  2.75it/s] 18%|â–ˆâ–Š        | 565/3145 [03:48<17:03,  2.52it/s] 18%|â–ˆâ–Š        | 558/3143 [03:35<15:06,  2.85it/s] 18%|â–ˆâ–Š        | 555/3145 [03:38<15:32,  2.78it/s] 18%|â–ˆâ–Š        | 553/3145 [03:35<16:02,  2.69it/s] 18%|â–ˆâ–Š        | 559/3143 [03:36<15:11,  2.83it/s] 18%|â–ˆâ–Š        | 566/3145 [03:48<17:20,  2.48it/s] 18%|â–ˆâ–Š        | 556/3145 [03:38<16:47,  2.57it/s] 18%|â–ˆâ–Š        | 554/3145 [03:36<16:11,  2.67it/s] 18%|â–ˆâ–Š        | 560/3143 [03:36<15:20,  2.81it/s] 18%|â–ˆâ–Š        | 567/3145 [03:49<17:07,  2.51it/s] 18%|â–ˆâ–Š        | 555/3145 [03:36<14:04,  3.07it/s] 18%|â–ˆâ–Š        | 557/3145 [03:39<17:09,  2.51it/s] 18%|â–ˆâ–Š        | 561/3143 [03:36<15:51,  2.71it/s] 18%|â–ˆâ–Š        | 556/3145 [03:36<14:00,  3.08it/s] 18%|â–ˆâ–Š        | 568/3145 [03:49<17:35,  2.44it/s] 18%|â–ˆâ–Š        | 558/3145 [03:39<17:48,  2.42it/s] 18%|â–ˆâ–Š        | 562/3143 [03:37<15:35,  2.76it/s] 18%|â–ˆâ–Š        | 557/3145 [03:37<14:26,  2.99it/s] 18%|â–ˆâ–Š        | 569/3145 [03:50<16:54,  2.54it/s] 18%|â–ˆâ–Š        | 570/3145 [03:50<14:56,  2.87it/s] 18%|â–ˆâ–Š        | 559/3145 [03:40<17:01,  2.53it/s] 18%|â–ˆâ–Š        | 558/3145 [03:37<14:39,  2.94it/s] 18%|â–ˆâ–Š        | 563/3143 [03:37<17:46,  2.42it/s] 18%|â–ˆâ–Š        | 571/3145 [03:50<14:43,  2.91it/s] 18%|â–ˆâ–Š        | 559/3145 [03:37<14:27,  2.98it/s] 18%|â–ˆâ–Š        | 560/3145 [03:40<17:30,  2.46it/s] 18%|â–ˆâ–Š        | 564/3143 [03:38<17:28,  2.46it/s] 18%|â–ˆâ–Š        | 572/3145 [03:51<15:19,  2.80it/s] 18%|â–ˆâ–Š        | 560/3145 [03:38<14:45,  2.92it/s] 18%|â–ˆâ–Š        | 561/3145 [03:40<17:02,  2.53it/s] 18%|â–ˆâ–Š        | 565/3143 [03:38<16:51,  2.55it/s] 18%|â–ˆâ–Š        | 573/3145 [03:51<15:19,  2.80it/s] 18%|â–ˆâ–Š        | 561/3145 [03:38<14:30,  2.97it/s] 18%|â–ˆâ–Š        | 562/3145 [03:41<16:56,  2.54it/s] 18%|â–ˆâ–Š        | 566/3143 [03:38<16:05,  2.67it/s] 18%|â–ˆâ–Š        | 562/3145 [03:38<13:47,  3.12it/s] 18%|â–ˆâ–Š        | 574/3145 [03:51<15:26,  2.78it/s] 18%|â–ˆâ–Š        | 563/3145 [03:41<16:52,  2.55it/s] 18%|â–ˆâ–Š        | 567/3143 [03:39<15:53,  2.70it/s] 18%|â–ˆâ–Š        | 563/3145 [03:39<14:17,  3.01it/s] 18%|â–ˆâ–Š        | 575/3145 [03:52<15:00,  2.85it/s] 18%|â–ˆâ–Š        | 564/3145 [03:41<15:33,  2.76it/s] 18%|â–ˆâ–Š        | 568/3143 [03:39<16:08,  2.66it/s] 18%|â–ˆâ–Š        | 564/3145 [03:39<14:59,  2.87it/s] 18%|â–ˆâ–Š        | 576/3145 [03:52<14:57,  2.86it/s] 18%|â–ˆâ–Š        | 565/3145 [03:42<14:32,  2.96it/s] 18%|â–ˆâ–Š        | 569/3143 [03:39<15:37,  2.75it/s] 18%|â–ˆâ–Š        | 566/3145 [03:42<13:44,  3.13it/s] 18%|â–ˆâ–Š        | 565/3145 [03:40<15:14,  2.82it/s] 18%|â–ˆâ–Š        | 577/3145 [03:52<15:33,  2.75it/s] 18%|â–ˆâ–Š        | 570/3143 [03:40<15:29,  2.77it/s] 18%|â–ˆâ–Š        | 567/3145 [03:42<15:00,  2.86it/s] 18%|â–ˆâ–Š        | 566/3145 [03:40<15:41,  2.74it/s] 18%|â–ˆâ–Š        | 578/3145 [03:53<15:46,  2.71it/s] 18%|â–ˆâ–Š        | 571/3143 [03:40<15:07,  2.84it/s] 18%|â–ˆâ–Š        | 568/3145 [03:43<15:06,  2.84it/s] 18%|â–ˆâ–Š        | 579/3145 [03:53<15:36,  2.74it/s] 18%|â–ˆâ–Š        | 567/3145 [03:40<16:05,  2.67it/s] 18%|â–ˆâ–Š        | 572/3143 [03:40<15:13,  2.81it/s] 18%|â–ˆâ–Š        | 569/3145 [03:43<15:11,  2.82it/s] 18%|â–ˆâ–Š        | 580/3145 [03:53<15:55,  2.69it/s] 18%|â–ˆâ–Š        | 568/3145 [03:41<17:05,  2.51it/s] 18%|â–ˆâ–Š        | 573/3143 [03:41<15:34,  2.75it/s] 18%|â–ˆâ–Š        | 570/3145 [03:44<16:21,  2.62it/s] 18%|â–ˆâ–Š        | 581/3145 [03:54<16:30,  2.59it/s] 18%|â–ˆâ–Š        | 569/3145 [03:41<17:27,  2.46it/s] 18%|â–ˆâ–Š        | 574/3143 [03:41<15:35,  2.75it/s] 19%|â–ˆâ–Š        | 582/3145 [03:54<16:03,  2.66it/s] 18%|â–ˆâ–Š        | 571/3145 [03:44<17:14,  2.49it/s] 18%|â–ˆâ–Š        | 570/3145 [03:42<16:27,  2.61it/s] 18%|â–ˆâ–Š        | 575/3143 [03:42<15:54,  2.69it/s] 19%|â–ˆâ–Š        | 583/3145 [03:55<15:11,  2.81it/s] 18%|â–ˆâ–Š        | 572/3145 [03:44<17:08,  2.50it/s] 18%|â–ˆâ–Š        | 571/3145 [03:42<17:08,  2.50it/s] 18%|â–ˆâ–Š        | 576/3143 [03:42<15:54,  2.69it/s] 19%|â–ˆâ–Š        | 584/3145 [03:55<14:35,  2.93it/s] 18%|â–ˆâ–Š        | 577/3143 [03:42<15:01,  2.84it/s] 18%|â–ˆâ–Š        | 573/3145 [03:45<16:38,  2.58it/s] 18%|â–ˆâ–Š        | 572/3145 [03:42<16:08,  2.66it/s] 19%|â–ˆâ–Š        | 585/3145 [03:55<15:29,  2.75it/s] 18%|â–ˆâ–Š        | 574/3145 [03:45<16:10,  2.65it/s] 18%|â–ˆâ–Š        | 573/3145 [03:43<15:50,  2.71it/s] 18%|â–ˆâ–Š        | 578/3143 [03:43<15:21,  2.78it/s] 19%|â–ˆâ–Š        | 586/3145 [03:56<16:10,  2.64it/s] 18%|â–ˆâ–Š        | 575/3145 [03:45<15:45,  2.72it/s] 18%|â–ˆâ–Š        | 574/3145 [03:43<15:49,  2.71it/s] 18%|â–ˆâ–Š        | 579/3143 [03:43<15:50,  2.70it/s] 19%|â–ˆâ–Š        | 587/3145 [03:56<16:41,  2.55it/s] 18%|â–ˆâ–Š        | 576/3145 [03:46<15:35,  2.75it/s] 18%|â–ˆâ–Š        | 575/3145 [03:43<15:56,  2.69it/s] 18%|â–ˆâ–Š        | 580/3143 [03:43<16:35,  2.57it/s] 18%|â–ˆâ–Š        | 576/3145 [03:44<15:24,  2.78it/s] 19%|â–ˆâ–Š        | 588/3145 [03:57<17:05,  2.49it/s] 18%|â–ˆâ–Š        | 577/3145 [03:46<16:28,  2.60it/s] 18%|â–ˆâ–Š        | 581/3143 [03:44<17:03,  2.50it/s] 18%|â–ˆâ–Š        | 577/3145 [03:44<15:22,  2.78it/s] 19%|â–ˆâ–Š        | 589/3145 [03:57<17:04,  2.49it/s] 18%|â–ˆâ–Š        | 578/3145 [03:47<16:35,  2.58it/s] 19%|â–ˆâ–Š        | 582/3143 [03:44<16:52,  2.53it/s] 18%|â–ˆâ–Š        | 578/3145 [03:44<15:43,  2.72it/s] 19%|â–ˆâ–‰        | 590/3145 [03:57<16:10,  2.63it/s] 18%|â–ˆâ–Š        | 579/3145 [03:47<16:07,  2.65it/s] 19%|â–ˆâ–Š        | 583/3143 [03:45<16:22,  2.61it/s] 18%|â–ˆâ–Š        | 579/3145 [03:45<15:16,  2.80it/s] 19%|â–ˆâ–‰        | 591/3145 [03:58<15:33,  2.74it/s] 18%|â–ˆâ–Š        | 580/3145 [03:47<16:13,  2.63it/s] 19%|â–ˆâ–Š        | 584/3143 [03:45<15:58,  2.67it/s] 18%|â–ˆâ–Š        | 580/3145 [03:45<15:37,  2.73it/s] 19%|â–ˆâ–‰        | 592/3145 [03:58<15:32,  2.74it/s] 18%|â–ˆâ–Š        | 581/3145 [03:48<16:51,  2.54it/s] 19%|â–ˆâ–Š        | 585/3143 [03:45<16:40,  2.56it/s] 19%|â–ˆâ–‰        | 593/3145 [03:58<15:19,  2.78it/s] 18%|â–ˆâ–Š        | 581/3145 [03:46<16:53,  2.53it/s] 19%|â–ˆâ–Š        | 582/3145 [03:48<16:22,  2.61it/s] 19%|â–ˆâ–‰        | 594/3145 [03:59<13:39,  3.11it/s] 19%|â–ˆâ–Š        | 586/3143 [03:46<16:14,  2.62it/s] 19%|â–ˆâ–Š        | 583/3145 [03:49<16:03,  2.66it/s] 19%|â–ˆâ–‰        | 595/3145 [03:59<13:45,  3.09it/s] 19%|â–ˆâ–Š        | 582/3145 [03:46<18:14,  2.34it/s] 19%|â–ˆâ–Š        | 587/3143 [03:46<16:19,  2.61it/s] 19%|â–ˆâ–Š        | 584/3145 [03:49<15:14,  2.80it/s] 19%|â–ˆâ–Š        | 583/3145 [03:46<17:20,  2.46it/s] 19%|â–ˆâ–‰        | 596/3145 [03:59<14:57,  2.84it/s] 19%|â–ˆâ–Š        | 588/3143 [03:47<16:12,  2.63it/s] 19%|â–ˆâ–Š        | 585/3145 [03:49<16:00,  2.67it/s] 19%|â–ˆâ–Š        | 584/3145 [03:47<17:06,  2.49it/s] 19%|â–ˆâ–Š        | 589/3143 [03:47<16:38,  2.56it/s] 19%|â–ˆâ–‰        | 597/3145 [04:00<16:20,  2.60it/s] 19%|â–ˆâ–Š        | 586/3145 [03:50<15:47,  2.70it/s] 19%|â–ˆâ–Š        | 585/3145 [03:47<16:33,  2.58it/s] 19%|â–ˆâ–‰        | 590/3143 [03:47<16:37,  2.56it/s] 19%|â–ˆâ–‰        | 598/3145 [04:00<17:17,  2.45it/s] 19%|â–ˆâ–Š        | 587/3145 [03:50<15:28,  2.75it/s] 19%|â–ˆâ–Š        | 586/3145 [03:48<15:48,  2.70it/s] 19%|â–ˆâ–‰        | 591/3143 [03:48<15:59,  2.66it/s] 19%|â–ˆâ–‰        | 599/3145 [04:01<16:20,  2.60it/s] 19%|â–ˆâ–Š        | 588/3145 [03:50<15:45,  2.71it/s] 19%|â–ˆâ–Š        | 587/3145 [03:48<15:58,  2.67it/s] 19%|â–ˆâ–‰        | 592/3143 [03:48<15:43,  2.70it/s] 19%|â–ˆâ–‰        | 600/3145 [04:01<16:17,  2.60it/s] 19%|â–ˆâ–Š        | 589/3145 [03:51<15:12,  2.80it/s] 19%|â–ˆâ–Š        | 588/3145 [03:48<16:05,  2.65it/s] 19%|â–ˆâ–‰        | 593/3143 [03:49<16:44,  2.54it/s] 19%|â–ˆâ–‰        | 590/3145 [03:51<14:53,  2.86it/s] 19%|â–ˆâ–‰        | 601/3145 [04:01<16:16,  2.60it/s] 19%|â–ˆâ–Š        | 589/3145 [03:49<15:59,  2.66it/s] 19%|â–ˆâ–‰        | 594/3143 [03:49<16:33,  2.56it/s] 19%|â–ˆâ–‰        | 591/3145 [03:51<15:07,  2.81it/s] 19%|â–ˆâ–‰        | 602/3145 [04:02<16:03,  2.64it/s] 19%|â–ˆâ–‰        | 590/3145 [03:49<16:18,  2.61it/s] 19%|â–ˆâ–‰        | 592/3145 [03:52<15:21,  2.77it/s] 19%|â–ˆâ–‰        | 595/3143 [03:49<16:31,  2.57it/s] 19%|â–ˆâ–‰        | 603/3145 [04:02<16:30,  2.57it/s] 19%|â–ˆâ–‰        | 591/3145 [03:49<14:23,  2.96it/s] 19%|â–ˆâ–‰        | 593/3145 [03:52<15:28,  2.75it/s] 19%|â–ˆâ–‰        | 596/3143 [03:50<16:32,  2.57it/s] 19%|â–ˆâ–‰        | 592/3145 [03:50<14:47,  2.88it/s] 19%|â–ˆâ–‰        | 604/3145 [04:03<17:05,  2.48it/s] 19%|â–ˆâ–‰        | 597/3143 [03:50<16:08,  2.63it/s] 19%|â–ˆâ–‰        | 594/3145 [03:53<15:46,  2.69it/s] 19%|â–ˆâ–‰        | 593/3145 [03:50<15:17,  2.78it/s] 19%|â–ˆâ–‰        | 605/3145 [04:03<16:58,  2.49it/s] 19%|â–ˆâ–‰        | 598/3143 [03:50<16:20,  2.60it/s] 19%|â–ˆâ–‰        | 595/3145 [03:53<16:04,  2.64it/s] 19%|â–ˆâ–‰        | 594/3145 [03:50<15:10,  2.80it/s] 19%|â–ˆâ–‰        | 606/3145 [04:03<17:53,  2.37it/s] 19%|â–ˆâ–‰        | 599/3143 [03:51<15:50,  2.68it/s] 19%|â–ˆâ–‰        | 596/3145 [03:53<16:19,  2.60it/s] 19%|â–ˆâ–‰        | 607/3145 [04:04<15:33,  2.72it/s] 19%|â–ˆâ–‰        | 595/3145 [03:51<16:01,  2.65it/s] 19%|â–ˆâ–‰        | 600/3143 [03:51<15:51,  2.67it/s] 19%|â–ˆâ–‰        | 597/3145 [03:54<16:21,  2.60it/s] 19%|â–ˆâ–‰        | 608/3145 [04:04<16:14,  2.60it/s] 19%|â–ˆâ–‰        | 596/3145 [03:51<16:52,  2.52it/s] 19%|â–ˆâ–‰        | 601/3143 [03:52<15:38,  2.71it/s] 19%|â–ˆâ–‰        | 598/3145 [03:54<15:57,  2.66it/s] 19%|â–ˆâ–‰        | 609/3145 [04:05<17:23,  2.43it/s] 19%|â–ˆâ–‰        | 597/3145 [03:52<17:35,  2.41it/s] 19%|â–ˆâ–‰        | 599/3145 [03:54<15:52,  2.67it/s] 19%|â–ˆâ–‰        | 602/3143 [03:52<17:14,  2.46it/s] 19%|â–ˆâ–‰        | 598/3145 [03:52<16:41,  2.54it/s] 19%|â–ˆâ–‰        | 610/3145 [04:05<17:32,  2.41it/s] 19%|â–ˆâ–‰        | 600/3145 [03:55<15:53,  2.67it/s] 19%|â–ˆâ–‰        | 603/3143 [03:52<16:36,  2.55it/s] 19%|â–ˆâ–‰        | 599/3145 [03:52<16:41,  2.54it/s] 19%|â–ˆâ–‰        | 611/3145 [04:05<17:32,  2.41it/s] 19%|â–ˆâ–‰        | 601/3145 [03:55<15:41,  2.70it/s] 19%|â–ˆâ–‰        | 604/3143 [03:53<17:20,  2.44it/s] 19%|â–ˆâ–‰        | 600/3145 [03:53<16:03,  2.64it/s] 19%|â–ˆâ–‰        | 612/3145 [04:06<16:32,  2.55it/s] 19%|â–ˆâ–‰        | 602/3145 [03:55<15:06,  2.81it/s] 19%|â–ˆâ–‰        | 605/3143 [03:53<15:04,  2.81it/s] 19%|â–ˆâ–‰        | 601/3145 [03:53<15:28,  2.74it/s] 19%|â–ˆâ–‰        | 603/3145 [03:56<14:12,  2.98it/s] 19%|â–ˆâ–‰        | 613/3145 [04:06<16:27,  2.56it/s] 19%|â–ˆâ–‰        | 606/3143 [03:53<15:07,  2.80it/s] 19%|â–ˆâ–‰        | 602/3145 [03:54<15:47,  2.68it/s] 19%|â–ˆâ–‰        | 604/3145 [03:56<14:31,  2.91it/s] 20%|â–ˆâ–‰        | 614/3145 [04:06<15:51,  2.66it/s] 19%|â–ˆâ–‰        | 607/3143 [03:54<15:27,  2.73it/s] 19%|â–ˆâ–‰        | 603/3145 [03:54<15:20,  2.76it/s] 19%|â–ˆâ–‰        | 605/3145 [03:56<14:26,  2.93it/s] 20%|â–ˆâ–‰        | 615/3145 [04:07<16:27,  2.56it/s] 19%|â–ˆâ–‰        | 608/3143 [03:54<15:41,  2.69it/s] 19%|â–ˆâ–‰        | 604/3145 [03:54<15:38,  2.71it/s] 19%|â–ˆâ–‰        | 606/3145 [03:57<14:42,  2.88it/s] 20%|â–ˆâ–‰        | 616/3145 [04:07<16:21,  2.58it/s] 19%|â–ˆâ–‰        | 609/3143 [03:55<15:53,  2.66it/s] 19%|â–ˆâ–‰        | 607/3145 [03:57<14:56,  2.83it/s] 19%|â–ˆâ–‰        | 605/3145 [03:55<16:42,  2.53it/s] 20%|â–ˆâ–‰        | 617/3145 [04:08<16:23,  2.57it/s] 19%|â–ˆâ–‰        | 610/3143 [03:55<15:27,  2.73it/s] 19%|â–ˆâ–‰        | 608/3145 [03:58<14:59,  2.82it/s] 19%|â–ˆâ–‰        | 606/3145 [03:55<16:57,  2.50it/s] 20%|â–ˆâ–‰        | 618/3145 [04:08<15:43,  2.68it/s] 19%|â–ˆâ–‰        | 611/3143 [03:55<14:54,  2.83it/s] 19%|â–ˆâ–‰        | 609/3145 [03:58<15:19,  2.76it/s] 19%|â–ˆâ–‰        | 612/3143 [03:56<14:48,  2.85it/s] 19%|â–ˆâ–‰        | 607/3145 [03:56<16:46,  2.52it/s] 20%|â–ˆâ–‰        | 619/3145 [04:08<15:57,  2.64it/s] 19%|â–ˆâ–‰        | 610/3145 [03:58<15:13,  2.78it/s] 20%|â–ˆâ–‰        | 620/3145 [04:09<15:22,  2.74it/s] 20%|â–ˆâ–‰        | 613/3143 [03:56<14:59,  2.81it/s] 19%|â–ˆâ–‰        | 608/3145 [03:56<17:39,  2.40it/s] 19%|â–ˆâ–‰        | 611/3145 [03:59<15:12,  2.78it/s] 20%|â–ˆâ–‰        | 614/3143 [03:56<14:58,  2.82it/s] 20%|â–ˆâ–‰        | 621/3145 [04:09<15:42,  2.68it/s] 19%|â–ˆâ–‰        | 609/3145 [03:56<17:22,  2.43it/s] 19%|â–ˆâ–‰        | 612/3145 [03:59<15:42,  2.69it/s] 20%|â–ˆâ–‰        | 615/3143 [03:57<15:13,  2.77it/s] 20%|â–ˆâ–‰        | 622/3145 [04:09<15:29,  2.72it/s] 19%|â–ˆâ–‰        | 610/3145 [03:57<16:59,  2.49it/s] 19%|â–ˆâ–‰        | 613/3145 [03:59<15:34,  2.71it/s] 20%|â–ˆâ–‰        | 623/3145 [04:10<15:26,  2.72it/s] 20%|â–ˆâ–‰        | 616/3143 [03:57<15:31,  2.71it/s] 19%|â–ˆâ–‰        | 611/3145 [03:57<16:24,  2.57it/s] 20%|â–ˆâ–‰        | 614/3145 [04:00<15:25,  2.73it/s] 20%|â–ˆâ–‰        | 624/3145 [04:10<15:26,  2.72it/s] 20%|â–ˆâ–‰        | 617/3143 [03:57<15:51,  2.65it/s] 19%|â–ˆâ–‰        | 612/3145 [03:58<17:11,  2.45it/s] 20%|â–ˆâ–‰        | 615/3145 [04:00<15:44,  2.68it/s] 20%|â–ˆâ–‰        | 618/3143 [03:58<14:18,  2.94it/s] 20%|â–ˆâ–‰        | 625/3145 [04:11<16:33,  2.54it/s] 20%|â–ˆâ–‰        | 616/3145 [04:01<15:36,  2.70it/s] 20%|â–ˆâ–‰        | 619/3143 [03:58<15:08,  2.78it/s] 19%|â–ˆâ–‰        | 613/3145 [03:58<18:14,  2.31it/s] 20%|â–ˆâ–‰        | 626/3145 [04:11<16:23,  2.56it/s] 20%|â–ˆâ–‰        | 617/3145 [04:01<15:35,  2.70it/s] 20%|â–ˆâ–‰        | 620/3143 [03:58<15:05,  2.79it/s] 20%|â–ˆâ–‰        | 614/3145 [03:58<17:48,  2.37it/s] 20%|â–ˆâ–‰        | 627/3145 [04:11<15:48,  2.66it/s] 20%|â–ˆâ–‰        | 618/3145 [04:01<15:32,  2.71it/s] 20%|â–ˆâ–‰        | 621/3143 [03:59<15:03,  2.79it/s] 20%|â–ˆâ–‰        | 615/3145 [03:59<16:55,  2.49it/s] 20%|â–ˆâ–‰        | 628/3145 [04:12<16:18,  2.57it/s] 20%|â–ˆâ–‰        | 619/3145 [04:02<15:24,  2.73it/s] 20%|â–ˆâ–‰        | 622/3143 [03:59<15:22,  2.73it/s] 20%|â–ˆâ–‰        | 616/3145 [03:59<16:23,  2.57it/s] 20%|â–ˆâ–ˆ        | 629/3145 [04:12<15:56,  2.63it/s] 20%|â–ˆâ–‰        | 620/3145 [04:02<15:18,  2.75it/s] 20%|â–ˆâ–‰        | 623/3143 [04:00<15:44,  2.67it/s] 20%|â–ˆâ–‰        | 617/3145 [04:00<16:23,  2.57it/s] 20%|â–ˆâ–ˆ        | 630/3145 [04:12<15:36,  2.68it/s] 20%|â–ˆâ–‰        | 621/3145 [04:02<15:15,  2.76it/s] 20%|â–ˆâ–‰        | 624/3143 [04:00<16:24,  2.56it/s] 20%|â–ˆâ–‰        | 618/3145 [04:00<16:45,  2.51it/s] 20%|â–ˆâ–ˆ        | 631/3145 [04:13<15:41,  2.67it/s] 20%|â–ˆâ–‰        | 622/3145 [04:03<15:10,  2.77it/s] 20%|â–ˆâ–‰        | 619/3145 [04:00<16:20,  2.58it/s] 20%|â–ˆâ–ˆ        | 632/3145 [04:13<15:25,  2.72it/s] 20%|â–ˆâ–‰        | 625/3143 [04:00<16:53,  2.48it/s] 20%|â–ˆâ–‰        | 623/3145 [04:03<15:15,  2.76it/s] 20%|â–ˆâ–‰        | 620/3145 [04:01<15:40,  2.69it/s] 20%|â–ˆâ–ˆ        | 633/3145 [04:14<15:25,  2.71it/s] 20%|â–ˆâ–‰        | 626/3143 [04:01<17:14,  2.43it/s] 20%|â–ˆâ–‰        | 624/3145 [04:03<15:35,  2.69it/s] 20%|â–ˆâ–‰        | 621/3145 [04:01<15:31,  2.71it/s] 20%|â–ˆâ–ˆ        | 634/3145 [04:14<15:15,  2.74it/s] 20%|â–ˆâ–‰        | 625/3145 [04:04<13:53,  3.02it/s] 20%|â–ˆâ–‰        | 627/3143 [04:01<16:37,  2.52it/s] 20%|â–ˆâ–‰        | 622/3145 [04:01<15:19,  2.74it/s] 20%|â–ˆâ–ˆ        | 635/3145 [04:14<15:19,  2.73it/s] 20%|â–ˆâ–‰        | 628/3143 [04:02<15:44,  2.66it/s] 20%|â–ˆâ–‰        | 626/3145 [04:04<14:37,  2.87it/s] 20%|â–ˆâ–‰        | 623/3145 [04:02<14:58,  2.81it/s] 20%|â–ˆâ–ˆ        | 636/3145 [04:15<15:33,  2.69it/s] 20%|â–ˆâ–ˆ        | 629/3143 [04:02<15:37,  2.68it/s] 20%|â–ˆâ–‰        | 627/3145 [04:04<15:14,  2.75it/s] 20%|â–ˆâ–‰        | 624/3145 [04:02<14:35,  2.88it/s] 20%|â–ˆâ–ˆ        | 630/3143 [04:02<15:11,  2.76it/s] 20%|â–ˆâ–ˆ        | 637/3145 [04:15<15:46,  2.65it/s] 20%|â–ˆâ–‰        | 628/3145 [04:05<15:32,  2.70it/s] 20%|â–ˆâ–‰        | 625/3145 [04:02<15:07,  2.78it/s] 20%|â–ˆâ–ˆ        | 631/3143 [04:03<15:12,  2.75it/s] 20%|â–ˆâ–ˆ        | 638/3145 [04:15<15:37,  2.67it/s] 20%|â–ˆâ–ˆ        | 629/3145 [04:05<15:31,  2.70it/s] 20%|â–ˆâ–‰        | 626/3145 [04:03<14:57,  2.81it/s] 20%|â–ˆâ–ˆ        | 632/3143 [04:03<13:40,  3.06it/s] 20%|â–ˆâ–ˆ        | 639/3145 [04:16<15:04,  2.77it/s] 20%|â–ˆâ–ˆ        | 630/3145 [04:06<15:21,  2.73it/s] 20%|â–ˆâ–‰        | 627/3145 [04:03<15:52,  2.64it/s] 20%|â–ˆâ–ˆ        | 633/3143 [04:03<14:32,  2.88it/s] 20%|â–ˆâ–ˆ        | 640/3145 [04:16<15:02,  2.78it/s] 20%|â–ˆâ–ˆ        | 631/3145 [04:06<15:37,  2.68it/s] 20%|â–ˆâ–‰        | 628/3145 [04:04<16:24,  2.56it/s] 20%|â–ˆâ–ˆ        | 634/3143 [04:04<15:32,  2.69it/s] 20%|â–ˆâ–ˆ        | 641/3145 [04:17<15:20,  2.72it/s] 20%|â–ˆâ–ˆ        | 632/3145 [04:06<16:13,  2.58it/s] 20%|â–ˆâ–ˆ        | 635/3143 [04:04<15:19,  2.73it/s] 20%|â–ˆâ–ˆ        | 642/3145 [04:17<15:13,  2.74it/s] 20%|â–ˆâ–ˆ        | 629/3145 [04:04<17:29,  2.40it/s] 20%|â–ˆâ–ˆ        | 633/3145 [04:07<16:08,  2.59it/s] 20%|â–ˆâ–ˆ        | 636/3143 [04:04<15:31,  2.69it/s] 20%|â–ˆâ–ˆ        | 643/3145 [04:17<15:01,  2.77it/s] 20%|â–ˆâ–ˆ        | 630/3145 [04:05<16:44,  2.50it/s] 20%|â–ˆâ–ˆ        | 634/3145 [04:07<15:27,  2.71it/s] 20%|â–ˆâ–ˆ        | 637/3143 [04:05<14:21,  2.91it/s] 20%|â–ˆâ–ˆ        | 644/3145 [04:18<14:58,  2.78it/s] 20%|â–ˆâ–ˆ        | 635/3145 [04:08<15:38,  2.67it/s] 20%|â–ˆâ–ˆ        | 631/3145 [04:05<18:09,  2.31it/s] 20%|â–ˆâ–ˆ        | 638/3143 [04:05<14:51,  2.81it/s] 21%|â–ˆâ–ˆ        | 645/3145 [04:18<15:15,  2.73it/s] 20%|â–ˆâ–ˆ        | 636/3145 [04:08<15:25,  2.71it/s] 20%|â–ˆâ–ˆ        | 632/3145 [04:05<17:33,  2.39it/s] 20%|â–ˆâ–ˆ        | 639/3143 [04:05<14:43,  2.83it/s] 21%|â–ˆâ–ˆ        | 646/3145 [04:18<15:27,  2.69it/s] 20%|â–ˆâ–ˆ        | 637/3145 [04:08<14:59,  2.79it/s] 20%|â–ˆâ–ˆ        | 633/3145 [04:06<15:50,  2.64it/s] 20%|â–ˆâ–ˆ        | 640/3143 [04:06<15:11,  2.75it/s] 21%|â–ˆâ–ˆ        | 647/3145 [04:19<15:59,  2.60it/s] 20%|â–ˆâ–ˆ        | 638/3145 [04:09<14:49,  2.82it/s] 20%|â–ˆâ–ˆ        | 634/3145 [04:06<15:24,  2.72it/s] 20%|â–ˆâ–ˆ        | 641/3143 [04:06<15:05,  2.76it/s] 20%|â–ˆâ–ˆ        | 635/3145 [04:06<13:38,  3.07it/s] 20%|â–ˆâ–ˆ        | 639/3145 [04:09<14:54,  2.80it/s] 21%|â–ˆâ–ˆ        | 648/3145 [04:19<16:29,  2.52it/s] 20%|â–ˆâ–ˆ        | 642/3143 [04:07<15:44,  2.65it/s] 20%|â–ˆâ–ˆ        | 636/3145 [04:07<14:06,  2.96it/s] 20%|â–ˆâ–ˆ        | 640/3145 [04:09<14:52,  2.81it/s] 21%|â–ˆâ–ˆ        | 649/3145 [04:20<15:59,  2.60it/s] 20%|â–ˆâ–ˆ        | 643/3143 [04:07<16:17,  2.56it/s] 20%|â–ˆâ–ˆ        | 637/3145 [04:07<15:17,  2.73it/s] 20%|â–ˆâ–ˆ        | 641/3145 [04:10<14:44,  2.83it/s] 21%|â–ˆâ–ˆ        | 650/3145 [04:20<16:33,  2.51it/s] 20%|â–ˆâ–ˆ        | 644/3143 [04:07<16:10,  2.57it/s] 20%|â–ˆâ–ˆ        | 642/3145 [04:10<14:47,  2.82it/s] 20%|â–ˆâ–ˆ        | 638/3145 [04:07<15:29,  2.70it/s] 21%|â–ˆâ–ˆ        | 651/3145 [04:20<16:26,  2.53it/s] 21%|â–ˆâ–ˆ        | 645/3143 [04:08<16:07,  2.58it/s] 20%|â–ˆâ–ˆ        | 639/3145 [04:08<15:33,  2.68it/s] 20%|â–ˆâ–ˆ        | 643/3145 [04:10<15:28,  2.69it/s] 20%|â–ˆâ–ˆ        | 640/3145 [04:08<13:40,  3.05it/s] 21%|â–ˆâ–ˆ        | 652/3145 [04:21<17:36,  2.36it/s] 21%|â–ˆâ–ˆ        | 646/3143 [04:08<15:21,  2.71it/s] 20%|â–ˆâ–ˆ        | 644/3145 [04:11<14:06,  2.95it/s] 20%|â–ˆâ–ˆ        | 641/3145 [04:08<14:24,  2.90it/s] 21%|â–ˆâ–ˆ        | 653/3145 [04:21<17:05,  2.43it/s] 21%|â–ˆâ–ˆ        | 647/3143 [04:08<15:14,  2.73it/s] 21%|â–ˆâ–ˆ        | 645/3145 [04:11<14:55,  2.79it/s] 21%|â–ˆâ–ˆ        | 646/3145 [04:11<13:38,  3.05it/s] 20%|â–ˆâ–ˆ        | 642/3145 [04:09<14:24,  2.90it/s] 21%|â–ˆâ–ˆ        | 654/3145 [04:22<16:39,  2.49it/s] 21%|â–ˆâ–ˆ        | 648/3143 [04:09<15:46,  2.64it/s] 20%|â–ˆâ–ˆ        | 643/3145 [04:09<14:24,  2.89it/s] 21%|â–ˆâ–ˆ        | 647/3145 [04:12<14:13,  2.93it/s] 21%|â–ˆâ–ˆ        | 655/3145 [04:22<16:33,  2.51it/s] 21%|â–ˆâ–ˆ        | 649/3143 [04:09<15:29,  2.68it/s] 20%|â–ˆâ–ˆ        | 644/3145 [04:10<14:57,  2.79it/s] 21%|â–ˆâ–ˆ        | 648/3145 [04:12<14:48,  2.81it/s] 21%|â–ˆâ–ˆ        | 650/3143 [04:10<15:03,  2.76it/s] 21%|â–ˆâ–ˆ        | 656/3145 [04:22<16:04,  2.58it/s] 21%|â–ˆâ–ˆ        | 649/3145 [04:12<14:33,  2.86it/s] 21%|â–ˆâ–ˆ        | 645/3145 [04:10<15:49,  2.63it/s] 21%|â–ˆâ–ˆ        | 657/3145 [04:23<16:16,  2.55it/s] 21%|â–ˆâ–ˆ        | 651/3143 [04:10<15:44,  2.64it/s] 21%|â–ˆâ–ˆ        | 650/3145 [04:13<14:18,  2.91it/s] 21%|â–ˆâ–ˆ        | 652/3143 [04:10<14:44,  2.82it/s] 21%|â–ˆâ–ˆ        | 646/3145 [04:10<16:21,  2.55it/s] 21%|â–ˆâ–ˆ        | 658/3145 [04:23<16:16,  2.55it/s] 21%|â–ˆâ–ˆ        | 651/3145 [04:13<14:10,  2.93it/s] 21%|â–ˆâ–ˆ        | 653/3143 [04:11<15:01,  2.76it/s] 21%|â–ˆâ–ˆ        | 659/3145 [04:24<16:01,  2.59it/s] 21%|â–ˆâ–ˆ        | 647/3145 [04:11<16:46,  2.48it/s] 21%|â–ˆâ–ˆ        | 652/3145 [04:13<14:27,  2.87it/s] 21%|â–ˆâ–ˆ        | 654/3143 [04:11<14:55,  2.78it/s] 21%|â–ˆâ–ˆ        | 660/3145 [04:24<15:25,  2.69it/s] 21%|â–ˆâ–ˆ        | 648/3145 [04:11<16:11,  2.57it/s] 21%|â–ˆâ–ˆ        | 653/3145 [04:14<14:37,  2.84it/s] 21%|â–ˆâ–ˆ        | 655/3143 [04:11<15:15,  2.72it/s] 21%|â–ˆâ–ˆ        | 661/3145 [04:24<15:14,  2.72it/s] 21%|â–ˆâ–ˆ        | 649/3145 [04:12<16:36,  2.50it/s] 21%|â–ˆâ–ˆ        | 654/3145 [04:14<15:02,  2.76it/s] 21%|â–ˆâ–ˆ        | 662/3145 [04:24<13:30,  3.06it/s] 21%|â–ˆâ–ˆ        | 656/3143 [04:12<16:11,  2.56it/s] 21%|â–ˆâ–ˆ        | 663/3145 [04:25<12:11,  3.39it/s] 21%|â–ˆâ–ˆ        | 650/3145 [04:12<16:29,  2.52it/s] 21%|â–ˆâ–ˆ        | 655/3145 [04:15<14:34,  2.85it/s] 21%|â–ˆâ–ˆ        | 657/3143 [04:12<15:31,  2.67it/s] 21%|â–ˆâ–ˆ        | 664/3145 [04:25<12:48,  3.23it/s] 21%|â–ˆâ–ˆ        | 651/3145 [04:12<16:09,  2.57it/s] 21%|â–ˆâ–ˆ        | 656/3145 [04:15<15:33,  2.67it/s] 21%|â–ˆâ–ˆ        | 658/3143 [04:13<16:00,  2.59it/s] 21%|â–ˆâ–ˆ        | 665/3145 [04:25<13:42,  3.02it/s] 21%|â–ˆâ–ˆ        | 652/3145 [04:13<16:08,  2.57it/s] 21%|â–ˆâ–ˆ        | 657/3145 [04:15<15:23,  2.69it/s] 21%|â–ˆâ–ˆ        | 659/3143 [04:13<16:08,  2.56it/s] 21%|â–ˆâ–ˆ        | 666/3145 [04:26<14:32,  2.84it/s] 21%|â–ˆâ–ˆ        | 653/3145 [04:13<15:23,  2.70it/s] 21%|â–ˆâ–ˆ        | 658/3145 [04:16<15:14,  2.72it/s] 21%|â–ˆâ–ˆ        | 660/3143 [04:13<16:08,  2.56it/s] 21%|â–ˆâ–ˆ        | 654/3145 [04:13<15:24,  2.69it/s] 21%|â–ˆâ–ˆ        | 667/3145 [04:26<15:12,  2.71it/s] 21%|â–ˆâ–ˆ        | 659/3145 [04:16<15:27,  2.68it/s] 21%|â–ˆâ–ˆ        | 661/3143 [04:14<15:45,  2.63it/s] 21%|â–ˆâ–ˆ        | 668/3145 [04:27<14:54,  2.77it/s] 21%|â–ˆâ–ˆ        | 655/3145 [04:14<15:47,  2.63it/s] 21%|â–ˆâ–ˆ        | 660/3145 [04:16<16:16,  2.54it/s] 21%|â–ˆâ–ˆ        | 662/3143 [04:14<14:13,  2.91it/s] 21%|â–ˆâ–ˆ        | 656/3145 [04:14<15:15,  2.72it/s] 21%|â–ˆâ–ˆâ–       | 669/3145 [04:27<15:19,  2.69it/s] 21%|â–ˆâ–ˆ        | 661/3145 [04:17<16:17,  2.54it/s] 21%|â–ˆâ–ˆ        | 663/3143 [04:14<14:20,  2.88it/s] 21%|â–ˆâ–ˆâ–       | 670/3145 [04:27<14:51,  2.78it/s] 21%|â–ˆâ–ˆ        | 657/3145 [04:15<16:08,  2.57it/s] 21%|â–ˆâ–ˆ        | 664/3143 [04:15<14:27,  2.86it/s] 21%|â–ˆâ–ˆ        | 662/3145 [04:17<16:10,  2.56it/s] 21%|â–ˆâ–ˆâ–       | 671/3145 [04:28<14:37,  2.82it/s] 21%|â–ˆâ–ˆ        | 658/3145 [04:15<15:31,  2.67it/s] 21%|â–ˆâ–ˆ        | 665/3143 [04:15<14:35,  2.83it/s] 21%|â–ˆâ–ˆ        | 663/3145 [04:18<17:08,  2.41it/s] 21%|â–ˆâ–ˆâ–       | 672/3145 [04:28<14:45,  2.79it/s] 21%|â–ˆâ–ˆ        | 659/3145 [04:15<15:23,  2.69it/s] 21%|â–ˆâ–ˆ        | 666/3143 [04:16<15:00,  2.75it/s] 21%|â–ˆâ–ˆ        | 664/3145 [04:18<16:46,  2.46it/s] 21%|â–ˆâ–ˆâ–       | 673/3145 [04:28<15:57,  2.58it/s] 21%|â–ˆâ–ˆ        | 660/3145 [04:16<15:48,  2.62it/s] 21%|â–ˆâ–ˆ        | 667/3143 [04:16<14:59,  2.75it/s] 21%|â–ˆâ–ˆâ–       | 674/3145 [04:29<15:29,  2.66it/s] 21%|â–ˆâ–ˆ        | 665/3145 [04:19<17:16,  2.39it/s] 21%|â–ˆâ–ˆ        | 661/3145 [04:16<15:47,  2.62it/s] 21%|â–ˆâ–ˆâ–       | 668/3143 [04:16<15:39,  2.64it/s] 21%|â–ˆâ–ˆâ–       | 675/3145 [04:29<15:04,  2.73it/s] 21%|â–ˆâ–ˆ        | 666/3145 [04:19<16:16,  2.54it/s] 21%|â–ˆâ–ˆ        | 662/3145 [04:16<15:18,  2.70it/s] 21%|â–ˆâ–ˆâ–       | 669/3143 [04:17<15:22,  2.68it/s] 21%|â–ˆâ–ˆâ–       | 676/3145 [04:30<14:51,  2.77it/s] 21%|â–ˆâ–ˆ        | 663/3145 [04:17<15:32,  2.66it/s] 21%|â–ˆâ–ˆ        | 667/3145 [04:19<17:10,  2.40it/s] 21%|â–ˆâ–ˆâ–       | 670/3143 [04:17<16:11,  2.55it/s] 21%|â–ˆâ–ˆ        | 664/3145 [04:17<13:44,  3.01it/s] 22%|â–ˆâ–ˆâ–       | 677/3145 [04:30<15:13,  2.70it/s] 21%|â–ˆâ–ˆ        | 668/3145 [04:20<16:32,  2.50it/s] 21%|â–ˆâ–ˆâ–       | 671/3143 [04:17<15:34,  2.65it/s] 21%|â–ˆâ–ˆ        | 665/3145 [04:17<13:57,  2.96it/s] 21%|â–ˆâ–ˆâ–       | 669/3145 [04:20<16:24,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 678/3145 [04:30<16:43,  2.46it/s] 21%|â–ˆâ–ˆâ–       | 672/3143 [04:18<15:28,  2.66it/s] 21%|â–ˆâ–ˆ        | 666/3145 [04:18<14:50,  2.78it/s] 21%|â–ˆâ–ˆâ–       | 670/3145 [04:20<15:51,  2.60it/s] 22%|â–ˆâ–ˆâ–       | 679/3145 [04:31<16:11,  2.54it/s] 21%|â–ˆâ–ˆ        | 667/3145 [04:18<14:50,  2.78it/s] 21%|â–ˆâ–ˆâ–       | 673/3143 [04:18<16:00,  2.57it/s] 21%|â–ˆâ–ˆâ–       | 671/3145 [04:21<15:16,  2.70it/s] 22%|â–ˆâ–ˆâ–       | 680/3145 [04:31<16:04,  2.56it/s] 21%|â–ˆâ–ˆ        | 668/3145 [04:19<14:49,  2.78it/s] 21%|â–ˆâ–ˆâ–       | 674/3143 [04:19<15:39,  2.63it/s] 21%|â–ˆâ–ˆâ–       | 672/3145 [04:21<15:15,  2.70it/s] 22%|â–ˆâ–ˆâ–       | 681/3145 [04:31<15:09,  2.71it/s] 21%|â–ˆâ–ˆâ–       | 675/3143 [04:19<13:50,  2.97it/s] 21%|â–ˆâ–ˆâ–       | 669/3145 [04:19<14:35,  2.83it/s] 21%|â–ˆâ–ˆâ–       | 673/3145 [04:22<14:59,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 676/3143 [04:19<12:33,  3.27it/s] 22%|â–ˆâ–ˆâ–       | 682/3145 [04:32<15:07,  2.71it/s] 21%|â–ˆâ–ˆâ–       | 670/3145 [04:19<14:32,  2.84it/s] 22%|â–ˆâ–ˆâ–       | 677/3143 [04:19<11:45,  3.50it/s] 22%|â–ˆâ–ˆâ–       | 683/3145 [04:32<14:47,  2.77it/s] 22%|â–ˆâ–ˆâ–       | 678/3143 [04:20<11:04,  3.71it/s] 21%|â–ˆâ–ˆâ–       | 674/3145 [04:22<16:37,  2.48it/s] 21%|â–ˆâ–ˆâ–       | 671/3145 [04:20<14:22,  2.87it/s] 22%|â–ˆâ–ˆâ–       | 684/3145 [04:33<14:28,  2.83it/s] 22%|â–ˆâ–ˆâ–       | 679/3143 [04:20<12:11,  3.37it/s] 21%|â–ˆâ–ˆâ–       | 672/3145 [04:20<14:33,  2.83it/s] 21%|â–ˆâ–ˆâ–       | 675/3145 [04:23<17:36,  2.34it/s] 22%|â–ˆâ–ˆâ–       | 685/3145 [04:33<14:35,  2.81it/s] 21%|â–ˆâ–ˆâ–       | 673/3145 [04:20<14:58,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 680/3143 [04:20<14:39,  2.80it/s] 21%|â–ˆâ–ˆâ–       | 676/3145 [04:23<16:44,  2.46it/s] 22%|â–ˆâ–ˆâ–       | 686/3145 [04:33<14:15,  2.87it/s] 21%|â–ˆâ–ˆâ–       | 674/3145 [04:21<13:41,  3.01it/s] 22%|â–ˆâ–ˆâ–       | 677/3145 [04:23<16:18,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 687/3145 [04:34<14:21,  2.85it/s] 22%|â–ˆâ–ˆâ–       | 681/3143 [04:21<16:23,  2.50it/s] 21%|â–ˆâ–ˆâ–       | 675/3145 [04:21<14:24,  2.86it/s] 22%|â–ˆâ–ˆâ–       | 688/3145 [04:34<13:05,  3.13it/s] 22%|â–ˆâ–ˆâ–       | 678/3145 [04:24<15:30,  2.65it/s] 22%|â–ˆâ–ˆâ–       | 679/3145 [04:24<13:43,  2.99it/s] 21%|â–ˆâ–ˆâ–       | 676/3145 [04:21<14:37,  2.81it/s] 22%|â–ˆâ–ˆâ–       | 682/3143 [04:21<17:34,  2.33it/s] 22%|â–ˆâ–ˆâ–       | 689/3145 [04:34<14:28,  2.83it/s] 22%|â–ˆâ–ˆâ–       | 680/3145 [04:24<14:39,  2.80it/s] 22%|â–ˆâ–ˆâ–       | 677/3145 [04:22<15:12,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 690/3145 [04:35<14:47,  2.77it/s] 22%|â–ˆâ–ˆâ–       | 683/3143 [04:22<18:31,  2.21it/s] 22%|â–ˆâ–ˆâ–       | 681/3145 [04:25<14:41,  2.80it/s] 22%|â–ˆâ–ˆâ–       | 678/3145 [04:22<14:50,  2.77it/s] 22%|â–ˆâ–ˆâ–       | 684/3143 [04:22<17:13,  2.38it/s] 22%|â–ˆâ–ˆâ–       | 691/3145 [04:35<15:04,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 682/3145 [04:25<14:46,  2.78it/s] 22%|â–ˆâ–ˆâ–       | 679/3145 [04:22<14:54,  2.76it/s] 22%|â–ˆâ–ˆâ–       | 685/3143 [04:23<16:50,  2.43it/s] 22%|â–ˆâ–ˆâ–       | 692/3145 [04:35<16:03,  2.55it/s] 22%|â–ˆâ–ˆâ–       | 683/3145 [04:25<14:21,  2.86it/s] 22%|â–ˆâ–ˆâ–       | 680/3145 [04:23<15:12,  2.70it/s] 22%|â–ˆâ–ˆâ–       | 686/3143 [04:23<16:13,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 693/3145 [04:36<15:56,  2.56it/s] 22%|â–ˆâ–ˆâ–       | 684/3145 [04:26<14:05,  2.91it/s] 22%|â–ˆâ–ˆâ–       | 681/3145 [04:23<14:45,  2.78it/s] 22%|â–ˆâ–ˆâ–       | 687/3143 [04:23<15:46,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 694/3145 [04:36<15:27,  2.64it/s] 22%|â–ˆâ–ˆâ–       | 685/3145 [04:26<14:23,  2.85it/s] 22%|â–ˆâ–ˆâ–       | 688/3143 [04:24<13:48,  2.96it/s] 22%|â–ˆâ–ˆâ–       | 682/3145 [04:24<15:10,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 695/3145 [04:37<14:46,  2.76it/s] 22%|â–ˆâ–ˆâ–       | 686/3145 [04:26<14:59,  2.73it/s] 22%|â–ˆâ–ˆâ–       | 689/3143 [04:24<14:01,  2.92it/s] 22%|â–ˆâ–ˆâ–       | 683/3145 [04:24<14:46,  2.78it/s] 22%|â–ˆâ–ˆâ–       | 696/3145 [04:37<15:21,  2.66it/s] 22%|â–ˆâ–ˆâ–       | 690/3143 [04:24<13:52,  2.95it/s] 22%|â–ˆâ–ˆâ–       | 684/3145 [04:24<15:06,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 687/3145 [04:27<16:07,  2.54it/s] 22%|â–ˆâ–ˆâ–       | 697/3145 [04:37<15:53,  2.57it/s] 22%|â–ˆâ–ˆâ–       | 691/3143 [04:25<13:54,  2.94it/s] 22%|â–ˆâ–ˆâ–       | 685/3145 [04:25<14:54,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 688/3145 [04:27<15:50,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 692/3143 [04:25<14:08,  2.89it/s] 22%|â–ˆâ–ˆâ–       | 698/3145 [04:38<15:47,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 686/3145 [04:25<14:07,  2.90it/s] 22%|â–ˆâ–ˆâ–       | 689/3145 [04:28<15:31,  2.64it/s] 22%|â–ˆâ–ˆâ–       | 687/3145 [04:25<14:08,  2.90it/s] 22%|â–ˆâ–ˆâ–       | 693/3143 [04:25<14:38,  2.79it/s] 22%|â–ˆâ–ˆâ–       | 699/3145 [04:38<16:21,  2.49it/s] 22%|â–ˆâ–ˆâ–       | 690/3145 [04:28<14:56,  2.74it/s] 22%|â–ˆâ–ˆâ–       | 688/3145 [04:26<14:35,  2.81it/s] 22%|â–ˆâ–ˆâ–       | 694/3143 [04:26<15:12,  2.68it/s] 22%|â–ˆâ–ˆâ–       | 691/3145 [04:28<15:22,  2.66it/s] 22%|â–ˆâ–ˆâ–       | 700/3145 [04:39<17:03,  2.39it/s] 22%|â–ˆâ–ˆâ–       | 695/3143 [04:26<15:02,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 692/3145 [04:29<14:37,  2.80it/s] 22%|â–ˆâ–ˆâ–       | 689/3145 [04:26<15:22,  2.66it/s] 22%|â–ˆâ–ˆâ–       | 701/3145 [04:39<16:19,  2.50it/s] 22%|â–ˆâ–ˆâ–       | 693/3145 [04:29<14:31,  2.81it/s] 22%|â–ˆâ–ˆâ–       | 690/3145 [04:26<15:35,  2.62it/s] 22%|â–ˆâ–ˆâ–       | 696/3143 [04:27<16:34,  2.46it/s] 22%|â–ˆâ–ˆâ–       | 702/3145 [04:39<16:08,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 694/3145 [04:29<14:58,  2.73it/s] 22%|â–ˆâ–ˆâ–       | 691/3145 [04:27<15:46,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 703/3145 [04:40<16:07,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 697/3143 [04:27<18:02,  2.26it/s] 22%|â–ˆâ–ˆâ–       | 695/3145 [04:30<14:51,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 692/3145 [04:27<14:59,  2.73it/s] 22%|â–ˆâ–ˆâ–       | 704/3145 [04:40<15:57,  2.55it/s] 22%|â–ˆâ–ˆâ–       | 698/3143 [04:27<17:00,  2.40it/s] 22%|â–ˆâ–ˆâ–       | 696/3145 [04:30<15:07,  2.70it/s] 22%|â–ˆâ–ˆâ–       | 693/3145 [04:28<15:03,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 705/3145 [04:41<16:14,  2.50it/s] 22%|â–ˆâ–ˆâ–       | 699/3143 [04:28<16:20,  2.49it/s] 22%|â–ˆâ–ˆâ–       | 697/3145 [04:31<15:49,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 694/3145 [04:28<16:57,  2.41it/s] 22%|â–ˆâ–ˆâ–       | 706/3145 [04:41<15:41,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 700/3143 [04:28<15:43,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 698/3145 [04:31<15:48,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 695/3145 [04:28<16:10,  2.52it/s] 22%|â–ˆâ–ˆâ–       | 707/3145 [04:41<15:23,  2.64it/s] 22%|â–ˆâ–ˆâ–       | 701/3143 [04:29<15:45,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 699/3145 [04:31<15:45,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 696/3145 [04:29<15:42,  2.60it/s] 23%|â–ˆâ–ˆâ–Ž       | 708/3145 [04:42<15:20,  2.65it/s] 22%|â–ˆâ–ˆâ–       | 702/3143 [04:29<15:21,  2.65it/s] 22%|â–ˆâ–ˆâ–       | 697/3145 [04:29<15:04,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 700/3145 [04:32<15:39,  2.60it/s] 23%|â–ˆâ–ˆâ–Ž       | 709/3145 [04:42<14:50,  2.74it/s] 22%|â–ˆâ–ˆâ–       | 703/3143 [04:29<15:27,  2.63it/s] 22%|â–ˆâ–ˆâ–       | 698/3145 [04:30<15:02,  2.71it/s] 22%|â–ˆâ–ˆâ–       | 701/3145 [04:32<15:50,  2.57it/s] 23%|â–ˆâ–ˆâ–Ž       | 710/3145 [04:42<14:51,  2.73it/s] 22%|â–ˆâ–ˆâ–       | 704/3143 [04:30<14:09,  2.87it/s] 22%|â–ˆâ–ˆâ–       | 699/3145 [04:30<14:40,  2.78it/s] 22%|â–ˆâ–ˆâ–       | 702/3145 [04:32<15:19,  2.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 711/3145 [04:43<15:07,  2.68it/s] 22%|â–ˆâ–ˆâ–       | 705/3143 [04:30<15:07,  2.69it/s] 22%|â–ˆâ–ˆâ–       | 700/3145 [04:30<14:09,  2.88it/s] 22%|â–ˆâ–ˆâ–       | 703/3145 [04:33<14:53,  2.73it/s] 23%|â–ˆâ–ˆâ–Ž       | 712/3145 [04:43<15:17,  2.65it/s] 22%|â–ˆâ–ˆâ–       | 706/3143 [04:30<14:55,  2.72it/s] 22%|â–ˆâ–ˆâ–       | 701/3145 [04:31<14:02,  2.90it/s] 22%|â–ˆâ–ˆâ–       | 704/3145 [04:33<14:46,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 707/3143 [04:31<14:47,  2.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 713/3145 [04:44<15:30,  2.61it/s] 22%|â–ˆâ–ˆâ–       | 702/3145 [04:31<13:56,  2.92it/s] 22%|â–ˆâ–ˆâ–       | 705/3145 [04:34<15:14,  2.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 708/3143 [04:31<14:39,  2.77it/s] 23%|â–ˆâ–ˆâ–Ž       | 714/3145 [04:44<15:08,  2.67it/s] 22%|â–ˆâ–ˆâ–       | 703/3145 [04:31<14:08,  2.88it/s] 22%|â–ˆâ–ˆâ–       | 706/3145 [04:34<15:00,  2.71it/s] 23%|â–ˆâ–ˆâ–Ž       | 709/3143 [04:31<14:35,  2.78it/s] 23%|â–ˆâ–ˆâ–Ž       | 715/3145 [04:44<15:33,  2.60it/s] 22%|â–ˆâ–ˆâ–       | 704/3145 [04:32<14:34,  2.79it/s] 23%|â–ˆâ–ˆâ–Ž       | 710/3143 [04:32<13:47,  2.94it/s] 22%|â–ˆâ–ˆâ–       | 707/3145 [04:34<15:21,  2.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 716/3145 [04:45<14:53,  2.72it/s] 22%|â–ˆâ–ˆâ–       | 705/3145 [04:32<13:48,  2.94it/s] 23%|â–ˆâ–ˆâ–Ž       | 708/3145 [04:35<15:25,  2.63it/s] 23%|â–ˆâ–ˆâ–Ž       | 711/3143 [04:32<14:47,  2.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 717/3145 [04:45<15:51,  2.55it/s] 22%|â–ˆâ–ˆâ–       | 706/3145 [04:32<14:46,  2.75it/s] 23%|â–ˆâ–ˆâ–Ž       | 709/3145 [04:35<15:27,  2.63it/s] 23%|â–ˆâ–ˆâ–Ž       | 712/3143 [04:33<15:00,  2.70it/s] 23%|â–ˆâ–ˆâ–Ž       | 718/3145 [04:45<15:24,  2.62it/s] 22%|â–ˆâ–ˆâ–       | 707/3145 [04:33<15:02,  2.70it/s] 23%|â–ˆâ–ˆâ–Ž       | 713/3143 [04:33<15:22,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 710/3145 [04:35<15:56,  2.55it/s] 23%|â–ˆâ–ˆâ–Ž       | 708/3145 [04:33<14:31,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 719/3145 [04:46<15:56,  2.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 711/3145 [04:36<15:16,  2.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 714/3143 [04:33<15:04,  2.69it/s] 23%|â–ˆâ–ˆâ–Ž       | 709/3145 [04:33<14:52,  2.73it/s] 23%|â–ˆâ–ˆâ–Ž       | 720/3145 [04:46<15:25,  2.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 712/3145 [04:36<15:24,  2.63it/s] 23%|â–ˆâ–ˆâ–Ž       | 715/3143 [04:34<15:28,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 710/3145 [04:34<14:53,  2.73it/s] 23%|â–ˆâ–ˆâ–Ž       | 721/3145 [04:47<16:54,  2.39it/s] 23%|â–ˆâ–ˆâ–Ž       | 716/3143 [04:34<15:29,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 713/3145 [04:37<15:56,  2.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 711/3145 [04:34<15:58,  2.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 722/3145 [04:47<16:13,  2.49it/s] 23%|â–ˆâ–ˆâ–Ž       | 717/3143 [04:34<15:10,  2.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 714/3145 [04:37<15:26,  2.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 712/3145 [04:35<15:50,  2.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 723/3145 [04:48<17:16,  2.34it/s] 23%|â–ˆâ–ˆâ–Ž       | 718/3143 [04:35<15:21,  2.63it/s] 23%|â–ˆâ–ˆâ–Ž       | 715/3145 [04:37<15:39,  2.59it/s] 23%|â–ˆâ–ˆâ–Ž       | 713/3145 [04:35<16:09,  2.51it/s] 23%|â–ˆâ–ˆâ–Ž       | 719/3143 [04:35<14:49,  2.72it/s] 23%|â–ˆâ–ˆâ–Ž       | 716/3145 [04:38<15:13,  2.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 724/3145 [04:48<17:48,  2.27it/s] 23%|â–ˆâ–ˆâ–Ž       | 714/3145 [04:35<15:49,  2.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 720/3143 [04:36<14:24,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 717/3145 [04:38<15:48,  2.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 725/3145 [04:49<18:27,  2.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 721/3143 [04:36<14:25,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 715/3145 [04:36<17:31,  2.31it/s] 23%|â–ˆâ–ˆâ–Ž       | 718/3145 [04:38<15:25,  2.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 726/3145 [04:49<16:54,  2.39it/s] 23%|â–ˆâ–ˆâ–Ž       | 722/3143 [04:36<15:07,  2.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 716/3145 [04:36<16:36,  2.44it/s] 23%|â–ˆâ–ˆâ–Ž       | 719/3145 [04:39<15:29,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 727/3145 [04:49<16:07,  2.50it/s] 23%|â–ˆâ–ˆâ–Ž       | 723/3143 [04:37<15:00,  2.69it/s] 23%|â–ˆâ–ˆâ–Ž       | 717/3145 [04:37<15:49,  2.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 720/3145 [04:39<15:18,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 728/3145 [04:50<15:37,  2.58it/s] 23%|â–ˆâ–ˆâ–Ž       | 724/3143 [04:37<14:41,  2.75it/s] 23%|â–ˆâ–ˆâ–Ž       | 718/3145 [04:37<15:29,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 721/3145 [04:40<14:45,  2.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 729/3145 [04:50<15:13,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 725/3143 [04:37<14:25,  2.79it/s] 23%|â–ˆâ–ˆâ–Ž       | 730/3145 [04:50<13:30,  2.98it/s] 23%|â–ˆâ–ˆâ–Ž       | 719/3145 [04:37<15:33,  2.60it/s] 23%|â–ˆâ–ˆâ–Ž       | 722/3145 [04:40<14:22,  2.81it/s] 23%|â–ˆâ–ˆâ–Ž       | 720/3145 [04:38<13:53,  2.91it/s] 23%|â–ˆâ–ˆâ–Ž       | 726/3143 [04:38<14:06,  2.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 731/3145 [04:51<14:06,  2.85it/s] 23%|â–ˆâ–ˆâ–Ž       | 723/3145 [04:40<14:53,  2.71it/s] 23%|â–ˆâ–ˆâ–Ž       | 721/3145 [04:38<14:06,  2.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 727/3143 [04:38<14:18,  2.82it/s] 23%|â–ˆâ–ˆâ–Ž       | 732/3145 [04:51<14:17,  2.81it/s] 23%|â–ˆâ–ˆâ–Ž       | 724/3145 [04:41<15:05,  2.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 722/3145 [04:38<13:56,  2.90it/s] 23%|â–ˆâ–ˆâ–Ž       | 728/3143 [04:38<14:06,  2.85it/s] 23%|â–ˆâ–ˆâ–Ž       | 733/3145 [04:51<14:00,  2.87it/s] 23%|â–ˆâ–ˆâ–Ž       | 725/3145 [04:41<15:17,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 723/3145 [04:39<14:27,  2.79it/s] 23%|â–ˆâ–ˆâ–Ž       | 729/3143 [04:39<14:36,  2.76it/s] 23%|â–ˆâ–ˆâ–Ž       | 734/3145 [04:52<14:03,  2.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 724/3145 [04:39<12:49,  3.15it/s] 23%|â–ˆâ–ˆâ–Ž       | 726/3145 [04:41<15:23,  2.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 730/3143 [04:39<14:51,  2.71it/s] 23%|â–ˆâ–ˆâ–Ž       | 735/3145 [04:52<14:09,  2.84it/s] 23%|â–ˆâ–ˆâ–Ž       | 725/3145 [04:39<13:18,  3.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 727/3145 [04:42<15:55,  2.53it/s] 23%|â–ˆâ–ˆâ–Ž       | 731/3143 [04:40<15:04,  2.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 736/3145 [04:52<14:35,  2.75it/s] 23%|â–ˆâ–ˆâ–Ž       | 726/3145 [04:40<13:39,  2.95it/s] 23%|â–ˆâ–ˆâ–Ž       | 728/3145 [04:42<15:50,  2.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 737/3145 [04:53<14:00,  2.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 732/3143 [04:40<15:12,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 727/3145 [04:40<13:54,  2.90it/s] 23%|â–ˆâ–ˆâ–Ž       | 729/3145 [04:43<15:57,  2.52it/s] 23%|â–ˆâ–ˆâ–Ž       | 738/3145 [04:53<14:26,  2.78it/s] 23%|â–ˆâ–ˆâ–Ž       | 733/3143 [04:40<15:20,  2.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 728/3145 [04:40<14:22,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 730/3145 [04:43<15:11,  2.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 739/3145 [04:53<15:05,  2.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 734/3143 [04:41<15:24,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 729/3145 [04:41<14:23,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 731/3145 [04:43<14:35,  2.76it/s] 24%|â–ˆâ–ˆâ–Ž       | 740/3145 [04:54<15:09,  2.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 735/3143 [04:41<15:30,  2.59it/s] 23%|â–ˆâ–ˆâ–Ž       | 730/3145 [04:41<14:20,  2.81it/s] 23%|â–ˆâ–ˆâ–Ž       | 732/3145 [04:44<14:32,  2.77it/s] 23%|â–ˆâ–ˆâ–Ž       | 736/3143 [04:41<15:12,  2.64it/s] 24%|â–ˆâ–ˆâ–Ž       | 741/3145 [04:54<15:22,  2.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 731/3145 [04:41<14:23,  2.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 733/3145 [04:44<14:30,  2.77it/s] 23%|â–ˆâ–ˆâ–Ž       | 732/3145 [04:42<12:55,  3.11it/s] 23%|â–ˆâ–ˆâ–Ž       | 737/3143 [04:42<14:50,  2.70it/s] 24%|â–ˆâ–ˆâ–Ž       | 742/3145 [04:55<15:01,  2.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 734/3145 [04:44<14:10,  2.83it/s] 23%|â–ˆâ–ˆâ–Ž       | 733/3145 [04:42<13:39,  2.94it/s] 23%|â–ˆâ–ˆâ–Ž       | 738/3143 [04:42<14:17,  2.81it/s] 24%|â–ˆâ–ˆâ–Ž       | 743/3145 [04:55<14:26,  2.77it/s] 23%|â–ˆâ–ˆâ–Ž       | 735/3145 [04:45<14:16,  2.82it/s] 24%|â–ˆâ–ˆâ–Ž       | 739/3143 [04:42<14:07,  2.84it/s] 23%|â–ˆâ–ˆâ–Ž       | 734/3145 [04:42<14:16,  2.81it/s] 24%|â–ˆâ–ˆâ–Ž       | 744/3145 [04:55<14:51,  2.69it/s] 23%|â–ˆâ–ˆâ–Ž       | 736/3145 [04:45<14:18,  2.80it/s] 24%|â–ˆâ–ˆâ–Ž       | 740/3143 [04:43<14:09,  2.83it/s] 23%|â–ˆâ–ˆâ–Ž       | 735/3145 [04:43<14:21,  2.80it/s] 24%|â–ˆâ–ˆâ–Ž       | 745/3145 [04:56<15:05,  2.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 737/3145 [04:46<15:28,  2.59it/s] 23%|â–ˆâ–ˆâ–Ž       | 736/3145 [04:43<13:06,  3.06it/s] 24%|â–ˆâ–ˆâ–Ž       | 741/3143 [04:43<14:15,  2.81it/s] 24%|â–ˆâ–ˆâ–Ž       | 746/3145 [04:56<14:52,  2.69it/s] 23%|â–ˆâ–ˆâ–Ž       | 738/3145 [04:46<14:51,  2.70it/s] 23%|â–ˆâ–ˆâ–Ž       | 737/3145 [04:43<13:30,  2.97it/s] 24%|â–ˆâ–ˆâ–Ž       | 742/3143 [04:44<14:34,  2.74it/s] 24%|â–ˆâ–ˆâ–       | 747/3145 [04:56<14:42,  2.72it/s] 23%|â–ˆâ–ˆâ–Ž       | 738/3145 [04:44<12:00,  3.34it/s] 23%|â–ˆâ–ˆâ–Ž       | 739/3145 [04:46<15:10,  2.64it/s] 24%|â–ˆâ–ˆâ–Ž       | 743/3143 [04:44<14:08,  2.83it/s] 23%|â–ˆâ–ˆâ–Ž       | 739/3145 [04:44<12:35,  3.18it/s] 24%|â–ˆâ–ˆâ–       | 748/3145 [04:57<16:16,  2.45it/s] 24%|â–ˆâ–ˆâ–Ž       | 740/3145 [04:47<14:57,  2.68it/s] 24%|â–ˆâ–ˆâ–Ž       | 744/3143 [04:44<14:01,  2.85it/s] 24%|â–ˆâ–ˆâ–Ž       | 740/3145 [04:44<13:04,  3.06it/s] 24%|â–ˆâ–ˆâ–Ž       | 741/3145 [04:47<14:29,  2.77it/s] 24%|â–ˆâ–ˆâ–       | 749/3145 [04:57<16:17,  2.45it/s] 24%|â–ˆâ–ˆâ–Ž       | 745/3143 [04:45<14:50,  2.69it/s] 24%|â–ˆâ–ˆâ–Ž       | 741/3145 [04:45<13:51,  2.89it/s] 24%|â–ˆâ–ˆâ–Ž       | 742/3145 [04:47<14:52,  2.69it/s] 24%|â–ˆâ–ˆâ–       | 750/3145 [04:58<15:48,  2.53it/s] 24%|â–ˆâ–ˆâ–Ž       | 746/3143 [04:45<14:36,  2.74it/s] 24%|â–ˆâ–ˆâ–Ž       | 742/3145 [04:45<14:12,  2.82it/s] 24%|â–ˆâ–ˆâ–       | 751/3145 [04:58<15:23,  2.59it/s] 24%|â–ˆâ–ˆâ–Ž       | 743/3145 [04:48<15:54,  2.52it/s] 24%|â–ˆâ–ˆâ–       | 747/3143 [04:45<15:18,  2.61it/s] 24%|â–ˆâ–ˆâ–Ž       | 743/3145 [04:46<14:24,  2.78it/s] 24%|â–ˆâ–ˆâ–Ž       | 744/3145 [04:48<15:08,  2.64it/s] 24%|â–ˆâ–ˆâ–       | 752/3145 [04:59<16:11,  2.46it/s] 24%|â–ˆâ–ˆâ–       | 748/3143 [04:46<15:00,  2.66it/s] 24%|â–ˆâ–ˆâ–Ž       | 744/3145 [04:46<14:41,  2.72it/s] 24%|â–ˆâ–ˆâ–       | 753/3145 [04:59<15:39,  2.55it/s] 24%|â–ˆâ–ˆâ–Ž       | 745/3145 [04:49<15:40,  2.55it/s] 24%|â–ˆâ–ˆâ–       | 749/3143 [04:46<14:49,  2.69it/s] 24%|â–ˆâ–ˆâ–       | 754/3145 [04:59<13:41,  2.91it/s] 24%|â–ˆâ–ˆâ–Ž       | 745/3145 [04:46<15:19,  2.61it/s] 24%|â–ˆâ–ˆâ–Ž       | 746/3145 [04:49<14:52,  2.69it/s] 24%|â–ˆâ–ˆâ–       | 750/3143 [04:47<14:38,  2.72it/s] 24%|â–ˆâ–ˆâ–Ž       | 746/3145 [04:47<15:01,  2.66it/s] 24%|â–ˆâ–ˆâ–       | 755/3145 [04:59<14:09,  2.81it/s] 24%|â–ˆâ–ˆâ–       | 747/3145 [04:49<14:41,  2.72it/s] 24%|â–ˆâ–ˆâ–       | 751/3143 [04:47<14:31,  2.75it/s] 24%|â–ˆâ–ˆâ–       | 756/3145 [05:00<14:18,  2.78it/s] 24%|â–ˆâ–ˆâ–       | 747/3145 [04:47<15:05,  2.65it/s] 24%|â–ˆâ–ˆâ–       | 748/3145 [04:50<15:03,  2.65it/s] 24%|â–ˆâ–ˆâ–       | 752/3143 [04:47<14:10,  2.81it/s] 24%|â–ˆâ–ˆâ–       | 757/3145 [05:00<14:17,  2.78it/s] 24%|â–ˆâ–ˆâ–       | 748/3145 [04:47<15:15,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 749/3145 [04:50<15:16,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 753/3143 [04:48<14:39,  2.72it/s] 24%|â–ˆâ–ˆâ–       | 758/3145 [05:01<14:14,  2.79it/s] 24%|â–ˆâ–ˆâ–       | 749/3145 [04:48<15:00,  2.66it/s] 24%|â–ˆâ–ˆâ–       | 754/3143 [04:48<14:35,  2.73it/s] 24%|â–ˆâ–ˆâ–       | 750/3145 [04:50<15:21,  2.60it/s] 24%|â–ˆâ–ˆâ–       | 759/3145 [05:01<14:14,  2.79it/s] 24%|â–ˆâ–ˆâ–       | 750/3145 [04:48<14:45,  2.71it/s] 24%|â–ˆâ–ˆâ–       | 755/3143 [04:48<14:31,  2.74it/s] 24%|â–ˆâ–ˆâ–       | 751/3145 [04:51<15:26,  2.58it/s] 24%|â–ˆâ–ˆâ–       | 760/3145 [05:01<14:15,  2.79it/s] 24%|â–ˆâ–ˆâ–       | 756/3143 [04:49<14:23,  2.76it/s] 24%|â–ˆâ–ˆâ–       | 751/3145 [04:49<16:20,  2.44it/s] 24%|â–ˆâ–ˆâ–       | 752/3145 [04:51<15:07,  2.64it/s] 24%|â–ˆâ–ˆâ–       | 761/3145 [05:02<12:44,  3.12it/s] 24%|â–ˆâ–ˆâ–       | 757/3143 [04:49<14:41,  2.71it/s] 24%|â–ˆâ–ˆâ–       | 762/3145 [05:02<13:02,  3.05it/s] 24%|â–ˆâ–ˆâ–       | 753/3145 [04:52<15:23,  2.59it/s] 24%|â–ˆâ–ˆâ–       | 752/3145 [04:49<16:59,  2.35it/s] 24%|â–ˆâ–ˆâ–       | 758/3143 [04:49<14:34,  2.73it/s] 24%|â–ˆâ–ˆâ–       | 763/3145 [05:02<13:29,  2.94it/s] 24%|â–ˆâ–ˆâ–       | 753/3145 [04:50<16:16,  2.45it/s] 24%|â–ˆâ–ˆâ–       | 754/3145 [04:52<15:24,  2.59it/s] 24%|â–ˆâ–ˆâ–       | 759/3143 [04:50<14:30,  2.74it/s] 24%|â–ˆâ–ˆâ–       | 764/3145 [05:03<13:36,  2.92it/s] 24%|â–ˆâ–ˆâ–       | 755/3145 [04:52<15:13,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 754/3145 [04:50<16:07,  2.47it/s] 24%|â–ˆâ–ˆâ–       | 765/3145 [05:03<14:08,  2.81it/s] 24%|â–ˆâ–ˆâ–       | 760/3143 [04:50<15:08,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 755/3145 [04:50<15:32,  2.56it/s] 24%|â–ˆâ–ˆâ–       | 756/3145 [04:53<15:51,  2.51it/s] 24%|â–ˆâ–ˆâ–       | 761/3143 [04:51<14:51,  2.67it/s] 24%|â–ˆâ–ˆâ–       | 766/3145 [05:03<14:24,  2.75it/s] 24%|â–ˆâ–ˆâ–       | 756/3145 [04:51<15:07,  2.63it/s] 24%|â–ˆâ–ˆâ–       | 757/3145 [04:53<15:32,  2.56it/s] 24%|â–ˆâ–ˆâ–       | 762/3143 [04:51<15:11,  2.61it/s] 24%|â–ˆâ–ˆâ–       | 767/3145 [05:04<15:07,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 757/3145 [04:51<15:14,  2.61it/s] 24%|â–ˆâ–ˆâ–       | 758/3145 [04:54<15:27,  2.57it/s] 24%|â–ˆâ–ˆâ–       | 763/3143 [04:51<15:12,  2.61it/s] 24%|â–ˆâ–ˆâ–       | 768/3145 [05:04<15:10,  2.61it/s] 24%|â–ˆâ–ˆâ–       | 758/3145 [04:51<16:00,  2.48it/s] 24%|â–ˆâ–ˆâ–       | 759/3145 [04:54<15:37,  2.55it/s] 24%|â–ˆâ–ˆâ–       | 769/3145 [05:05<15:15,  2.60it/s] 24%|â–ˆâ–ˆâ–       | 764/3143 [04:52<15:38,  2.53it/s] 24%|â–ˆâ–ˆâ–       | 759/3145 [04:52<15:51,  2.51it/s] 24%|â–ˆâ–ˆâ–       | 760/3145 [04:54<15:35,  2.55it/s] 24%|â–ˆâ–ˆâ–       | 765/3143 [04:52<15:37,  2.54it/s] 24%|â–ˆâ–ˆâ–       | 770/3145 [05:05<15:38,  2.53it/s] 24%|â–ˆâ–ˆâ–       | 760/3145 [04:52<15:22,  2.58it/s] 24%|â–ˆâ–ˆâ–       | 761/3145 [04:55<15:32,  2.56it/s] 25%|â–ˆâ–ˆâ–       | 771/3145 [05:05<15:32,  2.55it/s] 24%|â–ˆâ–ˆâ–       | 761/3145 [04:53<15:23,  2.58it/s] 24%|â–ˆâ–ˆâ–       | 766/3143 [04:53<16:15,  2.44it/s] 24%|â–ˆâ–ˆâ–       | 762/3145 [04:55<15:30,  2.56it/s] 25%|â–ˆâ–ˆâ–       | 772/3145 [05:06<14:37,  2.70it/s] 24%|â–ˆâ–ˆâ–       | 762/3145 [04:53<14:43,  2.70it/s] 24%|â–ˆâ–ˆâ–       | 763/3145 [04:56<15:08,  2.62it/s] 24%|â–ˆâ–ˆâ–       | 767/3143 [04:53<16:46,  2.36it/s] 25%|â–ˆâ–ˆâ–       | 773/3145 [05:06<14:31,  2.72it/s] 24%|â–ˆâ–ˆâ–       | 763/3145 [04:53<15:19,  2.59it/s] 24%|â–ˆâ–ˆâ–       | 764/3145 [04:56<15:19,  2.59it/s] 24%|â–ˆâ–ˆâ–       | 768/3143 [04:53<15:56,  2.48it/s] 25%|â–ˆâ–ˆâ–       | 774/3145 [05:06<15:02,  2.63it/s] 24%|â–ˆâ–ˆâ–       | 764/3145 [04:54<15:17,  2.60it/s] 24%|â–ˆâ–ˆâ–       | 765/3145 [04:56<15:03,  2.63it/s] 24%|â–ˆâ–ˆâ–       | 769/3143 [04:54<15:25,  2.56it/s] 25%|â–ˆâ–ˆâ–       | 775/3145 [05:07<15:09,  2.61it/s] 24%|â–ˆâ–ˆâ–       | 765/3145 [04:54<14:58,  2.65it/s] 24%|â–ˆâ–ˆâ–       | 766/3145 [04:57<14:50,  2.67it/s] 24%|â–ˆâ–ˆâ–       | 770/3143 [04:54<15:22,  2.57it/s] 25%|â–ˆâ–ˆâ–       | 776/3145 [05:07<14:30,  2.72it/s] 24%|â–ˆâ–ˆâ–       | 766/3145 [04:54<14:45,  2.69it/s] 25%|â–ˆâ–ˆâ–       | 771/3143 [04:55<14:44,  2.68it/s] 24%|â–ˆâ–ˆâ–       | 767/3145 [04:57<14:44,  2.69it/s] 24%|â–ˆâ–ˆâ–       | 768/3145 [04:57<13:00,  3.04it/s] 25%|â–ˆâ–ˆâ–       | 777/3145 [05:08<14:26,  2.73it/s] 24%|â–ˆâ–ˆâ–       | 767/3145 [04:55<14:02,  2.82it/s] 25%|â–ˆâ–ˆâ–       | 772/3143 [04:55<15:47,  2.50it/s] 24%|â–ˆâ–ˆâ–       | 768/3145 [04:55<13:34,  2.92it/s] 25%|â–ˆâ–ˆâ–       | 778/3145 [05:08<14:24,  2.74it/s] 24%|â–ˆâ–ˆâ–       | 769/3145 [04:58<13:51,  2.86it/s] 25%|â–ˆâ–ˆâ–       | 773/3143 [04:55<14:35,  2.71it/s] 24%|â–ˆâ–ˆâ–       | 769/3145 [04:55<12:26,  3.18it/s] 25%|â–ˆâ–ˆâ–       | 779/3145 [05:08<14:00,  2.81it/s] 24%|â–ˆâ–ˆâ–       | 770/3145 [04:58<14:03,  2.81it/s] 24%|â–ˆâ–ˆâ–       | 770/3145 [04:56<11:36,  3.41it/s] 25%|â–ˆâ–ˆâ–       | 774/3143 [04:56<14:45,  2.67it/s] 25%|â–ˆâ–ˆâ–       | 780/3145 [05:09<14:21,  2.75it/s] 25%|â–ˆâ–ˆâ–       | 771/3145 [04:56<11:34,  3.42it/s] 25%|â–ˆâ–ˆâ–       | 771/3145 [04:58<14:34,  2.71it/s] 25%|â–ˆâ–ˆâ–       | 775/3143 [04:56<14:45,  2.68it/s] 25%|â–ˆâ–ˆâ–       | 781/3145 [05:09<14:22,  2.74it/s] 25%|â–ˆâ–ˆâ–       | 772/3145 [04:56<12:05,  3.27it/s] 25%|â–ˆâ–ˆâ–       | 772/3145 [04:59<14:26,  2.74it/s] 25%|â–ˆâ–ˆâ–       | 776/3143 [04:56<14:32,  2.71it/s] 25%|â–ˆâ–ˆâ–       | 773/3145 [04:56<11:33,  3.42it/s] 25%|â–ˆâ–ˆâ–       | 782/3145 [05:09<14:22,  2.74it/s] 25%|â–ˆâ–ˆâ–       | 773/3145 [04:59<14:59,  2.64it/s] 25%|â–ˆâ–ˆâ–       | 777/3143 [04:57<14:09,  2.78it/s] 25%|â–ˆâ–ˆâ–       | 774/3145 [04:57<12:20,  3.20it/s] 25%|â–ˆâ–ˆâ–       | 783/3145 [05:10<14:36,  2.70it/s] 25%|â–ˆâ–ˆâ–       | 774/3145 [05:00<15:25,  2.56it/s] 25%|â–ˆâ–ˆâ–       | 778/3143 [04:57<14:35,  2.70it/s] 25%|â–ˆâ–ˆâ–       | 775/3145 [04:57<12:31,  3.15it/s] 25%|â–ˆâ–ˆâ–       | 784/3145 [05:10<14:08,  2.78it/s] 25%|â–ˆâ–ˆâ–       | 775/3145 [05:00<14:35,  2.71it/s] 25%|â–ˆâ–ˆâ–       | 779/3143 [04:57<14:12,  2.77it/s] 25%|â–ˆâ–ˆâ–       | 776/3145 [04:58<13:26,  2.94it/s] 25%|â–ˆâ–ˆâ–       | 785/3145 [05:10<14:26,  2.72it/s] 25%|â–ˆâ–ˆâ–       | 776/3145 [05:00<14:08,  2.79it/s] 25%|â–ˆâ–ˆâ–       | 780/3143 [04:58<13:45,  2.86it/s] 25%|â–ˆâ–ˆâ–       | 777/3145 [04:58<13:38,  2.89it/s] 25%|â–ˆâ–ˆâ–       | 786/3145 [05:11<14:25,  2.72it/s] 25%|â–ˆâ–ˆâ–       | 777/3145 [05:01<14:28,  2.73it/s] 25%|â–ˆâ–ˆâ–       | 781/3143 [04:58<14:15,  2.76it/s] 25%|â–ˆâ–ˆâ–       | 778/3145 [04:58<12:45,  3.09it/s] 25%|â–ˆâ–ˆâ–Œ       | 787/3145 [05:11<14:39,  2.68it/s] 25%|â–ˆâ–ˆâ–       | 778/3145 [05:01<14:50,  2.66it/s] 25%|â–ˆâ–ˆâ–       | 779/3145 [04:59<13:09,  3.00it/s] 25%|â–ˆâ–ˆâ–       | 782/3143 [04:59<14:30,  2.71it/s] 25%|â–ˆâ–ˆâ–Œ       | 788/3145 [05:12<14:02,  2.80it/s] 25%|â–ˆâ–ˆâ–       | 783/3143 [04:59<14:12,  2.77it/s] 25%|â–ˆâ–ˆâ–       | 779/3145 [05:01<15:06,  2.61it/s] 25%|â–ˆâ–ˆâ–       | 780/3145 [04:59<13:53,  2.84it/s] 25%|â–ˆâ–ˆâ–       | 781/3145 [04:59<12:49,  3.07it/s] 25%|â–ˆâ–ˆâ–Œ       | 789/3145 [05:12<15:41,  2.50it/s] 25%|â–ˆâ–ˆâ–       | 784/3143 [04:59<14:10,  2.77it/s] 25%|â–ˆâ–ˆâ–       | 780/3145 [05:02<14:47,  2.67it/s] 25%|â–ˆâ–ˆâ–       | 782/3145 [05:00<13:06,  3.00it/s] 25%|â–ˆâ–ˆâ–       | 785/3143 [05:00<13:46,  2.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 790/3145 [05:12<15:12,  2.58it/s] 25%|â–ˆâ–ˆâ–       | 781/3145 [05:02<14:55,  2.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 791/3145 [05:13<14:50,  2.64it/s] 25%|â–ˆâ–ˆâ–       | 783/3145 [05:00<13:57,  2.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 786/3143 [05:00<14:01,  2.80it/s] 25%|â–ˆâ–ˆâ–       | 782/3145 [05:03<14:37,  2.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 787/3143 [05:00<14:21,  2.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 792/3145 [05:13<15:02,  2.61it/s] 25%|â–ˆâ–ˆâ–       | 784/3145 [05:00<14:33,  2.70it/s] 25%|â–ˆâ–ˆâ–       | 783/3145 [05:03<14:31,  2.71it/s] 25%|â–ˆâ–ˆâ–Œ       | 788/3143 [05:01<14:14,  2.75it/s] 25%|â–ˆâ–ˆâ–       | 785/3145 [05:01<14:43,  2.67it/s] 25%|â–ˆâ–ˆâ–       | 784/3145 [05:03<14:43,  2.67it/s] 25%|â–ˆâ–ˆâ–Œ       | 793/3145 [05:14<16:04,  2.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 789/3143 [05:01<14:32,  2.70it/s] 25%|â–ˆâ–ˆâ–       | 785/3145 [05:04<14:32,  2.71it/s] 25%|â–ˆâ–ˆâ–       | 786/3145 [05:01<14:59,  2.62it/s] 25%|â–ˆâ–ˆâ–Œ       | 794/3145 [05:14<15:27,  2.53it/s] 25%|â–ˆâ–ˆâ–       | 786/3145 [05:04<14:23,  2.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 790/3143 [05:02<15:04,  2.60it/s] 25%|â–ˆâ–ˆâ–Œ       | 787/3145 [05:02<15:37,  2.51it/s] 25%|â–ˆâ–ˆâ–Œ       | 795/3145 [05:14<16:29,  2.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 787/3145 [05:04<14:16,  2.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 791/3143 [05:02<15:13,  2.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 788/3145 [05:02<15:16,  2.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 796/3145 [05:15<15:46,  2.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 788/3145 [05:05<12:49,  3.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 792/3143 [05:02<15:32,  2.52it/s] 25%|â–ˆâ–ˆâ–Œ       | 789/3145 [05:02<15:13,  2.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 797/3145 [05:15<15:00,  2.61it/s] 25%|â–ˆâ–ˆâ–Œ       | 789/3145 [05:05<13:28,  2.91it/s] 25%|â–ˆâ–ˆâ–Œ       | 790/3145 [05:03<15:14,  2.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 798/3145 [05:16<15:00,  2.61it/s] 25%|â–ˆâ–ˆâ–Œ       | 793/3143 [05:03<16:39,  2.35it/s] 25%|â–ˆâ–ˆâ–Œ       | 790/3145 [05:05<14:00,  2.80it/s] 25%|â–ˆâ–ˆâ–Œ       | 791/3145 [05:03<14:36,  2.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 799/3145 [05:16<14:41,  2.66it/s] 25%|â–ˆâ–ˆâ–Œ       | 794/3143 [05:03<16:11,  2.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 791/3145 [05:06<14:16,  2.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 792/3145 [05:03<13:54,  2.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 795/3143 [05:04<15:32,  2.52it/s] 25%|â–ˆâ–ˆâ–Œ       | 800/3145 [05:16<16:04,  2.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 792/3145 [05:06<15:16,  2.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 793/3145 [05:04<13:41,  2.86it/s] 25%|â–ˆâ–ˆâ–Œ       | 801/3145 [05:17<16:05,  2.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 794/3145 [05:04<13:16,  2.95it/s] 25%|â–ˆâ–ˆâ–Œ       | 796/3143 [05:04<16:32,  2.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 793/3145 [05:07<14:54,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 802/3145 [05:17<15:09,  2.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 795/3145 [05:04<13:15,  2.96it/s] 25%|â–ˆâ–ˆâ–Œ       | 797/3143 [05:04<16:07,  2.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 794/3145 [05:07<14:59,  2.61it/s] 25%|â–ˆâ–ˆâ–Œ       | 796/3145 [05:05<13:34,  2.88it/s] 25%|â–ˆâ–ˆâ–Œ       | 798/3143 [05:05<15:28,  2.53it/s] 26%|â–ˆâ–ˆâ–Œ       | 803/3145 [05:18<15:58,  2.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 795/3145 [05:07<15:41,  2.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 797/3145 [05:05<13:42,  2.86it/s] 25%|â–ˆâ–ˆâ–Œ       | 799/3143 [05:05<15:06,  2.59it/s] 25%|â–ˆâ–ˆâ–Œ       | 796/3145 [05:08<15:10,  2.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 804/3145 [05:18<16:27,  2.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 798/3145 [05:05<14:31,  2.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 800/3143 [05:06<15:07,  2.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 797/3145 [05:08<14:34,  2.69it/s] 26%|â–ˆâ–ˆâ–Œ       | 805/3145 [05:18<15:54,  2.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 801/3143 [05:06<14:46,  2.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 799/3145 [05:06<15:26,  2.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 798/3145 [05:08<14:51,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 806/3145 [05:19<16:04,  2.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 802/3143 [05:06<14:51,  2.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 799/3145 [05:09<14:56,  2.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 807/3145 [05:19<14:56,  2.61it/s] 25%|â–ˆâ–ˆâ–Œ       | 800/3145 [05:06<16:43,  2.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 801/3145 [05:07<14:25,  2.71it/s] 26%|â–ˆâ–ˆâ–Œ       | 803/3143 [05:07<15:26,  2.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 800/3145 [05:09<14:50,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 808/3145 [05:20<15:03,  2.59it/s] 26%|â–ˆâ–ˆâ–Œ       | 802/3145 [05:07<14:40,  2.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 804/3143 [05:07<15:43,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 809/3145 [05:20<15:02,  2.59it/s] 25%|â–ˆâ–ˆâ–Œ       | 801/3145 [05:10<15:24,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 802/3145 [05:10<14:42,  2.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 803/3145 [05:07<15:12,  2.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 810/3145 [05:20<15:06,  2.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 805/3143 [05:08<15:46,  2.47it/s] 26%|â–ˆâ–ˆâ–Œ       | 803/3145 [05:10<14:57,  2.61it/s] 26%|â–ˆâ–ˆâ–Œ       | 806/3143 [05:08<15:22,  2.53it/s] 26%|â–ˆâ–ˆâ–Œ       | 804/3145 [05:08<15:32,  2.51it/s] 26%|â–ˆâ–ˆâ–Œ       | 811/3145 [05:21<16:00,  2.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 804/3145 [05:11<14:40,  2.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 805/3145 [05:08<15:14,  2.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 807/3143 [05:08<15:21,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 812/3145 [05:21<16:28,  2.36it/s] 26%|â–ˆâ–ˆâ–Œ       | 805/3145 [05:11<14:48,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 808/3143 [05:09<14:44,  2.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 806/3145 [05:09<15:13,  2.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 813/3145 [05:22<16:02,  2.42it/s] 26%|â–ˆâ–ˆâ–Œ       | 809/3143 [05:09<14:31,  2.68it/s] 26%|â–ˆâ–ˆâ–Œ       | 806/3145 [05:12<15:20,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 807/3145 [05:09<15:09,  2.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 814/3145 [05:22<15:31,  2.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 810/3143 [05:09<14:43,  2.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 807/3145 [05:12<15:05,  2.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 808/3145 [05:09<15:40,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 815/3145 [05:22<15:53,  2.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 811/3143 [05:10<14:28,  2.69it/s] 26%|â–ˆâ–ˆâ–Œ       | 808/3145 [05:12<14:47,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 809/3145 [05:10<15:28,  2.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 816/3145 [05:23<15:19,  2.53it/s] 26%|â–ˆâ–ˆâ–Œ       | 812/3143 [05:10<13:52,  2.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 809/3145 [05:13<16:00,  2.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 810/3145 [05:10<15:41,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 817/3145 [05:23<15:23,  2.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 813/3143 [05:10<14:15,  2.72it/s] 26%|â–ˆâ–ˆâ–Œ       | 810/3145 [05:13<15:11,  2.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 811/3145 [05:11<15:09,  2.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 818/3145 [05:24<15:17,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 814/3143 [05:11<13:50,  2.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 811/3145 [05:14<14:50,  2.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 812/3145 [05:11<14:51,  2.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 819/3145 [05:24<14:34,  2.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 815/3143 [05:11<13:38,  2.84it/s] 26%|â–ˆâ–ˆâ–Œ       | 812/3145 [05:14<13:26,  2.89it/s] 26%|â–ˆâ–ˆâ–Œ       | 820/3145 [05:24<12:48,  3.02it/s] 26%|â–ˆâ–ˆâ–Œ       | 813/3145 [05:11<15:35,  2.49it/s] 26%|â–ˆâ–ˆâ–Œ       | 816/3143 [05:12<13:48,  2.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 821/3145 [05:25<13:23,  2.89it/s] 26%|â–ˆâ–ˆâ–Œ       | 813/3145 [05:14<15:09,  2.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 814/3145 [05:12<16:07,  2.41it/s] 26%|â–ˆâ–ˆâ–Œ       | 817/3143 [05:12<15:33,  2.49it/s] 26%|â–ˆâ–ˆâ–Œ       | 822/3145 [05:25<13:15,  2.92it/s] 26%|â–ˆâ–ˆâ–Œ       | 814/3145 [05:15<16:25,  2.37it/s] 26%|â–ˆâ–ˆâ–Œ       | 815/3145 [05:12<15:28,  2.51it/s] 26%|â–ˆâ–ˆâ–Œ       | 818/3143 [05:12<15:21,  2.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 823/3145 [05:25<14:01,  2.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 815/3145 [05:15<15:40,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 816/3145 [05:13<15:24,  2.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 824/3145 [05:26<13:59,  2.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 816/3145 [05:15<14:04,  2.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 819/3143 [05:13<17:17,  2.24it/s] 26%|â–ˆâ–ˆâ–Œ       | 817/3145 [05:13<14:54,  2.60it/s] 26%|â–ˆâ–ˆâ–Œ       | 825/3145 [05:26<14:42,  2.63it/s] 26%|â–ˆâ–ˆâ–Œ       | 817/3145 [05:16<14:24,  2.69it/s] 26%|â–ˆâ–ˆâ–Œ       | 818/3145 [05:13<15:37,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 820/3143 [05:14<18:13,  2.12it/s] 26%|â–ˆâ–ˆâ–‹       | 826/3145 [05:26<14:28,  2.67it/s] 26%|â–ˆâ–ˆâ–Œ       | 818/3145 [05:16<15:17,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 819/3145 [05:14<13:50,  2.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 821/3143 [05:14<17:16,  2.24it/s] 26%|â–ˆâ–ˆâ–‹       | 827/3145 [05:27<14:46,  2.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 820/3145 [05:14<14:17,  2.71it/s] 26%|â–ˆâ–ˆâ–Œ       | 819/3145 [05:17<15:38,  2.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 822/3143 [05:14<15:50,  2.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 820/3145 [05:17<13:42,  2.83it/s] 26%|â–ˆâ–ˆâ–‹       | 828/3145 [05:27<14:25,  2.68it/s] 26%|â–ˆâ–ˆâ–Œ       | 823/3143 [05:15<15:13,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 821/3145 [05:15<15:08,  2.56it/s] 26%|â–ˆâ–ˆâ–‹       | 829/3145 [05:28<14:04,  2.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 821/3145 [05:17<14:01,  2.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 822/3145 [05:15<14:35,  2.65it/s] 26%|â–ˆâ–ˆâ–Œ       | 824/3143 [05:15<15:24,  2.51it/s] 26%|â–ˆâ–ˆâ–‹       | 830/3145 [05:28<14:04,  2.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 822/3145 [05:18<13:59,  2.77it/s] 26%|â–ˆâ–ˆâ–Œ       | 823/3145 [05:15<14:51,  2.61it/s] 26%|â–ˆâ–ˆâ–Œ       | 825/3143 [05:15<15:20,  2.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 823/3145 [05:18<13:57,  2.77it/s] 26%|â–ˆâ–ˆâ–‹       | 831/3145 [05:28<16:01,  2.41it/s] 26%|â–ˆâ–ˆâ–Œ       | 824/3145 [05:16<14:15,  2.71it/s] 26%|â–ˆâ–ˆâ–‹       | 826/3143 [05:16<15:12,  2.54it/s] 26%|â–ˆâ–ˆâ–Œ       | 824/3145 [05:18<14:00,  2.76it/s] 26%|â–ˆâ–ˆâ–‹       | 832/3145 [05:29<14:54,  2.59it/s] 26%|â–ˆâ–ˆâ–Œ       | 825/3145 [05:16<14:02,  2.75it/s] 26%|â–ˆâ–ˆâ–‹       | 827/3143 [05:16<15:14,  2.53it/s] 26%|â–ˆâ–ˆâ–Œ       | 825/3145 [05:19<13:44,  2.81it/s] 26%|â–ˆâ–ˆâ–‹       | 833/3145 [05:29<14:36,  2.64it/s] 26%|â–ˆâ–ˆâ–‹       | 826/3145 [05:16<13:55,  2.77it/s] 26%|â–ˆâ–ˆâ–‹       | 826/3145 [05:19<13:30,  2.86it/s] 26%|â–ˆâ–ˆâ–‹       | 828/3143 [05:17<15:07,  2.55it/s] 26%|â–ˆâ–ˆâ–‹       | 827/3145 [05:17<13:53,  2.78it/s] 27%|â–ˆâ–ˆâ–‹       | 834/3145 [05:30<15:25,  2.50it/s] 26%|â–ˆâ–ˆâ–‹       | 827/3145 [05:19<13:56,  2.77it/s] 26%|â–ˆâ–ˆâ–‹       | 829/3143 [05:17<15:12,  2.53it/s] 26%|â–ˆâ–ˆâ–‹       | 828/3145 [05:17<14:28,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 835/3145 [05:30<15:12,  2.53it/s] 26%|â–ˆâ–ˆâ–‹       | 828/3145 [05:20<14:25,  2.68it/s] 26%|â–ˆâ–ˆâ–‹       | 830/3143 [05:17<15:03,  2.56it/s] 26%|â–ˆâ–ˆâ–‹       | 829/3145 [05:17<14:14,  2.71it/s] 27%|â–ˆâ–ˆâ–‹       | 836/3145 [05:30<14:52,  2.59it/s] 26%|â–ˆâ–ˆâ–‹       | 831/3143 [05:18<14:53,  2.59it/s] 26%|â–ˆâ–ˆâ–‹       | 829/3145 [05:20<14:34,  2.65it/s] 26%|â–ˆâ–ˆâ–‹       | 830/3145 [05:18<14:23,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 837/3145 [05:31<15:15,  2.52it/s] 26%|â–ˆâ–ˆâ–‹       | 830/3145 [05:21<14:30,  2.66it/s] 26%|â–ˆâ–ˆâ–‹       | 832/3143 [05:18<14:58,  2.57it/s] 27%|â–ˆâ–ˆâ–‹       | 838/3145 [05:31<14:44,  2.61it/s] 26%|â–ˆâ–ˆâ–‹       | 831/3145 [05:18<16:13,  2.38it/s] 26%|â–ˆâ–ˆâ–‹       | 831/3145 [05:21<14:38,  2.63it/s] 27%|â–ˆâ–ˆâ–‹       | 833/3143 [05:18<15:01,  2.56it/s] 27%|â–ˆâ–ˆâ–‹       | 839/3145 [05:31<14:15,  2.70it/s] 26%|â–ˆâ–ˆâ–‹       | 832/3145 [05:19<15:47,  2.44it/s] 26%|â–ˆâ–ˆâ–‹       | 832/3145 [05:21<14:45,  2.61it/s] 27%|â–ˆâ–ˆâ–‹       | 834/3143 [05:19<15:24,  2.50it/s] 27%|â–ˆâ–ˆâ–‹       | 840/3145 [05:32<13:56,  2.76it/s] 26%|â–ˆâ–ˆâ–‹       | 833/3145 [05:19<15:09,  2.54it/s] 26%|â–ˆâ–ˆâ–‹       | 833/3145 [05:22<14:16,  2.70it/s] 27%|â–ˆâ–ˆâ–‹       | 841/3145 [05:32<13:50,  2.77it/s] 27%|â–ˆâ–ˆâ–‹       | 835/3143 [05:19<15:37,  2.46it/s] 27%|â–ˆâ–ˆâ–‹       | 834/3145 [05:19<14:47,  2.60it/s] 27%|â–ˆâ–ˆâ–‹       | 834/3145 [05:22<14:05,  2.73it/s] 27%|â–ˆâ–ˆâ–‹       | 836/3143 [05:20<14:51,  2.59it/s] 27%|â–ˆâ–ˆâ–‹       | 842/3145 [05:32<13:53,  2.76it/s] 27%|â–ˆâ–ˆâ–‹       | 835/3145 [05:20<14:29,  2.66it/s] 27%|â–ˆâ–ˆâ–‹       | 835/3145 [05:22<13:43,  2.80it/s] 27%|â–ˆâ–ˆâ–‹       | 837/3143 [05:20<14:33,  2.64it/s] 27%|â–ˆâ–ˆâ–‹       | 843/3145 [05:33<13:56,  2.75it/s] 27%|â–ˆâ–ˆâ–‹       | 836/3145 [05:20<13:34,  2.83it/s] 27%|â–ˆâ–ˆâ–‹       | 836/3145 [05:23<14:08,  2.72it/s] 27%|â–ˆâ–ˆâ–‹       | 838/3143 [05:20<14:20,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 844/3145 [05:33<13:48,  2.78it/s] 27%|â–ˆâ–ˆâ–‹       | 837/3145 [05:21<14:39,  2.62it/s] 27%|â–ˆâ–ˆâ–‹       | 837/3145 [05:23<14:23,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 845/3145 [05:34<13:25,  2.86it/s] 27%|â–ˆâ–ˆâ–‹       | 839/3143 [05:21<14:13,  2.70it/s] 27%|â–ˆâ–ˆâ–‹       | 838/3145 [05:21<14:39,  2.62it/s] 27%|â–ˆâ–ˆâ–‹       | 840/3143 [05:21<12:56,  2.97it/s] 27%|â–ˆâ–ˆâ–‹       | 838/3145 [05:24<14:36,  2.63it/s] 27%|â–ˆâ–ˆâ–‹       | 846/3145 [05:34<13:28,  2.84it/s] 27%|â–ˆâ–ˆâ–‹       | 839/3145 [05:21<14:40,  2.62it/s] 27%|â–ˆâ–ˆâ–‹       | 841/3143 [05:21<13:09,  2.92it/s] 27%|â–ˆâ–ˆâ–‹       | 839/3145 [05:24<15:02,  2.56it/s] 27%|â–ˆâ–ˆâ–‹       | 847/3145 [05:34<14:58,  2.56it/s] 27%|â–ˆâ–ˆâ–‹       | 840/3145 [05:22<13:31,  2.84it/s] 27%|â–ˆâ–ˆâ–‹       | 842/3143 [05:22<13:17,  2.89it/s] 27%|â–ˆâ–ˆâ–‹       | 840/3145 [05:24<15:05,  2.55it/s] 27%|â–ˆâ–ˆâ–‹       | 848/3145 [05:35<15:00,  2.55it/s] 27%|â–ˆâ–ˆâ–‹       | 841/3145 [05:22<13:38,  2.81it/s] 27%|â–ˆâ–ˆâ–‹       | 843/3143 [05:22<13:45,  2.78it/s] 27%|â–ˆâ–ˆâ–‹       | 849/3145 [05:35<13:03,  2.93it/s] 27%|â–ˆâ–ˆâ–‹       | 841/3145 [05:25<14:26,  2.66it/s] 27%|â–ˆâ–ˆâ–‹       | 842/3145 [05:22<14:22,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 844/3143 [05:23<14:15,  2.69it/s] 27%|â–ˆâ–ˆâ–‹       | 850/3145 [05:35<14:15,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 843/3145 [05:23<14:10,  2.71it/s] 27%|â–ˆâ–ˆâ–‹       | 842/3145 [05:25<17:23,  2.21it/s] 27%|â–ˆâ–ˆâ–‹       | 851/3145 [05:36<12:32,  3.05it/s] 27%|â–ˆâ–ˆâ–‹       | 845/3143 [05:23<14:27,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 843/3145 [05:26<15:01,  2.55it/s] 27%|â–ˆâ–ˆâ–‹       | 844/3145 [05:23<14:00,  2.74it/s] 27%|â–ˆâ–ˆâ–‹       | 852/3145 [05:36<12:54,  2.96it/s] 27%|â–ˆâ–ˆâ–‹       | 846/3143 [05:23<14:32,  2.63it/s] 27%|â–ˆâ–ˆâ–‹       | 845/3145 [05:23<13:54,  2.76it/s] 27%|â–ˆâ–ˆâ–‹       | 844/3145 [05:26<16:18,  2.35it/s] 27%|â–ˆâ–ˆâ–‹       | 853/3145 [05:36<13:35,  2.81it/s] 27%|â–ˆâ–ˆâ–‹       | 847/3143 [05:24<14:27,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 845/3145 [05:26<14:29,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 846/3145 [05:24<14:07,  2.71it/s] 27%|â–ˆâ–ˆâ–‹       | 854/3145 [05:37<13:39,  2.80it/s] 27%|â–ˆâ–ˆâ–‹       | 848/3143 [05:24<14:51,  2.57it/s] 27%|â–ˆâ–ˆâ–‹       | 847/3145 [05:24<14:19,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 855/3145 [05:37<13:46,  2.77it/s] 27%|â–ˆâ–ˆâ–‹       | 846/3145 [05:27<15:47,  2.43it/s] 27%|â–ˆâ–ˆâ–‹       | 849/3143 [05:24<14:50,  2.58it/s] 27%|â–ˆâ–ˆâ–‹       | 848/3145 [05:25<14:27,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 856/3145 [05:37<13:46,  2.77it/s] 27%|â–ˆâ–ˆâ–‹       | 847/3145 [05:27<15:12,  2.52it/s] 27%|â–ˆâ–ˆâ–‹       | 850/3143 [05:25<14:17,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 849/3145 [05:25<13:05,  2.92it/s] 27%|â–ˆâ–ˆâ–‹       | 848/3145 [05:28<14:34,  2.63it/s] 27%|â–ˆâ–ˆâ–‹       | 857/3145 [05:38<13:54,  2.74it/s] 27%|â–ˆâ–ˆâ–‹       | 851/3143 [05:25<14:32,  2.63it/s] 27%|â–ˆâ–ˆâ–‹       | 850/3145 [05:25<13:17,  2.88it/s] 27%|â–ˆâ–ˆâ–‹       | 849/3145 [05:28<14:42,  2.60it/s] 27%|â–ˆâ–ˆâ–‹       | 858/3145 [05:38<14:08,  2.69it/s] 27%|â–ˆâ–ˆâ–‹       | 852/3143 [05:26<14:15,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 851/3145 [05:26<13:17,  2.87it/s] 27%|â–ˆâ–ˆâ–‹       | 850/3145 [05:28<13:35,  2.81it/s] 27%|â–ˆâ–ˆâ–‹       | 859/3145 [05:39<14:20,  2.66it/s] 27%|â–ˆâ–ˆâ–‹       | 853/3143 [05:26<14:07,  2.70it/s] 27%|â–ˆâ–ˆâ–‹       | 852/3145 [05:26<13:21,  2.86it/s] 27%|â–ˆâ–ˆâ–‹       | 851/3145 [05:29<13:40,  2.79it/s] 27%|â–ˆâ–ˆâ–‹       | 853/3145 [05:26<11:52,  3.22it/s] 27%|â–ˆâ–ˆâ–‹       | 860/3145 [05:39<14:51,  2.56it/s] 27%|â–ˆâ–ˆâ–‹       | 854/3143 [05:26<13:56,  2.74it/s] 27%|â–ˆâ–ˆâ–‹       | 852/3145 [05:29<12:15,  3.12it/s] 27%|â–ˆâ–ˆâ–‹       | 854/3145 [05:27<12:40,  3.01it/s] 27%|â–ˆâ–ˆâ–‹       | 855/3143 [05:27<13:59,  2.73it/s] 27%|â–ˆâ–ˆâ–‹       | 861/3145 [05:39<14:57,  2.54it/s] 27%|â–ˆâ–ˆâ–‹       | 853/3145 [05:29<13:49,  2.76it/s] 27%|â–ˆâ–ˆâ–‹       | 855/3145 [05:27<12:57,  2.94it/s] 27%|â–ˆâ–ˆâ–‹       | 856/3143 [05:27<14:13,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 862/3145 [05:40<15:16,  2.49it/s] 27%|â–ˆâ–ˆâ–‹       | 856/3145 [05:27<11:46,  3.24it/s] 27%|â–ˆâ–ˆâ–‹       | 854/3145 [05:30<14:16,  2.68it/s] 27%|â–ˆâ–ˆâ–‹       | 857/3143 [05:27<14:33,  2.62it/s] 27%|â–ˆâ–ˆâ–‹       | 863/3145 [05:40<15:06,  2.52it/s] 27%|â–ˆâ–ˆâ–‹       | 855/3145 [05:30<14:24,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 857/3145 [05:28<13:28,  2.83it/s] 27%|â–ˆâ–ˆâ–‹       | 858/3143 [05:28<14:54,  2.55it/s] 27%|â–ˆâ–ˆâ–‹       | 864/3145 [05:41<14:54,  2.55it/s] 27%|â–ˆâ–ˆâ–‹       | 856/3145 [05:30<14:11,  2.69it/s] 27%|â–ˆâ–ˆâ–‹       | 858/3145 [05:28<13:33,  2.81it/s] 27%|â–ˆâ–ˆâ–‹       | 859/3143 [05:28<14:14,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 865/3145 [05:41<14:15,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 857/3145 [05:31<14:00,  2.72it/s] 27%|â–ˆâ–ˆâ–‹       | 859/3145 [05:28<13:36,  2.80it/s] 27%|â–ˆâ–ˆâ–‹       | 860/3143 [05:29<14:03,  2.71it/s] 28%|â–ˆâ–ˆâ–Š       | 866/3145 [05:41<14:12,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 858/3145 [05:31<13:37,  2.80it/s] 27%|â–ˆâ–ˆâ–‹       | 860/3145 [05:29<13:54,  2.74it/s] 28%|â–ˆâ–ˆâ–Š       | 867/3145 [05:42<13:46,  2.76it/s] 27%|â–ˆâ–ˆâ–‹       | 859/3145 [05:31<13:27,  2.83it/s] 27%|â–ˆâ–ˆâ–‹       | 861/3145 [05:29<12:51,  2.96it/s] 27%|â–ˆâ–ˆâ–‹       | 861/3143 [05:29<16:20,  2.33it/s] 28%|â–ˆâ–ˆâ–Š       | 868/3145 [05:42<13:45,  2.76it/s] 27%|â–ˆâ–ˆâ–‹       | 860/3145 [05:32<13:29,  2.82it/s] 27%|â–ˆâ–ˆâ–‹       | 862/3145 [05:29<13:08,  2.89it/s] 27%|â–ˆâ–ˆâ–‹       | 862/3143 [05:30<16:17,  2.33it/s] 28%|â–ˆâ–ˆâ–Š       | 869/3145 [05:42<13:39,  2.78it/s] 27%|â–ˆâ–ˆâ–‹       | 863/3145 [05:30<13:15,  2.87it/s] 27%|â–ˆâ–ˆâ–‹       | 861/3145 [05:32<14:16,  2.67it/s] 27%|â–ˆâ–ˆâ–‹       | 863/3143 [05:30<14:36,  2.60it/s] 28%|â–ˆâ–ˆâ–Š       | 870/3145 [05:43<13:53,  2.73it/s] 27%|â–ˆâ–ˆâ–‹       | 864/3145 [05:30<13:02,  2.92it/s] 27%|â–ˆâ–ˆâ–‹       | 862/3145 [05:33<14:22,  2.65it/s] 27%|â–ˆâ–ˆâ–‹       | 864/3143 [05:30<14:35,  2.60it/s] 28%|â–ˆâ–ˆâ–Š       | 871/3145 [05:43<13:44,  2.76it/s] 28%|â–ˆâ–ˆâ–Š       | 865/3145 [05:30<13:11,  2.88it/s] 27%|â–ˆâ–ˆâ–‹       | 863/3145 [05:33<14:08,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 865/3143 [05:31<14:13,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 872/3145 [05:44<13:41,  2.77it/s] 28%|â–ˆâ–ˆâ–Š       | 866/3145 [05:31<13:58,  2.72it/s] 27%|â–ˆâ–ˆâ–‹       | 864/3145 [05:33<14:18,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 866/3143 [05:31<13:58,  2.71it/s] 28%|â–ˆâ–ˆâ–Š       | 873/3145 [05:44<12:15,  3.09it/s] 28%|â–ˆâ–ˆâ–Š       | 865/3145 [05:34<12:40,  3.00it/s] 28%|â–ˆâ–ˆâ–Š       | 867/3143 [05:31<12:27,  3.05it/s] 28%|â–ˆâ–ˆâ–Š       | 867/3145 [05:31<13:52,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 874/3145 [05:44<12:17,  3.08it/s] 28%|â–ˆâ–ˆâ–Š       | 868/3143 [05:31<12:27,  3.04it/s] 28%|â–ˆâ–ˆâ–Š       | 866/3145 [05:34<13:08,  2.89it/s] 28%|â–ˆâ–ˆâ–Š       | 868/3145 [05:31<13:19,  2.85it/s] 28%|â–ˆâ–ˆâ–Š       | 875/3145 [05:44<13:22,  2.83it/s] 28%|â–ˆâ–ˆâ–Š       | 869/3143 [05:32<13:05,  2.90it/s] 28%|â–ˆâ–ˆâ–Š       | 869/3145 [05:32<13:51,  2.74it/s] 28%|â–ˆâ–ˆâ–Š       | 867/3145 [05:34<13:59,  2.71it/s] 28%|â–ˆâ–ˆâ–Š       | 876/3145 [05:45<13:22,  2.83it/s] 28%|â–ˆâ–ˆâ–Š       | 870/3143 [05:32<13:52,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 868/3145 [05:35<14:13,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 870/3145 [05:32<14:54,  2.54it/s] 28%|â–ˆâ–ˆâ–Š       | 877/3145 [05:45<13:25,  2.81it/s] 28%|â–ˆâ–ˆâ–Š       | 871/3143 [05:33<13:44,  2.75it/s] 28%|â–ˆâ–ˆâ–Š       | 869/3145 [05:35<14:07,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 878/3145 [05:46<13:29,  2.80it/s] 28%|â–ˆâ–ˆâ–Š       | 871/3145 [05:33<15:38,  2.42it/s] 28%|â–ˆâ–ˆâ–Š       | 872/3143 [05:33<12:10,  3.11it/s] 28%|â–ˆâ–ˆâ–Š       | 870/3145 [05:36<13:47,  2.75it/s] 28%|â–ˆâ–ˆâ–Š       | 879/3145 [05:46<13:18,  2.84it/s] 28%|â–ˆâ–ˆâ–Š       | 872/3145 [05:33<15:16,  2.48it/s] 28%|â–ˆâ–ˆâ–Š       | 873/3143 [05:33<13:23,  2.83it/s] 28%|â–ˆâ–ˆâ–Š       | 871/3145 [05:36<14:09,  2.68it/s] 28%|â–ˆâ–ˆâ–Š       | 880/3145 [05:46<13:26,  2.81it/s] 28%|â–ˆâ–ˆâ–Š       | 873/3145 [05:34<14:26,  2.62it/s] 28%|â–ˆâ–ˆâ–Š       | 874/3143 [05:34<13:14,  2.86it/s] 28%|â–ˆâ–ˆâ–Š       | 872/3145 [05:36<13:43,  2.76it/s] 28%|â–ˆâ–ˆâ–Š       | 881/3145 [05:47<13:08,  2.87it/s] 28%|â–ˆâ–ˆâ–Š       | 874/3145 [05:34<14:49,  2.55it/s] 28%|â–ˆâ–ˆâ–Š       | 875/3143 [05:34<13:50,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 873/3145 [05:37<14:06,  2.68it/s] 28%|â–ˆâ–ˆâ–Š       | 882/3145 [05:47<13:30,  2.79it/s] 28%|â–ˆâ–ˆâ–Š       | 875/3145 [05:34<14:30,  2.61it/s] 28%|â–ˆâ–ˆâ–Š       | 876/3143 [05:34<13:50,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 874/3145 [05:37<14:15,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 883/3145 [05:47<13:35,  2.77it/s] 28%|â–ˆâ–ˆâ–Š       | 876/3145 [05:35<14:12,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 877/3143 [05:35<13:33,  2.78it/s] 28%|â–ˆâ–ˆâ–Š       | 875/3145 [05:37<13:48,  2.74it/s] 28%|â–ˆâ–ˆâ–Š       | 877/3145 [05:35<14:17,  2.64it/s] 28%|â–ˆâ–ˆâ–Š       | 884/3145 [05:48<15:09,  2.49it/s] 28%|â–ˆâ–ˆâ–Š       | 878/3143 [05:35<13:50,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 876/3145 [05:38<13:43,  2.75it/s] 28%|â–ˆâ–ˆâ–Š       | 878/3145 [05:35<14:02,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 885/3145 [05:48<14:45,  2.55it/s] 28%|â–ˆâ–ˆâ–Š       | 879/3143 [05:36<14:26,  2.61it/s] 28%|â–ˆâ–ˆâ–Š       | 877/3145 [05:38<13:59,  2.70it/s] 28%|â–ˆâ–ˆâ–Š       | 886/3145 [05:49<14:08,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 879/3145 [05:36<14:12,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 880/3143 [05:36<14:47,  2.55it/s] 28%|â–ˆâ–ˆâ–Š       | 878/3145 [05:39<14:28,  2.61it/s] 28%|â–ˆâ–ˆâ–Š       | 887/3145 [05:49<13:48,  2.72it/s] 28%|â–ˆâ–ˆâ–Š       | 880/3145 [05:36<14:17,  2.64it/s] 28%|â–ˆâ–ˆâ–Š       | 881/3143 [05:36<15:02,  2.51it/s] 28%|â–ˆâ–ˆâ–Š       | 879/3145 [05:39<14:33,  2.59it/s] 28%|â–ˆâ–ˆâ–Š       | 888/3145 [05:49<13:42,  2.74it/s] 28%|â–ˆâ–ˆâ–Š       | 881/3145 [05:37<14:01,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 882/3143 [05:37<14:31,  2.59it/s] 28%|â–ˆâ–ˆâ–Š       | 880/3145 [05:39<14:35,  2.59it/s] 28%|â–ˆâ–ˆâ–Š       | 889/3145 [05:50<13:58,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 882/3145 [05:37<14:10,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 883/3143 [05:37<14:16,  2.64it/s] 28%|â–ˆâ–ˆâ–Š       | 881/3145 [05:40<14:00,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 890/3145 [05:50<14:08,  2.66it/s] 28%|â–ˆâ–ˆâ–Š       | 883/3145 [05:37<14:01,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 882/3145 [05:40<13:35,  2.78it/s] 28%|â–ˆâ–ˆâ–Š       | 884/3143 [05:38<14:42,  2.56it/s] 28%|â–ˆâ–ˆâ–Š       | 891/3145 [05:50<14:12,  2.64it/s] 28%|â–ˆâ–ˆâ–Š       | 884/3145 [05:38<14:15,  2.64it/s] 28%|â–ˆâ–ˆâ–Š       | 885/3143 [05:38<14:17,  2.63it/s] 28%|â–ˆâ–ˆâ–Š       | 883/3145 [05:40<13:57,  2.70it/s] 28%|â–ˆâ–ˆâ–Š       | 892/3145 [05:51<14:21,  2.61it/s] 28%|â–ˆâ–ˆâ–Š       | 886/3143 [05:38<12:35,  2.99it/s] 28%|â–ˆâ–ˆâ–Š       | 885/3145 [05:38<14:47,  2.55it/s] 28%|â–ˆâ–ˆâ–Š       | 884/3145 [05:41<12:52,  2.93it/s] 28%|â–ˆâ–ˆâ–Š       | 893/3145 [05:51<14:26,  2.60it/s] 28%|â–ˆâ–ˆâ–Š       | 887/3143 [05:38<12:29,  3.01it/s] 28%|â–ˆâ–ˆâ–Š       | 886/3145 [05:38<14:27,  2.60it/s] 28%|â–ˆâ–ˆâ–Š       | 885/3145 [05:41<13:10,  2.86it/s] 28%|â–ˆâ–ˆâ–Š       | 894/3145 [05:52<14:04,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 888/3143 [05:39<13:13,  2.84it/s] 28%|â–ˆâ–ˆâ–Š       | 886/3145 [05:41<13:11,  2.86it/s] 28%|â–ˆâ–ˆâ–Š       | 887/3145 [05:39<14:37,  2.57it/s] 28%|â–ˆâ–ˆâ–Š       | 889/3143 [05:39<13:14,  2.84it/s] 28%|â–ˆâ–ˆâ–Š       | 895/3145 [05:52<14:32,  2.58it/s] 28%|â–ˆâ–ˆâ–Š       | 887/3145 [05:42<13:19,  2.82it/s] 28%|â–ˆâ–ˆâ–Š       | 888/3145 [05:39<14:49,  2.54it/s] 28%|â–ˆâ–ˆâ–Š       | 896/3145 [05:52<13:58,  2.68it/s] 28%|â–ˆâ–ˆâ–Š       | 890/3143 [05:40<13:36,  2.76it/s] 28%|â–ˆâ–ˆâ–Š       | 888/3145 [05:42<13:45,  2.73it/s] 28%|â–ˆâ–ˆâ–Š       | 889/3145 [05:40<14:48,  2.54it/s] 29%|â–ˆâ–ˆâ–Š       | 897/3145 [05:53<13:49,  2.71it/s] 28%|â–ˆâ–ˆâ–Š       | 891/3143 [05:40<13:32,  2.77it/s] 28%|â–ˆâ–ˆâ–Š       | 889/3145 [05:43<14:05,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 890/3145 [05:40<14:21,  2.62it/s] 29%|â–ˆâ–ˆâ–Š       | 898/3145 [05:53<13:30,  2.77it/s] 28%|â–ˆâ–ˆâ–Š       | 892/3143 [05:40<13:28,  2.79it/s] 28%|â–ˆâ–ˆâ–Š       | 890/3145 [05:43<13:36,  2.76it/s] 28%|â–ˆâ–ˆâ–Š       | 891/3145 [05:40<14:24,  2.61it/s] 29%|â–ˆâ–ˆâ–Š       | 899/3145 [05:53<13:25,  2.79it/s] 28%|â–ˆâ–ˆâ–Š       | 893/3143 [05:41<13:26,  2.79it/s] 28%|â–ˆâ–ˆâ–Š       | 892/3145 [05:41<13:26,  2.79it/s] 28%|â–ˆâ–ˆâ–Š       | 891/3145 [05:43<13:54,  2.70it/s] 28%|â–ˆâ–ˆâ–Š       | 894/3143 [05:41<13:24,  2.79it/s] 29%|â–ˆâ–ˆâ–Š       | 900/3145 [05:54<14:01,  2.67it/s] 28%|â–ˆâ–ˆâ–Š       | 893/3145 [05:41<12:44,  2.95it/s] 28%|â–ˆâ–ˆâ–Š       | 892/3145 [05:44<14:24,  2.60it/s] 28%|â–ˆâ–ˆâ–Š       | 894/3145 [05:41<11:50,  3.17it/s] 28%|â–ˆâ–ˆâ–Š       | 895/3143 [05:41<13:28,  2.78it/s] 29%|â–ˆâ–ˆâ–Š       | 901/3145 [05:54<14:37,  2.56it/s] 28%|â–ˆâ–ˆâ–Š       | 893/3145 [05:44<13:55,  2.69it/s] 28%|â–ˆâ–ˆâ–Š       | 895/3145 [05:42<12:23,  3.03it/s] 29%|â–ˆâ–ˆâ–Š       | 896/3143 [05:42<13:22,  2.80it/s] 29%|â–ˆâ–ˆâ–Š       | 902/3145 [05:55<14:52,  2.51it/s] 28%|â–ˆâ–ˆâ–Š       | 896/3145 [05:42<11:14,  3.33it/s] 28%|â–ˆâ–ˆâ–Š       | 894/3145 [05:44<13:54,  2.70it/s] 29%|â–ˆâ–ˆâ–Š       | 897/3143 [05:42<13:43,  2.73it/s] 29%|â–ˆâ–ˆâ–Š       | 897/3145 [05:42<11:51,  3.16it/s] 29%|â–ˆâ–ˆâ–Š       | 903/3145 [05:55<14:48,  2.52it/s] 28%|â–ˆâ–ˆâ–Š       | 895/3145 [05:45<14:15,  2.63it/s] 29%|â–ˆâ–ˆâ–Š       | 898/3143 [05:42<13:35,  2.75it/s] 29%|â–ˆâ–ˆâ–Š       | 898/3145 [05:43<12:45,  2.94it/s] 28%|â–ˆâ–ˆâ–Š       | 896/3145 [05:45<14:25,  2.60it/s] 29%|â–ˆâ–ˆâ–Š       | 904/3145 [05:55<15:22,  2.43it/s] 29%|â–ˆâ–ˆâ–Š       | 899/3143 [05:43<12:50,  2.91it/s] 29%|â–ˆâ–ˆâ–Š       | 899/3145 [05:43<13:10,  2.84it/s] 29%|â–ˆâ–ˆâ–Š       | 897/3145 [05:46<14:00,  2.67it/s] 29%|â–ˆâ–ˆâ–‰       | 905/3145 [05:56<15:13,  2.45it/s] 29%|â–ˆâ–ˆâ–Š       | 900/3143 [05:43<13:16,  2.82it/s] 29%|â–ˆâ–ˆâ–Š       | 900/3145 [05:43<13:12,  2.83it/s] 29%|â–ˆâ–ˆâ–‰       | 906/3145 [05:56<15:26,  2.42it/s] 29%|â–ˆâ–ˆâ–Š       | 901/3143 [05:44<13:34,  2.75it/s] 29%|â–ˆâ–ˆâ–Š       | 898/3145 [05:46<15:24,  2.43it/s] 29%|â–ˆâ–ˆâ–Š       | 901/3145 [05:44<13:03,  2.86it/s] 29%|â–ˆâ–ˆâ–‰       | 907/3145 [05:57<14:41,  2.54it/s] 29%|â–ˆâ–ˆâ–Š       | 902/3143 [05:44<13:37,  2.74it/s] 29%|â–ˆâ–ˆâ–Š       | 899/3145 [05:46<14:50,  2.52it/s] 29%|â–ˆâ–ˆâ–Š       | 902/3145 [05:44<13:11,  2.83it/s] 29%|â–ˆâ–ˆâ–‰       | 908/3145 [05:57<14:16,  2.61it/s] 29%|â–ˆâ–ˆâ–Š       | 903/3143 [05:44<13:40,  2.73it/s] 29%|â–ˆâ–ˆâ–Š       | 900/3145 [05:47<15:07,  2.47it/s] 29%|â–ˆâ–ˆâ–Š       | 903/3145 [05:44<13:19,  2.80it/s] 29%|â–ˆâ–ˆâ–Š       | 901/3145 [05:47<14:13,  2.63it/s] 29%|â–ˆâ–ˆâ–‰       | 904/3143 [05:45<13:54,  2.68it/s] 29%|â–ˆâ–ˆâ–‰       | 909/3145 [05:58<15:55,  2.34it/s] 29%|â–ˆâ–ˆâ–Š       | 904/3145 [05:45<13:22,  2.79it/s] 29%|â–ˆâ–ˆâ–Š       | 902/3145 [05:48<14:37,  2.56it/s] 29%|â–ˆâ–ˆâ–‰       | 905/3143 [05:45<14:27,  2.58it/s] 29%|â–ˆâ–ˆâ–‰       | 910/3145 [05:58<15:34,  2.39it/s] 29%|â–ˆâ–ˆâ–‰       | 905/3145 [05:45<13:58,  2.67it/s] 29%|â–ˆâ–ˆâ–‰       | 906/3143 [05:45<14:00,  2.66it/s] 29%|â–ˆâ–ˆâ–‰       | 906/3145 [05:45<12:47,  2.92it/s] 29%|â–ˆâ–ˆâ–Š       | 903/3145 [05:48<14:59,  2.49it/s] 29%|â–ˆâ–ˆâ–‰       | 911/3145 [05:58<15:06,  2.46it/s] 29%|â–ˆâ–ˆâ–‰       | 907/3143 [05:46<14:08,  2.64it/s] 29%|â–ˆâ–ˆâ–Š       | 904/3145 [05:48<14:23,  2.59it/s] 29%|â–ˆâ–ˆâ–‰       | 907/3145 [05:46<13:06,  2.85it/s] 29%|â–ˆâ–ˆâ–‰       | 912/3145 [05:59<14:31,  2.56it/s] 29%|â–ˆâ–ˆâ–‰       | 908/3143 [05:46<13:52,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 905/3145 [05:49<14:32,  2.57it/s] 29%|â–ˆâ–ˆâ–‰       | 908/3145 [05:46<14:03,  2.65it/s] 29%|â–ˆâ–ˆâ–‰       | 913/3145 [05:59<14:53,  2.50it/s] 29%|â–ˆâ–ˆâ–‰       | 909/3145 [05:46<12:17,  3.03it/s] 29%|â–ˆâ–ˆâ–‰       | 909/3143 [05:47<14:06,  2.64it/s] 29%|â–ˆâ–ˆâ–‰       | 906/3145 [05:49<13:58,  2.67it/s] 29%|â–ˆâ–ˆâ–‰       | 914/3145 [05:59<14:09,  2.63it/s] 29%|â–ˆâ–ˆâ–‰       | 910/3145 [05:47<12:34,  2.96it/s] 29%|â–ˆâ–ˆâ–‰       | 910/3143 [05:47<13:46,  2.70it/s] 29%|â–ˆâ–ˆâ–‰       | 907/3145 [05:49<13:53,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 915/3145 [06:00<13:42,  2.71it/s] 29%|â–ˆâ–ˆâ–‰       | 911/3145 [05:47<11:25,  3.26it/s] 29%|â–ˆâ–ˆâ–‰       | 908/3145 [05:50<12:40,  2.94it/s] 29%|â–ˆâ–ˆâ–‰       | 911/3143 [05:47<14:01,  2.65it/s] 29%|â–ˆâ–ˆâ–‰       | 916/3145 [06:00<13:57,  2.66it/s] 29%|â–ˆâ–ˆâ–‰       | 912/3145 [05:47<12:14,  3.04it/s] 29%|â–ˆâ–ˆâ–‰       | 909/3145 [05:50<12:51,  2.90it/s] 29%|â–ˆâ–ˆâ–‰       | 912/3143 [05:48<13:28,  2.76it/s] 29%|â–ˆâ–ˆâ–‰       | 917/3145 [06:00<13:42,  2.71it/s] 29%|â–ˆâ–ˆâ–‰       | 913/3145 [05:48<12:28,  2.98it/s] 29%|â–ˆâ–ˆâ–‰       | 910/3145 [05:50<12:44,  2.92it/s] 29%|â–ˆâ–ˆâ–‰       | 913/3143 [05:48<13:25,  2.77it/s] 29%|â–ˆâ–ˆâ–‰       | 918/3145 [06:01<13:37,  2.73it/s] 29%|â–ˆâ–ˆâ–‰       | 914/3145 [05:48<13:29,  2.76it/s] 29%|â–ˆâ–ˆâ–‰       | 911/3145 [05:51<12:55,  2.88it/s] 29%|â–ˆâ–ˆâ–‰       | 914/3143 [05:48<13:20,  2.78it/s] 29%|â–ˆâ–ˆâ–‰       | 919/3145 [06:01<13:55,  2.66it/s] 29%|â–ˆâ–ˆâ–‰       | 912/3145 [05:51<12:58,  2.87it/s] 29%|â–ˆâ–ˆâ–‰       | 915/3145 [05:49<14:03,  2.64it/s] 29%|â–ˆâ–ˆâ–‰       | 915/3143 [05:49<13:49,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 920/3145 [06:02<13:28,  2.75it/s] 29%|â–ˆâ–ˆâ–‰       | 916/3145 [05:49<13:02,  2.85it/s] 29%|â–ˆâ–ˆâ–‰       | 913/3145 [05:51<13:29,  2.76it/s] 29%|â–ˆâ–ˆâ–‰       | 916/3143 [05:49<13:56,  2.66it/s] 29%|â–ˆâ–ˆâ–‰       | 921/3145 [06:02<13:47,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 917/3145 [05:49<13:04,  2.84it/s] 29%|â–ˆâ–ˆâ–‰       | 914/3145 [05:52<14:28,  2.57it/s] 29%|â–ˆâ–ˆâ–‰       | 922/3145 [06:02<13:34,  2.73it/s] 29%|â–ˆâ–ˆâ–‰       | 917/3143 [05:50<14:48,  2.51it/s] 29%|â–ˆâ–ˆâ–‰       | 918/3145 [05:50<13:25,  2.76it/s] 29%|â–ˆâ–ˆâ–‰       | 915/3145 [05:52<13:55,  2.67it/s] 29%|â–ˆâ–ˆâ–‰       | 923/3145 [06:03<13:27,  2.75it/s] 29%|â–ˆâ–ˆâ–‰       | 918/3143 [05:50<14:31,  2.55it/s] 29%|â–ˆâ–ˆâ–‰       | 919/3145 [05:50<13:35,  2.73it/s] 29%|â–ˆâ–ˆâ–‰       | 916/3145 [05:53<13:48,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 924/3145 [06:03<13:42,  2.70it/s] 29%|â–ˆâ–ˆâ–‰       | 919/3143 [05:50<14:51,  2.49it/s] 29%|â–ˆâ–ˆâ–‰       | 920/3145 [05:50<13:30,  2.75it/s] 29%|â–ˆâ–ˆâ–‰       | 917/3145 [05:53<13:23,  2.77it/s] 29%|â–ˆâ–ˆâ–‰       | 925/3145 [06:03<13:10,  2.81it/s] 29%|â–ˆâ–ˆâ–‰       | 920/3143 [05:51<14:15,  2.60it/s] 29%|â–ˆâ–ˆâ–‰       | 921/3145 [05:51<13:30,  2.74it/s] 29%|â–ˆâ–ˆâ–‰       | 918/3145 [05:53<12:59,  2.86it/s] 29%|â–ˆâ–ˆâ–‰       | 926/3145 [06:04<12:53,  2.87it/s] 29%|â–ˆâ–ˆâ–‰       | 921/3143 [05:51<13:45,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 922/3145 [05:51<13:06,  2.83it/s] 29%|â–ˆâ–ˆâ–‰       | 919/3145 [05:54<13:43,  2.70it/s] 29%|â–ˆâ–ˆâ–‰       | 927/3145 [06:04<13:02,  2.83it/s] 29%|â–ˆâ–ˆâ–‰       | 922/3143 [05:51<13:43,  2.70it/s] 29%|â–ˆâ–ˆâ–‰       | 923/3145 [05:51<13:09,  2.82it/s] 29%|â–ˆâ–ˆâ–‰       | 920/3145 [05:54<13:31,  2.74it/s] 30%|â–ˆâ–ˆâ–‰       | 928/3145 [06:04<13:20,  2.77it/s] 29%|â–ˆâ–ˆâ–‰       | 924/3145 [05:52<12:52,  2.87it/s] 29%|â–ˆâ–ˆâ–‰       | 923/3143 [05:52<14:15,  2.59it/s] 29%|â–ˆâ–ˆâ–‰       | 921/3145 [05:54<13:32,  2.74it/s] 30%|â–ˆâ–ˆâ–‰       | 929/3145 [06:05<13:09,  2.81it/s] 29%|â–ˆâ–ˆâ–‰       | 925/3145 [05:52<12:57,  2.86it/s] 29%|â–ˆâ–ˆâ–‰       | 924/3143 [05:52<13:54,  2.66it/s] 29%|â–ˆâ–ˆâ–‰       | 922/3145 [05:55<13:25,  2.76it/s] 30%|â–ˆâ–ˆâ–‰       | 930/3145 [06:05<13:05,  2.82it/s] 29%|â–ˆâ–ˆâ–‰       | 926/3145 [05:52<12:58,  2.85it/s] 29%|â–ˆâ–ˆâ–‰       | 925/3143 [05:53<14:02,  2.63it/s] 29%|â–ˆâ–ˆâ–‰       | 923/3145 [05:55<13:43,  2.70it/s] 30%|â–ˆâ–ˆâ–‰       | 931/3145 [06:06<13:05,  2.82it/s] 29%|â–ˆâ–ˆâ–‰       | 927/3145 [05:53<13:00,  2.84it/s] 29%|â–ˆâ–ˆâ–‰       | 926/3143 [05:53<13:44,  2.69it/s] 29%|â–ˆâ–ˆâ–‰       | 924/3145 [05:56<13:21,  2.77it/s] 30%|â–ˆâ–ˆâ–‰       | 932/3145 [06:06<13:05,  2.82it/s] 30%|â–ˆâ–ˆâ–‰       | 928/3145 [05:53<12:46,  2.89it/s] 29%|â–ˆâ–ˆâ–‰       | 927/3143 [05:53<13:27,  2.74it/s] 29%|â–ˆâ–ˆâ–‰       | 925/3145 [05:56<13:19,  2.78it/s] 30%|â–ˆâ–ˆâ–‰       | 933/3145 [06:06<12:45,  2.89it/s] 30%|â–ˆâ–ˆâ–‰       | 929/3145 [05:54<12:53,  2.87it/s] 30%|â–ˆâ–ˆâ–‰       | 928/3143 [05:54<13:00,  2.84it/s] 29%|â–ˆâ–ˆâ–‰       | 926/3145 [05:56<13:35,  2.72it/s] 30%|â–ˆâ–ˆâ–‰       | 930/3145 [05:54<12:42,  2.91it/s] 30%|â–ˆâ–ˆâ–‰       | 934/3145 [06:07<14:46,  2.49it/s] 30%|â–ˆâ–ˆâ–‰       | 929/3143 [05:54<13:04,  2.82it/s] 29%|â–ˆâ–ˆâ–‰       | 927/3145 [05:57<13:46,  2.68it/s] 30%|â–ˆâ–ˆâ–‰       | 931/3145 [05:54<13:46,  2.68it/s] 30%|â–ˆâ–ˆâ–‰       | 930/3143 [05:54<13:07,  2.81it/s] 30%|â–ˆâ–ˆâ–‰       | 935/3145 [06:07<15:47,  2.33it/s] 30%|â–ˆâ–ˆâ–‰       | 928/3145 [05:57<13:57,  2.65it/s] 30%|â–ˆâ–ˆâ–‰       | 931/3143 [05:55<13:16,  2.78it/s] 30%|â–ˆâ–ˆâ–‰       | 932/3145 [05:55<14:17,  2.58it/s] 30%|â–ˆâ–ˆâ–‰       | 936/3145 [06:08<14:56,  2.46it/s] 30%|â–ˆâ–ˆâ–‰       | 929/3145 [05:57<13:48,  2.68it/s] 30%|â–ˆâ–ˆâ–‰       | 932/3143 [05:55<12:59,  2.84it/s] 30%|â–ˆâ–ˆâ–‰       | 933/3145 [05:55<14:37,  2.52it/s] 30%|â–ˆâ–ˆâ–‰       | 937/3145 [06:08<14:36,  2.52it/s] 30%|â–ˆâ–ˆâ–‰       | 930/3145 [05:58<13:44,  2.69it/s] 30%|â–ˆâ–ˆâ–‰       | 933/3143 [05:55<13:02,  2.82it/s] 30%|â–ˆâ–ˆâ–‰       | 938/3145 [06:08<14:12,  2.59it/s] 30%|â–ˆâ–ˆâ–‰       | 931/3145 [05:58<13:35,  2.71it/s] 30%|â–ˆâ–ˆâ–‰       | 934/3145 [05:56<15:50,  2.33it/s] 30%|â–ˆâ–ˆâ–‰       | 934/3143 [05:56<13:30,  2.72it/s] 30%|â–ˆâ–ˆâ–‰       | 939/3145 [06:09<13:52,  2.65it/s] 30%|â–ˆâ–ˆâ–‰       | 932/3145 [05:59<13:49,  2.67it/s] 30%|â–ˆâ–ˆâ–‰       | 935/3145 [05:56<15:43,  2.34it/s] 30%|â–ˆâ–ˆâ–‰       | 935/3143 [05:56<13:11,  2.79it/s] 30%|â–ˆâ–ˆâ–‰       | 940/3145 [06:09<13:42,  2.68it/s] 30%|â–ˆâ–ˆâ–‰       | 933/3145 [05:59<13:39,  2.70it/s] 30%|â–ˆâ–ˆâ–‰       | 936/3145 [05:56<15:33,  2.37it/s] 30%|â–ˆâ–ˆâ–‰       | 936/3143 [05:57<13:27,  2.73it/s] 30%|â–ˆâ–ˆâ–‰       | 941/3145 [06:09<13:49,  2.66it/s] 30%|â–ˆâ–ˆâ–‰       | 934/3145 [05:59<13:52,  2.66it/s] 30%|â–ˆâ–ˆâ–‰       | 937/3143 [05:57<13:30,  2.72it/s] 30%|â–ˆâ–ˆâ–‰       | 937/3145 [05:57<15:28,  2.38it/s] 30%|â–ˆâ–ˆâ–‰       | 942/3145 [06:10<13:35,  2.70it/s] 30%|â–ˆâ–ˆâ–‰       | 935/3145 [06:00<13:42,  2.69it/s] 30%|â–ˆâ–ˆâ–‰       | 938/3145 [05:57<14:44,  2.49it/s] 30%|â–ˆâ–ˆâ–‰       | 938/3143 [05:57<14:05,  2.61it/s] 30%|â–ˆâ–ˆâ–‰       | 943/3145 [06:10<13:11,  2.78it/s] 30%|â–ˆâ–ˆâ–‰       | 936/3145 [06:00<13:15,  2.78it/s] 30%|â–ˆâ–ˆâ–‰       | 939/3143 [05:58<13:34,  2.71it/s] 30%|â–ˆâ–ˆâ–‰       | 939/3145 [05:58<14:39,  2.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 944/3145 [06:10<13:26,  2.73it/s] 30%|â–ˆâ–ˆâ–‰       | 937/3145 [06:00<13:35,  2.71it/s] 30%|â–ˆâ–ˆâ–‰       | 940/3143 [05:58<13:30,  2.72it/s] 30%|â–ˆâ–ˆâ–‰       | 940/3145 [05:58<14:29,  2.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 945/3145 [06:11<13:25,  2.73it/s] 30%|â–ˆâ–ˆâ–‰       | 938/3145 [06:01<14:09,  2.60it/s] 30%|â–ˆâ–ˆâ–‰       | 941/3143 [05:58<13:45,  2.67it/s] 30%|â–ˆâ–ˆâ–‰       | 941/3145 [05:58<14:31,  2.53it/s] 30%|â–ˆâ–ˆâ–ˆ       | 946/3145 [06:11<13:41,  2.68it/s] 30%|â–ˆâ–ˆâ–‰       | 939/3145 [06:01<14:13,  2.59it/s] 30%|â–ˆâ–ˆâ–‰       | 942/3145 [05:59<14:01,  2.62it/s] 30%|â–ˆâ–ˆâ–‰       | 942/3143 [05:59<14:12,  2.58it/s] 30%|â–ˆâ–ˆâ–ˆ       | 947/3145 [06:12<13:30,  2.71it/s] 30%|â–ˆâ–ˆâ–‰       | 940/3145 [06:02<13:52,  2.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 948/3145 [06:12<12:43,  2.88it/s] 30%|â–ˆâ–ˆâ–‰       | 943/3145 [05:59<14:22,  2.55it/s] 30%|â–ˆâ–ˆâ–ˆ       | 943/3143 [05:59<14:36,  2.51it/s] 30%|â–ˆâ–ˆâ–‰       | 941/3145 [06:02<13:58,  2.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 949/3145 [06:12<13:16,  2.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 944/3145 [06:00<14:02,  2.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 944/3143 [06:00<14:28,  2.53it/s] 30%|â–ˆâ–ˆâ–‰       | 942/3145 [06:02<13:40,  2.68it/s] 30%|â–ˆâ–ˆâ–ˆ       | 950/3145 [06:13<13:30,  2.71it/s] 30%|â–ˆâ–ˆâ–ˆ       | 945/3145 [06:00<14:26,  2.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 945/3143 [06:00<14:22,  2.55it/s] 30%|â–ˆâ–ˆâ–‰       | 943/3145 [06:03<13:58,  2.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 951/3145 [06:13<13:10,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 946/3143 [06:00<14:01,  2.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 946/3145 [06:00<14:17,  2.56it/s] 30%|â–ˆâ–ˆâ–ˆ       | 944/3145 [06:03<14:08,  2.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 952/3145 [06:13<13:22,  2.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 947/3143 [06:01<13:34,  2.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 947/3145 [06:01<13:51,  2.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 945/3145 [06:03<13:58,  2.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 953/3145 [06:14<13:16,  2.75it/s] 30%|â–ˆâ–ˆâ–ˆ       | 948/3143 [06:01<13:28,  2.72it/s] 30%|â–ˆâ–ˆâ–ˆ       | 948/3145 [06:01<13:57,  2.62it/s] 30%|â–ˆâ–ˆâ–ˆ       | 946/3145 [06:04<13:42,  2.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 954/3145 [06:14<13:33,  2.69it/s] 30%|â–ˆâ–ˆâ–ˆ       | 949/3143 [06:01<13:42,  2.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 949/3145 [06:01<14:04,  2.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 947/3145 [06:04<13:32,  2.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 955/3145 [06:15<13:43,  2.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 950/3143 [06:02<13:55,  2.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 950/3145 [06:02<14:00,  2.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 948/3145 [06:05<13:34,  2.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 956/3145 [06:15<13:45,  2.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 951/3143 [06:02<14:21,  2.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 951/3145 [06:02<14:26,  2.53it/s] 30%|â–ˆâ–ˆâ–ˆ       | 949/3145 [06:05<13:09,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 957/3145 [06:15<13:21,  2.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 952/3143 [06:03<13:48,  2.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 950/3145 [06:05<13:06,  2.79it/s] 30%|â–ˆâ–ˆâ–ˆ       | 952/3145 [06:03<14:43,  2.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 958/3145 [06:16<13:50,  2.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 953/3143 [06:03<13:19,  2.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 951/3145 [06:06<13:08,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 953/3145 [06:03<14:28,  2.52it/s] 30%|â–ˆâ–ˆâ–ˆ       | 959/3145 [06:16<13:34,  2.68it/s] 30%|â–ˆâ–ˆâ–ˆ       | 954/3143 [06:03<13:12,  2.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 952/3145 [06:06<13:06,  2.79it/s] 30%|â–ˆâ–ˆâ–ˆ       | 954/3145 [06:03<13:45,  2.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 960/3145 [06:16<13:04,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 955/3143 [06:04<13:04,  2.79it/s] 30%|â–ˆâ–ˆâ–ˆ       | 953/3145 [06:06<13:08,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 955/3145 [06:04<13:52,  2.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 961/3145 [06:17<13:06,  2.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 956/3143 [06:04<13:01,  2.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 954/3145 [06:07<13:05,  2.79it/s] 30%|â–ˆâ–ˆâ–ˆ       | 956/3145 [06:04<13:40,  2.67it/s] 31%|â–ˆâ–ˆâ–ˆ       | 962/3145 [06:17<13:15,  2.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 957/3143 [06:04<13:01,  2.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 955/3145 [06:07<12:55,  2.82it/s] 30%|â–ˆâ–ˆâ–ˆ       | 957/3145 [06:05<13:42,  2.66it/s] 31%|â–ˆâ–ˆâ–ˆ       | 963/3145 [06:17<13:29,  2.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 958/3143 [06:05<13:01,  2.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 956/3145 [06:07<13:22,  2.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 958/3145 [06:05<13:30,  2.70it/s] 31%|â–ˆâ–ˆâ–ˆ       | 959/3143 [06:05<12:52,  2.83it/s] 31%|â–ˆâ–ˆâ–ˆ       | 964/3145 [06:18<13:35,  2.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 959/3145 [06:05<13:20,  2.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 957/3145 [06:08<13:57,  2.61it/s] 31%|â–ˆâ–ˆâ–ˆ       | 965/3145 [06:18<12:20,  2.94it/s] 31%|â–ˆâ–ˆâ–ˆ       | 960/3143 [06:05<12:34,  2.89it/s] 31%|â–ˆâ–ˆâ–ˆ       | 966/3145 [06:18<12:16,  2.96it/s] 31%|â–ˆâ–ˆâ–ˆ       | 960/3145 [06:06<13:35,  2.68it/s] 30%|â–ˆâ–ˆâ–ˆ       | 958/3145 [06:08<13:59,  2.60it/s] 31%|â–ˆâ–ˆâ–ˆ       | 961/3143 [06:06<12:36,  2.89it/s] 31%|â–ˆâ–ˆâ–ˆ       | 967/3145 [06:19<12:25,  2.92it/s] 30%|â–ˆâ–ˆâ–ˆ       | 959/3145 [06:09<13:46,  2.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 961/3145 [06:06<13:46,  2.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 962/3143 [06:06<12:36,  2.88it/s] 31%|â–ˆâ–ˆâ–ˆ       | 968/3145 [06:19<12:21,  2.93it/s] 31%|â–ˆâ–ˆâ–ˆ       | 960/3145 [06:09<13:34,  2.68it/s] 31%|â–ˆâ–ˆâ–ˆ       | 962/3145 [06:06<14:17,  2.55it/s] 31%|â–ˆâ–ˆâ–ˆ       | 963/3143 [06:07<13:47,  2.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 969/3145 [06:20<12:54,  2.81it/s] 31%|â–ˆâ–ˆâ–ˆ       | 961/3145 [06:09<14:12,  2.56it/s] 31%|â–ˆâ–ˆâ–ˆ       | 963/3145 [06:07<13:58,  2.60it/s] 31%|â–ˆâ–ˆâ–ˆ       | 964/3143 [06:07<14:30,  2.50it/s] 31%|â–ˆâ–ˆâ–ˆ       | 970/3145 [06:20<12:56,  2.80it/s] 31%|â–ˆâ–ˆâ–ˆ       | 964/3145 [06:07<13:31,  2.69it/s] 31%|â–ˆâ–ˆâ–ˆ       | 962/3145 [06:10<14:17,  2.54it/s] 31%|â–ˆâ–ˆâ–ˆ       | 965/3143 [06:07<13:22,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆ       | 971/3145 [06:20<11:35,  3.13it/s] 31%|â–ˆâ–ˆâ–ˆ       | 965/3145 [06:08<13:05,  2.78it/s] 31%|â–ˆâ–ˆâ–ˆ       | 963/3145 [06:10<13:57,  2.61it/s] 31%|â–ˆâ–ˆâ–ˆ       | 966/3143 [06:08<12:57,  2.80it/s] 31%|â–ˆâ–ˆâ–ˆ       | 972/3145 [06:21<12:20,  2.93it/s] 31%|â–ˆâ–ˆâ–ˆ       | 966/3145 [06:08<12:40,  2.87it/s] 31%|â–ˆâ–ˆâ–ˆ       | 967/3143 [06:08<12:40,  2.86it/s] 31%|â–ˆâ–ˆâ–ˆ       | 964/3145 [06:10<13:58,  2.60it/s] 31%|â–ˆâ–ˆâ–ˆ       | 973/3145 [06:21<12:51,  2.82it/s] 31%|â–ˆâ–ˆâ–ˆ       | 967/3145 [06:08<13:05,  2.77it/s] 31%|â–ˆâ–ˆâ–ˆ       | 968/3143 [06:08<13:04,  2.77it/s] 31%|â–ˆâ–ˆâ–ˆ       | 965/3145 [06:11<14:48,  2.45it/s] 31%|â–ˆâ–ˆâ–ˆ       | 974/3145 [06:21<12:51,  2.81it/s] 31%|â–ˆâ–ˆâ–ˆ       | 968/3145 [06:09<13:04,  2.78it/s] 31%|â–ˆâ–ˆâ–ˆ       | 969/3143 [06:09<12:47,  2.83it/s] 31%|â–ˆâ–ˆâ–ˆ       | 966/3145 [06:11<14:32,  2.50it/s] 31%|â–ˆâ–ˆâ–ˆ       | 975/3145 [06:22<13:18,  2.72it/s] 31%|â–ˆâ–ˆâ–ˆ       | 970/3143 [06:09<11:19,  3.20it/s] 31%|â–ˆâ–ˆâ–ˆ       | 969/3145 [06:09<12:43,  2.85it/s] 31%|â–ˆâ–ˆâ–ˆ       | 967/3145 [06:12<13:43,  2.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 971/3143 [06:09<11:52,  3.05it/s] 31%|â–ˆâ–ˆâ–ˆ       | 970/3145 [06:09<12:31,  2.89it/s] 31%|â–ˆâ–ˆâ–ˆ       | 976/3145 [06:22<13:55,  2.60it/s] 31%|â–ˆâ–ˆâ–ˆ       | 972/3143 [06:10<10:56,  3.31it/s] 31%|â–ˆâ–ˆâ–ˆ       | 968/3145 [06:12<13:30,  2.69it/s] 31%|â–ˆâ–ˆâ–ˆ       | 977/3145 [06:22<13:35,  2.66it/s] 31%|â–ˆâ–ˆâ–ˆ       | 971/3145 [06:10<13:34,  2.67it/s] 31%|â–ˆâ–ˆâ–ˆ       | 973/3143 [06:10<10:05,  3.59it/s] 31%|â–ˆâ–ˆâ–ˆ       | 969/3145 [06:12<13:41,  2.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 974/3143 [06:10<09:33,  3.78it/s] 31%|â–ˆâ–ˆâ–ˆ       | 972/3145 [06:10<13:22,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆ       | 978/3145 [06:23<14:20,  2.52it/s] 31%|â–ˆâ–ˆâ–ˆ       | 970/3145 [06:13<12:47,  2.83it/s] 31%|â–ˆâ–ˆâ–ˆ       | 975/3143 [06:10<10:26,  3.46it/s] 31%|â–ˆâ–ˆâ–ˆ       | 973/3145 [06:10<13:21,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆ       | 979/3145 [06:23<13:41,  2.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 971/3145 [06:13<13:08,  2.76it/s] 31%|â–ˆâ–ˆâ–ˆ       | 974/3145 [06:11<11:52,  3.05it/s] 31%|â–ˆâ–ˆâ–ˆ       | 980/3145 [06:23<12:05,  2.98it/s] 31%|â–ˆâ–ˆâ–ˆ       | 976/3143 [06:11<12:02,  3.00it/s] 31%|â–ˆâ–ˆâ–ˆ       | 975/3145 [06:11<12:02,  3.00it/s] 31%|â–ˆâ–ˆâ–ˆ       | 972/3145 [06:14<13:44,  2.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 981/3145 [06:24<12:36,  2.86it/s] 31%|â–ˆâ–ˆâ–ˆ       | 977/3143 [06:11<12:03,  2.99it/s] 31%|â–ˆâ–ˆâ–ˆ       | 976/3145 [06:11<12:23,  2.92it/s] 31%|â–ˆâ–ˆâ–ˆ       | 978/3143 [06:11<12:03,  2.99it/s] 31%|â–ˆâ–ˆâ–ˆ       | 973/3145 [06:14<14:18,  2.53it/s] 31%|â–ˆâ–ˆâ–ˆ       | 982/3145 [06:24<14:15,  2.53it/s] 31%|â–ˆâ–ˆâ–ˆ       | 979/3143 [06:12<10:57,  3.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 977/3145 [06:12<12:35,  2.87it/s] 31%|â–ˆâ–ˆâ–ˆ       | 974/3145 [06:14<13:56,  2.59it/s] 31%|â–ˆâ–ˆâ–ˆ       | 980/3143 [06:12<11:23,  3.16it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 983/3145 [06:25<15:29,  2.33it/s] 31%|â–ˆâ–ˆâ–ˆ       | 978/3145 [06:12<12:53,  2.80it/s] 31%|â–ˆâ–ˆâ–ˆ       | 975/3145 [06:15<14:20,  2.52it/s] 31%|â–ˆâ–ˆâ–ˆ       | 981/3143 [06:12<11:35,  3.11it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 984/3145 [06:25<14:42,  2.45it/s] 31%|â–ˆâ–ˆâ–ˆ       | 979/3145 [06:12<12:52,  2.80it/s] 31%|â–ˆâ–ˆâ–ˆ       | 976/3145 [06:15<13:58,  2.59it/s] 31%|â–ˆâ–ˆâ–ˆ       | 982/3143 [06:13<12:15,  2.94it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 985/3145 [06:26<14:26,  2.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 980/3145 [06:13<12:55,  2.79it/s] 31%|â–ˆâ–ˆâ–ˆ       | 977/3145 [06:15<13:20,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 983/3143 [06:13<12:26,  2.89it/s] 31%|â–ˆâ–ˆâ–ˆ       | 981/3145 [06:13<12:57,  2.78it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 986/3145 [06:26<14:19,  2.51it/s] 31%|â–ˆâ–ˆâ–ˆ       | 978/3145 [06:16<13:11,  2.74it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 984/3143 [06:13<12:32,  2.87it/s] 31%|â–ˆâ–ˆâ–ˆ       | 982/3145 [06:14<12:56,  2.78it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 987/3145 [06:26<14:17,  2.52it/s] 31%|â–ˆâ–ˆâ–ˆ       | 979/3145 [06:16<13:07,  2.75it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 985/3143 [06:14<12:55,  2.78it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 983/3145 [06:14<12:59,  2.77it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 988/3145 [06:27<14:29,  2.48it/s] 31%|â–ˆâ–ˆâ–ˆ       | 980/3145 [06:17<13:21,  2.70it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 986/3143 [06:14<12:38,  2.85it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 984/3145 [06:14<12:43,  2.83it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 987/3143 [06:14<11:20,  3.17it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 989/3145 [06:27<14:26,  2.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 981/3145 [06:17<13:34,  2.66it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 985/3145 [06:15<13:08,  2.74it/s] 31%|â–ˆâ–ˆâ–ˆ       | 982/3145 [06:17<13:19,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 990/3145 [06:28<14:26,  2.49it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 988/3143 [06:15<13:32,  2.65it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 986/3145 [06:15<12:48,  2.81it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 983/3145 [06:18<13:17,  2.71it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 989/3143 [06:15<12:57,  2.77it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 991/3145 [06:28<14:35,  2.46it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 987/3145 [06:15<12:52,  2.79it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 984/3145 [06:18<13:33,  2.66it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 990/3143 [06:16<13:04,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 992/3145 [06:28<14:40,  2.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 988/3145 [06:16<12:33,  2.86it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 991/3143 [06:16<13:16,  2.70it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 985/3145 [06:18<14:17,  2.52it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 989/3145 [06:16<12:38,  2.84it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 993/3145 [06:29<15:21,  2.34it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 986/3145 [06:19<13:40,  2.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 992/3143 [06:16<13:30,  2.65it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 990/3145 [06:16<13:04,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 994/3145 [06:29<14:46,  2.43it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 987/3145 [06:19<13:06,  2.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 991/3145 [06:17<11:38,  3.08it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 993/3143 [06:17<13:26,  2.67it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 995/3145 [06:30<14:53,  2.41it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 988/3145 [06:19<12:49,  2.80it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 994/3143 [06:17<12:58,  2.76it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 992/3145 [06:17<13:13,  2.71it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 996/3145 [06:30<14:33,  2.46it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 989/3145 [06:20<12:58,  2.77it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 993/3145 [06:17<13:03,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 995/3143 [06:18<14:20,  2.50it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 997/3145 [06:30<14:14,  2.52it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 994/3145 [06:18<11:29,  3.12it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 990/3145 [06:20<12:51,  2.79it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 996/3143 [06:18<13:50,  2.59it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 998/3145 [06:31<13:34,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 995/3145 [06:18<11:58,  2.99it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 991/3145 [06:21<13:14,  2.71it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 997/3143 [06:18<13:49,  2.59it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 996/3145 [06:18<11:46,  3.04it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 999/3145 [06:31<13:43,  2.61it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 992/3145 [06:21<14:06,  2.54it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 998/3143 [06:19<13:46,  2.60it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3145 [06:32<13:28,  2.65it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 993/3145 [06:21<12:29,  2.87it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 997/3145 [06:19<12:54,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 999/3143 [06:19<12:17,  2.91it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1001/3145 [06:32<13:17,  2.69it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 994/3145 [06:22<12:59,  2.76it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3143 [06:19<11:27,  3.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 998/3145 [06:19<13:13,  2.71it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1002/3145 [06:32<13:09,  2.71it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 995/3145 [06:22<12:41,  2.82it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1001/3143 [06:20<12:28,  2.86it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 999/3145 [06:20<13:57,  2.56it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1003/3145 [06:33<13:21,  2.67it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 996/3145 [06:22<13:07,  2.73it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1002/3143 [06:20<12:15,  2.91it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3145 [06:20<12:59,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1004/3145 [06:33<13:30,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 997/3145 [06:23<12:55,  2.77it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1003/3143 [06:20<12:30,  2.85it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1001/3145 [06:20<13:20,  2.68it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1005/3145 [06:33<13:32,  2.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1004/3143 [06:21<12:37,  2.82it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 998/3145 [06:23<13:34,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1002/3145 [06:21<13:17,  2.69it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1006/3145 [06:34<12:54,  2.76it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1005/3143 [06:21<12:58,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1003/3145 [06:21<13:18,  2.68it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 999/3145 [06:24<14:25,  2.48it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1007/3145 [06:34<12:50,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1006/3143 [06:21<12:02,  2.96it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1004/3145 [06:21<13:08,  2.72it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1000/3145 [06:24<14:17,  2.50it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1007/3143 [06:22<11:22,  3.13it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1008/3145 [06:34<12:48,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1005/3145 [06:22<12:49,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1001/3145 [06:24<14:09,  2.52it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1009/3145 [06:35<11:58,  2.97it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1008/3143 [06:22<11:45,  3.02it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1006/3145 [06:22<12:47,  2.79it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1009/3143 [06:22<12:08,  2.93it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1010/3145 [06:35<12:42,  2.80it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1002/3145 [06:25<14:51,  2.40it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1007/3145 [06:22<12:49,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1010/3143 [06:23<12:22,  2.87it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1011/3145 [06:36<13:23,  2.66it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1008/3145 [06:23<12:56,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1003/3145 [06:25<15:44,  2.27it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1011/3143 [06:23<12:48,  2.77it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1004/3145 [06:26<13:29,  2.65it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1012/3145 [06:36<13:21,  2.66it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1009/3145 [06:23<13:33,  2.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1005/3145 [06:26<12:32,  2.84it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1012/3143 [06:23<13:03,  2.72it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1013/3145 [06:36<13:08,  2.70it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1010/3145 [06:24<13:40,  2.60it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1006/3145 [06:26<12:54,  2.76it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1013/3143 [06:24<12:58,  2.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1014/3145 [06:37<13:18,  2.67it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1011/3145 [06:24<13:26,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1007/3145 [06:27<12:58,  2.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1014/3143 [06:24<12:56,  2.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1015/3145 [06:37<13:23,  2.65it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1012/3145 [06:24<13:16,  2.68it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1015/3143 [06:24<11:42,  3.03it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1008/3145 [06:27<12:54,  2.76it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1016/3145 [06:38<14:16,  2.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1016/3143 [06:25<11:43,  3.03it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1013/3145 [06:25<13:33,  2.62it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1009/3145 [06:27<13:10,  2.70it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1017/3143 [06:25<12:00,  2.95it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1014/3145 [06:25<13:25,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1017/3145 [06:38<15:28,  2.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1010/3145 [06:28<13:29,  2.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1018/3143 [06:25<11:50,  2.99it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1011/3145 [06:28<11:56,  2.98it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1015/3145 [06:26<13:37,  2.61it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1018/3145 [06:38<14:44,  2.40it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1019/3143 [06:26<12:04,  2.93it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1016/3145 [06:26<13:17,  2.67it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1012/3145 [06:28<12:31,  2.84it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1019/3145 [06:39<13:59,  2.53it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1020/3143 [06:26<12:31,  2.83it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1013/3145 [06:29<12:26,  2.86it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1017/3145 [06:26<13:33,  2.62it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1020/3145 [06:39<13:52,  2.55it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1021/3143 [06:27<12:02,  2.94it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1014/3145 [06:29<12:46,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1018/3145 [06:27<13:35,  2.61it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1021/3145 [06:39<13:26,  2.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1022/3143 [06:27<12:13,  2.89it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1015/3145 [06:30<13:08,  2.70it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1019/3145 [06:27<13:23,  2.65it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1022/3145 [06:40<13:44,  2.57it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1023/3143 [06:27<12:43,  2.78it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1016/3145 [06:30<13:41,  2.59it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1024/3143 [06:28<11:32,  3.06it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1023/3145 [06:40<13:40,  2.59it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1020/3145 [06:28<14:23,  2.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1025/3143 [06:28<12:16,  2.87it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1024/3145 [06:41<13:48,  2.56it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1017/3145 [06:31<15:14,  2.33it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1021/3145 [06:28<15:12,  2.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1026/3143 [06:28<12:42,  2.78it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1025/3145 [06:41<14:00,  2.52it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1018/3145 [06:31<15:06,  2.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1022/3145 [06:28<15:44,  2.25it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1027/3143 [06:29<13:22,  2.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1026/3145 [06:42<14:15,  2.48it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1019/3145 [06:31<14:08,  2.51it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1023/3145 [06:29<15:05,  2.34it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1028/3143 [06:29<13:00,  2.71it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1027/3145 [06:42<13:51,  2.55it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1020/3145 [06:32<13:31,  2.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1024/3145 [06:29<13:43,  2.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1029/3143 [06:29<13:10,  2.67it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1021/3145 [06:32<13:25,  2.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1028/3145 [06:42<14:29,  2.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1025/3145 [06:30<13:50,  2.55it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1030/3143 [06:30<12:58,  2.71it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1022/3145 [06:32<13:16,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1026/3145 [06:30<13:25,  2.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1029/3145 [06:43<15:22,  2.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1031/3143 [06:30<12:49,  2.74it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1023/3145 [06:33<13:04,  2.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1027/3145 [06:30<13:03,  2.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1030/3145 [06:43<15:12,  2.32it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1032/3143 [06:31<13:03,  2.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1024/3145 [06:33<12:57,  2.73it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1028/3145 [06:31<12:53,  2.74it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1033/3143 [06:31<11:22,  3.09it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1031/3145 [06:44<14:42,  2.40it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1025/3145 [06:33<12:52,  2.74it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1029/3145 [06:31<12:32,  2.81it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1034/3143 [06:31<12:18,  2.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1032/3145 [06:44<14:23,  2.45it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1026/3145 [06:34<12:31,  2.82it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1030/3145 [06:31<12:34,  2.80it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1035/3143 [06:31<12:00,  2.93it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1027/3145 [06:34<12:37,  2.80it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1033/3145 [06:44<14:12,  2.48it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1031/3145 [06:32<12:18,  2.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1036/3143 [06:32<12:26,  2.82it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1034/3145 [06:45<12:35,  2.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1028/3145 [06:34<12:37,  2.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1032/3145 [06:32<12:09,  2.90it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1037/3143 [06:32<12:46,  2.75it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1035/3145 [06:45<12:52,  2.73it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1029/3145 [06:35<13:00,  2.71it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1033/3145 [06:32<12:54,  2.73it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3143 [06:33<12:24,  2.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1036/3145 [06:45<13:05,  2.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1030/3145 [06:35<12:58,  2.72it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1034/3145 [06:33<13:25,  2.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1039/3143 [06:33<12:28,  2.81it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1037/3145 [06:46<12:42,  2.76it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1031/3145 [06:36<13:36,  2.59it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1035/3145 [06:33<13:30,  2.60it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1040/3143 [06:33<12:28,  2.81it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3145 [06:46<12:41,  2.77it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1032/3145 [06:36<13:17,  2.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1036/3145 [06:34<13:31,  2.60it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1041/3143 [06:34<12:33,  2.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1039/3145 [06:47<12:41,  2.76it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1033/3145 [06:36<13:08,  2.68it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1040/3145 [06:47<11:28,  3.06it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1037/3145 [06:34<13:24,  2.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1042/3143 [06:34<13:29,  2.60it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1034/3145 [06:37<13:00,  2.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1041/3145 [06:47<11:50,  2.96it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3145 [06:34<13:09,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1043/3143 [06:34<13:15,  2.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1035/3145 [06:37<12:53,  2.73it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1039/3145 [06:35<12:59,  2.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1042/3145 [06:48<12:23,  2.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1044/3143 [06:35<13:23,  2.61it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1043/3145 [06:48<11:31,  3.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1036/3145 [06:38<13:08,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1040/3145 [06:35<12:35,  2.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1045/3143 [06:35<13:28,  2.59it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1044/3145 [06:48<11:44,  2.98it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1041/3145 [06:35<12:32,  2.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1037/3145 [06:38<13:26,  2.61it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1046/3143 [06:36<13:03,  2.68it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1045/3145 [06:49<12:13,  2.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1038/3145 [06:38<13:24,  2.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1042/3145 [06:36<14:01,  2.50it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1047/3143 [06:36<13:10,  2.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1046/3145 [06:49<12:34,  2.78it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1039/3145 [06:39<13:35,  2.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1043/3145 [06:36<13:22,  2.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1048/3143 [06:36<12:58,  2.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1040/3145 [06:39<13:18,  2.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1047/3145 [06:49<14:05,  2.48it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1044/3145 [06:37<14:31,  2.41it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1049/3143 [06:37<13:08,  2.66it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1041/3145 [06:39<12:45,  2.75it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1048/3145 [06:50<13:55,  2.51it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1045/3145 [06:37<13:38,  2.57it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1050/3143 [06:37<13:02,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1042/3145 [06:40<13:46,  2.55it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1049/3145 [06:50<13:32,  2.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1051/3143 [06:38<13:10,  2.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1046/3145 [06:37<14:17,  2.45it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1043/3145 [06:40<13:33,  2.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1050/3145 [06:50<13:04,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1052/3143 [06:38<13:10,  2.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1047/3145 [06:38<15:12,  2.30it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1051/3145 [06:51<12:58,  2.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1044/3145 [06:41<13:41,  2.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1053/3143 [06:38<12:48,  2.72it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1052/3145 [06:51<11:28,  3.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1045/3145 [06:41<13:05,  2.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1048/3145 [06:38<15:21,  2.28it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1054/3143 [06:39<12:56,  2.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1053/3145 [06:51<11:20,  3.07it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1049/3145 [06:39<14:47,  2.36it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1046/3145 [06:41<13:37,  2.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1055/3143 [06:39<13:08,  2.65it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1054/3145 [06:52<12:10,  2.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1047/3145 [06:42<13:25,  2.60it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1050/3145 [06:39<15:05,  2.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1056/3143 [06:39<12:55,  2.69it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1055/3145 [06:52<13:14,  2.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1051/3145 [06:40<12:58,  2.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1048/3145 [06:42<13:30,  2.59it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1057/3143 [06:40<12:45,  2.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1056/3145 [06:53<12:06,  2.88it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1052/3145 [06:40<12:46,  2.73it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1049/3145 [06:42<13:13,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1058/3143 [06:40<12:55,  2.69it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1057/3145 [06:53<12:16,  2.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1053/3145 [06:40<12:33,  2.78it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1050/3145 [06:43<13:02,  2.68it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1058/3145 [06:53<12:20,  2.82it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1059/3143 [06:40<13:05,  2.65it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1054/3145 [06:41<12:59,  2.68it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1051/3145 [06:43<13:11,  2.65it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1060/3143 [06:41<12:45,  2.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1059/3145 [06:54<12:21,  2.81it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1055/3145 [06:41<12:37,  2.76it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1052/3145 [06:44<13:22,  2.61it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1060/3145 [06:54<12:25,  2.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1061/3143 [06:41<13:27,  2.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1056/3145 [06:41<12:34,  2.77it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1053/3145 [06:44<13:01,  2.68it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1061/3145 [06:54<12:31,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1062/3143 [06:42<13:19,  2.60it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1057/3145 [06:42<12:34,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1054/3145 [06:44<12:35,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1062/3145 [06:55<13:08,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1058/3145 [06:42<12:17,  2.83it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1063/3143 [06:42<13:38,  2.54it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1055/3145 [06:45<12:43,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1063/3145 [06:55<12:54,  2.69it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1059/3145 [06:42<12:45,  2.73it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1064/3143 [06:42<13:56,  2.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1056/3145 [06:45<12:41,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1064/3145 [06:55<12:47,  2.71it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1060/3145 [06:43<12:21,  2.81it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1065/3143 [06:43<13:28,  2.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1057/3145 [06:45<12:36,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1065/3145 [06:56<12:39,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1061/3145 [06:43<12:40,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1066/3143 [06:43<13:26,  2.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1058/3145 [06:46<12:32,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1066/3145 [06:56<12:34,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1062/3145 [06:43<12:33,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1067/3143 [06:44<12:32,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1059/3145 [06:46<12:25,  2.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1067/3145 [06:57<13:06,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1063/3145 [06:44<12:14,  2.84it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1068/3143 [06:44<12:30,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1060/3145 [06:47<12:41,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1064/3145 [06:44<12:08,  2.85it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1068/3145 [06:57<12:59,  2.67it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1069/3143 [06:44<12:36,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1061/3145 [06:47<13:16,  2.62it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1069/3145 [06:57<12:46,  2.71it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1065/3145 [06:45<12:35,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1070/3143 [06:45<12:31,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1062/3145 [06:47<12:59,  2.67it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1066/3145 [06:45<11:05,  3.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1070/3145 [06:58<12:54,  2.68it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1071/3143 [06:45<12:16,  2.81it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1063/3145 [06:48<12:37,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1067/3145 [06:45<11:22,  3.05it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1071/3145 [06:58<12:43,  2.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1072/3143 [06:45<12:23,  2.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1068/3145 [06:45<11:57,  2.89it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1064/3145 [06:48<14:00,  2.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1072/3145 [06:58<12:36,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1073/3143 [06:46<12:39,  2.73it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1069/3145 [06:46<12:07,  2.85it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1073/3145 [06:59<12:14,  2.82it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1065/3145 [06:48<13:33,  2.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1074/3143 [06:46<13:29,  2.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1074/3145 [06:59<10:52,  3.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1070/3145 [06:46<12:50,  2.69it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1075/3143 [06:47<13:07,  2.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1066/3145 [06:49<15:22,  2.25it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1075/3145 [06:59<11:36,  2.97it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1071/3145 [06:47<12:34,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1076/3143 [06:47<13:06,  2.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1072/3145 [06:47<12:27,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1076/3145 [07:00<12:28,  2.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1067/3145 [06:50<16:41,  2.07it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1077/3143 [06:47<12:54,  2.67it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1073/3145 [06:47<13:07,  2.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1077/3145 [07:00<13:04,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1068/3145 [06:50<16:04,  2.15it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1078/3143 [06:48<13:00,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1074/3145 [06:48<12:51,  2.68it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1078/3145 [07:01<12:56,  2.66it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1069/3145 [06:50<15:34,  2.22it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1079/3143 [06:48<12:49,  2.68it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1075/3145 [06:48<11:24,  3.02it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1079/3145 [07:01<12:31,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1080/3143 [06:48<11:12,  3.07it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1070/3145 [06:51<14:20,  2.41it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1076/3145 [06:48<11:35,  2.98it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1080/3145 [07:01<12:09,  2.83it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1081/3143 [06:49<11:29,  2.99it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1071/3145 [06:51<14:22,  2.40it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1077/3145 [06:49<11:46,  2.93it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1081/3145 [07:02<12:32,  2.74it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1082/3143 [06:49<11:41,  2.94it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1072/3145 [06:51<12:28,  2.77it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1078/3145 [06:49<12:00,  2.87it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1083/3143 [06:49<11:50,  2.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1082/3145 [07:02<13:50,  2.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1073/3145 [06:52<13:08,  2.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1079/3145 [06:49<11:48,  2.92it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1084/3143 [06:50<11:57,  2.87it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1083/3145 [07:02<13:21,  2.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1080/3145 [06:50<11:40,  2.95it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1074/3145 [06:52<13:55,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1085/3143 [06:50<11:48,  2.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1084/3145 [07:03<13:03,  2.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1081/3145 [06:50<11:51,  2.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1075/3145 [06:53<14:04,  2.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1086/3143 [06:50<11:50,  2.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1085/3145 [07:03<12:43,  2.70it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1082/3145 [06:50<11:37,  2.96it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1076/3145 [06:53<13:26,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1087/3143 [06:51<12:16,  2.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1083/3145 [06:51<11:38,  2.95it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1088/3143 [06:51<10:53,  3.14it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1086/3145 [07:04<14:24,  2.38it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1077/3145 [06:53<13:27,  2.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1084/3145 [06:51<11:37,  2.96it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1089/3143 [06:51<11:11,  3.06it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1078/3145 [06:54<13:06,  2.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1087/3145 [07:04<14:13,  2.41it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1085/3145 [06:51<11:42,  2.93it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1088/3145 [07:04<12:53,  2.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1090/3143 [06:52<11:33,  2.96it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1079/3145 [06:54<13:05,  2.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1086/3145 [06:52<11:41,  2.94it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1089/3145 [07:05<13:11,  2.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1091/3143 [06:52<12:37,  2.71it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1080/3145 [06:55<13:07,  2.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1087/3145 [06:52<11:40,  2.94it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1090/3145 [07:05<13:08,  2.61it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1081/3145 [06:55<12:43,  2.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1088/3145 [06:52<11:51,  2.89it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1092/3143 [06:53<13:10,  2.60it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1082/3145 [06:55<12:38,  2.72it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1091/3145 [07:06<13:23,  2.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1089/3145 [06:53<12:01,  2.85it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1093/3143 [06:53<12:45,  2.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1094/3143 [06:53<11:19,  3.01it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1092/3145 [07:06<12:58,  2.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1083/3145 [06:56<12:56,  2.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1090/3145 [06:53<12:12,  2.81it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1095/3143 [06:53<11:28,  2.97it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1093/3145 [07:06<13:02,  2.62it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1084/3145 [06:56<12:44,  2.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1091/3145 [06:54<12:37,  2.71it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1096/3143 [06:54<11:39,  2.93it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1092/3145 [06:54<12:27,  2.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1085/3145 [06:56<13:21,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1097/3143 [06:54<11:41,  2.91it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1094/3145 [07:07<15:02,  2.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1093/3145 [06:54<12:21,  2.77it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1086/3145 [06:57<13:00,  2.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1095/3145 [07:07<14:29,  2.36it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1098/3143 [06:55<12:15,  2.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1094/3145 [06:55<12:19,  2.77it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1087/3145 [06:57<13:17,  2.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1099/3143 [06:55<12:28,  2.73it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1096/3145 [07:08<14:24,  2.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1088/3145 [06:58<11:47,  2.91it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1095/3145 [06:55<12:54,  2.65it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1100/3143 [06:55<12:25,  2.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1089/3145 [06:58<12:11,  2.81it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1097/3145 [07:08<14:36,  2.34it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1096/3145 [06:55<13:19,  2.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1101/3143 [06:56<11:55,  2.85it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1098/3145 [07:09<14:23,  2.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1097/3145 [06:56<12:44,  2.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1090/3145 [06:58<13:35,  2.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1102/3143 [06:56<12:35,  2.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1099/3145 [07:09<14:28,  2.35it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1091/3145 [06:59<13:30,  2.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1098/3145 [06:56<14:07,  2.41it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1103/3143 [06:56<13:09,  2.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1100/3145 [07:09<14:21,  2.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1092/3145 [06:59<14:03,  2.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1099/3145 [06:57<14:06,  2.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1104/3143 [06:57<13:33,  2.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1101/3145 [07:10<14:07,  2.41it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1093/3145 [07:00<14:16,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1100/3145 [06:57<14:27,  2.36it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1105/3143 [06:57<14:16,  2.38it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1102/3145 [07:10<13:16,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1094/3145 [07:00<14:02,  2.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1101/3145 [06:58<13:47,  2.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1106/3143 [06:58<13:18,  2.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1103/3145 [07:11<13:32,  2.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1095/3145 [07:00<12:20,  2.77it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1102/3145 [06:58<13:30,  2.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1107/3143 [06:58<13:19,  2.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1104/3145 [07:11<13:15,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1096/3145 [07:01<13:42,  2.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1103/3145 [06:58<13:43,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1108/3143 [06:58<13:25,  2.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1105/3145 [07:11<13:34,  2.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1097/3145 [07:01<13:20,  2.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1104/3145 [06:59<13:41,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1109/3143 [06:59<13:21,  2.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1106/3145 [07:12<13:05,  2.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1098/3145 [07:02<13:20,  2.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1105/3145 [06:59<13:30,  2.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1110/3143 [06:59<13:12,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1107/3145 [07:12<13:05,  2.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1099/3145 [07:02<13:44,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1106/3145 [07:00<13:20,  2.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1111/3143 [07:00<13:48,  2.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1108/3145 [07:13<14:18,  2.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1100/3145 [07:02<14:17,  2.38it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1107/3145 [07:00<14:02,  2.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1112/3143 [07:00<13:39,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1109/3145 [07:13<14:08,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1101/3145 [07:03<13:44,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1108/3145 [07:00<13:58,  2.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1113/3143 [07:00<13:46,  2.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1110/3145 [07:13<13:35,  2.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1102/3145 [07:03<13:27,  2.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1109/3145 [07:01<13:17,  2.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1114/3143 [07:01<13:45,  2.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1111/3145 [07:14<12:34,  2.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1103/3145 [07:04<13:41,  2.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1110/3145 [07:01<13:31,  2.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1112/3145 [07:14<12:37,  2.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1115/3143 [07:01<14:06,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1104/3145 [07:04<13:20,  2.55it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1116/3143 [07:02<12:22,  2.73it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1111/3145 [07:02<13:54,  2.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1113/3145 [07:15<13:46,  2.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1105/3145 [07:04<14:03,  2.42it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1117/3143 [07:02<12:30,  2.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1112/3145 [07:02<13:42,  2.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1114/3145 [07:15<14:13,  2.38it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1106/3145 [07:05<13:42,  2.48it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1118/3143 [07:02<12:41,  2.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1113/3145 [07:02<14:05,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1115/3145 [07:15<14:49,  2.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1107/3145 [07:05<13:36,  2.50it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1119/3143 [07:03<13:23,  2.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1114/3145 [07:03<13:53,  2.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1116/3145 [07:16<14:29,  2.33it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1108/3145 [07:06<13:58,  2.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1115/3145 [07:03<13:30,  2.50it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1120/3143 [07:03<14:22,  2.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1117/3145 [07:16<13:36,  2.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1109/3145 [07:06<14:10,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1116/3145 [07:04<13:07,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1121/3143 [07:04<14:21,  2.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1118/3145 [07:17<13:00,  2.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1110/3145 [07:06<13:42,  2.47it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1117/3145 [07:04<13:15,  2.55it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1119/3145 [07:17<12:17,  2.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1122/3143 [07:04<14:07,  2.38it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1111/3145 [07:07<13:35,  2.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1118/3145 [07:04<13:08,  2.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1123/3143 [07:04<12:28,  2.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1120/3145 [07:17<12:43,  2.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1124/3143 [07:05<12:33,  2.68it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1119/3145 [07:05<13:06,  2.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1121/3145 [07:18<11:46,  2.87it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1112/3145 [07:07<14:22,  2.36it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1122/3145 [07:18<10:47,  3.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1125/3143 [07:05<12:47,  2.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1120/3145 [07:05<13:12,  2.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1113/3145 [07:08<14:16,  2.37it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1123/3145 [07:18<09:54,  3.40it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1126/3143 [07:06<12:42,  2.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1121/3145 [07:06<13:21,  2.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1114/3145 [07:08<13:41,  2.47it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1124/3145 [07:18<10:42,  3.15it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1122/3145 [07:06<13:32,  2.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1127/3143 [07:06<13:35,  2.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1115/3145 [07:08<13:39,  2.48it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1125/3145 [07:19<12:22,  2.72it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1123/3145 [07:06<12:50,  2.62it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1128/3143 [07:06<13:04,  2.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1116/3145 [07:09<14:01,  2.41it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1126/3145 [07:19<12:57,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1124/3145 [07:07<12:32,  2.69it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1129/3143 [07:07<12:42,  2.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1127/3145 [07:20<11:33,  2.91it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1117/3145 [07:09<13:45,  2.46it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1125/3145 [07:07<12:05,  2.78it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1130/3143 [07:07<12:46,  2.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1118/3145 [07:10<13:07,  2.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1128/3145 [07:20<11:58,  2.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1126/3145 [07:07<11:36,  2.90it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1131/3143 [07:07<12:44,  2.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1119/3145 [07:10<12:41,  2.66it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1127/3145 [07:08<11:55,  2.82it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1129/3145 [07:20<13:19,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1132/3143 [07:08<12:59,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1120/3145 [07:10<13:02,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1128/3145 [07:08<12:21,  2.72it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1130/3145 [07:21<13:35,  2.47it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1121/3145 [07:11<12:59,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1133/3143 [07:08<14:28,  2.31it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1129/3145 [07:08<12:34,  2.67it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1131/3145 [07:21<13:13,  2.54it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1122/3145 [07:11<12:57,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1134/3143 [07:09<14:13,  2.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1130/3145 [07:09<12:41,  2.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1132/3145 [07:22<13:17,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1123/3145 [07:12<12:59,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1135/3143 [07:09<13:21,  2.51it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1131/3145 [07:09<12:32,  2.68it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1133/3145 [07:22<13:47,  2.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1124/3145 [07:12<12:59,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1136/3143 [07:10<12:54,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1132/3145 [07:10<12:26,  2.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1134/3145 [07:23<13:53,  2.41it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1125/3145 [07:12<13:00,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1137/3143 [07:10<12:58,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1133/3145 [07:10<12:00,  2.79it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1135/3145 [07:23<13:18,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1126/3145 [07:13<13:00,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1138/3143 [07:10<12:44,  2.62it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1134/3145 [07:10<12:07,  2.76it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1136/3145 [07:23<13:46,  2.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1127/3145 [07:13<13:03,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1135/3145 [07:11<12:09,  2.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1139/3143 [07:11<12:53,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1137/3145 [07:24<13:15,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1140/3143 [07:11<12:37,  2.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1128/3145 [07:14<13:05,  2.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1136/3145 [07:11<12:39,  2.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1138/3145 [07:24<12:48,  2.61it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1141/3143 [07:11<12:13,  2.73it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1129/3145 [07:14<12:53,  2.61it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1137/3145 [07:11<12:46,  2.62it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1139/3145 [07:24<12:30,  2.67it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1130/3145 [07:14<12:24,  2.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1142/3143 [07:12<12:50,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1138/3145 [07:12<12:54,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1140/3145 [07:25<12:41,  2.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1139/3145 [07:12<11:23,  2.93it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1131/3145 [07:15<12:41,  2.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1143/3143 [07:12<12:38,  2.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1141/3145 [07:25<12:35,  2.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1140/3145 [07:12<11:50,  2.82it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1144/3143 [07:12<12:16,  2.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1132/3145 [07:15<12:38,  2.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1142/3145 [07:26<12:26,  2.68it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1141/3145 [07:13<11:40,  2.86it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1133/3145 [07:15<12:34,  2.67it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1145/3143 [07:13<13:12,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1143/3145 [07:26<12:21,  2.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1142/3145 [07:13<11:33,  2.89it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1134/3145 [07:16<12:17,  2.73it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1146/3143 [07:13<13:05,  2.54it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1144/3145 [07:26<12:50,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1143/3145 [07:14<12:25,  2.69it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1135/3145 [07:16<12:20,  2.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1147/3143 [07:14<12:44,  2.61it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1145/3145 [07:27<12:17,  2.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1144/3145 [07:14<12:20,  2.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1136/3145 [07:17<13:08,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1148/3143 [07:14<12:49,  2.59it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1146/3145 [07:27<12:26,  2.68it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1145/3145 [07:14<11:57,  2.79it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1137/3145 [07:17<12:53,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1147/3145 [07:27<11:05,  3.00it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1149/3143 [07:14<12:52,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1146/3145 [07:15<11:51,  2.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1138/3145 [07:17<12:40,  2.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1148/3145 [07:28<11:53,  2.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1150/3143 [07:15<13:10,  2.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1147/3145 [07:15<12:30,  2.66it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1139/3145 [07:18<12:48,  2.61it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1149/3145 [07:28<11:54,  2.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1151/3143 [07:15<12:47,  2.60it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1148/3145 [07:15<11:44,  2.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1140/3145 [07:18<12:12,  2.74it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1150/3145 [07:28<12:09,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1152/3143 [07:16<13:06,  2.53it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1149/3145 [07:16<12:05,  2.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1141/3145 [07:18<12:25,  2.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1151/3145 [07:29<12:09,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1153/3143 [07:16<12:21,  2.68it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1150/3145 [07:16<12:01,  2.77it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1142/3145 [07:19<11:56,  2.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1152/3145 [07:29<11:51,  2.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1154/3143 [07:16<12:51,  2.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1143/3145 [07:19<11:37,  2.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1151/3145 [07:17<13:07,  2.53it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1155/3143 [07:17<11:18,  2.93it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1153/3145 [07:30<12:13,  2.72it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1144/3145 [07:19<11:28,  2.91it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1152/3145 [07:17<13:02,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1156/3143 [07:17<11:46,  2.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1154/3145 [07:30<12:46,  2.60it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1145/3145 [07:20<11:28,  2.90it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1153/3145 [07:17<12:44,  2.60it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1157/3143 [07:17<11:50,  2.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1155/3145 [07:30<12:16,  2.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1146/3145 [07:20<11:38,  2.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1154/3145 [07:18<12:32,  2.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1158/3143 [07:18<11:57,  2.77it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1156/3145 [07:31<11:57,  2.77it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1147/3145 [07:20<12:04,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1155/3145 [07:18<12:28,  2.66it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1159/3143 [07:18<12:13,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1157/3145 [07:31<11:59,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1148/3145 [07:21<12:01,  2.77it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1156/3145 [07:18<12:17,  2.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1160/3143 [07:19<12:13,  2.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1158/3145 [07:31<12:13,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1149/3145 [07:21<12:17,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1157/3145 [07:19<11:58,  2.77it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1161/3143 [07:19<11:58,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1159/3145 [07:32<11:42,  2.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1150/3145 [07:22<12:27,  2.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1158/3145 [07:19<12:01,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1162/3143 [07:19<11:39,  2.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1160/3145 [07:32<11:46,  2.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1151/3145 [07:22<12:19,  2.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1163/3143 [07:20<11:42,  2.82it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1161/3145 [07:32<11:47,  2.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1159/3145 [07:20<13:25,  2.47it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1152/3145 [07:22<12:31,  2.65it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1164/3143 [07:20<11:59,  2.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1162/3145 [07:33<11:45,  2.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1160/3145 [07:20<14:53,  2.22it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1153/3145 [07:23<13:01,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1165/3143 [07:20<12:04,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1163/3145 [07:33<11:56,  2.77it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1161/3145 [07:21<15:18,  2.16it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1166/3143 [07:21<12:05,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1154/3145 [07:23<13:21,  2.48it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1164/3145 [07:34<12:15,  2.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1167/3143 [07:21<12:08,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1165/3145 [07:34<12:05,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1155/3145 [07:24<13:23,  2.48it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1162/3145 [07:21<15:19,  2.16it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1168/3143 [07:21<12:09,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1166/3145 [07:34<11:58,  2.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1163/3145 [07:21<14:21,  2.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1156/3145 [07:24<13:13,  2.51it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1167/3145 [07:35<11:42,  2.82it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1169/3143 [07:22<12:20,  2.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1157/3145 [07:24<12:45,  2.60it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1164/3145 [07:22<14:57,  2.21it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1168/3145 [07:35<11:44,  2.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1170/3143 [07:22<12:12,  2.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1158/3145 [07:25<12:29,  2.65it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1165/3145 [07:22<15:22,  2.15it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1171/3143 [07:23<11:52,  2.77it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1169/3145 [07:35<12:09,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1159/3145 [07:25<12:25,  2.66it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1166/3145 [07:23<14:24,  2.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1172/3143 [07:23<11:49,  2.78it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1170/3145 [07:36<12:20,  2.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1160/3145 [07:25<12:32,  2.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1167/3145 [07:23<13:58,  2.36it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1173/3143 [07:23<12:11,  2.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1171/3145 [07:36<12:31,  2.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1161/3145 [07:26<12:23,  2.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1168/3145 [07:24<13:36,  2.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1174/3143 [07:24<12:00,  2.73it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1172/3145 [07:36<12:07,  2.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1162/3145 [07:26<12:58,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1175/3143 [07:24<11:57,  2.74it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1169/3145 [07:24<13:10,  2.50it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1173/3145 [07:37<12:26,  2.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1163/3145 [07:27<12:42,  2.60it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1170/3145 [07:24<12:54,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1176/3143 [07:24<12:19,  2.66it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1174/3145 [07:37<12:23,  2.65it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1164/3145 [07:27<12:27,  2.65it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1171/3145 [07:25<12:53,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1177/3143 [07:25<12:32,  2.61it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1165/3145 [07:27<12:17,  2.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1175/3145 [07:38<13:03,  2.51it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1178/3143 [07:25<11:56,  2.74it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1172/3145 [07:25<12:51,  2.56it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1176/3145 [07:38<13:00,  2.52it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1166/3145 [07:28<13:41,  2.41it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1179/3143 [07:25<11:51,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1173/3145 [07:26<12:56,  2.54it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1177/3145 [07:38<12:43,  2.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1167/3145 [07:28<13:27,  2.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1180/3143 [07:26<11:33,  2.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1174/3145 [07:26<12:52,  2.55it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1178/3145 [07:39<12:32,  2.61it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1168/3145 [07:29<12:57,  2.54it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1181/3143 [07:26<11:29,  2.84it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1169/3145 [07:29<11:08,  2.96it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1179/3145 [07:39<12:30,  2.62it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1175/3145 [07:26<13:27,  2.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1182/3143 [07:27<11:24,  2.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1170/3145 [07:29<11:23,  2.89it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1180/3145 [07:40<12:27,  2.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1176/3145 [07:27<13:10,  2.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1183/3143 [07:27<11:14,  2.90it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1171/3145 [07:30<11:32,  2.85it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1181/3145 [07:40<12:16,  2.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1177/3145 [07:27<13:10,  2.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1184/3143 [07:27<11:25,  2.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1172/3145 [07:30<11:53,  2.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1182/3145 [07:40<12:05,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1185/3143 [07:28<11:30,  2.84it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1178/3145 [07:28<13:23,  2.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1173/3145 [07:30<11:45,  2.80it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1183/3145 [07:41<12:17,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1186/3143 [07:28<11:32,  2.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1179/3145 [07:28<12:53,  2.54it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1174/3145 [07:31<12:22,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1184/3145 [07:41<11:53,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1187/3143 [07:28<11:48,  2.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1180/3145 [07:28<12:47,  2.56it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1175/3145 [07:31<12:20,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1185/3145 [07:41<11:56,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1188/3143 [07:29<11:44,  2.78it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1181/3145 [07:29<12:48,  2.55it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1186/3145 [07:42<11:28,  2.84it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1176/3145 [07:31<12:34,  2.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1189/3143 [07:29<11:46,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1182/3145 [07:29<12:36,  2.60it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1187/3145 [07:42<11:21,  2.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1190/3143 [07:29<11:42,  2.78it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1177/3145 [07:32<13:15,  2.47it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1183/3145 [07:29<12:29,  2.62it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1188/3145 [07:42<11:46,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1191/3143 [07:30<11:39,  2.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1178/3145 [07:32<12:45,  2.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1184/3145 [07:30<12:14,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1189/3145 [07:43<11:45,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1192/3143 [07:30<11:46,  2.76it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1179/3145 [07:33<12:21,  2.65it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1185/3145 [07:30<12:04,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1190/3145 [07:43<11:54,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1193/3143 [07:30<11:38,  2.79it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1180/3145 [07:33<12:24,  2.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1186/3145 [07:31<12:16,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1191/3145 [07:44<11:51,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1194/3143 [07:31<11:52,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1187/3145 [07:31<12:07,  2.69it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1181/3145 [07:33<13:06,  2.50it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1192/3145 [07:44<11:54,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1195/3143 [07:31<12:05,  2.69it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1188/3145 [07:31<11:59,  2.72it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1182/3145 [07:34<13:20,  2.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1193/3145 [07:44<12:10,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1196/3143 [07:32<11:55,  2.72it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1189/3145 [07:32<12:11,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1183/3145 [07:34<12:52,  2.54it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1194/3145 [07:45<11:57,  2.72it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1197/3143 [07:32<11:36,  2.79it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1190/3145 [07:32<12:18,  2.65it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1184/3145 [07:35<12:33,  2.60it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1195/3145 [07:45<12:10,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1198/3143 [07:32<11:32,  2.81it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1185/3145 [07:35<12:04,  2.71it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1199/3143 [07:33<10:38,  3.04it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1191/3145 [07:33<13:47,  2.36it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1196/3145 [07:45<12:19,  2.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1186/3145 [07:35<11:54,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1200/3143 [07:33<11:27,  2.82it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1197/3145 [07:46<12:08,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1192/3145 [07:33<13:28,  2.42it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1187/3145 [07:36<11:37,  2.81it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1201/3143 [07:33<11:47,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1193/3145 [07:33<12:55,  2.52it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1198/3145 [07:46<12:12,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1188/3145 [07:36<11:37,  2.81it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1202/3143 [07:34<11:47,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1199/3145 [07:46<11:52,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1194/3145 [07:34<12:26,  2.62it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1189/3145 [07:36<12:16,  2.65it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1200/3145 [07:47<11:45,  2.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1203/3143 [07:34<12:16,  2.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1195/3145 [07:34<12:58,  2.50it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1190/3145 [07:37<11:46,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1201/3145 [07:47<11:41,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1204/3143 [07:34<11:58,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1196/3145 [07:34<12:15,  2.65it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1191/3145 [07:37<12:03,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1202/3145 [07:47<10:17,  3.15it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1197/3145 [07:35<10:50,  2.99it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1205/3143 [07:35<11:38,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1203/3145 [07:48<10:32,  3.07it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1192/3145 [07:38<12:39,  2.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1198/3145 [07:35<11:48,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1206/3143 [07:35<11:53,  2.71it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1204/3145 [07:48<11:07,  2.91it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1193/3145 [07:38<12:27,  2.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1199/3145 [07:35<11:49,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1207/3143 [07:36<12:22,  2.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1205/3145 [07:49<11:15,  2.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1194/3145 [07:38<12:12,  2.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1200/3145 [07:36<11:59,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1208/3143 [07:36<12:20,  2.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1206/3145 [07:49<11:22,  2.84it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1195/3145 [07:39<12:19,  2.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1201/3145 [07:36<11:52,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1209/3143 [07:36<12:05,  2.67it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1207/3145 [07:49<11:46,  2.74it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1196/3145 [07:39<12:00,  2.70it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1210/3143 [07:37<10:35,  3.04it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1202/3145 [07:37<11:45,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1208/3145 [07:50<11:26,  2.82it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1197/3145 [07:39<11:54,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1203/3145 [07:37<11:23,  2.84it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1211/3143 [07:37<11:48,  2.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1209/3145 [07:50<11:28,  2.81it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1198/3145 [07:40<11:34,  2.80it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1204/3145 [07:37<11:48,  2.74it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1212/3143 [07:37<12:41,  2.54it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1210/3145 [07:50<11:36,  2.78it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1199/3145 [07:40<11:39,  2.78it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1205/3145 [07:38<12:23,  2.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1213/3143 [07:38<12:19,  2.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1211/3145 [07:51<11:37,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1200/3145 [07:40<11:58,  2.71it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1206/3145 [07:38<11:46,  2.74it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1212/3145 [07:51<11:31,  2.79it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1201/3145 [07:41<11:51,  2.73it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1214/3143 [07:38<14:04,  2.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1207/3145 [07:38<12:28,  2.59it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1213/3145 [07:51<11:19,  2.84it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1202/3145 [07:41<12:13,  2.65it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1215/3143 [07:39<13:36,  2.36it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1214/3145 [07:52<11:20,  2.84it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1208/3145 [07:39<13:14,  2.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1216/3143 [07:39<12:59,  2.47it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1203/3145 [07:42<12:28,  2.59it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1215/3145 [07:52<11:42,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1209/3145 [07:39<13:06,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1217/3143 [07:39<11:40,  2.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1204/3145 [07:42<12:14,  2.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1218/3143 [07:40<10:51,  2.95it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1216/3145 [07:52<11:36,  2.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1210/3145 [07:40<13:20,  2.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1219/3143 [07:40<10:27,  3.07it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1205/3145 [07:42<13:06,  2.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1211/3145 [07:40<12:46,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1217/3145 [07:53<12:54,  2.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1220/3143 [07:40<10:47,  2.97it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1206/3145 [07:43<12:53,  2.51it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1218/3145 [07:53<12:38,  2.54it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1212/3145 [07:41<13:26,  2.40it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1221/3143 [07:41<11:36,  2.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1207/3145 [07:43<12:56,  2.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1219/3145 [07:54<12:32,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1213/3145 [07:41<12:45,  2.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1208/3145 [07:44<12:22,  2.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1222/3143 [07:41<12:17,  2.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1220/3145 [07:54<12:18,  2.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1214/3145 [07:41<12:38,  2.55it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1209/3145 [07:44<11:05,  2.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1223/3143 [07:42<12:11,  2.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1221/3145 [07:55<12:34,  2.55it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1210/3145 [07:44<11:28,  2.81it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1215/3145 [07:42<13:06,  2.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1224/3143 [07:42<12:17,  2.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1222/3145 [07:55<12:38,  2.53it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1216/3145 [07:42<12:46,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1211/3145 [07:45<12:41,  2.54it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1225/3143 [07:42<12:18,  2.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1212/3145 [07:45<11:20,  2.84it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1223/3145 [07:55<12:42,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1217/3145 [07:43<12:26,  2.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1226/3143 [07:43<12:47,  2.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1213/3145 [07:45<11:40,  2.76it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1218/3145 [07:43<12:03,  2.66it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1224/3145 [07:56<12:29,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1227/3143 [07:43<12:24,  2.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1214/3145 [07:46<11:11,  2.88it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1219/3145 [07:43<11:45,  2.73it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1225/3145 [07:56<12:32,  2.55it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1215/3145 [07:46<09:58,  3.22it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1228/3143 [07:44<12:08,  2.63it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1220/3145 [07:44<11:40,  2.75it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1226/3145 [07:56<12:42,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1216/3145 [07:46<09:54,  3.24it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1221/3145 [07:44<10:28,  3.06it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1229/3143 [07:44<12:29,  2.55it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1227/3145 [07:57<11:16,  2.84it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1217/3145 [07:47<10:44,  2.99it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1222/3145 [07:44<11:28,  2.79it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1228/3145 [07:57<11:16,  2.83it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1230/3143 [07:44<12:47,  2.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1218/3145 [07:47<10:41,  3.00it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1229/3145 [07:57<11:32,  2.76it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1223/3145 [07:45<12:49,  2.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1231/3143 [07:45<12:55,  2.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1219/3145 [07:47<11:13,  2.86it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1230/3145 [07:58<10:49,  2.95it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1232/3143 [07:45<12:11,  2.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1224/3145 [07:45<12:25,  2.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1220/3145 [07:48<11:08,  2.88it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1231/3145 [07:58<10:49,  2.95it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1233/3143 [07:45<12:13,  2.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1225/3145 [07:45<12:12,  2.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1221/3145 [07:48<11:31,  2.78it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1232/3145 [07:58<11:16,  2.83it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1234/3143 [07:46<11:47,  2.70it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1226/3145 [07:46<12:21,  2.59it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1233/3145 [07:59<10:28,  3.04it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1222/3145 [07:48<12:08,  2.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1235/3143 [07:46<10:55,  2.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1227/3145 [07:46<11:54,  2.68it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1223/3145 [07:49<12:01,  2.67it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1234/3145 [07:59<10:58,  2.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1236/3143 [07:46<11:04,  2.87it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1228/3145 [07:47<12:10,  2.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1235/3145 [08:00<11:20,  2.81it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1224/3145 [07:49<12:43,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1237/3143 [07:47<11:43,  2.71it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1229/3145 [07:47<12:29,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1225/3145 [07:50<12:29,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1236/3145 [08:00<12:25,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1238/3143 [07:47<12:37,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1230/3145 [07:47<12:27,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1226/3145 [07:50<12:33,  2.55it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1237/3145 [08:00<12:25,  2.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1239/3143 [07:48<12:32,  2.53it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1231/3145 [07:48<12:38,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1227/3145 [07:50<12:26,  2.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1238/3145 [08:01<12:23,  2.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1240/3143 [07:48<12:21,  2.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1232/3145 [07:48<12:33,  2.54it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1228/3145 [07:51<12:40,  2.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1239/3145 [08:01<12:38,  2.51it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1241/3143 [07:49<12:54,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1233/3145 [07:49<12:19,  2.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1229/3145 [07:51<12:33,  2.54it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1240/3145 [08:02<12:17,  2.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1242/3143 [07:49<12:49,  2.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1234/3145 [07:49<12:15,  2.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1230/3145 [07:52<12:25,  2.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1241/3145 [08:02<13:28,  2.35it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1235/3145 [07:49<12:16,  2.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1243/3143 [07:49<13:08,  2.41it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1231/3145 [07:52<13:40,  2.33it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1242/3145 [08:02<13:23,  2.37it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1236/3145 [07:50<12:18,  2.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1244/3143 [07:50<13:04,  2.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1232/3145 [07:53<13:56,  2.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1243/3145 [08:03<13:23,  2.37it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1237/3145 [07:50<12:17,  2.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1245/3143 [07:50<12:43,  2.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1238/3145 [07:50<11:56,  2.66it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1233/3145 [07:53<13:26,  2.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1244/3145 [08:03<13:21,  2.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1246/3143 [07:51<12:48,  2.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1239/3145 [07:51<12:01,  2.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1245/3145 [08:04<12:45,  2.48it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1234/3145 [07:54<14:19,  2.22it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1247/3143 [07:51<13:23,  2.36it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1240/3145 [07:51<12:17,  2.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1246/3145 [08:04<12:55,  2.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1248/3143 [07:51<12:40,  2.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1235/3145 [07:54<13:59,  2.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1241/3145 [07:52<12:05,  2.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1247/3145 [08:05<12:54,  2.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1236/3145 [07:54<13:38,  2.33it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1249/3143 [07:52<13:14,  2.38it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1242/3145 [07:52<12:39,  2.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1248/3145 [08:05<12:46,  2.47it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1237/3145 [07:55<13:36,  2.34it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1250/3143 [07:52<13:12,  2.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1249/3145 [08:05<12:29,  2.53it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1243/3145 [07:53<13:24,  2.36it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1238/3145 [07:55<13:11,  2.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1251/3143 [07:53<13:11,  2.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1250/3145 [08:06<12:41,  2.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1244/3145 [07:53<13:17,  2.38it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1239/3145 [07:56<12:52,  2.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1252/3143 [07:53<13:03,  2.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1251/3145 [08:06<12:55,  2.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1240/3145 [07:56<12:45,  2.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1245/3145 [07:53<13:39,  2.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1253/3143 [07:54<13:07,  2.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1252/3145 [08:07<12:54,  2.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1241/3145 [07:56<12:59,  2.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1246/3145 [07:54<13:44,  2.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1254/3143 [07:54<12:44,  2.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1253/3145 [08:07<12:35,  2.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1242/3145 [07:57<12:48,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1247/3145 [07:54<13:45,  2.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1255/3143 [07:54<13:45,  2.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1254/3145 [08:07<12:54,  2.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1243/3145 [07:57<12:37,  2.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1248/3145 [07:55<13:42,  2.31it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1256/3143 [07:55<13:27,  2.34it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1244/3145 [07:57<11:45,  2.69it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1255/3145 [08:08<13:48,  2.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1249/3145 [07:55<13:09,  2.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1245/3145 [07:58<12:15,  2.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1257/3143 [07:55<14:29,  2.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1256/3145 [08:08<13:26,  2.34it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1250/3145 [07:56<13:37,  2.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1246/3145 [07:58<12:04,  2.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1258/3143 [07:56<14:44,  2.13it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1257/3145 [08:09<13:24,  2.35it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1251/3145 [07:56<13:19,  2.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1247/3145 [07:59<11:56,  2.65it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1259/3143 [07:56<14:41,  2.14it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1252/3145 [07:56<12:48,  2.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1258/3145 [08:09<13:49,  2.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1248/3145 [07:59<12:01,  2.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1260/3143 [07:57<13:32,  2.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1253/3145 [07:57<12:44,  2.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1259/3145 [08:10<13:42,  2.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1249/3145 [07:59<12:26,  2.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1261/3143 [07:57<13:09,  2.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1254/3145 [07:57<12:40,  2.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1250/3145 [08:00<11:41,  2.70it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1260/3145 [08:10<13:48,  2.27it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1262/3143 [07:57<12:37,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1251/3145 [08:00<10:47,  2.93it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1255/3145 [07:58<12:40,  2.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1261/3145 [08:10<13:08,  2.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1256/3145 [07:58<11:18,  2.78it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1252/3145 [08:00<10:50,  2.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1263/3143 [07:58<14:04,  2.23it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1262/3145 [08:11<12:46,  2.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1253/3145 [08:01<10:14,  3.08it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1257/3145 [07:58<11:29,  2.74it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1264/3143 [07:58<13:18,  2.35it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1254/3145 [08:01<10:43,  2.94it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1263/3145 [08:11<13:48,  2.27it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1258/3145 [07:59<12:06,  2.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1265/3143 [07:59<12:41,  2.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1255/3145 [08:01<10:50,  2.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1264/3145 [08:12<13:39,  2.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1266/3143 [07:59<11:59,  2.61it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1259/3145 [07:59<12:31,  2.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1256/3145 [08:02<11:15,  2.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1265/3145 [08:12<13:12,  2.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1267/3143 [07:59<12:07,  2.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1260/3145 [07:59<12:45,  2.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1257/3145 [08:02<11:40,  2.70it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1266/3145 [08:12<12:29,  2.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1261/3145 [08:00<12:52,  2.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1268/3143 [08:00<13:13,  2.36it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1267/3145 [08:13<13:08,  2.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1258/3145 [08:03<12:57,  2.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1262/3145 [08:00<13:01,  2.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1269/3143 [08:00<12:53,  2.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1259/3145 [08:03<12:39,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1268/3145 [08:13<13:28,  2.32it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1263/3145 [08:01<13:07,  2.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1270/3143 [08:01<13:08,  2.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1260/3145 [08:03<12:01,  2.61it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1269/3145 [08:14<12:39,  2.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1264/3145 [08:01<12:53,  2.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1271/3143 [08:01<13:34,  2.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1270/3145 [08:14<12:30,  2.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1261/3145 [08:04<12:53,  2.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1265/3145 [08:02<12:36,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1271/3145 [08:15<12:29,  2.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1272/3143 [08:02<14:25,  2.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1266/3145 [08:02<11:10,  2.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1262/3145 [08:04<14:01,  2.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1272/3145 [08:15<12:15,  2.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1267/3145 [08:02<11:24,  2.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1273/3143 [08:02<14:51,  2.10it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1263/3145 [08:05<14:25,  2.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1273/3145 [08:15<11:48,  2.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1268/3145 [08:03<11:30,  2.72it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1264/3145 [08:05<12:47,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1274/3143 [08:03<13:58,  2.23it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1274/3145 [08:16<11:43,  2.66it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1269/3145 [08:03<11:54,  2.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1275/3143 [08:03<13:15,  2.35it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1265/3145 [08:06<12:39,  2.48it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1275/3145 [08:16<11:41,  2.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1270/3145 [08:03<12:17,  2.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1266/3145 [08:06<12:46,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1276/3143 [08:03<13:31,  2.30it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1276/3145 [08:16<11:52,  2.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1271/3145 [08:04<11:52,  2.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1277/3143 [08:04<12:11,  2.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1267/3145 [08:06<12:46,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1277/3145 [08:17<11:48,  2.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1272/3145 [08:04<12:04,  2.58it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1278/3143 [08:04<12:27,  2.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1268/3145 [08:07<12:39,  2.47it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1278/3145 [08:17<11:39,  2.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1273/3145 [08:04<12:05,  2.58it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1279/3143 [08:05<12:01,  2.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1279/3145 [08:17<10:25,  2.98it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1269/3145 [08:07<12:23,  2.52it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1280/3143 [08:05<12:05,  2.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1274/3145 [08:05<12:34,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1270/3145 [08:08<12:17,  2.54it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1280/3145 [08:18<11:54,  2.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1281/3143 [08:05<11:08,  2.78it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1275/3145 [08:05<12:34,  2.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1271/3145 [08:08<12:04,  2.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1281/3145 [08:18<12:20,  2.52it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1282/3143 [08:06<11:34,  2.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1272/3145 [08:08<12:09,  2.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1276/3145 [08:06<13:28,  2.31it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1282/3145 [08:19<12:37,  2.46it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1283/3143 [08:06<12:51,  2.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1273/3145 [08:09<12:47,  2.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1283/3145 [08:19<11:54,  2.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1277/3145 [08:06<13:48,  2.26it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1284/3143 [08:07<12:28,  2.48it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1284/3145 [08:19<12:02,  2.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1274/3145 [08:09<12:59,  2.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1278/3145 [08:07<13:35,  2.29it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1285/3143 [08:07<12:30,  2.47it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1279/3145 [08:07<11:51,  2.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1285/3145 [08:20<12:09,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1275/3145 [08:10<13:40,  2.28it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1286/3143 [08:07<12:15,  2.53it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1280/3145 [08:07<12:11,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1286/3145 [08:20<12:05,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1276/3145 [08:10<13:06,  2.37it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1287/3143 [08:08<11:43,  2.64it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1281/3145 [08:08<11:11,  2.78it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1287/3145 [08:21<12:33,  2.47it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1282/3145 [08:08<10:16,  3.02it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1277/3145 [08:11<13:29,  2.31it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1288/3143 [08:08<12:05,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1288/3145 [08:21<12:08,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1289/3143 [08:08<11:39,  2.65it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1283/3145 [08:08<11:25,  2.71it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1278/3145 [08:11<13:49,  2.25it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1289/3145 [08:21<12:08,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1290/3143 [08:09<11:56,  2.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1279/3145 [08:11<13:04,  2.38it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1284/3145 [08:09<12:06,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1290/3145 [08:22<12:56,  2.39it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1291/3143 [08:09<11:38,  2.65it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1280/3145 [08:12<12:47,  2.43it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1285/3145 [08:09<12:12,  2.54it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1291/3145 [08:22<12:37,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1292/3143 [08:10<11:44,  2.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1286/3145 [08:10<12:24,  2.50it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1281/3145 [08:12<13:11,  2.36it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1292/3145 [08:23<12:39,  2.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1293/3143 [08:10<12:04,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1287/3145 [08:10<12:39,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1282/3145 [08:13<13:23,  2.32it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1293/3145 [08:23<12:07,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1294/3143 [08:10<11:48,  2.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1288/3145 [08:10<12:25,  2.49it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1283/3145 [08:13<13:10,  2.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1294/3145 [08:23<12:22,  2.49it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1295/3143 [08:11<11:49,  2.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1284/3145 [08:13<12:43,  2.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1289/3145 [08:11<13:00,  2.38it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1295/3145 [08:24<12:03,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1296/3143 [08:11<11:47,  2.61it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1285/3145 [08:14<12:42,  2.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1290/3145 [08:11<12:54,  2.39it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1296/3145 [08:24<12:34,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1297/3143 [08:12<11:49,  2.60it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1286/3145 [08:14<12:17,  2.52it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1291/3145 [08:12<13:07,  2.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1298/3143 [08:12<11:50,  2.60it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1297/3145 [08:25<13:44,  2.24it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1287/3145 [08:15<12:15,  2.53it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1292/3145 [08:12<13:03,  2.37it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1299/3143 [08:12<11:58,  2.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1298/3145 [08:25<12:31,  2.46it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1288/3145 [08:15<12:08,  2.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1293/3145 [08:13<12:48,  2.41it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3143 [08:13<11:38,  2.64it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1299/3145 [08:26<12:21,  2.49it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1289/3145 [08:15<12:05,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1294/3145 [08:13<12:29,  2.47it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3145 [08:26<12:15,  2.51it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1301/3143 [08:13<12:46,  2.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1290/3145 [08:16<11:31,  2.68it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1295/3145 [08:13<12:00,  2.57it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1301/3145 [08:26<11:38,  2.64it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1302/3143 [08:14<12:08,  2.53it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1291/3145 [08:16<11:41,  2.64it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1296/3145 [08:14<11:44,  2.62it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1302/3145 [08:27<11:26,  2.68it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1303/3143 [08:14<11:48,  2.60it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1292/3145 [08:16<11:30,  2.68it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1304/3143 [08:14<10:23,  2.95it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1297/3145 [08:14<11:52,  2.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1303/3145 [08:27<11:10,  2.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1293/3145 [08:17<11:39,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1305/3143 [08:14<10:19,  2.97it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1298/3145 [08:14<11:40,  2.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1304/3145 [08:27<10:55,  2.81it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1294/3145 [08:17<11:28,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1306/3143 [08:15<10:46,  2.84it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1305/3145 [08:28<10:58,  2.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1299/3145 [08:15<12:14,  2.51it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1295/3145 [08:18<11:36,  2.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1307/3143 [08:15<11:08,  2.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1306/3145 [08:28<10:56,  2.80it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3145 [08:15<11:54,  2.58it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1296/3145 [08:18<11:26,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1308/3143 [08:15<10:11,  3.00it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1307/3145 [08:28<10:45,  2.85it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1301/3145 [08:16<11:27,  2.68it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1309/3143 [08:16<10:25,  2.93it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1302/3145 [08:16<10:12,  3.01it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1308/3145 [08:29<10:44,  2.85it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1297/3145 [08:18<12:23,  2.48it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1310/3143 [08:16<10:35,  2.88it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1303/3145 [08:16<10:13,  3.00it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1309/3145 [08:29<11:06,  2.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1298/3145 [08:19<13:25,  2.29it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1311/3143 [08:17<10:57,  2.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1304/3145 [08:17<10:44,  2.86it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1310/3145 [08:29<11:22,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1312/3143 [08:17<10:43,  2.85it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1299/3145 [08:19<14:29,  2.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1305/3145 [08:17<11:45,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1311/3145 [08:30<11:34,  2.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1313/3143 [08:17<09:38,  3.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3145 [08:20<13:30,  2.28it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1306/3145 [08:17<11:19,  2.71it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1312/3145 [08:30<11:34,  2.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1314/3143 [08:18<09:58,  3.05it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1301/3145 [08:20<12:47,  2.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1307/3145 [08:18<11:07,  2.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1315/3143 [08:18<10:31,  2.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1313/3145 [08:31<11:55,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1302/3145 [08:21<12:08,  2.53it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1308/3145 [08:18<11:44,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3143 [08:18<10:56,  2.78it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1314/3145 [08:31<11:55,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1303/3145 [08:21<12:02,  2.55it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1309/3145 [08:19<11:32,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1317/3143 [08:19<11:12,  2.71it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1315/3145 [08:31<11:54,  2.56it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1304/3145 [08:21<13:04,  2.35it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1310/3145 [08:19<12:15,  2.49it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3145 [08:32<11:22,  2.68it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1318/3143 [08:19<11:04,  2.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1305/3145 [08:22<12:33,  2.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1317/3145 [08:32<11:20,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1319/3143 [08:19<11:10,  2.72it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1311/3145 [08:19<12:46,  2.39it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1318/3145 [08:32<09:52,  3.08it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1306/3145 [08:22<12:08,  2.53it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1320/3143 [08:20<11:22,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1312/3145 [08:20<12:14,  2.50it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1319/3145 [08:33<10:22,  2.93it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1307/3145 [08:23<12:07,  2.53it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1313/3145 [08:20<11:39,  2.62it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1321/3143 [08:20<12:02,  2.52it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1320/3145 [08:33<11:01,  2.76it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1308/3145 [08:23<11:43,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1314/3145 [08:20<11:31,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1322/3143 [08:21<11:39,  2.60it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1309/3145 [08:23<11:23,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1321/3145 [08:34<11:29,  2.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1315/3145 [08:21<11:42,  2.60it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1323/3143 [08:21<11:32,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1310/3145 [08:24<11:20,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1322/3145 [08:34<11:32,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3145 [08:21<11:13,  2.72it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1324/3143 [08:21<11:32,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1317/3145 [08:21<09:48,  3.10it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1311/3145 [08:24<11:22,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1323/3145 [08:34<12:11,  2.49it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1325/3143 [08:22<11:19,  2.68it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1318/3145 [08:22<10:02,  3.03it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1312/3145 [08:24<11:19,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1324/3145 [08:35<11:49,  2.57it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1326/3143 [08:22<11:13,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1319/3145 [08:22<10:55,  2.79it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1313/3145 [08:25<11:18,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1325/3145 [08:35<11:45,  2.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1327/3143 [08:22<11:05,  2.73it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1314/3145 [08:25<10:58,  2.78it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1320/3145 [08:23<11:18,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1326/3145 [08:35<11:20,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1328/3143 [08:23<10:51,  2.79it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1315/3145 [08:26<11:19,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1321/3145 [08:23<11:16,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1327/3145 [08:36<11:16,  2.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1329/3143 [08:23<10:45,  2.81it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1322/3145 [08:23<11:23,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3145 [08:26<11:36,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1328/3145 [08:36<11:07,  2.72it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1330/3143 [08:23<10:32,  2.86it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1317/3145 [08:26<10:21,  2.94it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1329/3145 [08:37<11:03,  2.74it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1331/3143 [08:24<10:58,  2.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1323/3145 [08:24<12:11,  2.49it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1318/3145 [08:26<10:19,  2.95it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1330/3145 [08:37<10:43,  2.82it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1324/3145 [08:24<11:55,  2.54it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1332/3143 [08:24<11:23,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1319/3145 [08:27<10:17,  2.96it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1331/3145 [08:37<10:30,  2.88it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1325/3145 [08:25<11:39,  2.60it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1333/3143 [08:25<11:11,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1320/3145 [08:27<11:00,  2.76it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1332/3145 [08:38<10:19,  2.93it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1326/3145 [08:25<11:30,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1334/3143 [08:25<11:42,  2.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1321/3145 [08:28<10:51,  2.80it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1333/3145 [08:38<10:52,  2.78it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1327/3145 [08:25<11:09,  2.72it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1335/3143 [08:25<11:26,  2.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1322/3145 [08:28<10:50,  2.80it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1334/3145 [08:38<11:09,  2.71it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1328/3145 [08:26<11:11,  2.71it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1336/3143 [08:26<11:31,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1323/3145 [08:28<10:50,  2.80it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1335/3145 [08:39<11:53,  2.54it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1329/3145 [08:26<11:46,  2.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1337/3143 [08:26<11:31,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1324/3145 [08:29<11:29,  2.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1330/3145 [08:26<10:18,  2.93it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1336/3145 [08:39<11:31,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1338/3143 [08:27<11:44,  2.56it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1325/3145 [08:29<11:31,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1331/3145 [08:27<10:46,  2.81it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1337/3145 [08:40<11:34,  2.60it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1339/3143 [08:27<11:58,  2.51it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1326/3145 [08:30<11:52,  2.55it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1332/3145 [08:27<10:34,  2.86it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1338/3145 [08:40<11:33,  2.61it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1327/3145 [08:30<11:19,  2.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1340/3143 [08:27<11:47,  2.55it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1333/3145 [08:27<10:42,  2.82it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1339/3145 [08:40<11:00,  2.74it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1328/3145 [08:30<11:24,  2.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1341/3143 [08:28<11:37,  2.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1334/3145 [08:28<10:51,  2.78it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1340/3145 [08:41<10:57,  2.75it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1342/3143 [08:28<11:13,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1329/3145 [08:31<11:42,  2.59it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1335/3145 [08:28<11:09,  2.71it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1341/3145 [08:41<11:08,  2.70it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1343/3143 [08:28<11:20,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1330/3145 [08:31<11:21,  2.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1336/3145 [08:29<10:56,  2.76it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1342/3145 [08:41<11:00,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1344/3143 [08:29<11:10,  2.68it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1331/3145 [08:31<11:12,  2.70it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1337/3145 [08:29<10:45,  2.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1343/3145 [08:42<11:02,  2.72it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1345/3143 [08:29<11:32,  2.60it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1338/3145 [08:29<11:01,  2.73it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1332/3145 [08:32<11:41,  2.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1344/3145 [08:42<11:29,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1346/3143 [08:30<11:15,  2.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1333/3145 [08:32<11:28,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1339/3145 [08:30<11:14,  2.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1345/3145 [08:43<11:11,  2.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1347/3143 [08:30<11:20,  2.64it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1340/3145 [08:30<11:26,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1346/3145 [08:43<10:57,  2.74it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1334/3145 [08:33<11:54,  2.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1348/3143 [08:30<11:38,  2.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1347/3145 [08:43<10:57,  2.73it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1335/3145 [08:33<11:53,  2.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1341/3145 [08:30<11:47,  2.55it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1349/3143 [08:31<11:17,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1336/3145 [08:33<11:33,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1348/3145 [08:44<11:13,  2.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1342/3145 [08:31<11:53,  2.53it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1350/3143 [08:31<10:56,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1337/3145 [08:34<11:24,  2.64it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1349/3145 [08:44<11:13,  2.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1343/3145 [08:31<12:06,  2.48it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1351/3143 [08:31<10:55,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1338/3145 [08:34<11:26,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1350/3145 [08:44<11:41,  2.56it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1344/3145 [08:32<11:59,  2.50it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1352/3143 [08:32<10:35,  2.82it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1339/3145 [08:34<11:31,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1351/3145 [08:45<11:23,  2.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1345/3145 [08:32<11:56,  2.51it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1353/3143 [08:32<10:37,  2.81it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1352/3145 [08:45<11:23,  2.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1346/3145 [08:32<11:28,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1340/3145 [08:35<12:32,  2.40it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1354/3143 [08:32<10:24,  2.87it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1347/3145 [08:33<10:12,  2.94it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1353/3145 [08:46<11:14,  2.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1341/3145 [08:35<12:06,  2.48it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1355/3143 [08:33<10:43,  2.78it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1348/3145 [08:33<09:11,  3.26it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1354/3145 [08:46<11:05,  2.69it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1342/3145 [08:36<11:44,  2.56it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1356/3143 [08:33<10:47,  2.76it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1349/3145 [08:33<09:55,  3.01it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1355/3145 [08:46<11:01,  2.70it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1343/3145 [08:36<11:15,  2.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1357/3143 [08:34<10:34,  2.81it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1350/3145 [08:34<10:15,  2.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1356/3145 [08:47<11:25,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1344/3145 [08:36<11:11,  2.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1358/3143 [08:34<11:08,  2.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1351/3145 [08:34<10:32,  2.84it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1357/3145 [08:47<11:12,  2.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1345/3145 [08:37<11:05,  2.70it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1359/3143 [08:34<11:18,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1352/3145 [08:34<11:15,  2.65it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1346/3145 [08:37<11:02,  2.72it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1358/3145 [08:47<11:42,  2.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1360/3143 [08:35<10:32,  2.82it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1353/3145 [08:35<11:22,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1359/3145 [08:48<11:18,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1347/3145 [08:38<11:28,  2.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1361/3143 [08:35<10:53,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1354/3145 [08:35<11:24,  2.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1362/3143 [08:35<09:35,  3.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1360/3145 [08:48<11:19,  2.63it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1348/3145 [08:38<11:32,  2.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1355/3145 [08:36<11:45,  2.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1363/3143 [08:36<10:10,  2.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1361/3145 [08:49<11:10,  2.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1349/3145 [08:38<11:33,  2.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1356/3145 [08:36<11:41,  2.55it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1364/3143 [08:36<10:35,  2.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1362/3145 [08:49<10:47,  2.75it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1350/3145 [08:39<11:37,  2.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1365/3143 [08:36<10:36,  2.79it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1357/3145 [08:36<11:42,  2.55it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1363/3145 [08:49<10:48,  2.75it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1351/3145 [08:39<11:36,  2.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1366/3143 [08:37<10:51,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1364/3145 [08:50<10:48,  2.75it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1358/3145 [08:37<11:41,  2.55it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1352/3145 [08:40<11:35,  2.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1365/3145 [08:50<09:52,  3.01it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1359/3145 [08:37<11:29,  2.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1367/3143 [08:37<11:23,  2.60it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1353/3145 [08:40<11:19,  2.64it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1366/3145 [08:50<10:02,  2.95it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1360/3145 [08:38<10:57,  2.71it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1368/3143 [08:38<11:07,  2.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1354/3145 [08:40<11:05,  2.69it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1361/3145 [08:38<10:00,  2.97it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1367/3145 [08:51<10:13,  2.90it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1355/3145 [08:40<09:54,  3.01it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1369/3143 [08:38<11:02,  2.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1362/3145 [08:38<10:16,  2.89it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1368/3145 [08:51<10:35,  2.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1356/3145 [08:41<09:58,  2.99it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1370/3143 [08:38<10:50,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1363/3145 [08:39<10:23,  2.86it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1369/3145 [08:51<10:34,  2.80it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1371/3143 [08:39<10:29,  2.82it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1357/3145 [08:41<10:11,  2.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1364/3145 [08:39<10:14,  2.90it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1370/3145 [08:52<10:35,  2.79it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1358/3145 [08:41<10:09,  2.93it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1372/3143 [08:39<10:40,  2.77it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1365/3145 [08:39<10:29,  2.83it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1371/3145 [08:52<10:50,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1359/3145 [08:42<09:54,  3.01it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1373/3143 [08:39<11:14,  2.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1366/3145 [08:40<10:33,  2.81it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1372/3145 [08:52<10:44,  2.75it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1360/3145 [08:42<10:10,  2.92it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1374/3143 [08:40<10:48,  2.73it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1373/3145 [08:53<10:25,  2.83it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1367/3145 [08:40<10:55,  2.71it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1361/3145 [08:43<10:53,  2.73it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1375/3143 [08:40<11:13,  2.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1374/3145 [08:53<10:27,  2.82it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1368/3145 [08:40<11:09,  2.65it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1376/3143 [08:41<10:46,  2.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1362/3145 [08:43<11:35,  2.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1375/3145 [08:53<10:29,  2.81it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1369/3145 [08:41<11:00,  2.69it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1363/3145 [08:43<11:24,  2.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1377/3143 [08:41<10:50,  2.71it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1376/3145 [08:54<10:28,  2.81it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1370/3145 [08:41<11:06,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1378/3143 [08:41<11:01,  2.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1364/3145 [08:44<11:33,  2.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1377/3145 [08:54<10:48,  2.73it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1379/3143 [08:42<09:39,  3.04it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1371/3145 [08:42<11:18,  2.62it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1365/3145 [08:44<11:24,  2.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1378/3145 [08:55<10:36,  2.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1380/3143 [08:42<09:48,  3.00it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1372/3145 [08:42<11:22,  2.60it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1366/3145 [08:45<11:42,  2.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1379/3145 [08:55<10:49,  2.72it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1381/3143 [08:42<10:06,  2.91it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1373/3145 [08:42<11:23,  2.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1367/3145 [08:45<11:55,  2.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1380/3145 [08:55<10:45,  2.74it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1382/3143 [08:43<10:13,  2.87it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1374/3145 [08:43<11:44,  2.51it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1368/3145 [08:45<12:03,  2.46it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1381/3145 [08:56<11:34,  2.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1383/3143 [08:43<10:52,  2.70it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1375/3145 [08:43<11:12,  2.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1382/3145 [08:56<11:11,  2.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1384/3143 [08:43<10:42,  2.74it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1369/3145 [08:46<12:53,  2.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1376/3145 [08:43<11:04,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1385/3143 [08:44<10:39,  2.75it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1383/3145 [08:57<11:33,  2.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1377/3145 [08:44<10:58,  2.68it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1370/3145 [08:46<12:21,  2.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1386/3143 [08:44<10:38,  2.75it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1371/3145 [08:47<11:37,  2.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1384/3145 [08:57<11:46,  2.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1378/3145 [08:44<11:06,  2.65it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1387/3143 [08:44<10:10,  2.88it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1372/3145 [08:47<11:34,  2.55it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1385/3145 [08:57<11:37,  2.52it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1379/3145 [08:45<11:20,  2.59it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1388/3143 [08:45<10:14,  2.86it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1373/3145 [08:47<11:16,  2.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1386/3145 [08:58<11:30,  2.55it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1380/3145 [08:45<11:06,  2.65it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1389/3143 [08:45<10:31,  2.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1374/3145 [08:48<11:18,  2.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1387/3145 [08:58<11:13,  2.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1381/3145 [08:45<11:07,  2.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1390/3143 [08:46<11:50,  2.47it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1375/3145 [08:48<11:25,  2.58it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1388/3145 [08:58<11:07,  2.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1382/3145 [08:46<11:03,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1389/3145 [08:59<09:41,  3.02it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1391/3143 [08:46<11:39,  2.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1376/3145 [08:49<11:27,  2.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1383/3145 [08:46<10:57,  2.68it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1390/3145 [08:59<10:09,  2.88it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1392/3143 [08:46<11:20,  2.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1377/3145 [08:49<11:16,  2.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1384/3145 [08:46<10:53,  2.70it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1391/3145 [08:59<10:31,  2.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1378/3145 [08:49<11:20,  2.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1393/3143 [08:47<11:47,  2.47it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1385/3145 [08:47<11:07,  2.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1392/3145 [09:00<10:16,  2.84it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1394/3143 [08:47<10:25,  2.79it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1386/3145 [08:47<11:19,  2.59it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1379/3145 [08:50<12:28,  2.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1393/3145 [09:00<10:19,  2.83it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1395/3143 [08:47<10:12,  2.85it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1387/3145 [08:48<11:18,  2.59it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1380/3145 [08:50<12:10,  2.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1396/3143 [08:48<10:08,  2.87it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1394/3145 [09:01<10:46,  2.71it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1388/3145 [08:48<11:02,  2.65it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1381/3145 [08:51<11:54,  2.47it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1395/3145 [09:01<10:54,  2.67it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1397/3143 [08:48<10:43,  2.71it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1389/3145 [08:48<11:14,  2.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1396/3145 [09:01<10:31,  2.77it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1382/3145 [08:51<11:52,  2.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1398/3143 [08:49<10:50,  2.68it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1390/3145 [08:49<11:00,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1397/3145 [09:02<10:00,  2.91it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1383/3145 [08:51<11:49,  2.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1399/3143 [08:49<10:59,  2.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1398/3145 [09:02<10:34,  2.75it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1391/3145 [08:49<12:08,  2.41it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1400/3143 [08:49<10:48,  2.69it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1384/3145 [08:52<11:48,  2.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1399/3145 [09:02<11:02,  2.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1401/3143 [08:50<10:54,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1385/3145 [08:52<11:49,  2.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1392/3145 [08:50<12:56,  2.26it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1400/3145 [09:03<10:47,  2.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1402/3143 [08:50<10:59,  2.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1393/3145 [08:50<12:13,  2.39it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1386/3145 [08:53<11:58,  2.45it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1401/3145 [09:03<10:27,  2.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1403/3143 [08:50<10:54,  2.66it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1394/3145 [08:50<11:34,  2.52it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1387/3145 [08:53<12:35,  2.33it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1402/3145 [09:03<10:48,  2.69it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1395/3145 [08:51<11:16,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1404/3143 [08:51<11:56,  2.43it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1388/3145 [08:53<11:48,  2.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1403/3145 [09:04<10:27,  2.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1396/3145 [08:51<11:14,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1404/3145 [09:04<09:42,  2.99it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1405/3143 [08:51<11:42,  2.47it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1389/3145 [08:54<11:44,  2.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1405/3145 [09:04<08:55,  3.25it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1397/3145 [08:52<11:18,  2.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1406/3143 [08:52<11:32,  2.51it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1390/3145 [08:54<11:27,  2.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1406/3145 [09:05<09:20,  3.10it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1398/3145 [08:52<10:42,  2.72it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1407/3143 [08:52<11:01,  2.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1391/3145 [08:55<11:24,  2.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1399/3145 [08:52<10:28,  2.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1407/3145 [09:05<09:46,  2.97it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1408/3143 [08:52<11:02,  2.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1392/3145 [08:55<11:11,  2.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1408/3145 [09:05<09:54,  2.92it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1400/3145 [08:53<11:20,  2.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1393/3145 [08:55<10:56,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1409/3143 [08:53<11:20,  2.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1409/3145 [09:06<10:44,  2.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1401/3145 [08:53<11:41,  2.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1410/3143 [08:53<11:08,  2.59it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1394/3145 [08:56<11:03,  2.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1410/3145 [09:06<10:48,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1411/3143 [08:54<10:41,  2.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1402/3145 [08:54<11:51,  2.45it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1395/3145 [08:56<11:15,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1411/3145 [09:07<09:52,  2.93it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1403/3145 [08:54<11:26,  2.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1412/3143 [08:54<11:05,  2.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1396/3145 [08:57<11:14,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1412/3145 [09:07<10:02,  2.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1413/3143 [08:54<10:40,  2.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1404/3145 [08:54<11:27,  2.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1397/3145 [08:57<10:51,  2.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1413/3145 [09:07<10:24,  2.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1414/3143 [08:55<10:31,  2.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1405/3145 [08:55<11:06,  2.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1398/3145 [08:57<10:42,  2.72it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1414/3145 [09:08<10:13,  2.82it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1415/3143 [08:55<10:07,  2.85it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1399/3145 [08:58<10:14,  2.84it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1406/3145 [08:55<10:51,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1415/3145 [09:08<10:00,  2.88it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1416/3143 [08:55<10:25,  2.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1407/3145 [08:55<10:35,  2.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1400/3145 [08:58<10:38,  2.73it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1416/3145 [09:08<10:18,  2.80it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1417/3143 [08:56<10:39,  2.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1401/3145 [08:58<10:35,  2.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1408/3145 [08:56<11:08,  2.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1417/3145 [09:09<10:11,  2.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1418/3143 [08:56<10:32,  2.73it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1402/3145 [08:59<10:54,  2.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1409/3145 [08:56<11:27,  2.52it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1418/3145 [09:09<10:32,  2.73it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1419/3143 [08:56<10:27,  2.75it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1403/3145 [08:59<10:32,  2.75it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1419/3145 [09:09<10:25,  2.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1410/3145 [08:57<11:22,  2.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1420/3143 [08:57<10:24,  2.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1404/3145 [08:59<10:18,  2.82it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1420/3145 [09:10<10:01,  2.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1411/3145 [08:57<11:00,  2.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1421/3143 [08:57<09:10,  3.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1405/3145 [09:00<10:35,  2.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1421/3145 [09:10<10:07,  2.84it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1412/3145 [08:57<11:07,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1422/3143 [08:57<09:33,  3.00it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1406/3145 [09:00<10:47,  2.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1422/3145 [09:10<10:09,  2.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1423/3143 [08:58<09:32,  3.00it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1413/3145 [08:58<11:13,  2.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1423/3145 [09:11<09:57,  2.88it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1407/3145 [09:01<11:22,  2.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1424/3143 [08:58<09:39,  2.97it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1414/3145 [08:58<11:16,  2.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1424/3145 [09:11<10:18,  2.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1425/3143 [08:58<09:39,  2.97it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1408/3145 [09:01<11:20,  2.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1415/3145 [08:59<11:01,  2.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1425/3145 [09:12<10:13,  2.80it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1409/3145 [09:01<10:46,  2.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1426/3143 [08:59<10:12,  2.80it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1416/3145 [08:59<11:21,  2.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1426/3145 [09:12<10:26,  2.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1427/3143 [08:59<09:53,  2.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1410/3145 [09:02<11:14,  2.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1417/3145 [08:59<11:25,  2.52it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1427/3145 [09:12<10:09,  2.82it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1428/3143 [09:00<10:01,  2.85it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1411/3145 [09:02<11:30,  2.51it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1418/3145 [09:00<10:47,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1428/3145 [09:13<09:51,  2.90it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1429/3143 [09:00<09:40,  2.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1419/3145 [09:00<11:04,  2.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1412/3145 [09:03<11:57,  2.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1430/3143 [09:00<09:17,  3.07it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1429/3145 [09:13<10:06,  2.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1431/3143 [09:00<09:00,  3.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1413/3145 [09:03<11:51,  2.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1420/3145 [09:01<11:29,  2.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1430/3145 [09:13<10:27,  2.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1432/3143 [09:01<09:21,  3.05it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1414/3145 [09:03<11:54,  2.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1421/3145 [09:01<11:37,  2.47it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1431/3145 [09:14<10:58,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1433/3143 [09:01<09:55,  2.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1422/3145 [09:01<11:12,  2.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1415/3145 [09:04<11:43,  2.46it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1432/3145 [09:14<10:59,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1434/3143 [09:02<09:59,  2.85it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1416/3145 [09:04<11:21,  2.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1423/3145 [09:02<11:04,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1433/3145 [09:15<11:01,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1435/3143 [09:02<10:52,  2.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1417/3145 [09:05<11:05,  2.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1424/3145 [09:02<10:57,  2.62it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1434/3145 [09:15<10:43,  2.66it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1436/3143 [09:02<10:53,  2.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1425/3145 [09:02<11:11,  2.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1418/3145 [09:05<11:25,  2.52it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1435/3145 [09:15<10:47,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1437/3143 [09:03<10:40,  2.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1419/3145 [09:05<11:12,  2.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1426/3145 [09:03<11:06,  2.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1436/3145 [09:16<10:48,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1438/3143 [09:03<11:05,  2.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1420/3145 [09:06<10:39,  2.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1427/3145 [09:03<11:01,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1437/3145 [09:16<10:55,  2.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1421/3145 [09:06<10:45,  2.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1439/3143 [09:04<11:19,  2.51it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1428/3145 [09:04<10:53,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1438/3145 [09:16<11:01,  2.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1422/3145 [09:06<10:48,  2.66it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1440/3143 [09:04<10:57,  2.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1429/3145 [09:04<10:47,  2.65it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1439/3145 [09:17<10:41,  2.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1423/3145 [09:07<10:51,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1441/3143 [09:04<11:04,  2.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1430/3145 [09:04<10:53,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1440/3145 [09:17<11:34,  2.45it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1431/3145 [09:05<10:42,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1424/3145 [09:07<11:11,  2.56it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1441/3145 [09:18<11:06,  2.56it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1442/3143 [09:05<11:59,  2.37it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1432/3145 [09:05<10:14,  2.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1443/3143 [09:05<10:30,  2.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1425/3145 [09:08<11:48,  2.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1442/3145 [09:18<11:03,  2.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1433/3145 [09:05<09:28,  3.01it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1444/3143 [09:05<10:37,  2.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1426/3145 [09:08<11:44,  2.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1443/3145 [09:18<11:17,  2.51it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1434/3145 [09:06<09:44,  2.93it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1445/3143 [09:06<11:16,  2.51it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1427/3145 [09:08<11:07,  2.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1444/3145 [09:19<11:09,  2.54it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1435/3145 [09:06<10:07,  2.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1428/3145 [09:09<09:42,  2.95it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1446/3143 [09:06<10:46,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1445/3145 [09:19<11:19,  2.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1436/3145 [09:06<10:23,  2.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1447/3143 [09:07<09:35,  2.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1429/3145 [09:09<09:51,  2.90it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1446/3145 [09:20<11:09,  2.54it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1437/3145 [09:07<10:38,  2.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1448/3143 [09:07<09:42,  2.91it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1430/3145 [09:09<10:47,  2.65it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1447/3145 [09:20<10:53,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1438/3145 [09:07<10:24,  2.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1449/3143 [09:07<10:11,  2.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1431/3145 [09:10<11:13,  2.55it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1450/3143 [09:07<09:04,  3.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1448/3145 [09:20<10:51,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1439/3145 [09:08<10:39,  2.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1432/3145 [09:10<10:57,  2.61it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1451/3143 [09:08<09:40,  2.92it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1440/3145 [09:08<10:35,  2.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1449/3145 [09:21<10:53,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1433/3145 [09:11<10:48,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1452/3143 [09:08<09:48,  2.87it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1441/3145 [09:08<10:16,  2.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1450/3145 [09:21<10:57,  2.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1434/3145 [09:11<10:52,  2.62it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1453/3143 [09:09<09:51,  2.86it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1442/3145 [09:09<10:28,  2.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1451/3145 [09:22<10:54,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1435/3145 [09:11<10:49,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1454/3143 [09:09<09:51,  2.86it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1443/3145 [09:09<10:08,  2.80it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1452/3145 [09:22<10:51,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1436/3145 [09:12<10:25,  2.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1455/3143 [09:09<10:12,  2.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1444/3145 [09:09<10:21,  2.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1453/3145 [09:22<10:23,  2.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1437/3145 [09:12<10:59,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1456/3143 [09:10<10:07,  2.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1454/3145 [09:23<10:16,  2.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1445/3145 [09:10<11:08,  2.54it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1457/3143 [09:10<10:05,  2.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1438/3145 [09:13<11:43,  2.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1455/3145 [09:23<10:11,  2.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1446/3145 [09:10<11:07,  2.55it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1458/3143 [09:10<10:39,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1456/3145 [09:23<10:39,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1447/3145 [09:11<10:53,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1439/3145 [09:13<12:27,  2.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1459/3143 [09:11<10:31,  2.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1457/3145 [09:24<10:38,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1448/3145 [09:11<10:38,  2.66it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1440/3145 [09:14<12:37,  2.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1460/3143 [09:11<10:08,  2.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1458/3145 [09:24<10:28,  2.69it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1449/3145 [09:11<10:29,  2.69it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1461/3143 [09:11<09:48,  2.86it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1441/3145 [09:14<12:44,  2.23it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1450/3145 [09:12<10:19,  2.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1459/3145 [09:25<11:07,  2.53it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1462/3143 [09:12<09:53,  2.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1442/3145 [09:14<12:18,  2.31it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1451/3145 [09:12<10:02,  2.81it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1460/3145 [09:25<10:59,  2.55it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1463/3143 [09:12<10:32,  2.66it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1452/3145 [09:12<10:06,  2.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1443/3145 [09:15<12:30,  2.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1461/3145 [09:25<10:54,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1464/3143 [09:13<10:21,  2.70it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1453/3145 [09:13<10:06,  2.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1444/3145 [09:15<12:42,  2.23it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1462/3145 [09:26<10:42,  2.62it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1465/3143 [09:13<10:28,  2.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1454/3145 [09:13<10:07,  2.78it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1463/3145 [09:26<10:50,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1445/3145 [09:16<12:34,  2.25it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1466/3143 [09:13<10:17,  2.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1455/3145 [09:13<09:55,  2.84it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1446/3145 [09:16<11:48,  2.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1464/3145 [09:26<11:03,  2.53it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1467/3143 [09:14<10:22,  2.69it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1456/3145 [09:14<09:49,  2.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1465/3145 [09:27<10:48,  2.59it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1468/3143 [09:14<10:08,  2.75it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1457/3145 [09:14<09:48,  2.87it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1447/3145 [09:17<12:50,  2.20it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1469/3143 [09:14<10:05,  2.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1458/3145 [09:14<10:11,  2.76it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1466/3145 [09:27<11:28,  2.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1448/3145 [09:17<12:30,  2.26it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1470/3143 [09:15<10:16,  2.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1459/3145 [09:15<10:14,  2.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1449/3145 [09:18<12:03,  2.34it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1467/3145 [09:28<12:14,  2.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1471/3143 [09:15<10:24,  2.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1460/3145 [09:15<10:44,  2.61it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1450/3145 [09:18<11:28,  2.46it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1468/3145 [09:28<13:04,  2.14it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1472/3143 [09:16<10:13,  2.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1461/3145 [09:16<10:21,  2.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1451/3145 [09:18<11:18,  2.50it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1473/3143 [09:16<10:24,  2.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1462/3145 [09:16<10:21,  2.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1452/3145 [09:19<10:57,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1469/3145 [09:29<13:46,  2.03it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1474/3143 [09:16<10:29,  2.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1463/3145 [09:16<10:27,  2.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1453/3145 [09:19<10:46,  2.62it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1470/3145 [09:29<13:50,  2.02it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1475/3143 [09:17<10:20,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1464/3145 [09:17<10:37,  2.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1454/3145 [09:19<11:05,  2.54it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1471/3145 [09:30<12:53,  2.16it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1476/3143 [09:17<10:06,  2.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1465/3145 [09:17<10:31,  2.66it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1455/3145 [09:20<10:35,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1477/3143 [09:17<10:03,  2.76it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1472/3145 [09:30<12:31,  2.23it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1466/3145 [09:17<10:27,  2.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1456/3145 [09:20<10:26,  2.70it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1478/3143 [09:18<09:56,  2.79it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1473/3145 [09:31<11:52,  2.35it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1467/3145 [09:18<10:36,  2.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1457/3145 [09:20<10:34,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1474/3145 [09:31<11:30,  2.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1479/3143 [09:18<10:25,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1468/3145 [09:18<10:54,  2.56it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1458/3145 [09:21<10:39,  2.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1475/3145 [09:31<10:55,  2.55it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1480/3143 [09:19<10:49,  2.56it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1469/3145 [09:19<10:45,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1459/3145 [09:21<10:46,  2.61it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1476/3145 [09:32<10:30,  2.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1481/3143 [09:19<10:48,  2.56it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1470/3145 [09:19<10:46,  2.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1460/3145 [09:22<10:34,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1477/3145 [09:32<10:13,  2.72it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1482/3143 [09:19<10:46,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1471/3145 [09:19<10:44,  2.60it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1461/3145 [09:22<10:26,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1478/3145 [09:32<10:10,  2.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1462/3145 [09:22<09:14,  3.04it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1483/3143 [09:20<10:40,  2.59it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1479/3145 [09:33<09:06,  3.05it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1472/3145 [09:20<10:43,  2.60it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1484/3143 [09:20<10:26,  2.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1463/3145 [09:23<10:06,  2.77it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1480/3145 [09:33<09:30,  2.92it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1473/3145 [09:20<10:51,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1485/3143 [09:20<09:14,  2.99it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1464/3145 [09:23<10:50,  2.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1481/3145 [09:33<10:08,  2.73it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1474/3145 [09:21<10:35,  2.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1486/3143 [09:21<09:24,  2.94it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1465/3145 [09:23<10:24,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1482/3145 [09:34<09:51,  2.81it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1475/3145 [09:21<10:47,  2.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1487/3143 [09:21<09:28,  2.91it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1488/3143 [09:21<08:26,  3.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1466/3145 [09:24<10:33,  2.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1483/3145 [09:34<10:07,  2.74it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1476/3145 [09:21<10:23,  2.68it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1489/3143 [09:22<08:51,  3.11it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1467/3145 [09:24<10:30,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1484/3145 [09:34<10:10,  2.72it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1477/3145 [09:22<10:28,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1490/3143 [09:22<09:08,  3.01it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1468/3145 [09:25<10:25,  2.68it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1485/3145 [09:35<10:08,  2.73it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1478/3145 [09:22<10:32,  2.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1491/3143 [09:22<09:24,  2.93it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1469/3145 [09:25<10:22,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1486/3145 [09:35<10:38,  2.60it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1479/3145 [09:22<10:23,  2.67it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1492/3143 [09:23<09:46,  2.81it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1480/3145 [09:23<10:11,  2.72it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1470/3145 [09:25<10:51,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1487/3145 [09:36<11:08,  2.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1493/3143 [09:23<10:01,  2.74it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1481/3145 [09:23<10:00,  2.77it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1471/3145 [09:26<10:52,  2.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1488/3145 [09:36<10:44,  2.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1494/3143 [09:24<10:09,  2.70it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1472/3145 [09:26<10:36,  2.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1482/3145 [09:24<10:36,  2.61it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1489/3145 [09:36<10:50,  2.55it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1495/3143 [09:24<10:19,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1473/3145 [09:26<10:12,  2.73it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1483/3145 [09:24<10:29,  2.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1490/3145 [09:37<11:00,  2.51it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1496/3143 [09:24<09:45,  2.81it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1474/3145 [09:27<10:10,  2.74it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1484/3145 [09:24<10:22,  2.67it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1491/3145 [09:37<10:56,  2.52it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1497/3143 [09:25<10:02,  2.73it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1475/3145 [09:27<10:08,  2.74it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1485/3145 [09:25<10:29,  2.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1492/3145 [09:38<10:32,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1498/3143 [09:25<10:15,  2.67it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1476/3145 [09:28<10:06,  2.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1486/3145 [09:25<10:11,  2.71it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1493/3145 [09:38<10:33,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1499/3143 [09:25<09:58,  2.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1477/3145 [09:28<10:21,  2.68it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1487/3145 [09:25<10:07,  2.73it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1494/3145 [09:38<10:47,  2.55it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3143 [09:26<10:06,  2.71it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1478/3145 [09:28<10:05,  2.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1488/3145 [09:26<10:20,  2.67it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1495/3145 [09:39<10:44,  2.56it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1501/3143 [09:26<10:14,  2.67it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1479/3145 [09:29<10:23,  2.67it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1489/3145 [09:26<10:14,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1496/3145 [09:39<09:59,  2.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1480/3145 [09:29<09:27,  2.94it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1502/3143 [09:26<10:10,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1490/3145 [09:27<10:39,  2.59it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1497/3145 [09:39<09:54,  2.77it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1481/3145 [09:29<09:39,  2.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1503/3143 [09:27<11:28,  2.38it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1498/3145 [09:40<09:37,  2.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1491/3145 [09:27<10:54,  2.53it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1482/3145 [09:30<09:42,  2.85it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1499/3145 [09:40<09:44,  2.81it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1492/3145 [09:27<10:33,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1504/3143 [09:27<12:03,  2.27it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1483/3145 [09:30<09:51,  2.81it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1493/3145 [09:28<10:19,  2.67it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3145 [09:41<10:13,  2.68it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1484/3145 [09:30<09:44,  2.84it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1505/3143 [09:28<12:47,  2.13it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1485/3145 [09:31<08:47,  3.15it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1494/3145 [09:28<10:11,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1501/3145 [09:41<09:58,  2.75it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1506/3143 [09:28<11:41,  2.34it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1486/3145 [09:31<09:06,  3.03it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1502/3145 [09:41<09:58,  2.75it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1495/3145 [09:28<10:19,  2.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1507/3143 [09:29<11:20,  2.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1487/3145 [09:31<09:31,  2.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1503/3145 [09:42<09:56,  2.75it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1496/3145 [09:29<10:32,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1508/3143 [09:29<10:53,  2.50it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1488/3145 [09:32<09:38,  2.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1504/3145 [09:42<09:48,  2.79it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1497/3145 [09:29<10:11,  2.69it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1509/3143 [09:29<10:20,  2.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1505/3145 [09:42<09:34,  2.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1489/3145 [09:32<10:24,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1498/3145 [09:30<10:20,  2.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1510/3143 [09:30<10:37,  2.56it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1506/3145 [09:43<09:38,  2.83it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1490/3145 [09:33<10:17,  2.68it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1499/3145 [09:30<10:37,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1511/3143 [09:30<11:09,  2.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1507/3145 [09:43<10:14,  2.66it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1491/3145 [09:33<10:12,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3145 [09:30<10:38,  2.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1512/3143 [09:31<10:30,  2.59it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1508/3145 [09:43<10:08,  2.69it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1501/3145 [09:31<10:11,  2.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1492/3145 [09:33<10:34,  2.60it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1513/3143 [09:31<10:16,  2.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1493/3145 [09:34<10:11,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1502/3145 [09:31<10:06,  2.71it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1509/3145 [09:44<11:05,  2.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1514/3143 [09:31<10:19,  2.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1494/3145 [09:34<09:46,  2.82it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1510/3145 [09:44<10:17,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1503/3145 [09:32<10:21,  2.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1495/3145 [09:34<09:38,  2.85it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1515/3143 [09:32<10:22,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1511/3145 [09:45<10:13,  2.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1504/3145 [09:32<10:43,  2.55it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1516/3143 [09:32<10:28,  2.59it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1505/3145 [09:32<09:23,  2.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1512/3145 [09:45<10:04,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1496/3145 [09:35<10:29,  2.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1497/3145 [09:35<09:59,  2.75it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1506/3145 [09:33<09:32,  2.86it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1517/3143 [09:33<10:34,  2.56it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1513/3145 [09:45<10:20,  2.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1514/3145 [09:46<09:05,  2.99it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1498/3145 [09:35<09:45,  2.82it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1507/3145 [09:33<10:02,  2.72it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1518/3143 [09:33<10:58,  2.47it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1515/3145 [09:46<09:46,  2.78it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1499/3145 [09:36<10:28,  2.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1519/3143 [09:33<10:30,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1508/3145 [09:33<10:33,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1516/3145 [09:46<09:34,  2.84it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1500/3145 [09:36<10:20,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1520/3143 [09:34<10:19,  2.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1509/3145 [09:34<10:17,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1517/3145 [09:47<09:40,  2.81it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1501/3145 [09:37<10:10,  2.69it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1521/3143 [09:34<10:04,  2.68it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1510/3145 [09:34<10:41,  2.55it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1518/3145 [09:47<09:55,  2.73it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1502/3145 [09:37<10:07,  2.71it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1522/3143 [09:34<10:11,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1511/3145 [09:34<10:03,  2.71it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1519/3145 [09:47<09:45,  2.78it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1503/3145 [09:37<10:07,  2.70it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1523/3143 [09:35<10:46,  2.51it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1512/3145 [09:35<10:45,  2.53it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1520/3145 [09:48<09:55,  2.73it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1504/3145 [09:38<10:48,  2.53it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1513/3145 [09:35<10:27,  2.60it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1524/3143 [09:35<10:53,  2.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1521/3145 [09:48<10:06,  2.68it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1505/3145 [09:38<10:42,  2.55it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1514/3145 [09:36<10:28,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1525/3143 [09:36<11:12,  2.41it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1522/3145 [09:49<10:29,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1506/3145 [09:38<10:25,  2.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1515/3145 [09:36<10:29,  2.59it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1526/3143 [09:36<10:44,  2.51it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1523/3145 [09:49<10:13,  2.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1507/3145 [09:39<10:15,  2.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1516/3145 [09:36<10:31,  2.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1527/3143 [09:37<10:38,  2.53it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1524/3145 [09:49<10:14,  2.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1508/3145 [09:39<09:55,  2.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1528/3143 [09:37<09:17,  2.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1517/3145 [09:37<10:33,  2.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1525/3145 [09:50<10:20,  2.61it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1509/3145 [09:40<09:53,  2.76it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1529/3143 [09:37<10:25,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1518/3145 [09:37<10:33,  2.57it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1526/3145 [09:50<10:09,  2.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1510/3145 [09:40<09:51,  2.77it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1519/3145 [09:38<10:06,  2.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1530/3143 [09:38<10:22,  2.59it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1527/3145 [09:51<09:54,  2.72it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1511/3145 [09:40<10:06,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1531/3143 [09:38<09:22,  2.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1520/3145 [09:38<09:53,  2.74it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1528/3145 [09:51<09:48,  2.75it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1512/3145 [09:41<10:15,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1532/3143 [09:38<09:25,  2.85it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1521/3145 [09:38<10:26,  2.59it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1529/3145 [09:51<09:53,  2.72it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1513/3145 [09:41<10:10,  2.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1533/3143 [09:39<09:27,  2.84it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1530/3145 [09:52<09:01,  2.98it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1522/3145 [09:39<10:18,  2.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1514/3145 [09:41<10:05,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1534/3143 [09:39<09:27,  2.84it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1523/3145 [09:39<10:09,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1531/3145 [09:52<09:28,  2.84it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1535/3143 [09:39<09:30,  2.82it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1515/3145 [09:42<10:12,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1532/3145 [09:52<09:20,  2.88it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1524/3145 [09:39<09:55,  2.72it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1516/3145 [09:42<09:53,  2.74it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1536/3143 [09:40<09:31,  2.81it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1533/3145 [09:53<09:30,  2.83it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1537/3143 [09:40<08:44,  3.06it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1525/3145 [09:40<10:58,  2.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1517/3145 [09:43<10:57,  2.47it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1534/3145 [09:53<09:35,  2.80it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1526/3145 [09:40<10:35,  2.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1538/3143 [09:40<09:17,  2.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1535/3145 [09:53<09:37,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1539/3143 [09:41<09:15,  2.89it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1527/3145 [09:41<10:14,  2.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1518/3145 [09:43<12:01,  2.26it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1536/3145 [09:54<09:51,  2.72it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1528/3145 [09:41<09:23,  2.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1519/3145 [09:44<11:27,  2.36it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1540/3143 [09:41<10:13,  2.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1529/3145 [09:41<09:33,  2.82it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1537/3145 [09:54<10:28,  2.56it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1520/3145 [09:44<11:45,  2.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1541/3143 [09:42<10:23,  2.57it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1530/3145 [09:42<09:48,  2.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1538/3145 [09:55<10:13,  2.62it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1542/3143 [09:42<10:36,  2.52it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1521/3145 [09:44<11:55,  2.27it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1531/3145 [09:42<09:47,  2.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1539/3145 [09:55<10:07,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1543/3143 [09:42<10:26,  2.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1532/3145 [09:42<09:51,  2.73it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1522/3145 [09:45<12:25,  2.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1540/3145 [09:55<10:11,  2.62it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1544/3143 [09:43<10:32,  2.53it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1533/3145 [09:43<09:39,  2.78it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1523/3145 [09:45<11:36,  2.33it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1541/3145 [09:56<10:13,  2.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1545/3143 [09:43<10:19,  2.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1524/3145 [09:46<10:23,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1534/3145 [09:43<09:58,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1542/3145 [09:56<10:05,  2.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1525/3145 [09:46<09:58,  2.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1546/3143 [09:43<10:03,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1535/3145 [09:44<09:53,  2.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1543/3145 [09:56<10:02,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1547/3143 [09:44<10:03,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1526/3145 [09:46<10:25,  2.59it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1536/3145 [09:44<09:57,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1544/3145 [09:57<09:54,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1548/3143 [09:44<09:55,  2.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1537/3145 [09:44<09:41,  2.77it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1527/3145 [09:47<10:34,  2.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1545/3145 [09:57<10:07,  2.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1549/3143 [09:45<09:43,  2.73it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1528/3145 [09:47<10:05,  2.67it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1538/3145 [09:45<09:48,  2.73it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1546/3145 [09:58<10:13,  2.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1550/3143 [09:45<09:52,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1529/3145 [09:47<09:58,  2.70it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1539/3145 [09:45<10:02,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1547/3145 [09:58<10:31,  2.53it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1551/3143 [09:45<09:50,  2.70it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1530/3145 [09:48<09:52,  2.72it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1540/3145 [09:45<09:38,  2.77it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1548/3145 [09:58<10:42,  2.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1552/3143 [09:46<10:00,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1531/3145 [09:48<10:09,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1541/3145 [09:46<09:58,  2.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1549/3145 [09:59<10:22,  2.57it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1553/3143 [09:46<09:56,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1532/3145 [09:49<09:59,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1542/3145 [09:46<10:04,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1550/3145 [09:59<10:20,  2.57it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1554/3143 [09:46<09:59,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1543/3145 [09:46<09:56,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1533/3145 [09:49<10:57,  2.45it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1555/3143 [09:47<08:49,  3.00it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1551/3145 [10:00<10:04,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1544/3145 [09:47<10:05,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1534/3145 [09:50<11:13,  2.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1556/3143 [09:47<09:00,  2.93it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1552/3145 [10:00<10:09,  2.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1545/3145 [09:47<10:15,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1535/3145 [09:50<10:23,  2.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1557/3143 [09:47<09:34,  2.76it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1553/3145 [10:00<10:57,  2.42it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1536/3145 [09:50<09:50,  2.73it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1546/3145 [09:48<10:06,  2.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1558/3143 [09:48<09:20,  2.83it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1537/3145 [09:50<08:45,  3.06it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1554/3145 [10:01<10:29,  2.53it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1547/3145 [09:48<09:50,  2.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1559/3143 [09:48<09:35,  2.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1538/3145 [09:51<08:54,  3.01it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1555/3145 [10:01<10:01,  2.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1548/3145 [09:48<09:44,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1560/3143 [09:48<09:11,  2.87it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1539/3145 [09:51<09:14,  2.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1556/3145 [10:01<10:03,  2.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1549/3145 [09:49<09:41,  2.74it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1540/3145 [09:51<08:18,  3.22it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1561/3143 [09:49<09:29,  2.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1557/3145 [10:02<10:07,  2.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1550/3145 [09:49<10:05,  2.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1562/3143 [09:49<09:26,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1541/3145 [09:52<09:30,  2.81it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1558/3145 [10:02<10:10,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1551/3145 [09:50<10:24,  2.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1563/3143 [09:50<09:29,  2.77it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1542/3145 [09:52<09:47,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1559/3145 [10:03<10:09,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1552/3145 [09:50<10:11,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1564/3143 [09:50<09:25,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1543/3145 [09:53<09:58,  2.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1560/3145 [10:03<10:38,  2.48it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1553/3145 [09:50<10:03,  2.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1565/3143 [09:50<09:25,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1544/3145 [09:53<09:49,  2.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1561/3145 [10:03<10:04,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1566/3143 [09:51<09:24,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1554/3145 [09:51<09:59,  2.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1545/3145 [09:53<09:44,  2.74it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1562/3145 [10:04<09:45,  2.70it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1555/3145 [09:51<09:53,  2.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1567/3143 [09:51<09:41,  2.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1546/3145 [09:54<09:53,  2.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1563/3145 [10:04<09:39,  2.73it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1556/3145 [09:51<09:35,  2.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1568/3143 [09:51<09:50,  2.67it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1547/3145 [09:54<09:45,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1564/3145 [10:04<09:46,  2.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1557/3145 [09:52<09:36,  2.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1569/3143 [09:52<09:41,  2.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1548/3145 [09:54<10:08,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1565/3145 [10:05<09:40,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1558/3145 [09:52<09:32,  2.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1570/3143 [09:52<09:23,  2.79it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1549/3145 [09:55<09:53,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1559/3145 [09:52<09:15,  2.85it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1566/3145 [10:05<09:54,  2.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1571/3143 [09:52<09:25,  2.78it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1550/3145 [09:55<10:02,  2.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1560/3145 [09:53<09:10,  2.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1567/3145 [10:06<10:04,  2.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1572/3143 [09:53<09:44,  2.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1551/3145 [09:56<10:09,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1561/3145 [09:53<09:17,  2.84it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1568/3145 [10:06<10:24,  2.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1573/3143 [09:53<09:57,  2.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1552/3145 [09:56<09:57,  2.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1562/3145 [09:53<09:37,  2.74it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1569/3145 [10:06<10:17,  2.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1574/3143 [09:54<10:02,  2.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1553/3145 [09:56<09:37,  2.75it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1563/3145 [09:54<09:28,  2.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1570/3145 [10:07<09:57,  2.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1575/3143 [09:54<09:48,  2.66it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1554/3145 [09:57<09:48,  2.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1564/3145 [09:54<09:26,  2.79it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1576/3143 [09:54<09:38,  2.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1571/3145 [10:07<10:53,  2.41it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1555/3145 [09:57<09:55,  2.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1565/3145 [09:55<09:29,  2.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1577/3143 [09:55<09:46,  2.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1572/3145 [10:08<10:26,  2.51it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1556/3145 [09:57<09:35,  2.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1566/3145 [09:55<09:39,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1578/3143 [09:55<09:51,  2.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1573/3145 [10:08<10:05,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1557/3145 [09:58<09:36,  2.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1567/3145 [09:55<09:52,  2.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1579/3143 [09:56<09:34,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1574/3145 [10:08<10:03,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1558/3145 [09:58<10:09,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1568/3145 [09:56<09:45,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1580/3143 [09:56<09:55,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1575/3145 [10:09<10:03,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1559/3145 [09:59<10:09,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1569/3145 [09:56<09:44,  2.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1581/3143 [09:56<09:35,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1560/3145 [09:59<08:56,  2.96it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1576/3145 [10:09<10:06,  2.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1570/3145 [09:56<09:37,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1582/3143 [09:57<09:41,  2.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1561/3145 [09:59<09:49,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1577/3145 [10:10<09:58,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1571/3145 [09:57<09:29,  2.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1562/3145 [09:59<08:45,  3.01it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1583/3143 [09:57<09:34,  2.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1578/3145 [10:10<09:52,  2.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1572/3145 [09:57<09:46,  2.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1563/3145 [10:00<08:16,  3.19it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1584/3143 [09:57<09:30,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1579/3145 [10:10<08:54,  2.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1573/3145 [09:57<08:49,  2.97it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1564/3145 [10:00<08:49,  2.99it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1574/3145 [09:58<07:58,  3.28it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1585/3143 [09:58<09:38,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1580/3145 [10:11<09:29,  2.75it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1565/3145 [10:01<09:00,  2.92it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1575/3145 [09:58<08:28,  3.09it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1586/3143 [09:58<09:32,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1581/3145 [10:11<09:41,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1566/3145 [10:01<09:06,  2.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1576/3145 [09:58<08:45,  2.98it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1587/3143 [09:58<09:41,  2.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1582/3145 [10:11<09:30,  2.74it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1577/3145 [09:59<08:00,  3.27it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1567/3145 [10:01<09:11,  2.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1583/3145 [10:12<09:28,  2.75it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1588/3143 [09:59<10:04,  2.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1578/3145 [09:59<08:28,  3.08it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1568/3145 [10:02<10:14,  2.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1584/3145 [10:12<09:11,  2.83it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1579/3145 [09:59<07:59,  3.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1589/3143 [09:59<10:05,  2.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1585/3145 [10:12<09:01,  2.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1580/3145 [10:00<08:17,  3.15it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1569/3145 [10:02<10:49,  2.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1590/3143 [10:00<10:54,  2.37it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1586/3145 [10:13<09:24,  2.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1581/3145 [10:00<08:51,  2.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1570/3145 [10:03<11:00,  2.39it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1591/3143 [10:00<10:25,  2.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1582/3145 [10:00<08:30,  3.06it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1587/3145 [10:13<09:39,  2.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1571/3145 [10:03<10:19,  2.54it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1592/3143 [10:01<10:15,  2.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1583/3145 [10:01<09:01,  2.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1588/3145 [10:14<09:43,  2.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1572/3145 [10:03<09:50,  2.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1593/3143 [10:01<09:51,  2.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1584/3145 [10:01<09:04,  2.87it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1589/3145 [10:14<09:34,  2.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1573/3145 [10:04<10:14,  2.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1594/3143 [10:01<09:39,  2.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1585/3145 [10:01<09:09,  2.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1590/3145 [10:14<09:26,  2.74it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1595/3143 [10:02<09:16,  2.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1574/3145 [10:04<10:15,  2.55it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1591/3145 [10:15<09:10,  2.82it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1586/3145 [10:02<09:26,  2.75it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1596/3143 [10:02<09:26,  2.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1575/3145 [10:04<10:03,  2.60it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1592/3145 [10:15<08:58,  2.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1587/3145 [10:02<10:26,  2.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1576/3145 [10:05<09:55,  2.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1597/3143 [10:02<09:30,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1593/3145 [10:15<09:17,  2.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1588/3145 [10:03<10:17,  2.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1577/3145 [10:05<09:50,  2.65it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1598/3143 [10:03<09:31,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1594/3145 [10:16<09:20,  2.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1599/3143 [10:03<09:20,  2.75it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1589/3145 [10:03<10:18,  2.52it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1595/3145 [10:16<08:20,  3.10it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1578/3145 [10:06<10:51,  2.41it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1600/3143 [10:03<09:16,  2.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1579/3145 [10:06<09:26,  2.76it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1590/3145 [10:03<10:36,  2.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1596/3145 [10:16<09:26,  2.74it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1601/3143 [10:04<09:01,  2.85it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1580/3145 [10:06<09:37,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1591/3145 [10:04<10:14,  2.53it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1597/3145 [10:17<09:56,  2.60it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1602/3143 [10:04<09:07,  2.82it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1581/3145 [10:07<09:34,  2.72it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1592/3145 [10:04<10:11,  2.54it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1598/3145 [10:17<09:54,  2.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1582/3145 [10:07<09:18,  2.80it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1603/3143 [10:05<09:39,  2.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1593/3145 [10:05<09:42,  2.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1599/3145 [10:18<10:11,  2.53it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1583/3145 [10:07<09:20,  2.79it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1604/3143 [10:05<09:30,  2.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1594/3145 [10:05<10:07,  2.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1584/3145 [10:08<09:13,  2.82it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1600/3145 [10:18<10:40,  2.41it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1605/3143 [10:05<09:58,  2.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1595/3145 [10:05<09:41,  2.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1585/3145 [10:08<09:36,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1606/3143 [10:06<09:28,  2.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1596/3145 [10:06<09:32,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1601/3145 [10:19<11:16,  2.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1607/3143 [10:06<09:13,  2.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1586/3145 [10:09<09:38,  2.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1602/3145 [10:19<10:45,  2.39it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1597/3145 [10:06<09:45,  2.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1608/3143 [10:06<09:24,  2.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1587/3145 [10:09<09:51,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1598/3145 [10:06<09:33,  2.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1603/3145 [10:19<10:28,  2.45it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1609/3143 [10:07<09:20,  2.74it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1599/3145 [10:07<09:09,  2.81it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1588/3145 [10:09<10:13,  2.54it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1604/3145 [10:20<10:44,  2.39it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1600/3145 [10:07<08:05,  3.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1610/3143 [10:07<09:23,  2.72it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1589/3145 [10:10<09:52,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1605/3145 [10:20<10:15,  2.50it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1601/3145 [10:07<08:38,  2.97it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1611/3143 [10:07<09:09,  2.79it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1590/3145 [10:10<08:58,  2.89it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1606/3145 [10:20<09:02,  2.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1602/3145 [10:08<08:15,  3.11it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1612/3143 [10:08<09:12,  2.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1607/3145 [10:21<08:53,  2.89it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1591/3145 [10:10<09:53,  2.62it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1603/3145 [10:08<07:31,  3.41it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1613/3143 [10:08<09:23,  2.71it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1608/3145 [10:21<09:12,  2.78it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1604/3145 [10:08<08:04,  3.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1592/3145 [10:11<10:03,  2.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1614/3143 [10:09<09:29,  2.68it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1593/3145 [10:11<09:12,  2.81it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1609/3145 [10:21<09:32,  2.68it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1605/3145 [10:09<09:01,  2.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1615/3143 [10:09<09:12,  2.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1594/3145 [10:11<09:01,  2.86it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1606/3145 [10:09<08:57,  2.86it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1610/3145 [10:22<09:58,  2.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1595/3145 [10:12<08:54,  2.90it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1616/3143 [10:09<10:13,  2.49it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1607/3145 [10:09<09:08,  2.80it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1611/3145 [10:22<10:08,  2.52it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1596/3145 [10:12<08:23,  3.08it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1617/3143 [10:10<09:56,  2.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1597/3145 [10:12<08:06,  3.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1608/3145 [10:10<09:38,  2.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1612/3145 [10:23<10:09,  2.52it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1598/3145 [10:13<08:27,  3.05it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1618/3143 [10:10<10:21,  2.46it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1609/3145 [10:10<09:37,  2.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1613/3145 [10:23<09:51,  2.59it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1599/3145 [10:13<08:45,  2.94it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1619/3143 [10:11<10:02,  2.53it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1610/3145 [10:11<09:32,  2.68it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1614/3145 [10:23<09:55,  2.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1600/3145 [10:13<08:56,  2.88it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1620/3143 [10:11<09:49,  2.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1611/3145 [10:11<09:43,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1615/3145 [10:24<09:40,  2.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1616/3145 [10:24<08:31,  2.99it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1601/3145 [10:14<08:52,  2.90it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1621/3143 [10:11<09:38,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1612/3145 [10:11<09:22,  2.73it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1617/3145 [10:24<08:00,  3.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1602/3145 [10:14<09:26,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1622/3143 [10:12<09:38,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1613/3145 [10:12<09:34,  2.67it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1618/3145 [10:25<08:21,  3.05it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1603/3145 [10:15<09:16,  2.77it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1623/3143 [10:12<09:48,  2.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1614/3145 [10:12<09:44,  2.62it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1619/3145 [10:25<08:32,  2.98it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1604/3145 [10:15<09:15,  2.77it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1624/3143 [10:12<09:48,  2.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1615/3145 [10:12<09:39,  2.64it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1620/3145 [10:25<08:56,  2.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1605/3145 [10:15<09:18,  2.76it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1616/3145 [10:13<09:19,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1625/3143 [10:13<09:33,  2.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1621/3145 [10:26<08:52,  2.86it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1606/3145 [10:16<09:29,  2.70it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1626/3143 [10:13<09:29,  2.67it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1617/3145 [10:13<09:35,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1622/3145 [10:26<08:55,  2.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1607/3145 [10:16<09:24,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1627/3143 [10:14<09:10,  2.75it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1618/3145 [10:14<09:26,  2.69it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1623/3145 [10:26<09:00,  2.82it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1619/3145 [10:14<08:25,  3.02it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1608/3145 [10:16<09:10,  2.79it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1628/3143 [10:14<09:01,  2.80it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1624/3145 [10:27<08:49,  2.87it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1609/3145 [10:17<08:23,  3.05it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1620/3145 [10:14<08:39,  2.94it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1629/3143 [10:14<08:58,  2.81it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1625/3145 [10:27<08:45,  2.89it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1610/3145 [10:17<09:05,  2.81it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1621/3145 [10:15<09:00,  2.82it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1630/3143 [10:15<09:00,  2.80it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1626/3145 [10:27<08:43,  2.90it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1611/3145 [10:17<09:05,  2.81it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1631/3143 [10:15<09:02,  2.79it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1622/3145 [10:15<09:17,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1627/3145 [10:28<09:07,  2.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1612/3145 [10:18<09:02,  2.83it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1632/3143 [10:15<08:50,  2.85it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1623/3145 [10:15<09:15,  2.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1628/3145 [10:28<09:07,  2.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1613/3145 [10:18<09:23,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1624/3145 [10:16<09:01,  2.81it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1629/3145 [10:29<09:09,  2.76it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1633/3143 [10:16<10:30,  2.39it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1614/3145 [10:19<09:28,  2.69it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1625/3145 [10:16<09:27,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1630/3145 [10:29<08:53,  2.84it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1634/3143 [10:16<10:29,  2.40it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1615/3145 [10:19<09:17,  2.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1626/3145 [10:16<09:12,  2.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1631/3145 [10:29<08:55,  2.83it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1635/3143 [10:17<10:14,  2.45it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1616/3145 [10:19<09:34,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1627/3145 [10:17<09:29,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1632/3145 [10:30<08:49,  2.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1636/3143 [10:17<10:09,  2.47it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1617/3145 [10:20<09:42,  2.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1628/3145 [10:17<09:06,  2.78it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1633/3145 [10:30<09:13,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1637/3143 [10:17<09:48,  2.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1618/3145 [10:20<09:29,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1629/3145 [10:18<09:10,  2.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1634/3145 [10:30<09:03,  2.78it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1638/3143 [10:18<09:33,  2.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1619/3145 [10:20<09:17,  2.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1630/3145 [10:18<09:11,  2.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1635/3145 [10:31<08:56,  2.81it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1639/3143 [10:18<09:37,  2.60it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1620/3145 [10:21<08:56,  2.84it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1631/3145 [10:18<09:05,  2.77it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1636/3145 [10:31<09:09,  2.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1640/3143 [10:19<09:27,  2.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1632/3145 [10:19<09:19,  2.70it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1621/3145 [10:21<09:48,  2.59it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1637/3145 [10:31<09:16,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1641/3143 [10:19<09:21,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1622/3145 [10:21<09:29,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1633/3145 [10:19<09:36,  2.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1638/3145 [10:32<09:21,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1642/3143 [10:19<09:11,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1634/3145 [10:19<08:19,  3.02it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1623/3145 [10:22<09:39,  2.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1639/3145 [10:32<09:25,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1643/3143 [10:20<09:22,  2.67it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1635/3145 [10:20<09:08,  2.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1624/3145 [10:22<09:18,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1640/3145 [10:33<09:37,  2.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1644/3143 [10:20<09:14,  2.70it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1636/3145 [10:20<09:16,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1625/3145 [10:23<09:29,  2.67it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1641/3145 [10:33<09:36,  2.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1645/3143 [10:20<09:13,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1626/3145 [10:23<09:08,  2.77it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1637/3145 [10:20<09:13,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1642/3145 [10:33<09:34,  2.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1646/3143 [10:21<09:02,  2.76it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1638/3145 [10:21<09:12,  2.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1627/3145 [10:23<09:29,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1643/3145 [10:34<09:36,  2.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1647/3143 [10:21<09:42,  2.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1639/3145 [10:21<09:25,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1628/3145 [10:24<09:51,  2.56it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1644/3145 [10:34<09:13,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1648/3143 [10:22<09:21,  2.66it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1629/3145 [10:24<09:50,  2.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1645/3145 [10:34<09:07,  2.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1640/3145 [10:22<10:37,  2.36it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1649/3143 [10:22<09:01,  2.76it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1646/3145 [10:35<09:02,  2.76it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1630/3145 [10:25<10:01,  2.52it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1641/3145 [10:22<10:10,  2.46it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1650/3143 [10:22<09:10,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1647/3145 [10:35<09:12,  2.71it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1631/3145 [10:25<09:53,  2.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1642/3145 [10:22<09:47,  2.56it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1651/3143 [10:23<09:44,  2.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1643/3145 [10:23<09:29,  2.64it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1632/3145 [10:25<10:11,  2.47it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1648/3145 [10:36<09:48,  2.54it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1652/3143 [10:23<09:39,  2.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1644/3145 [10:23<09:00,  2.78it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1633/3145 [10:26<09:53,  2.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1649/3145 [10:36<09:51,  2.53it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1653/3143 [10:23<09:51,  2.52it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1645/3145 [10:23<09:11,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1634/3145 [10:26<09:52,  2.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1650/3145 [10:36<09:29,  2.62it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1654/3143 [10:24<09:21,  2.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1646/3145 [10:24<09:33,  2.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1651/3145 [10:37<09:09,  2.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1635/3145 [10:26<09:32,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1655/3143 [10:24<08:13,  3.02it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1647/3145 [10:24<09:36,  2.60it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1652/3145 [10:37<09:07,  2.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1656/3143 [10:24<08:18,  2.98it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1636/3145 [10:27<09:42,  2.59it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1648/3145 [10:25<08:21,  2.99it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1657/3143 [10:25<08:27,  2.93it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1637/3145 [10:27<09:43,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1653/3145 [10:38<09:38,  2.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1649/3145 [10:25<08:23,  2.97it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1658/3143 [10:25<07:35,  3.26it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1638/3145 [10:28<09:48,  2.56it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1654/3145 [10:38<09:56,  2.50it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1650/3145 [10:25<08:35,  2.90it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1659/3143 [10:25<07:57,  3.11it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1639/3145 [10:28<08:40,  2.89it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1655/3145 [10:38<09:52,  2.51it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1651/3145 [10:26<08:57,  2.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1660/3143 [10:26<08:10,  3.02it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1640/3145 [10:28<08:44,  2.87it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1656/3145 [10:39<09:56,  2.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1661/3143 [10:26<08:28,  2.91it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1652/3145 [10:26<09:13,  2.70it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1641/3145 [10:29<09:01,  2.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1653/3145 [10:26<09:24,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1662/3143 [10:26<08:53,  2.77it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1657/3145 [10:39<10:11,  2.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1642/3145 [10:29<08:59,  2.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1654/3145 [10:27<08:22,  2.97it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1663/3143 [10:27<09:02,  2.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1658/3145 [10:40<10:04,  2.46it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1643/3145 [10:29<09:02,  2.77it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1655/3145 [10:27<08:06,  3.06it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1664/3143 [10:27<08:57,  2.75it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1659/3145 [10:40<09:42,  2.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1644/3145 [10:30<09:13,  2.71it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1656/3145 [10:27<08:20,  2.98it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1645/3145 [10:30<08:57,  2.79it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1665/3143 [10:28<09:34,  2.57it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1660/3145 [10:40<09:54,  2.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1657/3145 [10:28<08:01,  3.09it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1646/3145 [10:30<08:52,  2.82it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1666/3143 [10:28<09:17,  2.65it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1661/3145 [10:41<09:41,  2.55it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1658/3145 [10:28<08:31,  2.91it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1667/3143 [10:28<09:09,  2.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1647/3145 [10:31<09:09,  2.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1659/3145 [10:28<08:38,  2.87it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1662/3145 [10:41<09:46,  2.53it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1660/3145 [10:29<08:40,  2.85it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1668/3143 [10:29<09:27,  2.60it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1648/3145 [10:31<09:29,  2.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1663/3145 [10:42<09:59,  2.47it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1669/3143 [10:29<09:05,  2.70it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1661/3145 [10:29<09:02,  2.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1649/3145 [10:32<09:36,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1664/3145 [10:42<10:02,  2.46it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1670/3143 [10:29<09:11,  2.67it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1662/3145 [10:29<08:51,  2.79it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1650/3145 [10:32<09:36,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1665/3145 [10:42<09:55,  2.48it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1663/3145 [10:30<08:57,  2.76it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1671/3143 [10:30<09:16,  2.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1651/3145 [10:32<09:21,  2.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1666/3145 [10:43<09:51,  2.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1664/3145 [10:30<09:00,  2.74it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1672/3143 [10:30<09:11,  2.67it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1652/3145 [10:33<09:25,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1667/3145 [10:43<09:10,  2.69it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1665/3145 [10:31<09:08,  2.70it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1653/3145 [10:33<09:16,  2.68it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1673/3143 [10:31<09:36,  2.55it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1668/3145 [10:43<09:08,  2.69it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1674/3143 [10:31<09:12,  2.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1666/3145 [10:31<09:17,  2.65it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1654/3145 [10:34<09:17,  2.67it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1669/3145 [10:44<09:56,  2.47it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1667/3145 [10:31<08:16,  2.98it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1655/3145 [10:34<09:01,  2.75it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1675/3143 [10:31<09:26,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1668/3145 [10:32<08:39,  2.84it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1670/3145 [10:44<10:17,  2.39it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1656/3145 [10:34<09:24,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1676/3143 [10:32<09:26,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1669/3145 [10:32<07:43,  3.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1671/3145 [10:45<09:39,  2.54it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1677/3143 [10:32<09:14,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1657/3145 [10:35<09:28,  2.62it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1670/3145 [10:32<08:15,  2.98it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1672/3145 [10:45<09:51,  2.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1678/3143 [10:33<09:16,  2.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1658/3145 [10:35<09:32,  2.60it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1671/3145 [10:33<08:38,  2.84it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1673/3145 [10:46<09:34,  2.56it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1659/3145 [10:35<09:10,  2.70it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1679/3143 [10:33<09:21,  2.61it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1672/3145 [10:33<08:53,  2.76it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1674/3145 [10:46<09:57,  2.46it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1660/3145 [10:36<09:31,  2.60it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1673/3145 [10:33<08:44,  2.81it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1680/3143 [10:33<09:51,  2.47it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1661/3145 [10:36<09:19,  2.65it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1675/3145 [10:47<10:50,  2.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1681/3143 [10:34<09:43,  2.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1674/3145 [10:34<09:28,  2.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1662/3145 [10:37<09:35,  2.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1682/3143 [10:34<09:39,  2.52it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1675/3145 [10:34<09:16,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1676/3145 [10:47<11:33,  2.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1663/3145 [10:37<09:07,  2.71it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1676/3145 [10:35<09:08,  2.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1683/3143 [10:35<09:52,  2.47it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1677/3145 [10:47<10:39,  2.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1664/3145 [10:37<08:50,  2.79it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1684/3143 [10:35<09:21,  2.60it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1677/3145 [10:35<09:48,  2.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1678/3145 [10:48<10:14,  2.39it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1665/3145 [10:38<09:06,  2.71it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1685/3143 [10:35<09:12,  2.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1679/3145 [10:48<10:07,  2.41it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1678/3145 [10:35<10:31,  2.32it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1686/3143 [10:36<08:53,  2.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1666/3145 [10:38<10:03,  2.45it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1680/3145 [10:49<09:53,  2.47it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1667/3145 [10:38<08:45,  2.81it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1679/3145 [10:36<10:16,  2.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1687/3143 [10:36<08:52,  2.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1681/3145 [10:49<09:20,  2.61it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1668/3145 [10:39<08:18,  2.96it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1680/3145 [10:36<10:17,  2.37it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1688/3143 [10:36<09:01,  2.69it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1682/3145 [10:49<09:09,  2.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1669/3145 [10:39<08:42,  2.83it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1681/3145 [10:37<09:51,  2.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1689/3143 [10:37<09:07,  2.66it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1683/3145 [10:50<09:03,  2.69it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1670/3145 [10:39<08:17,  2.96it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1682/3145 [10:37<09:43,  2.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1690/3143 [10:37<08:58,  2.70it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1684/3145 [10:50<08:54,  2.74it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1671/3145 [10:40<08:31,  2.88it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1683/3145 [10:37<09:39,  2.52it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1691/3143 [10:37<08:52,  2.73it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1685/3145 [10:50<08:43,  2.79it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1672/3145 [10:40<08:37,  2.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1692/3143 [10:38<08:42,  2.77it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1686/3145 [10:51<08:31,  2.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1684/3145 [10:38<10:04,  2.42it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1673/3145 [10:40<08:55,  2.75it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1693/3143 [10:38<08:53,  2.72it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1687/3145 [10:51<08:32,  2.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1685/3145 [10:38<10:37,  2.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1674/3145 [10:41<09:46,  2.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1694/3143 [10:39<08:37,  2.80it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1688/3145 [10:51<09:14,  2.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1675/3145 [10:41<09:39,  2.54it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1686/3145 [10:39<11:00,  2.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1695/3143 [10:39<08:42,  2.77it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1689/3145 [10:52<09:44,  2.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1676/3145 [10:42<09:51,  2.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1687/3145 [10:39<10:33,  2.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1696/3143 [10:39<08:57,  2.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1690/3145 [10:52<09:37,  2.52it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1677/3145 [10:42<09:41,  2.53it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1697/3143 [10:40<08:57,  2.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1688/3145 [10:40<10:32,  2.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1691/3145 [10:53<09:17,  2.61it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1698/3143 [10:40<09:02,  2.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1678/3145 [10:43<09:48,  2.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1689/3145 [10:40<10:28,  2.32it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1692/3145 [10:53<09:04,  2.67it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1679/3145 [10:43<09:45,  2.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1690/3145 [10:41<10:09,  2.39it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1699/3143 [10:41<09:54,  2.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1693/3145 [10:53<09:17,  2.60it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1680/3145 [10:43<09:38,  2.53it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1691/3145 [10:41<09:36,  2.52it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1700/3143 [10:41<09:45,  2.46it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1694/3145 [10:54<09:03,  2.67it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1681/3145 [10:44<09:38,  2.53it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1692/3145 [10:41<09:21,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1701/3143 [10:41<09:42,  2.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1695/3145 [10:54<09:25,  2.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1693/3145 [10:42<08:58,  2.70it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1682/3145 [10:44<09:27,  2.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1702/3143 [10:42<09:28,  2.54it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1696/3145 [10:55<09:10,  2.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1694/3145 [10:42<08:42,  2.78it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1683/3145 [10:44<09:21,  2.60it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1703/3143 [10:42<09:16,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1697/3145 [10:55<09:15,  2.61it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1695/3145 [10:42<08:32,  2.83it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1684/3145 [10:45<09:39,  2.52it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1704/3143 [10:42<09:16,  2.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1698/3145 [10:55<09:19,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1696/3145 [10:43<08:45,  2.76it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1685/3145 [10:45<09:27,  2.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1705/3143 [10:43<09:15,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1699/3145 [10:56<09:20,  2.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1697/3145 [10:43<08:43,  2.76it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1686/3145 [10:46<09:19,  2.61it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1700/3145 [10:56<09:21,  2.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1706/3143 [10:43<10:05,  2.37it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1698/3145 [10:43<08:54,  2.71it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1687/3145 [10:46<09:08,  2.66it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1701/3145 [10:56<09:04,  2.65it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1699/3145 [10:44<08:42,  2.77it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1688/3145 [10:46<08:04,  3.01it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1707/3143 [10:44<10:48,  2.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1702/3145 [10:57<08:56,  2.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1689/3145 [10:47<08:04,  3.00it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1700/3145 [10:44<08:59,  2.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1708/3143 [10:44<10:20,  2.31it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1703/3145 [10:57<08:48,  2.73it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1690/3145 [10:47<08:14,  2.94it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1701/3145 [10:44<09:08,  2.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1709/3143 [10:45<10:03,  2.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1704/3145 [10:58<08:58,  2.67it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1691/3145 [10:47<08:35,  2.82it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1702/3145 [10:45<09:10,  2.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3143 [10:45<10:37,  2.25it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1692/3145 [10:48<08:38,  2.80it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1705/3145 [10:58<09:16,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1703/3145 [10:45<09:16,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1711/3143 [10:45<09:46,  2.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1706/3145 [10:58<09:05,  2.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1693/3145 [10:48<09:15,  2.61it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1704/3145 [10:46<09:15,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1712/3143 [10:46<09:34,  2.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1707/3145 [10:59<08:58,  2.67it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1694/3145 [10:49<09:19,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1705/3145 [10:46<08:53,  2.70it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1713/3143 [10:46<09:14,  2.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1708/3145 [10:59<09:03,  2.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1695/3145 [10:49<09:09,  2.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1706/3145 [10:46<09:07,  2.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1714/3143 [10:47<09:39,  2.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1709/3145 [11:00<09:36,  2.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1707/3145 [10:47<09:16,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1696/3145 [10:49<09:41,  2.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1715/3143 [10:47<09:29,  2.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3145 [11:00<09:55,  2.41it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1708/3145 [10:47<09:22,  2.55it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1697/3145 [10:50<09:51,  2.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1716/3143 [10:47<09:17,  2.56it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1709/3145 [10:48<08:58,  2.67it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1711/3145 [11:00<09:45,  2.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1698/3145 [10:50<09:31,  2.53it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1717/3143 [10:48<09:17,  2.56it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3145 [10:48<08:53,  2.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1712/3145 [11:01<09:49,  2.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1699/3145 [10:51<10:03,  2.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1718/3143 [10:48<08:51,  2.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1711/3145 [10:48<09:01,  2.65it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1713/3145 [11:01<09:24,  2.53it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1700/3145 [10:51<09:35,  2.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1719/3143 [10:49<09:10,  2.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1712/3145 [10:49<08:51,  2.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1714/3145 [11:01<09:07,  2.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1701/3145 [10:51<09:17,  2.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1720/3143 [10:49<08:45,  2.71it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1715/3145 [11:02<08:44,  2.73it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1713/3145 [10:49<09:01,  2.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1702/3145 [10:52<09:09,  2.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1721/3143 [10:49<09:07,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1716/3145 [11:02<08:41,  2.74it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1714/3145 [10:49<09:05,  2.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1703/3145 [10:52<09:12,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1722/3143 [10:50<08:41,  2.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1717/3145 [11:03<08:36,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1715/3145 [10:50<09:34,  2.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1704/3145 [10:52<09:15,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1723/3143 [10:50<08:49,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1718/3145 [11:03<08:33,  2.78it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1705/3145 [10:53<08:04,  2.97it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1716/3145 [10:50<09:20,  2.55it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1724/3143 [10:50<08:41,  2.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1719/3145 [11:03<08:48,  2.70it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1706/3145 [10:53<08:13,  2.92it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1717/3145 [10:51<09:06,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1725/3143 [10:51<08:49,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1720/3145 [11:04<08:56,  2.66it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1707/3145 [10:53<08:33,  2.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1718/3145 [10:51<09:35,  2.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1726/3143 [10:51<08:38,  2.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1721/3145 [11:04<08:57,  2.65it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1708/3145 [10:54<09:06,  2.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1719/3145 [10:51<09:30,  2.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1727/3143 [10:52<09:02,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1722/3145 [11:04<08:51,  2.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1709/3145 [10:54<08:50,  2.71it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1720/3145 [10:52<09:07,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1728/3143 [10:52<08:38,  2.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1723/3145 [11:05<08:57,  2.65it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3145 [10:55<09:13,  2.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1721/3145 [10:52<08:48,  2.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1729/3143 [10:52<08:48,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1724/3145 [11:05<08:36,  2.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1722/3145 [10:52<07:46,  3.05it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1711/3145 [10:55<09:17,  2.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1730/3143 [10:53<08:36,  2.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1725/3145 [11:06<08:35,  2.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1723/3145 [10:53<08:02,  2.95it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1712/3145 [10:55<09:02,  2.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1731/3143 [10:53<08:33,  2.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1724/3145 [10:53<08:18,  2.85it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1726/3145 [11:06<09:14,  2.56it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1713/3145 [10:56<08:53,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1732/3143 [10:53<08:48,  2.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1725/3145 [10:53<07:42,  3.07it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1727/3145 [11:06<08:48,  2.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1714/3145 [10:56<09:09,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1726/3145 [10:54<07:48,  3.03it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1733/3143 [10:54<09:01,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1728/3145 [11:07<09:10,  2.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1715/3145 [10:57<09:09,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1727/3145 [10:54<07:45,  3.05it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1734/3143 [10:54<08:50,  2.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1729/3145 [11:07<08:47,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1716/3145 [10:57<09:10,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1728/3145 [10:54<08:10,  2.89it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1735/3143 [10:54<08:37,  2.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1730/3145 [11:07<08:54,  2.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1729/3145 [10:55<08:09,  2.89it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1717/3145 [10:57<09:14,  2.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1736/3143 [10:55<08:43,  2.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1731/3145 [11:08<08:53,  2.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1718/3145 [10:58<09:15,  2.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1730/3145 [10:55<08:37,  2.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1737/3143 [10:55<09:02,  2.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1732/3145 [11:08<07:58,  2.95it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1731/3145 [10:56<08:41,  2.71it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1719/3145 [10:58<09:36,  2.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1738/3143 [10:56<08:47,  2.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1733/3145 [11:09<08:35,  2.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1720/3145 [10:58<09:06,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1732/3145 [10:56<08:48,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1739/3143 [10:56<08:55,  2.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1734/3145 [11:09<08:39,  2.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1721/3145 [10:59<08:19,  2.85it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1735/3145 [11:09<07:42,  3.05it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1733/3145 [10:56<08:45,  2.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1740/3143 [10:56<08:55,  2.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1736/3145 [11:09<07:11,  3.27it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1734/3145 [10:57<08:31,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1722/3145 [10:59<09:03,  2.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1741/3143 [10:57<08:45,  2.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1737/3145 [11:10<07:34,  3.10it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1723/3145 [10:59<08:09,  2.90it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1735/3145 [10:57<08:36,  2.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1742/3143 [10:57<08:26,  2.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1724/3145 [11:00<08:17,  2.86it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1738/3145 [11:10<07:59,  2.93it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1736/3145 [10:57<08:23,  2.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1743/3143 [10:57<08:37,  2.70it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1739/3145 [11:10<08:09,  2.87it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1725/3145 [11:00<08:37,  2.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1737/3145 [10:58<08:23,  2.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1744/3143 [10:58<08:28,  2.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1726/3145 [11:01<08:34,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1740/3145 [11:11<08:27,  2.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1738/3145 [10:58<08:33,  2.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1745/3143 [10:58<08:26,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1727/3145 [11:01<08:24,  2.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1741/3145 [11:11<08:29,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1739/3145 [10:59<08:44,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1746/3143 [10:59<08:22,  2.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1728/3145 [11:01<08:21,  2.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1742/3145 [11:12<08:25,  2.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1740/3145 [10:59<08:42,  2.69it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1747/3143 [10:59<08:21,  2.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1729/3145 [11:02<08:24,  2.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1743/3145 [11:12<08:24,  2.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1748/3143 [10:59<08:14,  2.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1741/3145 [10:59<09:18,  2.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1744/3145 [11:12<08:15,  2.83it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1730/3145 [11:02<08:38,  2.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1749/3143 [11:00<08:11,  2.83it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1742/3145 [11:00<09:02,  2.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1745/3145 [11:13<08:39,  2.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1731/3145 [11:02<09:05,  2.59it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1750/3143 [11:00<08:17,  2.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1743/3145 [11:00<09:17,  2.52it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1746/3145 [11:13<08:38,  2.70it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1732/3145 [11:03<09:02,  2.61it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1751/3143 [11:00<08:40,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1744/3145 [11:00<09:02,  2.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1747/3145 [11:13<08:28,  2.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1733/3145 [11:03<08:52,  2.65it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1752/3143 [11:01<09:17,  2.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1745/3145 [11:01<09:01,  2.59it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1748/3145 [11:14<08:26,  2.76it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1734/3145 [11:04<08:57,  2.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1753/3143 [11:01<08:59,  2.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1749/3145 [11:14<07:27,  3.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1746/3145 [11:01<08:51,  2.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1735/3145 [11:04<08:48,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1754/3143 [11:02<08:45,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1750/3145 [11:14<07:55,  2.93it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1747/3145 [11:02<08:44,  2.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1736/3145 [11:04<09:19,  2.52it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1755/3143 [11:02<08:38,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1748/3145 [11:02<08:33,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1751/3145 [11:15<08:19,  2.79it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1756/3143 [11:02<08:31,  2.71it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1737/3145 [11:05<09:44,  2.41it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1749/3145 [11:02<08:43,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1752/3145 [11:15<08:20,  2.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1757/3143 [11:03<08:37,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1753/3145 [11:15<08:11,  2.83it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1750/3145 [11:03<08:39,  2.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1738/3145 [11:05<09:51,  2.38it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1758/3143 [11:03<08:29,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1751/3145 [11:03<08:24,  2.76it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1754/3145 [11:16<08:41,  2.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1739/3145 [11:06<09:27,  2.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1759/3143 [11:03<08:36,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1752/3145 [11:03<08:22,  2.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1755/3145 [11:16<08:31,  2.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1740/3145 [11:06<09:34,  2.45it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1753/3145 [11:04<08:14,  2.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1760/3143 [11:04<08:42,  2.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1741/3145 [11:06<08:58,  2.61it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1756/3145 [11:17<08:54,  2.60it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1754/3145 [11:04<08:28,  2.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1761/3143 [11:04<08:47,  2.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1742/3145 [11:07<09:02,  2.59it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1757/3145 [11:17<09:35,  2.41it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1755/3145 [11:04<08:24,  2.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1762/3143 [11:05<08:36,  2.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1743/3145 [11:07<09:50,  2.37it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1756/3145 [11:05<08:36,  2.69it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1758/3145 [11:18<10:21,  2.23it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1763/3143 [11:05<08:48,  2.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1744/3145 [11:08<09:38,  2.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1757/3145 [11:05<08:30,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1759/3145 [11:18<10:01,  2.30it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1764/3143 [11:05<09:09,  2.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1745/3145 [11:08<09:25,  2.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1758/3145 [11:06<08:25,  2.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1765/3143 [11:06<08:42,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1760/3145 [11:19<09:58,  2.31it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1746/3145 [11:08<08:59,  2.59it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1759/3145 [11:06<08:35,  2.69it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1766/3143 [11:06<08:27,  2.71it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1747/3145 [11:09<07:51,  2.97it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1761/3145 [11:19<09:57,  2.31it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1760/3145 [11:06<08:35,  2.69it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1767/3143 [11:06<08:33,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1748/3145 [11:09<07:58,  2.92it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1762/3145 [11:19<10:08,  2.27it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1761/3145 [11:07<08:42,  2.65it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1768/3143 [11:07<08:36,  2.66it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1749/3145 [11:09<08:23,  2.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1763/3145 [11:20<09:35,  2.40it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1762/3145 [11:07<09:01,  2.55it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1769/3143 [11:07<08:40,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1750/3145 [11:10<08:33,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1764/3145 [11:20<08:57,  2.57it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1751/3145 [11:10<07:30,  3.09it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1763/3145 [11:08<08:43,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1770/3143 [11:08<08:22,  2.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1765/3145 [11:20<08:45,  2.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1752/3145 [11:10<07:48,  2.98it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1764/3145 [11:08<08:33,  2.69it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1771/3143 [11:08<08:23,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1766/3145 [11:21<08:21,  2.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1753/3145 [11:11<08:14,  2.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1765/3145 [11:08<08:37,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1772/3143 [11:08<08:30,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1767/3145 [11:21<08:47,  2.61it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1754/3145 [11:11<08:22,  2.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1773/3143 [11:09<08:24,  2.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1766/3145 [11:09<09:38,  2.38it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1768/3145 [11:22<08:47,  2.61it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1755/3145 [11:11<08:23,  2.76it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1774/3143 [11:09<08:19,  2.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1769/3145 [11:22<08:41,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1767/3145 [11:09<09:27,  2.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1775/3143 [11:09<08:26,  2.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1756/3145 [11:12<08:48,  2.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1770/3145 [11:22<08:22,  2.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1768/3145 [11:10<09:15,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1776/3143 [11:10<08:25,  2.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1757/3145 [11:12<08:39,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1771/3145 [11:23<08:32,  2.68it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1769/3145 [11:10<09:03,  2.53it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1758/3145 [11:13<08:17,  2.79it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1777/3143 [11:10<08:23,  2.71it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1772/3145 [11:23<07:40,  2.98it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1770/3145 [11:10<08:50,  2.59it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1778/3143 [11:10<07:38,  2.97it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1759/3145 [11:13<08:23,  2.76it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1771/3145 [11:11<07:46,  2.95it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1773/3145 [11:23<08:04,  2.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1779/3143 [11:11<07:47,  2.92it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1760/3145 [11:13<08:19,  2.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1772/3145 [11:11<07:19,  3.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1774/3145 [11:24<08:09,  2.80it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1780/3143 [11:11<07:32,  3.01it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1761/3145 [11:14<08:18,  2.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1773/3145 [11:11<07:41,  2.98it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1775/3145 [11:24<08:19,  2.74it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1781/3143 [11:11<07:55,  2.87it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1762/3145 [11:14<08:19,  2.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1774/3145 [11:12<08:16,  2.76it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1776/3145 [11:24<08:13,  2.77it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1782/3143 [11:12<08:09,  2.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1763/3145 [11:14<08:31,  2.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1775/3145 [11:12<08:15,  2.77it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1777/3145 [11:25<08:12,  2.78it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1783/3143 [11:12<08:04,  2.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1764/3145 [11:15<08:36,  2.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1776/3145 [11:12<08:09,  2.80it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1778/3145 [11:25<08:16,  2.75it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1784/3143 [11:13<07:52,  2.87it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1765/3145 [11:15<08:32,  2.69it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1777/3145 [11:13<08:25,  2.71it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1779/3145 [11:26<08:14,  2.76it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1785/3143 [11:13<07:49,  2.90it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1766/3145 [11:15<07:37,  3.01it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1778/3145 [11:13<07:29,  3.04it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1780/3145 [11:26<08:01,  2.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1786/3143 [11:13<08:04,  2.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1767/3145 [11:16<08:05,  2.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1779/3145 [11:13<08:09,  2.79it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1781/3145 [11:26<08:14,  2.76it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1787/3143 [11:14<08:04,  2.80it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1780/3145 [11:14<07:32,  3.02it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1768/3145 [11:16<08:01,  2.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1782/3145 [11:27<08:09,  2.79it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1788/3143 [11:14<08:05,  2.79it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1781/3145 [11:14<07:37,  2.98it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1769/3145 [11:17<08:10,  2.80it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1789/3143 [11:14<07:56,  2.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1783/3145 [11:27<09:06,  2.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1782/3145 [11:14<08:18,  2.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1770/3145 [11:17<08:36,  2.66it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1784/3145 [11:27<08:45,  2.59it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1790/3143 [11:15<08:23,  2.69it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1783/3145 [11:15<08:36,  2.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1771/3145 [11:17<09:23,  2.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1785/3145 [11:28<09:00,  2.52it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1791/3143 [11:15<08:25,  2.67it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1784/3145 [11:15<08:33,  2.65it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1772/3145 [11:18<09:29,  2.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1792/3143 [11:16<08:41,  2.59it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1786/3145 [11:28<09:18,  2.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1785/3145 [11:16<08:34,  2.65it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1773/3145 [11:18<09:13,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1793/3143 [11:16<08:44,  2.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1787/3145 [11:29<09:14,  2.45it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1786/3145 [11:16<08:52,  2.55it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1794/3143 [11:16<08:24,  2.67it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1788/3145 [11:29<09:10,  2.46it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1774/3145 [11:19<10:34,  2.16it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1787/3145 [11:16<08:34,  2.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1795/3143 [11:17<08:39,  2.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1789/3145 [11:29<08:56,  2.53it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1788/3145 [11:17<08:50,  2.56it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1775/3145 [11:19<11:23,  2.00it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1796/3143 [11:17<08:33,  2.62it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1790/3145 [11:30<09:26,  2.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1789/3145 [11:17<09:01,  2.50it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1797/3143 [11:17<08:32,  2.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1790/3145 [11:17<08:05,  2.79it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1776/3145 [11:20<12:09,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1791/3145 [11:30<09:39,  2.34it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1798/3143 [11:18<08:42,  2.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1791/3145 [11:18<08:20,  2.70it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1777/3145 [11:21<11:56,  1.91it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1792/3145 [11:31<10:08,  2.23it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1799/3143 [11:18<08:41,  2.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1792/3145 [11:18<08:34,  2.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1778/3145 [11:21<10:54,  2.09it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1793/3145 [11:31<09:46,  2.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1800/3143 [11:19<08:21,  2.68it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1793/3145 [11:19<08:31,  2.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1779/3145 [11:21<10:26,  2.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1794/3145 [11:19<07:42,  2.92it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1801/3143 [11:19<08:30,  2.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1794/3145 [11:32<09:42,  2.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1780/3145 [11:22<09:48,  2.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1795/3145 [11:32<09:10,  2.45it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1802/3143 [11:19<08:41,  2.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1795/3145 [11:19<09:07,  2.47it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1781/3145 [11:22<09:30,  2.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1803/3143 [11:20<08:51,  2.52it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1796/3145 [11:33<09:36,  2.34it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1796/3145 [11:20<08:55,  2.52it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1782/3145 [11:22<09:16,  2.45it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1797/3145 [11:33<09:20,  2.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1797/3145 [11:20<08:41,  2.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1804/3143 [11:20<09:24,  2.37it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1783/3145 [11:23<09:18,  2.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1798/3145 [11:33<09:11,  2.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1798/3145 [11:21<08:36,  2.61it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1805/3143 [11:21<09:28,  2.35it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1784/3145 [11:23<09:06,  2.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1799/3145 [11:34<08:45,  2.56it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1799/3145 [11:21<09:01,  2.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1806/3143 [11:21<09:15,  2.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1785/3145 [11:24<08:58,  2.53it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1800/3145 [11:34<08:32,  2.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1786/3145 [11:24<07:59,  2.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1807/3143 [11:21<09:14,  2.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1800/3145 [11:21<09:28,  2.37it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1801/3145 [11:34<07:40,  2.92it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1787/3145 [11:24<08:25,  2.69it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1808/3143 [11:22<09:06,  2.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1801/3145 [11:22<09:28,  2.36it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1802/3145 [11:35<08:17,  2.70it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1809/3143 [11:22<08:56,  2.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1802/3145 [11:22<09:30,  2.35it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1788/3145 [11:25<09:24,  2.40it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1803/3145 [11:35<08:32,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1810/3143 [11:23<08:46,  2.53it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1789/3145 [11:25<09:16,  2.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1804/3145 [11:36<08:48,  2.54it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1803/3145 [11:23<09:42,  2.31it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1811/3143 [11:23<08:37,  2.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1790/3145 [11:26<09:21,  2.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1805/3145 [11:36<09:00,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1804/3145 [11:23<09:30,  2.35it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1812/3143 [11:23<08:49,  2.51it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1805/3145 [11:24<09:19,  2.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1791/3145 [11:26<09:41,  2.33it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1806/3145 [11:36<09:26,  2.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1813/3143 [11:24<08:51,  2.50it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1792/3145 [11:27<09:26,  2.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1806/3145 [11:24<09:35,  2.33it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1807/3145 [11:37<09:39,  2.31it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1814/3143 [11:24<08:42,  2.54it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1793/3145 [11:27<09:23,  2.40it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1807/3145 [11:24<09:21,  2.38it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1808/3145 [11:37<09:11,  2.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1815/3143 [11:25<08:51,  2.50it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1794/3145 [11:27<09:07,  2.47it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1809/3145 [11:38<09:03,  2.46it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1816/3143 [11:25<08:39,  2.56it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1808/3145 [11:25<10:11,  2.19it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1795/3145 [11:28<08:54,  2.53it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1810/3145 [11:38<09:01,  2.47it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1817/3143 [11:25<08:38,  2.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1809/3145 [11:25<10:20,  2.15it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1796/3145 [11:28<09:11,  2.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1811/3145 [11:38<08:59,  2.47it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1818/3143 [11:26<08:57,  2.46it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1810/3145 [11:26<10:41,  2.08it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1797/3145 [11:29<09:02,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1812/3145 [11:39<09:06,  2.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1819/3143 [11:26<08:46,  2.52it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1798/3145 [11:29<08:58,  2.50it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1811/3145 [11:26<10:41,  2.08it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1813/3145 [11:39<08:56,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1820/3143 [11:27<08:49,  2.50it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1799/3145 [11:29<09:01,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1814/3145 [11:40<08:40,  2.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1812/3145 [11:27<10:25,  2.13it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1821/3143 [11:27<08:57,  2.46it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1815/3145 [11:40<08:34,  2.59it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1800/3145 [11:30<09:15,  2.42it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1813/3145 [11:27<10:03,  2.21it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1822/3143 [11:27<08:36,  2.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1816/3145 [11:40<08:42,  2.55it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1801/3145 [11:30<09:12,  2.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1823/3143 [11:28<08:54,  2.47it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1814/3145 [11:28<10:14,  2.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1802/3145 [11:30<08:23,  2.67it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1817/3145 [11:41<08:59,  2.46it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1815/3145 [11:28<09:55,  2.23it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1824/3143 [11:28<09:11,  2.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1803/3145 [11:31<08:41,  2.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1818/3145 [11:41<09:26,  2.34it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1816/3145 [11:29<09:43,  2.28it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1825/3143 [11:29<09:18,  2.36it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1804/3145 [11:31<08:27,  2.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1819/3145 [11:42<09:05,  2.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1826/3143 [11:29<08:50,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1817/3145 [11:29<09:33,  2.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1805/3145 [11:32<08:46,  2.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1820/3145 [11:42<09:09,  2.41it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1818/3145 [11:29<09:03,  2.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1827/3143 [11:29<08:57,  2.45it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1806/3145 [11:32<08:22,  2.67it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1821/3145 [11:42<08:40,  2.55it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1807/3145 [11:32<07:31,  2.96it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1819/3145 [11:30<08:54,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1828/3143 [11:30<09:00,  2.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1822/3145 [11:43<08:27,  2.61it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1808/3145 [11:33<07:50,  2.84it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1820/3145 [11:30<08:47,  2.51it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1829/3143 [11:30<08:38,  2.53it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1823/3145 [11:43<08:19,  2.65it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1809/3145 [11:33<08:08,  2.73it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1830/3143 [11:31<08:49,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1821/3145 [11:31<09:20,  2.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1824/3145 [11:44<08:15,  2.67it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1810/3145 [11:33<08:05,  2.75it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1822/3145 [11:31<08:48,  2.50it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1831/3143 [11:31<08:57,  2.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1825/3145 [11:44<08:08,  2.70it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1811/3145 [11:34<08:29,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1823/3145 [11:31<08:45,  2.51it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1832/3143 [11:31<08:41,  2.51it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1826/3145 [11:44<08:02,  2.73it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1812/3145 [11:34<08:25,  2.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1824/3145 [11:32<08:31,  2.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1833/3143 [11:32<08:44,  2.50it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1827/3145 [11:45<08:09,  2.69it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1813/3145 [11:34<07:45,  2.86it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1825/3145 [11:32<08:20,  2.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1828/3145 [11:45<07:32,  2.91it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1834/3143 [11:32<08:29,  2.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1814/3145 [11:35<08:02,  2.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1826/3145 [11:32<08:11,  2.68it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1829/3145 [11:45<07:37,  2.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1835/3143 [11:33<08:21,  2.61it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1815/3145 [11:35<08:02,  2.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1827/3145 [11:33<08:18,  2.65it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1836/3143 [11:33<08:00,  2.72it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1830/3145 [11:46<08:34,  2.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1816/3145 [11:36<08:55,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1828/3145 [11:33<08:21,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1837/3143 [11:33<08:16,  2.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1831/3145 [11:46<08:36,  2.54it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1817/3145 [11:36<08:39,  2.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1829/3145 [11:34<08:14,  2.66it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1838/3143 [11:34<08:09,  2.67it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1832/3145 [11:47<08:48,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1818/3145 [11:37<08:49,  2.51it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1830/3145 [11:34<08:06,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1839/3143 [11:34<08:20,  2.61it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1833/3145 [11:47<08:29,  2.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1819/3145 [11:37<08:26,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1831/3145 [11:34<08:33,  2.56it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1840/3143 [11:35<08:20,  2.61it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1834/3145 [11:47<08:29,  2.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1820/3145 [11:37<08:38,  2.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1832/3145 [11:35<08:21,  2.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1841/3143 [11:35<08:16,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1835/3145 [11:48<08:17,  2.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1833/3145 [11:35<07:32,  2.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1821/3145 [11:38<08:27,  2.61it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1842/3143 [11:35<07:57,  2.73it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1836/3145 [11:48<07:52,  2.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1834/3145 [11:35<07:01,  3.11it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1822/3145 [11:38<08:17,  2.66it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1843/3143 [11:36<07:52,  2.75it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1837/3145 [11:48<07:31,  2.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1835/3145 [11:36<07:06,  3.07it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1823/3145 [11:38<08:38,  2.55it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1844/3143 [11:36<07:45,  2.79it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1838/3145 [11:49<07:52,  2.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1836/3145 [11:36<07:18,  2.99it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1845/3143 [11:36<07:50,  2.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1839/3145 [11:49<08:03,  2.70it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1824/3145 [11:39<09:12,  2.39it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1837/3145 [11:36<07:44,  2.82it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1846/3143 [11:37<07:58,  2.71it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1840/3145 [11:50<07:59,  2.72it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1825/3145 [11:39<08:40,  2.54it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1838/3145 [11:37<07:56,  2.74it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1826/3145 [11:39<07:36,  2.89it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1847/3143 [11:37<07:52,  2.74it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1841/3145 [11:50<08:10,  2.66it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1839/3145 [11:37<07:51,  2.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1827/3145 [11:40<08:07,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1848/3143 [11:37<08:00,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1840/3145 [11:38<07:51,  2.77it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1842/3145 [11:50<08:28,  2.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1828/3145 [11:40<08:04,  2.72it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1849/3143 [11:38<08:04,  2.67it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1841/3145 [11:38<07:51,  2.77it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1843/3145 [11:51<08:42,  2.49it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1829/3145 [11:41<07:30,  2.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1850/3143 [11:38<08:14,  2.61it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1842/3145 [11:38<07:54,  2.74it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1844/3145 [11:51<08:28,  2.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1830/3145 [11:41<07:51,  2.79it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1851/3143 [11:39<08:15,  2.61it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1843/3145 [11:39<07:49,  2.77it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1845/3145 [11:52<08:25,  2.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1831/3145 [11:41<08:07,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1852/3143 [11:39<08:20,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1844/3145 [11:39<07:51,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1846/3145 [11:52<08:24,  2.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1832/3145 [11:42<08:14,  2.66it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1845/3145 [11:39<07:56,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1853/3143 [11:39<08:31,  2.52it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1847/3145 [11:52<08:21,  2.59it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1833/3145 [11:42<08:18,  2.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1846/3145 [11:40<07:53,  2.74it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1854/3143 [11:40<08:11,  2.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1834/3145 [11:42<07:19,  2.98it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1848/3145 [11:53<08:22,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1847/3145 [11:40<08:04,  2.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1855/3143 [11:40<08:13,  2.61it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1835/3145 [11:43<07:28,  2.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1849/3145 [11:53<08:22,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1848/3145 [11:40<07:59,  2.71it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1836/3145 [11:43<07:33,  2.89it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1856/3143 [11:41<08:59,  2.39it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1850/3145 [11:53<08:38,  2.50it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1849/3145 [11:41<07:46,  2.78it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1837/3145 [11:43<07:54,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1857/3143 [11:41<08:51,  2.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1851/3145 [11:54<08:21,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1850/3145 [11:41<07:47,  2.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1838/3145 [11:44<08:03,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1858/3143 [11:41<08:41,  2.46it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1852/3145 [11:54<08:25,  2.56it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1851/3145 [11:42<07:57,  2.71it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1839/3145 [11:44<07:22,  2.95it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1853/3145 [11:55<08:01,  2.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1859/3143 [11:42<08:45,  2.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1852/3145 [11:42<08:07,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1840/3145 [11:45<07:43,  2.81it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1860/3143 [11:42<07:36,  2.81it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1854/3145 [11:55<07:57,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1853/3145 [11:42<08:13,  2.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1841/3145 [11:45<08:00,  2.72it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1861/3143 [11:42<08:03,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1855/3145 [11:55<08:05,  2.66it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1842/3145 [11:45<07:14,  3.00it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1854/3145 [11:43<08:07,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1862/3143 [11:43<08:03,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1856/3145 [11:56<08:13,  2.61it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1843/3145 [11:46<07:37,  2.85it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1855/3145 [11:43<08:00,  2.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1863/3143 [11:43<07:15,  2.94it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1857/3145 [11:56<07:52,  2.72it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1856/3145 [11:43<07:46,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1844/3145 [11:46<07:51,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1864/3143 [11:43<07:15,  2.94it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1858/3145 [11:56<07:50,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1845/3145 [11:46<08:01,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1865/3143 [11:44<07:20,  2.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1857/3145 [11:44<08:23,  2.56it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1859/3145 [11:57<07:38,  2.80it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1866/3143 [11:44<07:27,  2.86it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1846/3145 [11:47<08:06,  2.67it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1858/3145 [11:44<08:10,  2.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1860/3145 [11:57<07:37,  2.81it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1847/3145 [11:47<08:02,  2.69it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1867/3143 [11:45<07:44,  2.74it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1859/3145 [11:45<08:17,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1861/3145 [11:57<07:34,  2.83it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1868/3143 [11:45<07:42,  2.75it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1860/3145 [11:45<08:17,  2.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1848/3145 [11:48<08:39,  2.49it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1862/3145 [11:58<07:46,  2.75it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1869/3143 [11:45<07:40,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1849/3145 [11:48<08:14,  2.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1861/3145 [11:45<08:23,  2.55it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1863/3145 [11:58<07:57,  2.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1870/3143 [11:46<06:46,  3.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1871/3143 [11:46<06:12,  3.41it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1862/3145 [11:46<08:12,  2.60it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1850/3145 [11:48<08:31,  2.53it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1864/3145 [11:59<07:50,  2.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1872/3143 [11:46<06:38,  3.19it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1863/3145 [11:46<08:04,  2.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1865/3145 [11:59<07:43,  2.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1851/3145 [11:49<08:36,  2.50it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1852/3145 [11:49<07:53,  2.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1873/3143 [11:47<07:06,  2.98it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1866/3145 [11:59<08:01,  2.66it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1864/3145 [11:47<08:23,  2.54it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1853/3145 [11:49<07:52,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1867/3145 [12:00<07:46,  2.74it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1874/3143 [11:47<07:47,  2.71it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1865/3145 [11:47<08:13,  2.59it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1854/3145 [11:50<07:23,  2.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1868/3145 [12:00<07:11,  2.96it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1875/3143 [11:47<07:44,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1869/3145 [12:00<06:44,  3.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1866/3145 [11:47<09:07,  2.34it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1876/3143 [11:48<07:08,  2.96it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1855/3145 [11:50<07:54,  2.72it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1870/3145 [12:01<06:38,  3.20it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1867/3145 [11:48<09:05,  2.34it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1856/3145 [11:50<07:42,  2.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1877/3143 [11:48<07:56,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1871/3145 [12:01<07:14,  2.94it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1868/3145 [11:48<08:37,  2.47it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1857/3145 [11:51<07:42,  2.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1878/3143 [11:48<07:48,  2.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1872/3145 [12:01<06:49,  3.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1869/3145 [11:49<08:16,  2.57it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1858/3145 [11:51<07:58,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1879/3143 [11:49<07:58,  2.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1873/3145 [12:02<07:30,  2.82it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1870/3145 [11:49<07:55,  2.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1859/3145 [11:52<07:49,  2.74it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1880/3143 [11:49<07:59,  2.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1874/3145 [12:02<07:45,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1871/3145 [11:49<07:52,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1860/3145 [11:52<07:56,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1875/3145 [12:02<07:08,  2.96it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1881/3143 [11:50<08:03,  2.61it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1872/3145 [11:50<07:46,  2.73it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1861/3145 [11:52<07:55,  2.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1876/3145 [12:03<06:50,  3.09it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1882/3143 [11:50<07:52,  2.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1873/3145 [11:50<07:44,  2.74it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1877/3145 [12:03<06:21,  3.32it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1862/3145 [11:53<07:53,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1883/3143 [11:50<07:36,  2.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1874/3145 [11:50<07:38,  2.77it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1863/3145 [11:53<07:01,  3.04it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1878/3145 [12:03<06:43,  3.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1884/3143 [11:51<07:25,  2.83it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1864/3145 [11:53<07:15,  2.94it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1879/3145 [12:04<06:47,  3.10it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1875/3145 [11:51<08:10,  2.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1885/3143 [11:51<07:17,  2.88it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1865/3145 [11:54<07:25,  2.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1880/3145 [12:04<07:10,  2.94it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1876/3145 [11:51<07:51,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1886/3143 [11:51<07:10,  2.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1866/3145 [11:54<07:26,  2.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1877/3145 [11:52<07:57,  2.65it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1887/3143 [11:52<07:25,  2.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1881/3145 [12:04<08:11,  2.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1878/3145 [11:52<07:49,  2.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1867/3145 [11:54<08:14,  2.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1882/3145 [12:05<07:56,  2.65it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1888/3143 [11:52<07:39,  2.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1879/3145 [11:52<08:08,  2.59it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1868/3145 [11:55<08:45,  2.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1889/3143 [11:53<08:15,  2.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1883/3145 [12:05<09:07,  2.31it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1880/3145 [11:53<07:57,  2.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1869/3145 [11:55<08:30,  2.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1890/3143 [11:53<08:27,  2.47it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1881/3145 [11:53<07:50,  2.68it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1884/3145 [12:06<09:40,  2.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1870/3145 [11:56<08:15,  2.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1891/3143 [11:53<08:41,  2.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1882/3145 [11:53<07:45,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1885/3145 [12:06<08:55,  2.35it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1871/3145 [11:56<08:17,  2.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1883/3145 [11:54<07:24,  2.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1892/3143 [11:54<08:30,  2.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1886/3145 [12:07<08:43,  2.41it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1872/3145 [11:56<08:14,  2.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1884/3145 [11:54<07:18,  2.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1893/3143 [11:54<08:21,  2.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1887/3145 [12:07<08:33,  2.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1873/3145 [11:57<08:25,  2.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1885/3145 [11:54<07:13,  2.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1894/3143 [11:55<08:03,  2.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1888/3145 [12:07<08:01,  2.61it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1874/3145 [11:57<07:52,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1886/3145 [11:55<07:26,  2.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1895/3143 [11:55<08:14,  2.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1875/3145 [11:57<07:09,  2.95it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1889/3145 [12:08<08:44,  2.39it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1887/3145 [11:55<07:17,  2.88it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1896/3143 [11:55<07:49,  2.66it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1876/3145 [11:58<07:33,  2.80it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1888/3145 [11:55<07:31,  2.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1890/3145 [12:08<09:06,  2.30it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1897/3143 [11:56<07:42,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1877/3145 [11:58<07:33,  2.80it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1889/3145 [11:56<07:42,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1878/3145 [11:59<07:22,  2.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1891/3145 [12:09<09:30,  2.20it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1898/3143 [11:56<07:58,  2.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1890/3145 [11:56<08:13,  2.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1879/3145 [11:59<07:27,  2.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1892/3145 [12:09<09:01,  2.32it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1899/3143 [11:56<08:06,  2.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1891/3145 [11:57<08:02,  2.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1880/3145 [11:59<07:35,  2.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1900/3143 [11:57<08:14,  2.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1893/3145 [12:10<09:25,  2.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1892/3145 [11:57<07:46,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1881/3145 [12:00<07:50,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1894/3145 [12:10<08:36,  2.42it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1901/3143 [11:57<08:20,  2.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1893/3145 [11:57<07:40,  2.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1882/3145 [12:00<07:37,  2.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1895/3145 [12:10<08:15,  2.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1902/3143 [11:58<08:14,  2.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1894/3145 [11:58<07:49,  2.66it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1883/3145 [12:00<07:35,  2.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1896/3145 [12:11<08:01,  2.59it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1903/3143 [11:58<07:49,  2.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1884/3145 [12:01<07:24,  2.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1895/3145 [11:58<07:53,  2.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1897/3145 [12:11<08:05,  2.57it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1904/3143 [11:58<07:39,  2.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1896/3145 [11:59<07:47,  2.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1885/3145 [12:01<07:30,  2.80it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1898/3145 [12:11<07:54,  2.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1905/3143 [11:59<07:37,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1897/3145 [11:59<07:40,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1886/3145 [12:01<07:35,  2.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1899/3145 [12:12<07:46,  2.67it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1906/3143 [11:59<07:36,  2.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1898/3145 [11:59<07:27,  2.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1887/3145 [12:02<07:49,  2.68it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1907/3143 [11:59<07:32,  2.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1900/3145 [12:12<08:04,  2.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1888/3145 [12:02<06:50,  3.06it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1899/3145 [12:00<07:36,  2.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1908/3143 [12:00<07:27,  2.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1889/3145 [12:02<07:02,  2.97it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1901/3145 [12:13<08:19,  2.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1900/3145 [12:00<07:32,  2.75it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1909/3143 [12:00<07:25,  2.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1890/3145 [12:03<07:12,  2.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1902/3145 [12:13<08:08,  2.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1901/3145 [12:00<07:41,  2.69it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1910/3143 [12:01<07:34,  2.71it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1903/3145 [12:13<08:05,  2.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1891/3145 [12:03<07:41,  2.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1902/3145 [12:01<07:46,  2.66it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1892/3145 [12:03<06:45,  3.09it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1911/3143 [12:01<07:56,  2.58it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1904/3145 [12:14<08:11,  2.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1903/3145 [12:01<07:40,  2.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1893/3145 [12:04<07:08,  2.92it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1912/3143 [12:01<08:06,  2.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1905/3145 [12:14<07:58,  2.59it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1904/3145 [12:01<07:36,  2.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1894/3145 [12:04<07:22,  2.83it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1913/3143 [12:02<08:15,  2.48it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1905/3145 [12:02<07:26,  2.78it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1906/3145 [12:15<08:09,  2.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1895/3145 [12:05<07:45,  2.69it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1914/3143 [12:02<08:07,  2.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1906/3145 [12:02<07:34,  2.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1907/3145 [12:15<08:19,  2.48it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1915/3143 [12:02<07:02,  2.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1896/3145 [12:05<07:50,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1907/3145 [12:03<07:43,  2.67it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1908/3145 [12:15<08:03,  2.56it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1916/3143 [12:03<06:53,  2.96it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1897/3145 [12:05<07:49,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1909/3145 [12:16<08:04,  2.55it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1908/3145 [12:03<08:09,  2.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1917/3143 [12:03<07:13,  2.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1898/3145 [12:06<07:53,  2.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1909/3145 [12:03<07:10,  2.87it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1918/3143 [12:03<07:10,  2.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1910/3145 [12:16<08:15,  2.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1899/3145 [12:06<07:44,  2.68it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1910/3145 [12:04<07:14,  2.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1919/3143 [12:04<07:20,  2.78it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1911/3145 [12:17<08:16,  2.49it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1911/3145 [12:04<07:11,  2.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1900/3145 [12:07<08:02,  2.58it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1912/3145 [12:17<08:08,  2.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1920/3143 [12:04<07:39,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1912/3145 [12:04<07:15,  2.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1901/3145 [12:07<08:23,  2.47it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1921/3143 [12:05<07:18,  2.79it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1913/3145 [12:17<08:10,  2.51it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1913/3145 [12:05<07:43,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1922/3143 [12:05<06:27,  3.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1902/3145 [12:07<08:29,  2.44it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1914/3145 [12:18<08:15,  2.48it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1914/3145 [12:05<07:43,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1923/3143 [12:05<06:53,  2.95it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1903/3145 [12:08<08:24,  2.46it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1915/3145 [12:18<07:57,  2.57it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1915/3145 [12:05<07:44,  2.65it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1924/3143 [12:06<06:59,  2.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1904/3145 [12:08<08:42,  2.37it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1916/3145 [12:19<07:48,  2.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1925/3143 [12:06<06:55,  2.93it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1916/3145 [12:06<08:02,  2.55it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1917/3145 [12:19<06:53,  2.97it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1905/3145 [12:09<08:54,  2.32it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1926/3143 [12:06<07:01,  2.88it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1918/3145 [12:19<06:58,  2.93it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1917/3145 [12:06<08:28,  2.41it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1906/3145 [12:09<08:38,  2.39it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1927/3143 [12:07<07:16,  2.79it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1919/3145 [12:20<07:15,  2.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1918/3145 [12:07<08:32,  2.39it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1907/3145 [12:09<08:08,  2.54it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1928/3143 [12:07<07:24,  2.74it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1920/3145 [12:20<07:15,  2.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1908/3145 [12:10<07:46,  2.65it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1919/3145 [12:07<08:46,  2.33it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1929/3143 [12:07<07:41,  2.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1921/3145 [12:20<07:15,  2.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1909/3145 [12:10<07:51,  2.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1920/3145 [12:08<08:37,  2.37it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1930/3143 [12:08<07:42,  2.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1922/3145 [12:21<07:23,  2.76it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1910/3145 [12:11<07:53,  2.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1921/3145 [12:08<08:29,  2.40it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1931/3143 [12:08<07:34,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1923/3145 [12:21<07:21,  2.77it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1911/3145 [12:11<07:43,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1932/3143 [12:09<07:28,  2.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1922/3145 [12:08<08:26,  2.42it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1924/3145 [12:21<07:13,  2.82it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1912/3145 [12:11<07:46,  2.65it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1923/3145 [12:09<08:00,  2.54it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3143 [12:09<07:25,  2.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1925/3145 [12:22<07:38,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1913/3145 [12:12<07:34,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1934/3143 [12:09<06:53,  2.92it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1924/3145 [12:09<08:02,  2.53it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1926/3145 [12:22<07:44,  2.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1935/3143 [12:10<06:58,  2.89it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1914/3145 [12:12<07:42,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1927/3145 [12:22<07:26,  2.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1925/3145 [12:10<08:40,  2.34it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1915/3145 [12:12<07:34,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1936/3143 [12:10<07:02,  2.86it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1928/3145 [12:23<06:38,  3.06it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1926/3145 [12:10<08:05,  2.51it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1929/3145 [12:23<06:00,  3.37it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1937/3143 [12:10<06:56,  2.89it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1916/3145 [12:13<07:44,  2.65it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1927/3145 [12:10<07:50,  2.59it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1930/3145 [12:23<06:31,  3.10it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1938/3143 [12:11<06:54,  2.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1917/3145 [12:13<07:34,  2.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1928/3145 [12:11<07:18,  2.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1939/3143 [12:11<06:59,  2.87it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1931/3145 [12:24<06:54,  2.93it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1918/3145 [12:14<07:40,  2.66it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1929/3145 [12:11<06:49,  2.97it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1940/3143 [12:11<07:00,  2.86it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1930/3145 [12:11<06:26,  3.15it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1919/3145 [12:14<07:26,  2.75it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1932/3145 [12:24<07:33,  2.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1941/3143 [12:12<06:24,  3.13it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1931/3145 [12:12<05:53,  3.44it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1920/3145 [12:14<07:33,  2.70it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3145 [12:25<07:38,  2.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1942/3143 [12:12<06:32,  3.06it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1932/3145 [12:12<06:20,  3.19it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1934/3145 [12:25<07:41,  2.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1943/3143 [12:12<06:34,  3.04it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1921/3145 [12:15<08:16,  2.46it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3145 [12:12<06:40,  3.02it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1934/3145 [12:12<06:00,  3.36it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1935/3145 [12:25<07:31,  2.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1944/3143 [12:13<06:44,  2.96it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1922/3145 [12:15<08:43,  2.34it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1935/3145 [12:13<06:20,  3.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1936/3145 [12:26<07:25,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1945/3143 [12:13<06:51,  2.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1923/3145 [12:16<08:21,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1936/3145 [12:13<06:39,  3.03it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1937/3145 [12:26<07:34,  2.66it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1946/3143 [12:13<07:06,  2.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1924/3145 [12:16<08:14,  2.47it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1938/3145 [12:26<07:17,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1937/3145 [12:14<06:53,  2.92it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1947/3143 [12:14<07:16,  2.74it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1925/3145 [12:16<07:51,  2.59it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1938/3145 [12:14<06:45,  2.98it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1939/3145 [12:27<07:10,  2.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1948/3143 [12:14<07:07,  2.80it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1926/3145 [12:17<07:44,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1940/3145 [12:27<07:09,  2.81it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1939/3145 [12:14<07:00,  2.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1949/3143 [12:14<06:50,  2.91it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1940/3145 [12:14<06:20,  3.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1927/3145 [12:17<07:35,  2.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1941/3145 [12:27<07:10,  2.79it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1950/3143 [12:15<06:40,  2.98it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1928/3145 [12:17<07:20,  2.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1941/3145 [12:15<06:51,  2.93it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1951/3143 [12:15<06:38,  2.99it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1942/3145 [12:28<07:22,  2.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1929/3145 [12:18<07:31,  2.69it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1942/3145 [12:15<06:58,  2.88it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1952/3143 [12:15<06:40,  2.97it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1943/3145 [12:28<07:08,  2.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1943/3145 [12:16<06:26,  3.11it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1930/3145 [12:18<07:26,  2.72it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1953/3143 [12:16<06:43,  2.95it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1944/3145 [12:28<06:59,  2.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1944/3145 [12:16<07:02,  2.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1931/3145 [12:18<07:24,  2.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1945/3145 [12:29<06:55,  2.89it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1954/3143 [12:16<06:59,  2.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1932/3145 [12:19<07:16,  2.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1945/3145 [12:16<07:14,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1955/3143 [12:16<06:55,  2.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1946/3145 [12:29<07:01,  2.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3145 [12:19<07:16,  2.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1946/3145 [12:17<07:04,  2.83it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1947/3145 [12:30<06:57,  2.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1956/3143 [12:17<06:59,  2.83it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1934/3145 [12:20<07:14,  2.79it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1947/3145 [12:17<07:09,  2.79it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1948/3145 [12:30<06:58,  2.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1957/3143 [12:17<07:09,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1935/3145 [12:20<07:16,  2.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1949/3145 [12:30<06:53,  2.89it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1948/3145 [12:17<07:12,  2.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1958/3143 [12:18<07:16,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1950/3145 [12:31<06:57,  2.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1936/3145 [12:20<07:25,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1949/3145 [12:18<07:20,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1959/3143 [12:18<06:58,  2.83it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1951/3145 [12:31<07:03,  2.82it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1937/3145 [12:21<07:30,  2.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1950/3145 [12:18<07:18,  2.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1960/3143 [12:18<07:25,  2.65it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1952/3145 [12:31<07:08,  2.79it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1938/3145 [12:21<07:38,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1951/3145 [12:19<07:43,  2.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1961/3143 [12:19<07:29,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1953/3145 [12:32<07:19,  2.72it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1939/3145 [12:21<07:31,  2.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1952/3145 [12:19<07:42,  2.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1962/3143 [12:19<07:30,  2.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1954/3145 [12:32<07:04,  2.81it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1940/3145 [12:22<07:37,  2.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1963/3143 [12:19<07:23,  2.66it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1953/3145 [12:19<07:45,  2.56it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1955/3145 [12:32<06:57,  2.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1941/3145 [12:22<07:35,  2.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1954/3145 [12:20<07:42,  2.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1964/3143 [12:20<07:28,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1956/3145 [12:33<06:49,  2.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1942/3145 [12:23<07:27,  2.69it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1965/3143 [12:20<07:43,  2.54it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1955/3145 [12:20<08:09,  2.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1957/3145 [12:33<07:03,  2.81it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1943/3145 [12:23<07:23,  2.71it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1966/3143 [12:21<07:29,  2.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1958/3145 [12:33<06:49,  2.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1956/3145 [12:21<08:04,  2.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1944/3145 [12:23<07:32,  2.65it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1967/3143 [12:21<07:25,  2.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1959/3145 [12:34<07:28,  2.65it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1957/3145 [12:21<08:14,  2.40it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1945/3145 [12:24<07:29,  2.67it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1968/3143 [12:21<07:10,  2.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1960/3145 [12:34<07:10,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1958/3145 [12:21<08:05,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1946/3145 [12:24<07:25,  2.69it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1961/3145 [12:34<06:25,  3.07it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1969/3143 [12:22<07:18,  2.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1959/3145 [12:22<07:57,  2.48it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1962/3145 [12:35<06:36,  2.99it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1947/3145 [12:25<07:58,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1970/3143 [12:22<07:13,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1960/3145 [12:22<06:56,  2.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1948/3145 [12:25<07:34,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1963/3145 [12:35<06:56,  2.84it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1961/3145 [12:22<06:55,  2.85it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1971/3143 [12:22<07:36,  2.57it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1964/3145 [12:35<06:36,  2.98it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1972/3143 [12:23<06:38,  2.94it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1949/3145 [12:25<07:42,  2.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1962/3145 [12:23<07:23,  2.66it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1965/3145 [12:36<06:43,  2.93it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1973/3143 [12:23<06:49,  2.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1950/3145 [12:26<07:42,  2.59it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1963/3145 [12:23<07:19,  2.69it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1966/3145 [12:36<06:38,  2.96it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1974/3143 [12:23<07:02,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1951/3145 [12:26<07:41,  2.59it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1964/3145 [12:24<07:14,  2.72it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1967/3145 [12:36<06:36,  2.97it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1975/3143 [12:24<07:03,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1952/3145 [12:26<07:58,  2.49it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1965/3145 [12:24<07:24,  2.66it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1968/3145 [12:37<06:42,  2.93it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1976/3143 [12:24<07:15,  2.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1953/3145 [12:27<07:35,  2.62it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1966/3145 [12:24<07:38,  2.57it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1969/3145 [12:37<06:59,  2.81it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1977/3143 [12:25<07:30,  2.59it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1970/3145 [12:38<06:40,  2.93it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1954/3145 [12:27<08:16,  2.40it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1967/3145 [12:25<07:38,  2.57it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1978/3143 [12:25<07:47,  2.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1971/3145 [12:38<06:35,  2.97it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1968/3145 [12:25<07:25,  2.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1955/3145 [12:28<08:05,  2.45it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1972/3145 [12:38<06:53,  2.84it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1979/3143 [12:26<08:10,  2.37it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1956/3145 [12:28<07:46,  2.55it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1969/3145 [12:26<07:30,  2.61it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1973/3145 [12:39<07:06,  2.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1957/3145 [12:28<07:46,  2.55it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1970/3145 [12:26<07:48,  2.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1980/3143 [12:26<08:24,  2.30it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1974/3145 [12:39<07:11,  2.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1958/3145 [12:29<07:34,  2.61it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1981/3143 [12:26<08:04,  2.40it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1971/3145 [12:26<07:50,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1975/3145 [12:39<07:01,  2.77it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1982/3143 [12:27<07:02,  2.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1959/3145 [12:29<07:26,  2.66it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1972/3145 [12:27<07:06,  2.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1983/3143 [12:27<06:25,  3.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1976/3145 [12:40<07:08,  2.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1960/3145 [12:30<07:19,  2.70it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1973/3145 [12:27<07:12,  2.71it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1984/3143 [12:27<06:17,  3.07it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1977/3145 [12:40<07:03,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1961/3145 [12:30<07:08,  2.76it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1974/3145 [12:27<07:09,  2.73it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1985/3143 [12:28<06:39,  2.90it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1978/3145 [12:40<07:02,  2.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1962/3145 [12:30<07:10,  2.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1975/3145 [12:28<07:01,  2.77it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1986/3143 [12:28<06:54,  2.79it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1979/3145 [12:41<06:59,  2.78it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1976/3145 [12:28<06:42,  2.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1963/3145 [12:31<07:09,  2.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1987/3143 [12:28<06:45,  2.85it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1977/3145 [12:28<06:11,  3.14it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1980/3145 [12:41<06:46,  2.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1964/3145 [12:31<07:20,  2.68it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1981/3145 [12:41<06:26,  3.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1988/3143 [12:29<06:57,  2.76it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1978/3145 [12:29<06:27,  3.01it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1965/3145 [12:31<07:11,  2.73it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1982/3145 [12:42<06:33,  2.96it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1989/3143 [12:29<07:02,  2.73it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1979/3145 [12:29<07:13,  2.69it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1966/3145 [12:32<07:18,  2.69it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1983/3145 [12:42<06:39,  2.91it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1990/3143 [12:29<07:10,  2.68it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1980/3145 [12:30<07:20,  2.64it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1967/3145 [12:32<07:36,  2.58it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1984/3145 [12:42<06:42,  2.88it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1981/3145 [12:30<07:02,  2.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1991/3143 [12:30<07:40,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1968/3145 [12:32<07:26,  2.64it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1985/3145 [12:43<06:45,  2.86it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1982/3145 [12:30<06:51,  2.82it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1969/3145 [12:33<06:30,  3.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1992/3143 [12:30<07:49,  2.45it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1986/3145 [12:43<06:49,  2.83it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1970/3145 [12:33<06:32,  2.99it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1983/3145 [12:31<07:03,  2.74it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1987/3145 [12:44<06:44,  2.86it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1971/3145 [12:33<05:52,  3.33it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1993/3143 [12:31<08:15,  2.32it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1984/3145 [12:31<06:59,  2.77it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1988/3145 [12:44<06:38,  2.90it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1994/3143 [12:31<07:58,  2.40it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1972/3145 [12:34<06:48,  2.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1985/3145 [12:31<07:00,  2.76it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1989/3145 [12:44<06:54,  2.79it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1995/3143 [12:32<07:38,  2.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1973/3145 [12:34<06:43,  2.91it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1986/3145 [12:32<07:02,  2.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1990/3145 [12:45<07:40,  2.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1974/3145 [12:34<07:11,  2.71it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1996/3143 [12:32<07:51,  2.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1987/3145 [12:32<07:02,  2.74it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1975/3145 [12:35<06:33,  2.97it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1997/3143 [12:32<07:38,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1988/3145 [12:32<06:53,  2.80it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1991/3145 [12:45<08:36,  2.24it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1976/3145 [12:35<06:36,  2.95it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1998/3143 [12:33<07:31,  2.54it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1989/3145 [12:33<07:04,  2.73it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1992/3145 [12:46<07:54,  2.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1977/3145 [12:35<06:56,  2.81it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1993/3145 [12:46<07:03,  2.72it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1999/3143 [12:33<07:29,  2.54it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1990/3145 [12:33<07:43,  2.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1978/3145 [12:36<07:08,  2.72it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2000/3143 [12:34<07:26,  2.56it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1994/3145 [12:46<07:15,  2.64it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1991/3145 [12:34<07:37,  2.52it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1979/3145 [12:36<07:06,  2.74it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1995/3145 [12:47<07:05,  2.70it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2001/3143 [12:34<07:37,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1992/3145 [12:34<07:57,  2.42it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1980/3145 [12:37<07:09,  2.71it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2002/3143 [12:34<07:32,  2.52it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1996/3145 [12:47<08:01,  2.39it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1981/3145 [12:37<07:05,  2.73it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1993/3145 [12:34<07:56,  2.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2003/3143 [12:35<07:29,  2.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1997/3145 [12:48<07:46,  2.46it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1994/3145 [12:35<07:37,  2.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1982/3145 [12:37<07:21,  2.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2004/3143 [12:35<07:34,  2.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1998/3145 [12:48<07:42,  2.48it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1983/3145 [12:38<07:18,  2.65it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1995/3145 [12:35<07:43,  2.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2005/3143 [12:36<07:34,  2.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1999/3145 [12:48<07:34,  2.52it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1996/3145 [12:36<07:39,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1984/3145 [12:38<07:35,  2.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2006/3143 [12:36<07:31,  2.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2000/3145 [12:49<07:48,  2.44it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1997/3145 [12:36<07:39,  2.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1985/3145 [12:39<07:32,  2.56it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2007/3143 [12:36<07:28,  2.53it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1998/3145 [12:36<07:30,  2.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2001/3145 [12:49<08:17,  2.30it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1986/3145 [12:39<07:48,  2.47it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2008/3143 [12:37<07:21,  2.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1999/3145 [12:37<06:43,  2.84it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2002/3145 [12:50<08:05,  2.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1987/3145 [12:39<07:59,  2.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2000/3145 [12:37<06:37,  2.88it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2009/3143 [12:37<07:42,  2.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2003/3145 [12:50<07:52,  2.41it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1988/3145 [12:40<07:38,  2.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2001/3145 [12:38<07:29,  2.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2010/3143 [12:38<07:42,  2.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2004/3145 [12:50<07:43,  2.46it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1989/3145 [12:40<07:47,  2.47it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2002/3145 [12:38<07:30,  2.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2011/3143 [12:38<07:59,  2.36it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2005/3145 [12:51<07:37,  2.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1990/3145 [12:41<08:23,  2.29it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2003/3145 [12:38<07:35,  2.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2006/3145 [12:51<07:20,  2.58it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2012/3143 [12:39<08:19,  2.26it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1991/3145 [12:41<08:05,  2.38it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2007/3145 [12:52<07:18,  2.59it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2004/3145 [12:39<07:57,  2.39it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1992/3145 [12:41<07:06,  2.70it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2013/3143 [12:39<08:41,  2.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1993/3145 [12:42<06:57,  2.76it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2008/3145 [12:52<07:39,  2.47it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2005/3145 [12:39<08:08,  2.34it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2014/3143 [12:39<08:33,  2.20it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2009/3145 [12:52<07:29,  2.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1994/3145 [12:42<07:27,  2.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2006/3145 [12:40<08:01,  2.37it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2015/3143 [12:40<08:11,  2.30it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2010/3145 [12:53<07:15,  2.61it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1995/3145 [12:43<07:40,  2.50it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2007/3145 [12:40<08:11,  2.31it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2016/3143 [12:40<07:56,  2.37it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2011/3145 [12:53<07:06,  2.66it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1996/3145 [12:43<07:23,  2.59it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2017/3143 [12:41<07:43,  2.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2008/3145 [12:41<08:41,  2.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2012/3145 [12:54<07:28,  2.52it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1997/3145 [12:43<07:30,  2.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2018/3143 [12:41<07:27,  2.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2009/3145 [12:41<08:29,  2.23it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2013/3145 [12:54<07:49,  2.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1998/3145 [12:44<07:50,  2.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2019/3143 [12:41<07:36,  2.46it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2010/3145 [12:42<08:33,  2.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2014/3145 [12:54<08:00,  2.36it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1999/3145 [12:44<07:49,  2.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2020/3143 [12:42<07:30,  2.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2011/3145 [12:42<08:25,  2.24it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2015/3145 [12:55<07:51,  2.40it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2000/3145 [12:45<07:35,  2.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2021/3143 [12:42<07:24,  2.53it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2016/3145 [12:55<07:27,  2.53it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2001/3145 [12:45<07:32,  2.53it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2012/3145 [12:43<09:05,  2.08it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2022/3143 [12:43<07:12,  2.59it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2017/3145 [12:56<07:26,  2.53it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2002/3145 [12:45<07:36,  2.50it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2013/3145 [12:43<08:44,  2.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2023/3143 [12:43<07:30,  2.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2018/3145 [12:56<07:43,  2.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2003/3145 [12:46<07:48,  2.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2014/3145 [12:43<08:29,  2.22it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2024/3143 [12:43<07:31,  2.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2019/3145 [12:56<07:32,  2.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2004/3145 [12:46<08:00,  2.37it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2015/3145 [12:44<08:14,  2.29it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2025/3143 [12:44<07:30,  2.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2020/3145 [12:57<07:07,  2.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2005/3145 [12:47<07:39,  2.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2026/3143 [12:44<07:18,  2.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2016/3145 [12:44<08:06,  2.32it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2021/3145 [12:57<07:00,  2.67it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2017/3145 [12:44<07:01,  2.68it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2027/3143 [12:45<07:06,  2.61it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2006/3145 [12:47<07:35,  2.50it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2022/3145 [12:58<07:13,  2.59it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2018/3145 [12:45<07:09,  2.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2007/3145 [12:47<07:31,  2.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2028/3143 [12:45<07:28,  2.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2023/3145 [12:58<07:24,  2.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2019/3145 [12:45<07:22,  2.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2008/3145 [12:48<07:31,  2.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2029/3143 [12:45<07:29,  2.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2024/3145 [12:58<07:33,  2.47it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2020/3145 [12:46<07:18,  2.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2009/3145 [12:48<07:18,  2.59it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2030/3143 [12:46<07:16,  2.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2025/3145 [12:59<07:20,  2.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2021/3145 [12:46<07:12,  2.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2010/3145 [12:49<07:33,  2.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2031/3143 [12:46<07:11,  2.58it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2026/3145 [12:59<07:06,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2032/3143 [12:46<06:27,  2.87it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2011/3145 [12:49<07:20,  2.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2022/3145 [12:47<08:08,  2.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2033/3143 [12:47<05:47,  3.19it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2027/3145 [12:59<07:00,  2.66it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2012/3145 [12:49<07:31,  2.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2023/3145 [12:47<07:59,  2.34it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2034/3143 [12:47<06:15,  2.95it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2028/3145 [13:00<07:09,  2.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2013/3145 [12:50<06:51,  2.75it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2024/3145 [12:47<07:34,  2.47it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2035/3143 [12:47<06:44,  2.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2029/3145 [13:00<07:12,  2.58it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2014/3145 [12:50<06:54,  2.73it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2025/3145 [12:48<07:23,  2.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2036/3143 [12:48<06:54,  2.67it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2030/3145 [13:01<07:08,  2.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2015/3145 [12:50<07:10,  2.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2026/3145 [12:48<07:02,  2.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2037/3143 [12:48<06:41,  2.75it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2031/3145 [13:01<06:58,  2.66it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2016/3145 [12:51<06:59,  2.69it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2027/3145 [12:48<06:45,  2.76it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2038/3143 [12:48<06:31,  2.82it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2032/3145 [13:01<06:55,  2.68it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2017/3145 [12:51<06:55,  2.72it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2028/3145 [12:49<06:43,  2.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2039/3143 [12:49<06:31,  2.82it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2033/3145 [13:02<06:52,  2.70it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2018/3145 [12:52<06:42,  2.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2029/3145 [12:49<06:56,  2.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2040/3143 [12:49<06:29,  2.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2034/3145 [13:02<06:40,  2.77it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2019/3145 [12:52<06:53,  2.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2030/3145 [12:49<06:52,  2.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2035/3145 [13:02<06:34,  2.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2041/3143 [12:50<07:00,  2.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2020/3145 [12:52<07:09,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2031/3145 [12:50<07:03,  2.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2036/3145 [13:03<06:22,  2.90it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2042/3143 [12:50<06:58,  2.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2021/3145 [12:53<07:12,  2.60it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2032/3145 [12:50<07:04,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2037/3145 [13:03<06:38,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2043/3143 [12:50<06:53,  2.66it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2022/3145 [12:53<07:25,  2.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2033/3145 [12:51<07:07,  2.60it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2038/3145 [13:03<06:29,  2.84it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2044/3143 [12:51<07:06,  2.58it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2023/3145 [12:54<07:18,  2.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2039/3145 [13:04<06:31,  2.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2045/3143 [12:51<06:18,  2.90it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2034/3145 [12:51<07:20,  2.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2024/3145 [12:54<07:03,  2.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2040/3145 [13:04<06:28,  2.84it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2046/3143 [12:51<06:16,  2.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2035/3145 [12:51<07:07,  2.59it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2025/3145 [12:54<06:56,  2.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2041/3145 [13:05<06:23,  2.88it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2047/3143 [12:52<06:35,  2.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2036/3145 [12:52<07:30,  2.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2042/3145 [13:05<06:29,  2.83it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2026/3145 [12:55<07:00,  2.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2048/3143 [12:52<06:31,  2.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2037/3145 [12:52<07:11,  2.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2043/3145 [13:05<06:23,  2.87it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2027/3145 [12:55<06:47,  2.75it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2049/3143 [12:53<06:35,  2.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2038/3145 [12:53<07:32,  2.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2044/3145 [13:06<06:22,  2.88it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2028/3145 [12:55<07:08,  2.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2050/3143 [12:53<06:46,  2.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2039/3145 [12:53<07:25,  2.48it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2045/3145 [13:06<06:28,  2.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2029/3145 [12:56<06:54,  2.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2051/3143 [12:53<06:41,  2.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2040/3145 [12:53<07:09,  2.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2046/3145 [13:06<06:39,  2.75it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2030/3145 [12:56<07:10,  2.59it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2052/3143 [12:54<06:57,  2.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2041/3145 [12:54<07:08,  2.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2047/3145 [13:07<06:34,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2053/3143 [12:54<06:48,  2.67it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2031/3145 [12:57<07:34,  2.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2048/3145 [13:07<05:48,  3.15it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2042/3145 [12:54<07:18,  2.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2054/3143 [12:54<06:42,  2.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2032/3145 [12:57<07:14,  2.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2049/3145 [13:07<06:01,  3.03it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2043/3145 [12:55<07:06,  2.59it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2055/3143 [12:55<06:27,  2.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2033/3145 [12:57<07:04,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2050/3145 [13:08<06:21,  2.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2044/3145 [12:55<07:20,  2.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2056/3143 [12:55<06:27,  2.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2034/3145 [12:58<06:57,  2.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2051/3145 [13:08<06:33,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2045/3145 [12:55<07:08,  2.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2057/3143 [12:55<06:36,  2.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2035/3145 [12:58<07:02,  2.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2052/3145 [13:08<06:35,  2.76it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2058/3143 [12:56<06:15,  2.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2046/3145 [12:56<07:01,  2.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2053/3145 [13:09<06:34,  2.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2036/3145 [12:59<07:26,  2.48it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2059/3143 [12:56<06:29,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2047/3145 [12:56<07:03,  2.59it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2037/3145 [12:59<07:22,  2.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2048/3145 [12:56<06:07,  2.98it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2060/3143 [12:56<06:19,  2.86it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2054/3145 [13:09<07:29,  2.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2049/3145 [12:57<06:07,  2.98it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2061/3143 [12:57<06:23,  2.82it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2038/3145 [12:59<07:56,  2.32it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2055/3145 [13:10<07:26,  2.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2050/3145 [12:57<06:35,  2.77it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2062/3143 [12:57<06:25,  2.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2039/3145 [13:00<07:41,  2.40it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2056/3145 [13:10<08:08,  2.23it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2063/3143 [12:58<06:27,  2.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2051/3145 [12:58<06:56,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2040/3145 [13:00<07:35,  2.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2052/3145 [12:58<07:01,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2064/3143 [12:58<06:49,  2.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2057/3145 [13:11<08:46,  2.07it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2041/3145 [13:01<07:25,  2.48it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2042/3145 [13:01<06:37,  2.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2053/3145 [12:58<06:52,  2.65it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2065/3143 [12:58<06:41,  2.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2058/3145 [13:11<08:48,  2.06it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2066/3143 [12:59<05:55,  3.03it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2043/3145 [13:01<06:34,  2.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2054/3145 [12:59<06:49,  2.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2059/3145 [13:12<07:56,  2.28it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2067/3143 [12:59<05:34,  3.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2044/3145 [13:02<06:35,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2055/3145 [12:59<06:48,  2.67it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2060/3145 [13:12<07:25,  2.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2068/3143 [12:59<05:59,  2.99it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2045/3145 [13:02<06:45,  2.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2056/3145 [12:59<06:51,  2.65it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2061/3145 [13:12<07:19,  2.47it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2069/3143 [13:00<06:16,  2.86it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2057/3145 [13:00<06:54,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2046/3145 [13:02<07:06,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2062/3145 [13:13<07:17,  2.47it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2070/3143 [13:00<06:16,  2.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2058/3145 [13:00<06:16,  2.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2047/3145 [13:03<06:59,  2.62it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2063/3145 [13:13<06:53,  2.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2059/3145 [13:00<05:37,  3.21it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2071/3143 [13:00<06:37,  2.70it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2048/3145 [13:03<06:52,  2.66it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2064/3145 [13:13<07:07,  2.53it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2060/3145 [13:01<06:15,  2.89it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2072/3143 [13:01<07:21,  2.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2049/3145 [13:03<06:49,  2.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2065/3145 [13:14<07:01,  2.56it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2061/3145 [13:01<06:20,  2.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2073/3143 [13:01<07:03,  2.53it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2050/3145 [13:04<06:43,  2.71it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2066/3145 [13:14<06:57,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2062/3145 [13:01<06:24,  2.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2074/3143 [13:02<06:49,  2.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2051/3145 [13:04<06:49,  2.67it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2067/3145 [13:15<06:51,  2.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2052/3145 [13:04<06:13,  2.93it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2075/3143 [13:02<06:53,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2063/3145 [13:02<07:34,  2.38it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2068/3145 [13:15<07:03,  2.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2053/3145 [13:05<06:18,  2.89it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2076/3143 [13:02<06:52,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2064/3145 [13:03<07:47,  2.31it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2069/3145 [13:15<07:11,  2.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2054/3145 [13:05<06:30,  2.79it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2077/3143 [13:03<06:43,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2065/3145 [13:03<07:27,  2.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2055/3145 [13:06<06:41,  2.71it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2070/3145 [13:16<07:19,  2.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2078/3143 [13:03<06:35,  2.70it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2066/3145 [13:03<07:22,  2.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2071/3145 [13:16<06:57,  2.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2056/3145 [13:06<06:48,  2.66it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2079/3143 [13:04<06:53,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2067/3145 [13:04<07:08,  2.52it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2072/3145 [13:17<06:42,  2.67it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2057/3145 [13:06<06:40,  2.71it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2080/3143 [13:04<06:44,  2.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2068/3145 [13:04<06:58,  2.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2073/3145 [13:17<06:25,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2058/3145 [13:07<06:28,  2.80it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2081/3143 [13:04<06:36,  2.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2069/3145 [13:04<07:11,  2.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2074/3145 [13:17<06:23,  2.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2059/3145 [13:07<06:33,  2.76it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2082/3143 [13:05<06:58,  2.54it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2070/3145 [13:05<07:08,  2.51it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2075/3145 [13:18<06:33,  2.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2060/3145 [13:07<06:32,  2.77it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2083/3143 [13:05<06:48,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2076/3145 [13:18<06:26,  2.76it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2071/3145 [13:05<06:53,  2.60it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2061/3145 [13:08<06:41,  2.70it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2084/3143 [13:05<06:40,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2077/3145 [13:18<06:28,  2.75it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2072/3145 [13:06<07:00,  2.55it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2062/3145 [13:08<06:41,  2.70it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2085/3143 [13:06<06:53,  2.56it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2078/3145 [13:19<06:34,  2.70it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2073/3145 [13:06<06:50,  2.61it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2063/3145 [13:09<06:49,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2086/3143 [13:06<06:40,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2074/3145 [13:06<06:42,  2.66it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2079/3145 [13:19<06:39,  2.67it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2087/3143 [13:06<05:50,  3.02it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2064/3145 [13:09<06:57,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2075/3145 [13:07<06:39,  2.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2080/3145 [13:20<06:59,  2.54it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2088/3143 [13:07<06:06,  2.88it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2065/3145 [13:09<07:13,  2.49it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2076/3145 [13:07<06:36,  2.69it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2089/3143 [13:07<06:03,  2.90it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2081/3145 [13:20<06:56,  2.55it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2066/3145 [13:10<07:11,  2.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2077/3145 [13:07<06:32,  2.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2090/3143 [13:08<06:10,  2.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2082/3145 [13:20<06:55,  2.56it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2067/3145 [13:10<06:57,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2078/3145 [13:08<06:27,  2.76it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2091/3143 [13:08<06:08,  2.86it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2083/3145 [13:21<06:53,  2.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2068/3145 [13:11<06:54,  2.60it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2079/3145 [13:08<06:23,  2.78it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2092/3143 [13:08<06:18,  2.78it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2084/3145 [13:21<06:37,  2.67it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2069/3145 [13:11<06:58,  2.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2080/3145 [13:09<06:44,  2.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2093/3143 [13:09<06:18,  2.77it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2085/3145 [13:21<06:40,  2.65it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2070/3145 [13:11<07:06,  2.52it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2094/3143 [13:09<06:17,  2.78it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2081/3145 [13:09<06:51,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2086/3145 [13:22<06:28,  2.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2071/3145 [13:12<07:07,  2.51it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2095/3143 [13:09<06:25,  2.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2087/3145 [13:22<06:17,  2.80it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2082/3145 [13:09<07:11,  2.46it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2072/3145 [13:12<06:08,  2.91it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2088/3145 [13:22<06:14,  2.82it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2096/3143 [13:10<06:28,  2.69it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2083/3145 [13:10<06:50,  2.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2073/3145 [13:12<06:10,  2.90it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2084/3145 [13:10<06:15,  2.83it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2097/3143 [13:10<06:21,  2.74it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2089/3145 [13:23<06:37,  2.65it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2074/3145 [13:13<06:53,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2085/3145 [13:10<06:06,  2.89it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2098/3143 [13:10<06:17,  2.77it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2090/3145 [13:23<06:22,  2.76it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2075/3145 [13:13<06:46,  2.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2086/3145 [13:11<06:00,  2.94it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2091/3145 [13:24<06:31,  2.69it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2099/3143 [13:11<06:35,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2076/3145 [13:14<06:39,  2.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2087/3145 [13:11<06:09,  2.86it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2092/3145 [13:24<06:30,  2.70it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2100/3143 [13:11<06:53,  2.52it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2088/3145 [13:11<06:15,  2.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2077/3145 [13:14<06:47,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2093/3145 [13:24<06:36,  2.66it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2101/3143 [13:12<06:42,  2.59it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2078/3145 [13:14<06:41,  2.65it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2089/3145 [13:12<06:21,  2.77it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2094/3145 [13:25<06:20,  2.76it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2102/3143 [13:12<06:44,  2.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2090/3145 [13:12<06:33,  2.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2079/3145 [13:15<07:13,  2.46it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2095/3145 [13:25<06:23,  2.74it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2103/3143 [13:12<06:34,  2.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2091/3145 [13:13<06:42,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2096/3145 [13:25<06:19,  2.76it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2080/3145 [13:15<07:30,  2.37it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2104/3143 [13:13<06:25,  2.70it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2092/3145 [13:13<06:56,  2.53it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2097/3145 [13:26<06:28,  2.70it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2081/3145 [13:16<07:13,  2.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2105/3143 [13:13<06:27,  2.68it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2093/3145 [13:13<06:40,  2.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2082/3145 [13:16<07:09,  2.47it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2106/3143 [13:14<06:30,  2.65it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2098/3145 [13:26<06:57,  2.51it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2094/3145 [13:14<06:42,  2.61it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2083/3145 [13:16<06:49,  2.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2099/3145 [13:27<06:49,  2.55it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2107/3143 [13:14<06:32,  2.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2095/3145 [13:14<06:43,  2.60it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2084/3145 [13:17<06:50,  2.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2100/3145 [13:27<06:44,  2.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2108/3143 [13:14<06:29,  2.66it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2096/3145 [13:14<06:25,  2.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2085/3145 [13:17<06:50,  2.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2101/3145 [13:27<06:39,  2.61it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2109/3143 [13:15<06:50,  2.52it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2097/3145 [13:15<06:35,  2.65it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2102/3145 [13:28<06:51,  2.54it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2086/3145 [13:18<07:15,  2.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2110/3143 [13:15<06:48,  2.53it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2098/3145 [13:15<06:19,  2.76it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2087/3145 [13:18<07:09,  2.46it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2103/3145 [13:28<07:08,  2.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2111/3143 [13:16<07:00,  2.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2099/3145 [13:16<06:27,  2.70it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2100/3145 [13:16<05:47,  3.01it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2088/3145 [13:18<07:06,  2.48it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2104/3145 [13:29<06:59,  2.48it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2112/3143 [13:16<06:57,  2.47it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2101/3145 [13:16<05:57,  2.92it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2089/3145 [13:19<07:03,  2.49it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2105/3145 [13:29<07:04,  2.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2113/3143 [13:16<07:12,  2.38it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2102/3145 [13:17<05:52,  2.96it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2090/3145 [13:19<06:52,  2.56it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2114/3143 [13:17<06:54,  2.48it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2106/3145 [13:30<07:15,  2.38it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2103/3145 [13:17<06:09,  2.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2091/3145 [13:20<06:44,  2.60it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2115/3143 [13:17<06:48,  2.51it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2107/3145 [13:30<07:08,  2.42it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2104/3145 [13:17<06:11,  2.80it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2092/3145 [13:20<06:42,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2116/3143 [13:17<06:32,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2105/3145 [13:18<05:54,  2.93it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2108/3145 [13:30<07:33,  2.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2117/3143 [13:18<05:43,  2.99it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2093/3145 [13:20<06:44,  2.60it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2118/3143 [13:18<05:11,  3.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2106/3145 [13:18<06:09,  2.81it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2109/3145 [13:31<07:20,  2.35it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2094/3145 [13:21<06:25,  2.72it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2107/3145 [13:18<05:27,  3.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2119/3143 [13:18<05:36,  3.04it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2095/3145 [13:21<06:31,  2.68it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2110/3145 [13:31<07:25,  2.32it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2120/3143 [13:19<05:37,  3.03it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2108/3145 [13:19<06:17,  2.74it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2121/3143 [13:19<05:06,  3.34it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2096/3145 [13:21<06:40,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2111/3145 [13:32<07:35,  2.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2109/3145 [13:19<06:14,  2.76it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2122/3143 [13:19<04:44,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2097/3145 [13:22<06:38,  2.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2123/3143 [13:19<04:38,  3.67it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2110/3145 [13:19<06:17,  2.74it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2112/3145 [13:32<07:55,  2.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2098/3145 [13:22<06:45,  2.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2111/3145 [13:20<06:19,  2.73it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2113/3145 [13:33<07:24,  2.32it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2124/3143 [13:20<05:39,  3.00it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2099/3145 [13:23<06:42,  2.60it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2112/3145 [13:20<06:28,  2.66it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2114/3145 [13:33<07:41,  2.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2125/3143 [13:20<06:22,  2.66it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2100/3145 [13:23<06:38,  2.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2113/3145 [13:21<06:40,  2.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2115/3145 [13:33<07:16,  2.36it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2126/3143 [13:21<06:43,  2.52it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2101/3145 [13:23<06:47,  2.56it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2114/3145 [13:21<06:40,  2.57it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2116/3145 [13:34<07:30,  2.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2102/3145 [13:24<06:41,  2.60it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2127/3143 [13:21<07:07,  2.38it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2115/3145 [13:21<06:49,  2.51it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2103/3145 [13:24<06:39,  2.61it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2117/3145 [13:34<07:37,  2.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2128/3143 [13:22<06:54,  2.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2116/3145 [13:22<05:56,  2.89it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2129/3143 [13:22<06:03,  2.79it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2118/3145 [13:35<06:41,  2.56it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2104/3145 [13:24<06:36,  2.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2117/3145 [13:22<06:25,  2.67it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2130/3143 [13:22<05:30,  3.07it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2119/3145 [13:35<06:39,  2.57it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2105/3145 [13:25<07:12,  2.40it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2118/3145 [13:22<06:45,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2131/3143 [13:23<06:03,  2.78it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2120/3145 [13:35<06:46,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2132/3143 [13:23<05:29,  3.07it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2106/3145 [13:25<07:15,  2.39it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2119/3145 [13:23<06:44,  2.53it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2121/3145 [13:36<06:47,  2.51it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2133/3143 [13:23<05:39,  2.97it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2120/3145 [13:23<06:13,  2.74it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2107/3145 [13:26<07:26,  2.33it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2122/3145 [13:36<06:44,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2134/3143 [13:24<05:52,  2.86it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2121/3145 [13:24<06:19,  2.70it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2108/3145 [13:26<07:34,  2.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2123/3145 [13:37<06:35,  2.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2122/3145 [13:24<06:03,  2.81it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2135/3143 [13:24<06:00,  2.79it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2124/3145 [13:37<06:25,  2.65it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2109/3145 [13:27<07:19,  2.36it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2123/3145 [13:24<06:20,  2.69it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2136/3143 [13:24<06:20,  2.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2125/3145 [13:37<06:25,  2.65it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2110/3145 [13:27<07:09,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2124/3145 [13:25<06:25,  2.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2137/3143 [13:25<06:23,  2.62it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2126/3145 [13:38<06:31,  2.61it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2111/3145 [13:28<07:11,  2.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2138/3143 [13:25<06:16,  2.67it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2125/3145 [13:25<06:36,  2.57it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2112/3145 [13:28<06:53,  2.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2127/3145 [13:38<06:42,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2126/3145 [13:25<06:06,  2.78it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2139/3143 [13:25<06:15,  2.67it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2127/3145 [13:26<06:12,  2.74it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2113/3145 [13:28<07:11,  2.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2140/3143 [13:26<06:13,  2.69it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2128/3145 [13:39<07:12,  2.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2128/3145 [13:26<06:24,  2.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2141/3143 [13:26<06:15,  2.67it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2129/3145 [13:39<06:57,  2.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2114/3145 [13:29<07:35,  2.26it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2130/3145 [13:39<06:46,  2.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2142/3143 [13:27<06:27,  2.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2129/3145 [13:27<06:48,  2.49it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2115/3145 [13:29<07:17,  2.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2130/3145 [13:27<06:47,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2131/3145 [13:40<07:00,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2143/3143 [13:27<06:57,  2.40it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2116/3145 [13:30<07:17,  2.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2131/3145 [13:27<06:45,  2.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2132/3145 [13:40<06:51,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2144/3143 [13:28<07:00,  2.37it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2117/3145 [13:30<07:26,  2.30it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2133/3145 [13:41<06:37,  2.54it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2132/3145 [13:28<06:59,  2.42it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2145/3143 [13:28<06:49,  2.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2118/3145 [13:31<07:24,  2.31it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2134/3145 [13:41<06:24,  2.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2133/3145 [13:28<06:40,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2146/3143 [13:28<06:59,  2.37it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2119/3145 [13:31<07:18,  2.34it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2135/3145 [13:41<06:03,  2.78it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2147/3143 [13:29<06:20,  2.62it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2134/3145 [13:29<07:04,  2.38it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2120/3145 [13:31<07:12,  2.37it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2136/3145 [13:42<06:15,  2.68it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2148/3143 [13:29<06:24,  2.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2135/3145 [13:29<06:51,  2.45it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2137/3145 [13:42<05:43,  2.94it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2121/3145 [13:32<07:03,  2.42it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2149/3143 [13:29<06:14,  2.66it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2136/3145 [13:30<06:58,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2138/3145 [13:42<06:10,  2.72it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2122/3145 [13:32<07:05,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2150/3143 [13:30<05:46,  2.87it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2137/3145 [13:30<07:03,  2.38it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2139/3145 [13:43<06:15,  2.68it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2123/3145 [13:33<06:55,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2151/3143 [13:30<06:00,  2.75it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2140/3145 [13:43<05:46,  2.90it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2152/3143 [13:30<05:24,  3.06it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2138/3145 [13:30<06:58,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2124/3145 [13:33<07:03,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2141/3145 [13:43<05:57,  2.81it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2125/3145 [13:33<06:17,  2.70it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2153/3143 [13:31<05:56,  2.78it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2139/3145 [13:31<07:08,  2.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2142/3145 [13:44<06:01,  2.77it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2154/3143 [13:31<05:55,  2.78it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2126/3145 [13:34<06:40,  2.55it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2140/3145 [13:31<06:51,  2.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2143/3145 [13:44<05:57,  2.80it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2155/3143 [13:32<06:11,  2.66it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2141/3145 [13:32<06:51,  2.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2127/3145 [13:34<06:53,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2144/3145 [13:45<06:08,  2.72it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2128/3145 [13:34<06:38,  2.55it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2142/3145 [13:32<06:50,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2156/3143 [13:32<06:36,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2145/3145 [13:45<06:10,  2.70it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2129/3145 [13:35<06:43,  2.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2157/3143 [13:32<06:27,  2.54it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2143/3145 [13:32<06:57,  2.40it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2146/3145 [13:45<06:17,  2.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2130/3145 [13:35<06:40,  2.53it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2158/3143 [13:33<06:30,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2144/3145 [13:33<06:50,  2.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2147/3145 [13:46<06:22,  2.61it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2159/3143 [13:33<06:29,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2131/3145 [13:36<07:00,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2148/3145 [13:46<06:27,  2.57it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2145/3145 [13:33<07:20,  2.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2160/3143 [13:34<06:22,  2.57it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2132/3145 [13:36<06:45,  2.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2146/3145 [13:34<06:54,  2.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2149/3145 [13:47<06:30,  2.55it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2133/3145 [13:36<06:22,  2.65it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2161/3143 [13:34<06:27,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2150/3145 [13:47<06:30,  2.55it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2147/3145 [13:34<06:50,  2.43it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2134/3145 [13:37<06:09,  2.74it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2162/3143 [13:34<06:17,  2.60it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2148/3145 [13:34<06:39,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2151/3145 [13:47<06:39,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2135/3145 [13:37<06:19,  2.66it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2163/3143 [13:35<06:26,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2149/3145 [13:35<06:19,  2.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2152/3145 [13:48<06:24,  2.59it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2136/3145 [13:38<06:18,  2.66it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2164/3143 [13:35<06:25,  2.54it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2150/3145 [13:35<06:12,  2.67it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2153/3145 [13:48<06:24,  2.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2137/3145 [13:38<06:21,  2.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2151/3145 [13:35<05:44,  2.88it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2165/3143 [13:36<06:32,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2154/3145 [13:49<06:37,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2138/3145 [13:38<06:20,  2.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2152/3145 [13:36<05:52,  2.82it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2166/3143 [13:36<06:37,  2.46it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2155/3145 [13:49<06:43,  2.45it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2139/3145 [13:39<06:22,  2.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2153/3145 [13:36<06:00,  2.75it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2167/3143 [13:36<06:23,  2.55it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2140/3145 [13:39<06:23,  2.62it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2156/3145 [13:49<06:54,  2.38it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2154/3145 [13:37<06:22,  2.59it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2168/3143 [13:37<06:25,  2.53it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2157/3145 [13:50<06:53,  2.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2141/3145 [13:40<06:49,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2169/3143 [13:37<06:15,  2.59it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2155/3145 [13:37<06:54,  2.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2158/3145 [13:50<06:32,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2142/3145 [13:40<06:55,  2.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2170/3143 [13:37<06:10,  2.62it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2156/3145 [13:38<06:53,  2.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2159/3145 [13:51<06:40,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2143/3145 [13:40<06:59,  2.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2171/3143 [13:38<06:40,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2157/3145 [13:38<06:43,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2160/3145 [13:51<06:30,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2144/3145 [13:41<06:47,  2.46it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2172/3143 [13:38<06:27,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2158/3145 [13:38<06:49,  2.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2161/3145 [13:51<06:27,  2.54it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2173/3143 [13:39<05:45,  2.80it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2145/3145 [13:41<06:56,  2.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2159/3145 [13:39<06:58,  2.36it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2162/3145 [13:52<06:27,  2.54it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2174/3143 [13:39<05:54,  2.73it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2146/3145 [13:42<07:06,  2.34it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2160/3145 [13:39<06:42,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2163/3145 [13:52<06:20,  2.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2175/3143 [13:39<05:55,  2.72it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2147/3145 [13:42<07:04,  2.35it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2161/3145 [13:40<06:40,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2164/3145 [13:53<06:33,  2.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2176/3143 [13:40<06:16,  2.57it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2148/3145 [13:43<07:08,  2.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2162/3145 [13:40<06:43,  2.43it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2177/3143 [13:40<06:16,  2.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2165/3145 [13:53<06:50,  2.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2163/3145 [13:40<06:43,  2.43it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2149/3145 [13:43<07:08,  2.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2166/3145 [13:53<06:44,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2178/3143 [13:41<06:24,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2164/3145 [13:41<06:37,  2.47it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2150/3145 [13:43<07:14,  2.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2167/3145 [13:54<06:37,  2.46it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2179/3143 [13:41<06:23,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2165/3145 [13:41<06:31,  2.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2168/3145 [13:54<06:35,  2.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2180/3143 [13:41<06:26,  2.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2151/3145 [13:44<08:04,  2.05it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2166/3145 [13:42<06:27,  2.53it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2152/3145 [13:44<06:50,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2169/3145 [13:55<06:28,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2181/3143 [13:42<06:34,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2167/3145 [13:42<06:16,  2.60it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2153/3145 [13:45<06:35,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2170/3145 [13:55<06:21,  2.55it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2182/3143 [13:42<06:37,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2168/3145 [13:42<06:12,  2.62it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2154/3145 [13:45<06:34,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2171/3145 [13:55<06:31,  2.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2169/3145 [13:43<06:15,  2.60it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2183/3143 [13:43<07:08,  2.24it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2155/3145 [13:45<06:27,  2.56it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2172/3145 [13:56<06:36,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2184/3143 [13:43<06:21,  2.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2170/3145 [13:43<06:07,  2.65it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2156/3145 [13:46<06:05,  2.71it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2173/3145 [13:56<06:28,  2.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2185/3143 [13:44<06:28,  2.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2171/3145 [13:44<06:19,  2.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2157/3145 [13:46<06:01,  2.73it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2174/3145 [13:56<05:48,  2.79it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2172/3145 [13:44<06:17,  2.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2186/3143 [13:44<06:30,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2158/3145 [13:47<06:15,  2.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2175/3145 [13:57<06:04,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2187/3143 [13:44<06:23,  2.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2173/3145 [13:44<06:26,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2159/3145 [13:47<06:13,  2.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2176/3145 [13:57<06:21,  2.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2188/3143 [13:45<06:28,  2.46it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2174/3145 [13:45<06:24,  2.53it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2160/3145 [13:47<06:20,  2.59it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2177/3145 [13:58<06:20,  2.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2189/3143 [13:45<06:37,  2.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2161/3145 [13:48<06:31,  2.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2175/3145 [13:45<07:06,  2.27it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2178/3145 [13:58<06:56,  2.32it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2190/3143 [13:46<06:21,  2.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2162/3145 [13:48<06:32,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2176/3145 [13:46<06:52,  2.35it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2179/3145 [13:59<06:48,  2.37it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2177/3145 [13:46<06:24,  2.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2191/3143 [13:46<06:50,  2.32it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2163/3145 [13:49<06:45,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2180/3145 [13:59<06:33,  2.45it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2192/3143 [13:46<06:32,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2178/3145 [13:46<06:34,  2.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2164/3145 [13:49<06:44,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2181/3145 [13:59<06:38,  2.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2179/3145 [13:47<06:15,  2.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2193/3143 [13:47<06:23,  2.48it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2165/3145 [13:49<06:37,  2.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2182/3145 [14:00<06:48,  2.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2194/3143 [13:47<06:09,  2.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2166/3145 [13:50<06:35,  2.48it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2180/3145 [13:47<06:51,  2.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2195/3143 [13:48<06:16,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2183/3145 [14:00<07:07,  2.25it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2167/3145 [13:50<06:17,  2.59it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2181/3145 [13:48<06:41,  2.40it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2196/3143 [13:48<06:06,  2.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2168/3145 [13:50<06:05,  2.68it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2184/3145 [14:01<06:52,  2.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2182/3145 [13:48<06:24,  2.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2197/3143 [13:48<06:00,  2.62it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2169/3145 [13:51<06:10,  2.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2185/3145 [14:01<06:42,  2.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2183/3145 [13:48<06:42,  2.39it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2198/3143 [13:49<06:01,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2186/3145 [14:01<06:25,  2.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2170/3145 [13:51<06:19,  2.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2199/3143 [13:49<05:44,  2.74it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2184/3145 [13:49<07:11,  2.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2187/3145 [14:02<06:21,  2.51it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2171/3145 [13:52<06:12,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2200/3143 [13:49<05:49,  2.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2188/3145 [14:02<05:56,  2.69it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2185/3145 [13:49<07:09,  2.24it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2172/3145 [13:52<06:06,  2.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2186/3145 [13:50<06:08,  2.60it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2173/3145 [13:52<05:35,  2.89it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2201/3143 [13:50<06:02,  2.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2189/3145 [14:03<06:02,  2.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2187/3145 [13:50<05:23,  2.96it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2174/3145 [13:53<05:38,  2.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2190/3145 [14:03<05:56,  2.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2188/3145 [13:50<04:55,  3.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2202/3143 [13:50<06:12,  2.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2191/3145 [14:03<05:43,  2.78it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2175/3145 [13:53<05:51,  2.76it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2189/3145 [13:51<05:12,  3.06it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2203/3143 [13:51<06:17,  2.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2192/3145 [14:04<05:40,  2.80it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2190/3145 [13:51<05:21,  2.97it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2176/3145 [13:53<06:05,  2.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2204/3143 [13:51<06:07,  2.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2193/3145 [14:04<05:32,  2.86it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2205/3143 [13:51<05:20,  2.92it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2191/3145 [13:51<05:28,  2.90it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2177/3145 [13:54<06:41,  2.41it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2206/3143 [13:51<04:51,  3.22it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2194/3145 [14:04<05:34,  2.85it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2192/3145 [13:52<05:40,  2.80it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2207/3143 [13:52<04:44,  3.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2178/3145 [13:54<06:27,  2.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2195/3145 [14:05<05:45,  2.75it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2193/3145 [13:52<05:54,  2.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2208/3143 [13:52<05:00,  3.11it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2179/3145 [13:55<06:27,  2.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2196/3145 [14:05<06:10,  2.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2194/3145 [13:52<06:01,  2.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2209/3143 [13:52<05:17,  2.94it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2180/3145 [13:55<06:23,  2.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2197/3145 [14:05<05:55,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2195/3145 [13:53<06:06,  2.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2210/3143 [13:53<05:23,  2.89it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2181/3145 [13:55<06:09,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2198/3145 [14:06<05:49,  2.71it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2182/3145 [13:56<05:37,  2.85it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2196/3145 [13:53<06:10,  2.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2211/3143 [13:53<05:45,  2.70it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2183/3145 [13:56<05:33,  2.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2199/3145 [14:06<06:20,  2.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2197/3145 [13:54<06:00,  2.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2212/3143 [13:54<05:33,  2.79it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2200/3145 [14:07<06:20,  2.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2198/3145 [13:54<05:56,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2213/3143 [13:54<05:39,  2.74it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2184/3145 [13:57<06:28,  2.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2199/3145 [13:54<05:50,  2.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2214/3143 [13:54<05:35,  2.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2201/3145 [14:07<06:26,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2185/3145 [13:57<06:11,  2.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2200/3145 [13:55<05:45,  2.74it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2215/3143 [13:55<05:35,  2.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2202/3145 [14:08<06:10,  2.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2186/3145 [13:57<06:51,  2.33it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2216/3143 [13:55<05:34,  2.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2203/3145 [14:08<06:00,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2201/3145 [13:55<06:42,  2.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2217/3143 [13:55<05:40,  2.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2187/3145 [13:58<07:22,  2.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2204/3145 [14:08<06:10,  2.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2202/3145 [13:56<06:30,  2.42it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2218/3143 [13:56<05:43,  2.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2205/3145 [14:09<06:00,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2188/3145 [13:58<07:20,  2.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2203/3145 [13:56<06:13,  2.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2189/3145 [13:59<06:35,  2.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2206/3145 [14:09<06:10,  2.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2204/3145 [13:56<06:08,  2.56it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2219/3143 [13:56<06:40,  2.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2190/3145 [13:59<05:59,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2207/3145 [14:09<05:52,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2205/3145 [13:57<06:08,  2.55it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2220/3143 [13:57<06:46,  2.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2208/3145 [14:10<05:38,  2.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2191/3145 [14:00<06:45,  2.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2206/3145 [13:57<06:05,  2.57it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2221/3143 [13:57<06:23,  2.41it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2209/3145 [14:10<05:36,  2.78it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2222/3143 [13:58<05:54,  2.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2207/3145 [13:58<06:04,  2.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2192/3145 [14:00<07:05,  2.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2210/3145 [14:11<06:01,  2.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2208/3145 [13:58<05:57,  2.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2223/3143 [13:58<05:57,  2.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2193/3145 [14:01<07:09,  2.22it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2211/3145 [14:11<05:51,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2209/3145 [13:58<05:51,  2.67it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2224/3143 [13:58<05:47,  2.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2194/3145 [14:01<06:35,  2.40it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2212/3145 [14:11<05:41,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2225/3143 [13:59<05:40,  2.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2210/3145 [13:59<05:54,  2.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2195/3145 [14:01<06:55,  2.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2213/3145 [14:12<05:59,  2.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2226/3143 [13:59<05:43,  2.67it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2211/3145 [13:59<06:03,  2.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2196/3145 [14:02<06:41,  2.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2214/3145 [14:12<05:58,  2.60it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2227/3143 [13:59<05:38,  2.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2212/3145 [13:59<05:58,  2.60it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2228/3143 [14:00<04:58,  3.07it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2215/3145 [14:12<05:48,  2.67it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2197/3145 [14:02<06:30,  2.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2213/3145 [14:00<05:54,  2.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2229/3143 [14:00<05:06,  2.98it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2216/3145 [14:13<05:43,  2.71it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2198/3145 [14:02<06:16,  2.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2214/3145 [14:00<05:40,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2230/3143 [14:00<05:09,  2.95it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2199/3145 [14:03<05:59,  2.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2217/3145 [14:13<05:48,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2215/3145 [14:00<05:28,  2.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2231/3143 [14:01<05:04,  3.00it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2218/3145 [14:14<05:44,  2.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2216/3145 [14:01<05:28,  2.83it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2200/3145 [14:03<06:33,  2.40it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2232/3143 [14:01<05:02,  3.01it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2219/3145 [14:14<05:13,  2.95it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2217/3145 [14:01<05:29,  2.81it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2220/3145 [14:14<04:39,  3.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2201/3145 [14:04<06:21,  2.48it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2233/3143 [14:01<05:16,  2.88it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2221/3145 [14:14<04:54,  3.14it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2218/3145 [14:02<05:39,  2.73it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2202/3145 [14:04<06:10,  2.54it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2234/3143 [14:02<05:25,  2.80it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2222/3145 [14:15<05:05,  3.02it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2203/3145 [14:04<06:15,  2.51it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2219/3145 [14:02<05:59,  2.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2235/3143 [14:02<05:32,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2223/3145 [14:15<05:28,  2.81it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2220/3145 [14:02<06:01,  2.56it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2236/3143 [14:03<05:38,  2.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2204/3145 [14:05<06:53,  2.28it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2224/3145 [14:16<05:45,  2.66it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2221/3145 [14:03<06:10,  2.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2237/3143 [14:03<05:31,  2.73it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2205/3145 [14:05<06:49,  2.30it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2225/3145 [14:16<05:48,  2.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2238/3143 [14:03<05:29,  2.75it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2222/3145 [14:03<06:20,  2.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2206/3145 [14:06<06:34,  2.38it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2226/3145 [14:16<06:00,  2.55it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2239/3143 [14:04<05:37,  2.68it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2223/3145 [14:04<06:07,  2.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2207/3145 [14:06<06:04,  2.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2227/3145 [14:17<05:49,  2.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2240/3143 [14:04<05:40,  2.65it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2224/3145 [14:04<05:55,  2.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2208/3145 [14:07<06:02,  2.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2209/3145 [14:07<05:13,  2.99it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2228/3145 [14:17<05:58,  2.56it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2225/3145 [14:04<05:51,  2.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2241/3143 [14:04<05:45,  2.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2210/3145 [14:07<05:26,  2.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2229/3145 [14:17<05:48,  2.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2226/3145 [14:05<05:44,  2.67it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2242/3143 [14:05<05:38,  2.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2211/3145 [14:07<05:26,  2.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2243/3143 [14:05<05:04,  2.95it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2230/3145 [14:18<05:42,  2.67it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2227/3145 [14:05<05:41,  2.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2212/3145 [14:08<05:36,  2.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2244/3143 [14:05<05:16,  2.84it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2228/3145 [14:05<05:35,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2231/3145 [14:18<06:15,  2.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2213/3145 [14:08<05:35,  2.78it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2245/3143 [14:06<05:33,  2.69it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2229/3145 [14:06<05:45,  2.65it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2232/3145 [14:19<06:16,  2.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2246/3143 [14:06<04:59,  2.99it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2214/3145 [14:09<05:36,  2.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2230/3145 [14:06<05:32,  2.75it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2247/3143 [14:06<04:27,  3.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2233/3145 [14:19<06:08,  2.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2215/3145 [14:09<05:48,  2.67it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2231/3145 [14:07<05:31,  2.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2248/3143 [14:07<04:41,  3.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2234/3145 [14:19<05:53,  2.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2232/3145 [14:07<05:04,  3.00it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2216/3145 [14:09<05:44,  2.70it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2249/3143 [14:07<04:43,  3.15it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2235/3145 [14:20<05:51,  2.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2233/3145 [14:07<05:19,  2.86it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2217/3145 [14:10<05:54,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2250/3143 [14:07<04:54,  3.03it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2236/3145 [14:20<05:31,  2.74it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2251/3143 [14:08<04:25,  3.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2234/3145 [14:08<05:21,  2.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2218/3145 [14:10<05:42,  2.70it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2237/3145 [14:21<05:37,  2.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2252/3143 [14:08<04:37,  3.21it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2235/3145 [14:08<05:33,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2219/3145 [14:10<05:39,  2.72it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2238/3145 [14:21<05:39,  2.67it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2253/3143 [14:08<04:49,  3.07it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2220/3145 [14:11<05:33,  2.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2236/3145 [14:08<05:39,  2.68it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2239/3145 [14:21<05:43,  2.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2254/3143 [14:09<05:02,  2.94it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2221/3145 [14:11<05:33,  2.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2237/3145 [14:09<05:42,  2.65it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2240/3145 [14:22<05:07,  2.94it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2255/3143 [14:09<05:11,  2.85it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2222/3145 [14:12<05:38,  2.73it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2241/3145 [14:22<05:10,  2.91it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2238/3145 [14:09<06:15,  2.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3143 [14:09<04:59,  2.96it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2223/3145 [14:12<05:43,  2.69it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2242/3145 [14:22<05:20,  2.81it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2239/3145 [14:10<05:55,  2.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2257/3143 [14:10<05:04,  2.91it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2240/3145 [14:10<05:06,  2.95it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2224/3145 [14:12<05:49,  2.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2243/3145 [14:23<05:15,  2.86it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2258/3143 [14:10<05:06,  2.89it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2241/3145 [14:10<05:07,  2.94it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2225/3145 [14:13<05:47,  2.65it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2244/3145 [14:23<05:15,  2.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2259/3143 [14:10<05:07,  2.88it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2242/3145 [14:10<05:14,  2.87it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2245/3145 [14:23<05:10,  2.90it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2226/3145 [14:13<05:50,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2260/3143 [14:11<05:19,  2.77it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2227/3145 [14:13<05:04,  3.02it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2243/3145 [14:11<05:12,  2.89it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2246/3145 [14:24<05:21,  2.79it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2244/3145 [14:11<05:07,  2.93it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2228/3145 [14:14<05:19,  2.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2261/3143 [14:11<05:55,  2.48it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2247/3145 [14:24<05:21,  2.79it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2262/3143 [14:11<05:07,  2.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2229/3145 [14:14<05:16,  2.90it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2248/3145 [14:24<05:13,  2.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2245/3145 [14:12<06:00,  2.50it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2263/3143 [14:12<05:24,  2.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2249/3145 [14:25<04:45,  3.14it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2230/3145 [14:14<05:21,  2.84it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2246/3145 [14:12<05:56,  2.52it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2250/3145 [14:25<04:57,  3.01it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2264/3143 [14:12<05:28,  2.68it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2231/3145 [14:15<05:23,  2.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2247/3145 [14:12<05:46,  2.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2232/3145 [14:15<05:18,  2.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2251/3145 [14:25<05:14,  2.84it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2265/3143 [14:13<05:39,  2.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2248/3145 [14:13<05:45,  2.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2233/3145 [14:15<05:22,  2.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2252/3145 [14:26<05:08,  2.89it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2266/3143 [14:13<05:42,  2.56it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2249/3145 [14:13<05:29,  2.72it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2234/3145 [14:16<05:17,  2.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2253/3145 [14:26<05:14,  2.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2267/3143 [14:13<05:42,  2.56it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2250/3145 [14:14<05:34,  2.68it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2235/3145 [14:16<05:21,  2.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2254/3145 [14:26<05:12,  2.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2268/3143 [14:14<05:34,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2251/3145 [14:14<05:21,  2.78it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2236/3145 [14:17<05:30,  2.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2255/3145 [14:27<05:21,  2.77it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2269/3143 [14:14<05:50,  2.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2252/3145 [14:14<06:07,  2.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2237/3145 [14:17<05:37,  2.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3145 [14:27<05:33,  2.66it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2270/3143 [14:15<05:56,  2.45it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2253/3145 [14:15<05:49,  2.55it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2238/3145 [14:17<05:28,  2.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2257/3145 [14:28<05:24,  2.73it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2271/3143 [14:15<05:53,  2.47it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2239/3145 [14:18<05:27,  2.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2254/3145 [14:15<05:49,  2.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2258/3145 [14:28<05:58,  2.48it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2272/3143 [14:15<05:50,  2.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2240/3145 [14:18<05:25,  2.78it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2255/3145 [14:15<05:40,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2259/3145 [14:28<05:26,  2.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2273/3143 [14:16<05:37,  2.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2241/3145 [14:18<05:21,  2.81it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3145 [14:16<05:44,  2.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2260/3145 [14:29<05:27,  2.70it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2242/3145 [14:19<05:26,  2.77it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2274/3143 [14:16<05:38,  2.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2257/3145 [14:16<05:47,  2.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2261/3145 [14:29<05:33,  2.65it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2258/3145 [14:17<05:02,  2.93it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2243/3145 [14:19<05:25,  2.77it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2275/3143 [14:17<06:03,  2.39it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2262/3145 [14:30<05:28,  2.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2259/3145 [14:17<05:01,  2.94it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2244/3145 [14:19<05:21,  2.80it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2276/3143 [14:17<05:42,  2.53it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2263/3145 [14:30<05:30,  2.67it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2260/3145 [14:17<05:08,  2.87it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2245/3145 [14:20<05:24,  2.77it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2277/3143 [14:17<05:14,  2.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2264/3145 [14:30<05:25,  2.70it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2261/3145 [14:17<04:49,  3.05it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2246/3145 [14:20<05:25,  2.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2278/3143 [14:18<05:22,  2.68it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2265/3145 [14:31<05:24,  2.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2262/3145 [14:18<04:57,  2.96it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2247/3145 [14:21<05:21,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2279/3143 [14:18<05:25,  2.66it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2266/3145 [14:31<05:27,  2.68it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2263/3145 [14:18<05:01,  2.92it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2248/3145 [14:21<05:28,  2.73it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2267/3145 [14:31<05:24,  2.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2264/3145 [14:19<05:05,  2.88it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2280/3143 [14:19<05:52,  2.45it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2249/3145 [14:21<05:43,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2268/3145 [14:32<05:28,  2.67it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2281/3143 [14:19<05:38,  2.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2265/3145 [14:19<05:36,  2.61it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2250/3145 [14:22<05:54,  2.53it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2269/3145 [14:32<05:27,  2.68it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2282/3143 [14:19<05:30,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2266/3145 [14:19<05:26,  2.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2251/3145 [14:22<05:41,  2.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2283/3143 [14:20<05:21,  2.68it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2267/3145 [14:20<05:25,  2.70it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2270/3145 [14:33<06:17,  2.31it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2252/3145 [14:22<05:42,  2.61it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2268/3145 [14:20<05:20,  2.73it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2284/3143 [14:20<05:43,  2.50it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2271/3145 [14:33<06:01,  2.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2253/3145 [14:23<05:43,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2269/3145 [14:20<05:13,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2285/3143 [14:21<05:32,  2.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2254/3145 [14:23<05:04,  2.93it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2272/3145 [14:33<05:52,  2.47it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2270/3145 [14:21<05:15,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2286/3143 [14:21<05:26,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2255/3145 [14:23<05:06,  2.91it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2273/3145 [14:34<05:47,  2.51it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2271/3145 [14:21<05:16,  2.76it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2287/3143 [14:21<05:35,  2.55it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2256/3145 [14:24<05:11,  2.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2274/3145 [14:34<05:34,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2257/3145 [14:24<04:36,  3.21it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2272/3145 [14:22<05:23,  2.70it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2288/3143 [14:22<05:20,  2.66it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2275/3145 [14:35<05:34,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2258/3145 [14:24<04:49,  3.06it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2273/3145 [14:22<05:28,  2.65it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2289/3143 [14:22<05:42,  2.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2276/3145 [14:35<05:25,  2.67it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2259/3145 [14:25<04:54,  3.01it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2274/3145 [14:22<05:44,  2.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2290/3143 [14:22<05:38,  2.52it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2277/3145 [14:35<05:27,  2.65it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2260/3145 [14:25<05:00,  2.94it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2275/3145 [14:23<05:41,  2.55it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2291/3143 [14:23<05:32,  2.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2278/3145 [14:36<05:25,  2.66it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2261/3145 [14:25<05:07,  2.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2276/3145 [14:23<05:38,  2.56it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2292/3143 [14:23<05:38,  2.52it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2279/3145 [14:36<05:37,  2.56it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2262/3145 [14:26<05:10,  2.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2277/3145 [14:23<04:55,  2.94it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2293/3143 [14:24<05:27,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2280/3145 [14:36<05:33,  2.59it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2278/3145 [14:24<04:59,  2.90it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2263/3145 [14:26<05:49,  2.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2294/3143 [14:24<05:16,  2.68it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2281/3145 [14:37<05:19,  2.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2279/3145 [14:24<05:03,  2.85it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2295/3143 [14:24<05:10,  2.73it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2264/3145 [14:27<06:16,  2.34it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2282/3145 [14:37<05:16,  2.72it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2280/3145 [14:24<04:58,  2.89it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2265/3145 [14:27<05:40,  2.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2296/3143 [14:25<05:16,  2.68it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2283/3145 [14:37<05:08,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2281/3145 [14:25<05:02,  2.86it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2266/3145 [14:27<05:17,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2297/3143 [14:25<05:22,  2.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2284/3145 [14:38<05:17,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2282/3145 [14:25<04:58,  2.89it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2267/3145 [14:28<05:18,  2.75it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2285/3145 [14:38<05:17,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2298/3143 [14:25<05:24,  2.60it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2283/3145 [14:25<05:03,  2.84it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2268/3145 [14:28<04:58,  2.94it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2286/3145 [14:39<05:10,  2.77it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2269/3145 [14:28<04:41,  3.11it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2284/3145 [14:26<05:02,  2.85it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2299/3143 [14:26<05:34,  2.52it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2270/3145 [14:29<04:44,  3.08it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2287/3145 [14:39<05:17,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2285/3145 [14:26<05:09,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2300/3143 [14:26<05:24,  2.60it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2271/3145 [14:29<04:56,  2.95it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2288/3145 [14:39<05:15,  2.72it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2301/3143 [14:27<05:16,  2.66it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2286/3145 [14:27<05:16,  2.72it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2272/3145 [14:29<04:58,  2.92it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2302/3143 [14:27<05:05,  2.75it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2287/3145 [14:27<05:08,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2289/3145 [14:40<05:32,  2.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2273/3145 [14:30<05:04,  2.87it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2303/3143 [14:27<05:14,  2.67it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2290/3145 [14:40<05:31,  2.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2288/3145 [14:27<05:34,  2.56it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2274/3145 [14:30<05:27,  2.66it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2304/3143 [14:28<05:12,  2.68it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2291/3145 [14:41<05:26,  2.61it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2289/3145 [14:28<05:27,  2.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2305/3143 [14:28<05:02,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2292/3145 [14:41<05:10,  2.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2275/3145 [14:31<05:28,  2.65it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2290/3145 [14:28<05:38,  2.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2306/3143 [14:28<05:00,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2293/3145 [14:41<05:17,  2.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2276/3145 [14:31<05:28,  2.65it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2291/3145 [14:29<05:32,  2.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2307/3143 [14:29<05:00,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2294/3145 [14:42<05:05,  2.79it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2277/3145 [14:31<05:16,  2.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2292/3145 [14:29<05:32,  2.56it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2308/3143 [14:29<04:55,  2.83it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2295/3145 [14:42<05:05,  2.78it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2278/3145 [14:32<05:16,  2.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2293/3145 [14:29<05:25,  2.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2279/3145 [14:32<04:42,  3.07it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2309/3143 [14:29<04:56,  2.81it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2296/3145 [14:42<05:11,  2.73it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2294/3145 [14:30<05:12,  2.72it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2280/3145 [14:32<04:41,  3.07it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2297/3145 [14:43<04:34,  3.09it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2310/3143 [14:30<04:54,  2.83it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2295/3145 [14:30<05:10,  2.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2281/3145 [14:33<04:47,  3.00it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2298/3145 [14:43<04:42,  3.00it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2311/3143 [14:30<04:57,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2296/3145 [14:30<05:13,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2299/3145 [14:43<04:55,  2.86it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2282/3145 [14:33<05:13,  2.75it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2312/3143 [14:31<04:57,  2.79it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2300/3145 [14:44<04:24,  3.20it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2297/3145 [14:31<05:18,  2.66it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2283/3145 [14:33<05:19,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2313/3143 [14:31<04:59,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2301/3145 [14:44<04:35,  3.07it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2298/3145 [14:31<05:21,  2.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2284/3145 [14:34<05:18,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2314/3143 [14:31<05:13,  2.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2302/3145 [14:44<04:49,  2.91it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2299/3145 [14:32<05:16,  2.67it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2285/3145 [14:34<05:22,  2.67it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2315/3143 [14:32<05:08,  2.69it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2303/3145 [14:45<04:51,  2.89it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2300/3145 [14:32<05:18,  2.65it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2316/3143 [14:32<04:57,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2286/3145 [14:35<05:27,  2.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2301/3145 [14:32<05:10,  2.72it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2304/3145 [14:45<05:27,  2.56it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2287/3145 [14:35<05:09,  2.77it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2317/3143 [14:32<04:56,  2.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2302/3145 [14:33<05:08,  2.73it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2318/3143 [14:33<04:52,  2.82it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2288/3145 [14:35<05:15,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2305/3145 [14:46<05:56,  2.36it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2303/3145 [14:33<05:14,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2319/3143 [14:33<04:53,  2.81it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2289/3145 [14:36<05:20,  2.67it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2306/3145 [14:46<05:52,  2.38it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2320/3143 [14:33<04:52,  2.81it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2304/3145 [14:33<05:19,  2.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2290/3145 [14:36<05:24,  2.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2307/3145 [14:46<05:59,  2.33it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2321/3143 [14:34<04:53,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2305/3145 [14:34<05:24,  2.59it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2291/3145 [14:36<05:13,  2.72it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2308/3145 [14:47<05:24,  2.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2322/3143 [14:34<04:52,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2306/3145 [14:34<05:29,  2.55it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2309/3145 [14:47<05:01,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2292/3145 [14:37<05:26,  2.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2323/3143 [14:35<04:52,  2.80it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2310/3145 [14:47<04:59,  2.79it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2307/3145 [14:35<05:28,  2.55it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2293/3145 [14:37<05:47,  2.45it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2311/3145 [14:48<04:42,  2.95it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2324/3143 [14:35<05:06,  2.67it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2308/3145 [14:35<05:17,  2.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2294/3145 [14:38<05:33,  2.55it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2312/3145 [14:48<05:01,  2.77it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2325/3143 [14:35<05:18,  2.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2309/3145 [14:35<05:29,  2.53it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2295/3145 [14:38<05:39,  2.50it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2313/3145 [14:48<04:44,  2.92it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2326/3143 [14:36<05:17,  2.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2310/3145 [14:36<05:20,  2.61it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2296/3145 [14:38<05:35,  2.53it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2314/3145 [14:49<04:42,  2.94it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2327/3143 [14:36<05:23,  2.53it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2315/3145 [14:49<04:16,  3.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2311/3145 [14:36<05:30,  2.52it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2297/3145 [14:39<05:25,  2.60it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2328/3143 [14:37<05:10,  2.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2312/3145 [14:37<05:22,  2.59it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2316/3145 [14:49<04:39,  2.97it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2298/3145 [14:39<05:25,  2.60it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2329/3143 [14:37<05:10,  2.62it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2313/3145 [14:37<05:12,  2.66it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2317/3145 [14:50<04:48,  2.87it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2299/3145 [14:40<05:33,  2.54it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2330/3143 [14:37<04:58,  2.72it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2314/3145 [14:37<05:07,  2.70it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2300/3145 [14:40<05:23,  2.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2318/3145 [14:50<05:13,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2331/3143 [14:38<04:44,  2.86it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2315/3145 [14:38<05:17,  2.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2301/3145 [14:40<05:20,  2.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2319/3145 [14:51<05:12,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2332/3143 [14:38<04:53,  2.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2316/3145 [14:38<05:17,  2.61it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2302/3145 [14:41<05:23,  2.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2320/3145 [14:51<05:17,  2.60it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2333/3143 [14:38<04:41,  2.88it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2317/3145 [14:38<05:05,  2.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2303/3145 [14:41<05:20,  2.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2321/3145 [14:51<05:12,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2334/3143 [14:39<05:00,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2318/3145 [14:39<05:02,  2.74it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2335/3143 [14:39<04:21,  3.09it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2304/3145 [14:41<05:12,  2.69it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2322/3145 [14:52<05:24,  2.53it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2319/3145 [14:39<05:07,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2336/3143 [14:39<04:42,  2.86it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2305/3145 [14:42<05:10,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2323/3145 [14:52<05:15,  2.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2320/3145 [14:39<04:31,  3.04it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2337/3143 [14:40<04:10,  3.21it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2306/3145 [14:42<05:12,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2338/3143 [14:40<03:48,  3.52it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2324/3145 [14:53<05:14,  2.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2321/3145 [14:40<04:38,  2.95it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2307/3145 [14:43<05:17,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2339/3143 [14:40<04:03,  3.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2322/3145 [14:40<04:53,  2.81it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2325/3145 [14:53<06:01,  2.27it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2323/3145 [14:40<04:20,  3.15it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2340/3143 [14:40<04:09,  3.22it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2308/3145 [14:43<05:17,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2324/3145 [14:41<03:59,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2326/3145 [14:53<05:46,  2.36it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2341/3143 [14:41<04:26,  3.00it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2309/3145 [14:43<05:08,  2.71it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2325/3145 [14:41<03:44,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2327/3145 [14:54<05:51,  2.33it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2342/3143 [14:41<04:39,  2.87it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2326/3145 [14:41<04:11,  3.25it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2310/3145 [14:44<05:24,  2.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2343/3143 [14:42<04:48,  2.77it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2311/3145 [14:44<05:21,  2.60it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2327/3145 [14:42<04:37,  2.95it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2328/3145 [14:54<06:05,  2.24it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2312/3145 [14:44<04:50,  2.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2344/3143 [14:42<04:49,  2.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2329/3145 [14:55<05:44,  2.37it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2328/3145 [14:42<04:46,  2.85it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2313/3145 [14:45<04:52,  2.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2345/3143 [14:42<04:43,  2.81it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2329/3145 [14:42<04:46,  2.85it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2330/3145 [14:55<05:40,  2.40it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2314/3145 [14:45<05:02,  2.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2346/3143 [14:43<04:43,  2.81it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2330/3145 [14:43<05:02,  2.69it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2331/3145 [14:56<05:43,  2.37it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2347/3143 [14:43<04:44,  2.80it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2315/3145 [14:46<05:09,  2.69it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2332/3145 [14:56<05:06,  2.66it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2331/3145 [14:43<05:16,  2.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2348/3143 [14:43<04:47,  2.77it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2316/3145 [14:46<05:06,  2.71it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2332/3145 [14:44<05:07,  2.65it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2333/3145 [14:56<05:35,  2.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2349/3143 [14:44<04:43,  2.80it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2317/3145 [14:46<05:01,  2.75it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2333/3145 [14:44<04:33,  2.96it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2334/3145 [14:57<05:53,  2.29it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2318/3145 [14:47<05:06,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2334/3145 [14:44<04:38,  2.91it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2350/3143 [14:44<05:31,  2.39it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2335/3145 [14:57<05:43,  2.36it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2319/3145 [14:47<05:12,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2335/3145 [14:45<05:06,  2.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2351/3143 [14:45<05:37,  2.35it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2336/3145 [14:58<05:13,  2.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2320/3145 [14:47<05:07,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2336/3145 [14:45<05:08,  2.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2352/3143 [14:45<05:20,  2.47it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2337/3145 [14:58<05:01,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2321/3145 [14:48<05:18,  2.59it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2337/3145 [14:45<05:03,  2.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2353/3143 [14:45<05:08,  2.56it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2338/3145 [14:58<05:06,  2.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2322/3145 [14:48<05:04,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2354/3143 [14:46<04:30,  2.92it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2338/3145 [14:46<04:58,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2339/3145 [14:59<05:01,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2355/3143 [14:46<04:07,  3.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2323/3145 [14:48<04:57,  2.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2339/3145 [14:46<04:57,  2.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2356/3143 [14:46<04:10,  3.14it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2340/3145 [14:59<04:55,  2.73it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2324/3145 [14:49<05:11,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2340/3145 [14:46<05:04,  2.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2357/3143 [14:47<04:19,  3.03it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2341/3145 [14:59<04:58,  2.69it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2325/3145 [14:49<04:59,  2.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2358/3143 [14:47<04:17,  3.05it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2342/3145 [15:00<04:50,  2.77it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2341/3145 [14:47<05:29,  2.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2326/3145 [14:50<05:04,  2.69it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2343/3145 [15:00<04:41,  2.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2359/3143 [14:47<04:54,  2.66it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2342/3145 [14:47<05:34,  2.40it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2327/3145 [14:50<05:01,  2.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2344/3145 [15:00<04:49,  2.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2360/3143 [14:48<04:56,  2.65it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2343/3145 [14:48<05:28,  2.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2328/3145 [14:50<05:05,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2345/3145 [15:01<04:45,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2361/3143 [14:48<04:50,  2.69it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2344/3145 [14:48<05:18,  2.52it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2329/3145 [14:51<05:01,  2.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2346/3145 [15:01<04:12,  3.16it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2362/3143 [14:49<04:54,  2.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2345/3145 [14:49<05:11,  2.57it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2330/3145 [14:51<05:02,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2347/3145 [15:01<04:43,  2.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2363/3143 [14:49<04:51,  2.67it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2346/3145 [14:49<05:11,  2.56it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2331/3145 [14:51<05:07,  2.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2348/3145 [15:02<05:04,  2.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2364/3143 [14:49<04:42,  2.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2332/3145 [14:52<04:54,  2.76it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2347/3145 [14:49<05:12,  2.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2349/3145 [15:02<04:54,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2348/3145 [14:50<04:35,  2.89it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2365/3143 [14:50<04:47,  2.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2333/3145 [14:52<05:01,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2350/3145 [15:03<04:56,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2349/3145 [14:50<04:33,  2.91it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2366/3143 [14:50<04:44,  2.74it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2334/3145 [14:53<04:49,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2351/3145 [15:03<05:00,  2.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2350/3145 [14:50<04:46,  2.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2367/3143 [14:50<04:51,  2.66it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2335/3145 [14:53<04:59,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2352/3145 [15:03<05:00,  2.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2351/3145 [14:51<04:56,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2368/3143 [14:51<04:54,  2.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2336/3145 [14:53<04:56,  2.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2353/3145 [15:04<05:00,  2.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2352/3145 [14:51<04:47,  2.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2337/3145 [14:54<04:56,  2.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2369/3143 [14:51<05:03,  2.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2354/3145 [15:04<04:54,  2.69it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2353/3145 [14:51<04:42,  2.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2370/3143 [14:52<04:48,  2.68it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2338/3145 [14:54<05:08,  2.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2355/3145 [15:05<04:49,  2.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2354/3145 [14:52<04:43,  2.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2371/3143 [14:52<04:44,  2.71it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2339/3145 [14:54<05:01,  2.67it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2356/3145 [15:05<04:18,  3.05it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2355/3145 [14:52<04:57,  2.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2372/3143 [14:52<04:33,  2.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2357/3145 [15:05<04:20,  3.03it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2340/3145 [14:55<05:03,  2.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2356/3145 [14:53<04:51,  2.70it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2373/3143 [14:53<04:36,  2.79it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2341/3145 [14:55<05:00,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2358/3145 [15:05<04:38,  2.83it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2357/3145 [14:53<04:49,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2374/3143 [14:53<04:34,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2359/3145 [15:06<04:34,  2.86it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2342/3145 [14:56<05:12,  2.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2358/3145 [14:53<04:53,  2.68it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2375/3143 [14:53<04:53,  2.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2360/3145 [15:06<04:41,  2.79it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2343/3145 [14:56<05:46,  2.32it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2359/3145 [14:54<04:51,  2.70it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2376/3143 [14:54<04:48,  2.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2361/3145 [15:07<04:39,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2360/3145 [14:54<04:53,  2.67it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2377/3143 [14:54<04:44,  2.69it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2344/3145 [14:57<05:53,  2.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2362/3145 [15:07<04:45,  2.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2361/3145 [14:54<04:51,  2.69it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2378/3143 [14:54<04:35,  2.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2345/3145 [14:57<05:43,  2.33it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2363/3145 [15:07<04:41,  2.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2362/3145 [14:55<04:47,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2379/3143 [14:55<04:41,  2.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2346/3145 [14:57<05:23,  2.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2364/3145 [15:08<04:40,  2.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2363/3145 [14:55<04:46,  2.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2347/3145 [14:58<05:06,  2.60it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2380/3143 [14:55<04:44,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2365/3145 [15:08<04:38,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2364/3145 [14:55<04:39,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2348/3145 [14:58<05:01,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2381/3143 [14:56<04:49,  2.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2366/3145 [15:08<04:42,  2.76it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2365/3145 [14:56<04:36,  2.83it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2382/3143 [14:56<04:30,  2.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2349/3145 [14:58<05:04,  2.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2367/3145 [15:09<04:40,  2.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2366/3145 [14:56<04:35,  2.82it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2383/3143 [14:56<04:31,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2368/3145 [15:09<04:32,  2.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2350/3145 [14:59<05:16,  2.51it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2384/3143 [14:56<04:03,  3.12it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2367/3145 [14:57<04:43,  2.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2369/3145 [15:09<04:30,  2.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2351/3145 [14:59<04:55,  2.69it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2385/3143 [14:57<04:20,  2.91it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2370/3145 [15:10<04:23,  2.94it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2368/3145 [14:57<05:05,  2.54it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2352/3145 [15:00<04:59,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2386/3143 [14:57<04:36,  2.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2371/3145 [15:10<04:36,  2.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2369/3145 [14:57<05:00,  2.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2353/3145 [15:00<04:50,  2.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2372/3145 [15:10<04:28,  2.87it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2387/3143 [14:58<04:40,  2.69it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2354/3145 [15:00<04:47,  2.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2370/3145 [14:58<04:59,  2.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2388/3143 [14:58<04:31,  2.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2373/3145 [15:11<04:35,  2.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2371/3145 [14:58<04:56,  2.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2355/3145 [15:01<04:58,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2389/3143 [14:58<04:36,  2.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2374/3145 [15:11<04:39,  2.76it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2356/3145 [15:01<04:47,  2.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2372/3145 [14:59<04:58,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2390/3143 [14:59<04:36,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2375/3145 [15:12<04:44,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2357/3145 [15:01<04:46,  2.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2373/3145 [14:59<05:24,  2.38it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2391/3143 [14:59<04:33,  2.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2358/3145 [15:02<04:44,  2.76it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2376/3145 [15:12<05:12,  2.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2374/3145 [14:59<05:25,  2.37it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2392/3143 [15:00<04:53,  2.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2359/3145 [15:02<04:45,  2.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2377/3145 [15:12<05:06,  2.50it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2375/3145 [15:00<05:27,  2.35it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2393/3143 [15:00<04:53,  2.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2360/3145 [15:02<04:45,  2.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2378/3145 [15:13<04:56,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2376/3145 [15:00<05:04,  2.52it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2394/3143 [15:00<04:45,  2.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2361/3145 [15:03<05:02,  2.60it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2379/3145 [15:13<04:55,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2395/3143 [15:01<04:19,  2.88it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2377/3145 [15:01<05:03,  2.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2362/3145 [15:03<04:48,  2.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2380/3145 [15:14<04:54,  2.60it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2396/3143 [15:01<04:28,  2.78it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2378/3145 [15:01<04:56,  2.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2363/3145 [15:04<04:45,  2.73it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2381/3145 [15:14<04:47,  2.66it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2379/3145 [15:01<04:37,  2.76it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2397/3143 [15:01<04:40,  2.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2364/3145 [15:04<04:38,  2.80it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2380/3145 [15:02<04:42,  2.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2382/3145 [15:14<05:17,  2.40it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2398/3143 [15:02<04:34,  2.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2365/3145 [15:04<04:33,  2.85it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2381/3145 [15:02<04:49,  2.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2366/3145 [15:05<04:34,  2.84it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2399/3143 [15:02<04:37,  2.68it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2383/3145 [15:15<05:34,  2.27it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2382/3145 [15:02<04:44,  2.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2367/3145 [15:05<04:37,  2.81it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2400/3143 [15:03<04:40,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2384/3145 [15:15<05:18,  2.39it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2368/3145 [15:05<04:31,  2.86it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2383/3145 [15:03<04:49,  2.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2385/3145 [15:16<05:03,  2.51it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2401/3143 [15:03<04:43,  2.62it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2384/3145 [15:03<04:51,  2.61it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2386/3145 [15:16<04:53,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2402/3143 [15:03<04:45,  2.60it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2369/3145 [15:06<05:16,  2.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2387/3145 [15:16<04:37,  2.73it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2385/3145 [15:04<04:47,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2403/3143 [15:04<04:33,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2370/3145 [15:06<05:03,  2.55it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2386/3145 [15:04<04:19,  2.93it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2388/3145 [15:17<04:48,  2.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2371/3145 [15:07<05:00,  2.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2404/3143 [15:04<05:08,  2.39it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2387/3145 [15:04<04:30,  2.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2372/3145 [15:07<04:52,  2.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2389/3145 [15:17<05:18,  2.37it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2388/3145 [15:05<04:29,  2.81it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2405/3143 [15:05<05:21,  2.30it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2373/3145 [15:07<04:57,  2.60it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2389/3145 [15:05<04:38,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2390/3145 [15:18<05:41,  2.21it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2406/3143 [15:05<05:34,  2.20it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2374/3145 [15:08<04:46,  2.69it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2390/3145 [15:05<04:38,  2.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2391/3145 [15:18<05:25,  2.32it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2407/3143 [15:05<05:14,  2.34it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2375/3145 [15:08<04:55,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2408/3143 [15:06<04:31,  2.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2392/3145 [15:19<05:02,  2.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2391/3145 [15:06<04:46,  2.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2376/3145 [15:08<04:48,  2.66it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2393/3145 [15:19<04:52,  2.57it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2409/3143 [15:06<04:34,  2.67it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2392/3145 [15:06<04:49,  2.60it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2377/3145 [15:09<04:42,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2394/3145 [15:19<04:45,  2.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2410/3143 [15:06<04:31,  2.70it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2393/3145 [15:07<04:59,  2.51it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2378/3145 [15:09<04:53,  2.62it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2395/3145 [15:20<04:33,  2.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2411/3143 [15:07<04:30,  2.70it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2379/3145 [15:10<04:42,  2.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2394/3145 [15:07<05:26,  2.30it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2396/3145 [15:20<04:37,  2.69it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2412/3143 [15:07<04:23,  2.77it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2395/3145 [15:07<04:59,  2.51it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2380/3145 [15:10<04:48,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2397/3145 [15:20<04:40,  2.66it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2413/3143 [15:08<04:33,  2.67it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2396/3145 [15:08<04:49,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2398/3145 [15:21<04:30,  2.76it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2381/3145 [15:10<05:12,  2.45it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2414/3143 [15:08<04:33,  2.66it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2397/3145 [15:08<04:42,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2399/3145 [15:21<04:15,  2.92it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2415/3143 [15:08<04:27,  2.72it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2382/3145 [15:11<05:06,  2.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2398/3145 [15:08<04:43,  2.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2400/3145 [15:21<04:17,  2.89it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2383/3145 [15:11<05:02,  2.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2416/3143 [15:09<04:35,  2.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2401/3145 [15:22<04:20,  2.86it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2399/3145 [15:09<04:54,  2.53it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2417/3143 [15:09<04:25,  2.73it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2384/3145 [15:12<05:19,  2.38it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2402/3145 [15:22<04:21,  2.84it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2418/3143 [15:09<03:56,  3.07it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2400/3145 [15:09<05:00,  2.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2385/3145 [15:12<04:37,  2.74it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2403/3145 [15:22<04:22,  2.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2419/3143 [15:10<04:10,  2.90it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2401/3145 [15:10<04:45,  2.61it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2386/3145 [15:12<04:35,  2.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2404/3145 [15:23<04:34,  2.70it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2420/3143 [15:10<04:18,  2.80it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2402/3145 [15:10<04:53,  2.54it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2387/3145 [15:13<04:34,  2.76it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2405/3145 [15:23<04:43,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2421/3143 [15:10<04:22,  2.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2403/3145 [15:10<04:46,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2388/3145 [15:13<05:21,  2.35it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2422/3143 [15:11<04:20,  2.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2406/3145 [15:24<04:42,  2.61it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2404/3145 [15:11<04:53,  2.52it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2389/3145 [15:14<05:06,  2.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2407/3145 [15:24<04:31,  2.71it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2423/3143 [15:11<04:26,  2.70it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2405/3145 [15:11<04:45,  2.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2408/3145 [15:24<04:02,  3.04it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2424/3143 [15:11<03:54,  3.07it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2390/3145 [15:14<05:27,  2.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2406/3145 [15:12<04:39,  2.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2409/3145 [15:25<04:07,  2.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2425/3143 [15:12<04:01,  2.98it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2391/3145 [15:14<05:15,  2.39it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2407/3145 [15:12<04:42,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2410/3145 [15:25<04:04,  3.01it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2426/3143 [15:12<03:59,  2.99it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2411/3145 [15:25<03:58,  3.08it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2392/3145 [15:15<05:16,  2.38it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2427/3143 [15:12<04:03,  2.94it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2408/3145 [15:12<04:54,  2.50it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2412/3145 [15:26<04:20,  2.82it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2428/3143 [15:13<04:11,  2.84it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2393/3145 [15:15<05:35,  2.24it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2409/3145 [15:13<05:07,  2.39it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2413/3145 [15:26<04:20,  2.81it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2429/3143 [15:13<04:07,  2.88it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2410/3145 [15:13<04:46,  2.56it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2394/3145 [15:16<05:18,  2.36it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2414/3145 [15:26<04:17,  2.84it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2395/3145 [15:16<05:05,  2.46it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2430/3143 [15:14<04:42,  2.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2411/3145 [15:14<04:59,  2.45it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2415/3145 [15:27<04:23,  2.77it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2396/3145 [15:17<04:59,  2.50it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2431/3143 [15:14<04:33,  2.60it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2412/3145 [15:14<04:54,  2.49it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2416/3145 [15:27<04:23,  2.77it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2397/3145 [15:17<04:50,  2.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2432/3143 [15:14<04:34,  2.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2413/3145 [15:14<04:58,  2.45it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2417/3145 [15:28<04:51,  2.49it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2433/3143 [15:15<04:27,  2.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2398/3145 [15:17<04:59,  2.50it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2414/3145 [15:15<04:47,  2.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2418/3145 [15:28<04:40,  2.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2399/3145 [15:18<05:11,  2.40it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2415/3145 [15:15<04:46,  2.55it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2434/3143 [15:15<04:53,  2.42it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2419/3145 [15:28<04:40,  2.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2400/3145 [15:18<04:53,  2.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2435/3143 [15:16<04:41,  2.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2416/3145 [15:16<04:58,  2.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2420/3145 [15:29<04:30,  2.68it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2401/3145 [15:18<04:26,  2.79it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2436/3143 [15:16<04:40,  2.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2417/3145 [15:16<04:48,  2.52it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2402/3145 [15:19<04:29,  2.76it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2421/3145 [15:29<04:52,  2.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2418/3145 [15:16<04:41,  2.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2437/3143 [15:16<04:44,  2.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2403/3145 [15:19<04:27,  2.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2422/3145 [15:29<04:40,  2.57it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2419/3145 [15:17<04:33,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2438/3143 [15:17<04:35,  2.56it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2423/3145 [15:30<04:29,  2.68it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2404/3145 [15:20<04:43,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2420/3145 [15:17<04:25,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2439/3143 [15:17<04:37,  2.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2424/3145 [15:30<04:25,  2.71it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2421/3145 [15:17<03:57,  3.05it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2405/3145 [15:20<04:51,  2.54it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2440/3143 [15:18<04:30,  2.60it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2425/3145 [15:30<04:22,  2.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2422/3145 [15:18<04:03,  2.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2406/3145 [15:20<05:02,  2.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2441/3143 [15:18<04:22,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2426/3145 [15:31<04:26,  2.69it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2423/3145 [15:18<04:13,  2.84it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2407/3145 [15:21<04:57,  2.48it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2442/3143 [15:18<04:24,  2.65it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2427/3145 [15:31<04:18,  2.78it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2424/3145 [15:18<04:09,  2.89it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2443/3143 [15:19<04:14,  2.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2428/3145 [15:32<04:11,  2.86it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2425/3145 [15:19<03:56,  3.04it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2408/3145 [15:21<05:30,  2.23it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2444/3143 [15:19<04:19,  2.69it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2426/3145 [15:19<04:03,  2.95it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2429/3145 [15:32<04:19,  2.76it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2409/3145 [15:22<05:18,  2.31it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2445/3143 [15:19<04:29,  2.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2427/3145 [15:20<04:21,  2.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2430/3145 [15:32<04:30,  2.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2410/3145 [15:22<05:00,  2.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2446/3143 [15:20<04:29,  2.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2431/3145 [15:33<04:29,  2.65it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2428/3145 [15:20<04:28,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2411/3145 [15:22<04:55,  2.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2447/3143 [15:20<04:34,  2.53it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2432/3145 [15:33<04:26,  2.68it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2429/3145 [15:20<04:33,  2.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2412/3145 [15:23<04:57,  2.46it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2433/3145 [15:33<04:23,  2.70it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2448/3143 [15:21<04:38,  2.50it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2430/3145 [15:21<04:29,  2.66it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2413/3145 [15:23<04:55,  2.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2434/3145 [15:34<04:21,  2.72it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2414/3145 [15:24<04:18,  2.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2431/3145 [15:21<04:20,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2449/3143 [15:21<04:35,  2.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2435/3145 [15:34<04:22,  2.71it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2450/3143 [15:21<04:21,  2.65it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2415/3145 [15:24<04:26,  2.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2432/3145 [15:21<04:27,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2436/3145 [15:35<04:18,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2451/3143 [15:22<04:19,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2416/3145 [15:24<04:32,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2433/3145 [15:22<04:30,  2.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2434/3145 [15:22<04:12,  2.81it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2452/3143 [15:22<04:22,  2.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2437/3145 [15:35<04:30,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2417/3145 [15:25<04:27,  2.72it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2435/3145 [15:22<04:08,  2.86it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2453/3143 [15:22<04:12,  2.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2418/3145 [15:25<04:23,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2438/3145 [15:35<04:59,  2.36it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2436/3145 [15:23<04:06,  2.88it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2454/3143 [15:23<04:10,  2.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2419/3145 [15:25<04:23,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2439/3145 [15:36<04:45,  2.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2437/3145 [15:23<04:04,  2.90it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2455/3143 [15:23<04:10,  2.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2420/3145 [15:26<04:22,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2440/3145 [15:36<04:34,  2.57it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2438/3145 [15:23<04:03,  2.90it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2456/3143 [15:24<04:05,  2.80it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2421/3145 [15:26<04:17,  2.81it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2441/3145 [15:37<04:48,  2.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2439/3145 [15:24<04:12,  2.80it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2422/3145 [15:26<04:12,  2.86it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2457/3143 [15:24<04:11,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2440/3145 [15:24<04:04,  2.88it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2442/3145 [15:37<04:36,  2.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2423/3145 [15:27<04:14,  2.83it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2458/3143 [15:24<04:09,  2.75it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2443/3145 [15:37<04:34,  2.55it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2459/3143 [15:25<04:07,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2441/3145 [15:25<04:37,  2.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2424/3145 [15:27<04:35,  2.61it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2444/3145 [15:38<04:26,  2.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2425/3145 [15:27<04:04,  2.94it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2460/3143 [15:25<04:01,  2.83it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2442/3145 [15:25<04:30,  2.60it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2426/3145 [15:28<03:43,  3.22it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2445/3145 [15:38<04:16,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2461/3143 [15:25<04:04,  2.79it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2443/3145 [15:25<04:32,  2.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2427/3145 [15:28<03:59,  3.00it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2446/3145 [15:38<04:08,  2.81it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2462/3143 [15:26<04:03,  2.80it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2428/3145 [15:28<03:31,  3.39it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2444/3145 [15:26<04:23,  2.66it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2447/3145 [15:39<04:08,  2.81it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2463/3143 [15:26<04:09,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2445/3145 [15:26<04:27,  2.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2429/3145 [15:29<03:54,  3.05it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2448/3145 [15:39<04:15,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2464/3143 [15:26<04:10,  2.71it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2430/3145 [15:29<04:04,  2.92it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2446/3145 [15:27<04:31,  2.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2449/3145 [15:40<04:27,  2.61it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2465/3143 [15:27<04:05,  2.76it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2431/3145 [15:29<04:06,  2.90it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2447/3145 [15:27<04:23,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2450/3145 [15:40<04:28,  2.59it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2466/3143 [15:27<04:16,  2.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2432/3145 [15:30<04:10,  2.85it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2448/3145 [15:27<04:23,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2451/3145 [15:40<04:16,  2.70it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2449/3145 [15:28<03:53,  2.98it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2467/3143 [15:28<04:18,  2.61it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2433/3145 [15:30<04:24,  2.69it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2452/3145 [15:41<04:19,  2.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2434/3145 [15:30<03:58,  2.99it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2450/3145 [15:28<04:06,  2.82it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2468/3143 [15:28<04:14,  2.65it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2435/3145 [15:31<03:41,  3.21it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2453/3145 [15:41<04:20,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2451/3145 [15:28<04:16,  2.70it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2469/3143 [15:28<04:16,  2.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2454/3145 [15:41<04:15,  2.71it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2436/3145 [15:31<04:03,  2.91it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2470/3143 [15:29<04:05,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2452/3145 [15:29<04:21,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2455/3145 [15:42<04:10,  2.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2437/3145 [15:32<04:14,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2471/3143 [15:29<04:04,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2453/3145 [15:29<04:32,  2.54it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2456/3145 [15:42<04:09,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2438/3145 [15:32<04:17,  2.75it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2472/3143 [15:30<04:15,  2.62it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2454/3145 [15:30<04:19,  2.66it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2457/3145 [15:42<04:09,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2439/3145 [15:32<04:11,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2473/3143 [15:30<04:12,  2.66it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2455/3145 [15:30<04:18,  2.67it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2458/3145 [15:43<04:16,  2.68it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2440/3145 [15:33<04:31,  2.60it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2474/3143 [15:30<04:05,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2456/3145 [15:30<04:27,  2.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2459/3145 [15:43<04:17,  2.67it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2441/3145 [15:33<04:31,  2.59it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2475/3143 [15:31<04:32,  2.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2457/3145 [15:31<04:42,  2.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2460/3145 [15:44<04:16,  2.67it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2442/3145 [15:33<04:25,  2.65it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2476/3143 [15:31<04:42,  2.36it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2461/3145 [15:44<04:07,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2458/3145 [15:31<04:50,  2.36it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2443/3145 [15:34<04:22,  2.68it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2462/3145 [15:44<04:00,  2.84it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2459/3145 [15:32<04:36,  2.48it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2477/3143 [15:32<04:50,  2.29it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2444/3145 [15:34<04:24,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2463/3145 [15:45<04:09,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2460/3145 [15:32<04:26,  2.57it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2478/3143 [15:32<04:45,  2.33it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2445/3145 [15:35<04:28,  2.61it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2464/3145 [15:45<04:06,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2461/3145 [15:32<04:16,  2.67it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2479/3143 [15:32<04:26,  2.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2446/3145 [15:35<04:36,  2.53it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2465/3145 [15:45<04:05,  2.77it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2462/3145 [15:33<04:13,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2480/3143 [15:33<04:19,  2.56it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2447/3145 [15:35<04:23,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2466/3145 [15:46<04:15,  2.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2481/3143 [15:33<04:13,  2.61it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2463/3145 [15:33<04:41,  2.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2448/3145 [15:36<04:32,  2.56it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2467/3145 [15:46<04:18,  2.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2482/3143 [15:33<04:09,  2.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2464/3145 [15:34<04:35,  2.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2449/3145 [15:36<04:31,  2.56it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2465/3145 [15:34<03:58,  2.85it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2483/3143 [15:34<04:10,  2.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2468/3145 [15:47<04:34,  2.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2450/3145 [15:37<04:23,  2.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2466/3145 [15:34<04:01,  2.82it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2469/3145 [15:47<04:25,  2.55it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2484/3143 [15:34<04:10,  2.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2451/3145 [15:37<04:13,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2467/3145 [15:34<04:01,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2470/3145 [15:47<04:19,  2.60it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2485/3143 [15:35<04:13,  2.60it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2452/3145 [15:37<04:13,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2468/3145 [15:35<04:01,  2.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2471/3145 [15:48<04:01,  2.79it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2486/3143 [15:35<04:06,  2.66it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2453/3145 [15:38<04:08,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2469/3145 [15:35<04:02,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2472/3145 [15:48<04:00,  2.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2487/3143 [15:35<04:09,  2.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2454/3145 [15:38<04:17,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2470/3145 [15:36<03:57,  2.85it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2473/3145 [15:48<03:57,  2.83it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2488/3143 [15:36<04:09,  2.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2455/3145 [15:38<04:16,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2474/3145 [15:49<03:58,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2471/3145 [15:36<04:13,  2.65it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2489/3143 [15:36<04:04,  2.68it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2456/3145 [15:39<04:10,  2.75it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2475/3145 [15:49<03:54,  2.86it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2472/3145 [15:36<04:05,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2490/3143 [15:36<03:58,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2457/3145 [15:39<04:07,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2476/3145 [15:49<03:55,  2.84it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2473/3145 [15:37<04:08,  2.71it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2491/3143 [15:37<03:53,  2.79it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2458/3145 [15:39<04:03,  2.82it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2477/3145 [15:50<03:58,  2.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2474/3145 [15:37<04:11,  2.67it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2492/3143 [15:37<03:57,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2459/3145 [15:40<04:11,  2.73it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2478/3145 [15:50<03:58,  2.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2475/3145 [15:37<04:15,  2.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2493/3143 [15:38<03:54,  2.78it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2460/3145 [15:40<04:15,  2.68it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2479/3145 [15:51<03:54,  2.84it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2476/3145 [15:38<04:11,  2.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2494/3143 [15:38<03:53,  2.78it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2461/3145 [15:41<04:18,  2.64it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2480/3145 [15:51<04:02,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2477/3145 [15:38<04:09,  2.68it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2495/3143 [15:38<03:57,  2.73it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2462/3145 [15:41<04:14,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2481/3145 [15:51<04:12,  2.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2478/3145 [15:39<04:07,  2.70it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2496/3143 [15:39<03:55,  2.74it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2463/3145 [15:41<04:16,  2.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2497/3143 [15:39<03:27,  3.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2482/3145 [15:52<04:04,  2.71it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2479/3145 [15:39<03:59,  2.78it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2464/3145 [15:42<04:20,  2.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2483/3145 [15:52<03:57,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2498/3143 [15:39<03:45,  2.86it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2480/3145 [15:39<04:01,  2.76it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2465/3145 [15:42<04:17,  2.64it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2484/3145 [15:52<03:57,  2.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2499/3143 [15:40<03:54,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2481/3145 [15:40<04:21,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2500/3143 [15:40<03:24,  3.15it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2485/3145 [15:53<03:50,  2.86it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2466/3145 [15:42<04:12,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2482/3145 [15:40<04:19,  2.56it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2467/3145 [15:43<04:06,  2.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2501/3143 [15:40<03:37,  2.96it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2486/3145 [15:53<03:57,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2483/3145 [15:41<04:19,  2.55it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2468/3145 [15:43<04:02,  2.79it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2487/3145 [15:53<03:58,  2.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2502/3143 [15:41<03:47,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2484/3145 [15:41<04:21,  2.53it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2469/3145 [15:43<04:07,  2.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2503/3143 [15:41<03:42,  2.88it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2488/3145 [15:54<04:00,  2.73it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2485/3145 [15:41<04:15,  2.59it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2470/3145 [15:44<04:06,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2489/3145 [15:54<03:52,  2.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2504/3143 [15:41<03:50,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2471/3145 [15:44<04:00,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2486/3145 [15:42<04:20,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2505/3143 [15:42<03:45,  2.83it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2490/3145 [15:55<04:00,  2.72it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2491/3145 [15:55<03:44,  2.92it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2472/3145 [15:45<04:01,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2487/3145 [15:42<04:22,  2.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2506/3143 [15:42<04:11,  2.53it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2492/3145 [15:55<03:20,  3.25it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2473/3145 [15:45<04:03,  2.76it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2493/3145 [15:55<03:04,  3.53it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2488/3145 [15:42<04:20,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2507/3143 [15:43<04:15,  2.49it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2474/3145 [15:45<04:01,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2494/3145 [15:56<03:18,  3.28it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2489/3145 [15:43<04:23,  2.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2508/3143 [15:43<04:04,  2.60it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2495/3145 [15:56<03:03,  3.54it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2475/3145 [15:46<04:01,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2490/3145 [15:43<04:20,  2.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2509/3143 [15:43<04:03,  2.60it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2496/3145 [15:56<03:05,  3.51it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2476/3145 [15:46<04:06,  2.72it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2491/3145 [15:44<04:17,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2510/3143 [15:44<03:57,  2.67it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2477/3145 [15:46<04:03,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2497/3145 [15:57<03:48,  2.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2511/3143 [15:44<03:54,  2.70it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2478/3145 [15:47<03:36,  3.08it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2492/3145 [15:44<04:18,  2.53it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2498/3145 [15:57<03:57,  2.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2512/3143 [15:44<03:46,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2479/3145 [15:47<03:51,  2.88it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2493/3145 [15:45<04:26,  2.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2499/3145 [15:57<03:58,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2513/3143 [15:45<03:46,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2480/3145 [15:47<03:58,  2.79it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2494/3145 [15:45<04:23,  2.47it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2500/3145 [15:58<03:53,  2.76it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2514/3143 [15:45<03:56,  2.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2481/3145 [15:48<04:00,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2495/3145 [15:45<04:10,  2.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2501/3145 [15:58<03:49,  2.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2515/3143 [15:46<03:48,  2.75it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2496/3145 [15:46<04:00,  2.70it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2482/3145 [15:48<04:11,  2.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2502/3145 [15:59<03:56,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2516/3143 [15:46<03:46,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2483/3145 [15:49<04:19,  2.55it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2497/3145 [15:46<04:22,  2.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2503/3145 [15:59<04:04,  2.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2517/3143 [15:46<03:51,  2.71it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2484/3145 [15:49<03:58,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2498/3145 [15:46<04:11,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2504/3145 [15:59<04:04,  2.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2518/3143 [15:47<03:48,  2.74it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2485/3145 [15:49<03:57,  2.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2499/3145 [15:47<04:06,  2.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2505/3145 [16:00<04:09,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2519/3143 [15:47<03:41,  2.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2486/3145 [15:50<03:58,  2.77it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2500/3145 [15:47<04:14,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2506/3145 [16:00<04:08,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2520/3143 [15:47<03:47,  2.73it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2487/3145 [15:50<03:57,  2.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2501/3145 [15:48<04:04,  2.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2507/3145 [16:00<04:08,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2521/3143 [15:48<03:51,  2.68it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2488/3145 [15:50<03:57,  2.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2502/3145 [15:48<04:01,  2.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2522/3143 [15:48<03:55,  2.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2508/3145 [16:01<04:21,  2.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2489/3145 [15:51<04:16,  2.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2503/3145 [15:48<03:57,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2523/3143 [15:49<03:54,  2.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2509/3145 [16:01<04:31,  2.34it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2490/3145 [15:51<04:14,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2504/3145 [15:49<04:04,  2.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2524/3143 [15:49<04:01,  2.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2510/3145 [16:02<04:23,  2.41it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2491/3145 [15:52<04:14,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2505/3145 [15:49<04:12,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2525/3143 [15:49<04:00,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2511/3145 [16:02<04:12,  2.51it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2492/3145 [15:52<04:04,  2.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2506/3145 [15:50<04:11,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2526/3143 [15:50<04:00,  2.57it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2493/3145 [15:52<04:01,  2.70it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2512/3145 [16:03<04:10,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2507/3145 [15:50<04:06,  2.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2527/3143 [15:50<03:49,  2.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2513/3145 [16:03<03:58,  2.65it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2494/3145 [15:53<03:56,  2.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2508/3145 [15:50<04:07,  2.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2528/3143 [15:50<03:48,  2.69it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2495/3145 [15:53<03:57,  2.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2514/3145 [16:03<04:05,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2509/3145 [15:51<04:14,  2.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2529/3143 [15:51<03:46,  2.71it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2496/3145 [15:53<03:59,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2515/3145 [16:04<04:00,  2.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2530/3143 [15:51<03:32,  2.88it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2510/3145 [15:51<04:04,  2.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2516/3145 [16:04<03:56,  2.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2497/3145 [15:54<04:02,  2.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2531/3143 [15:51<03:34,  2.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2511/3145 [15:51<03:59,  2.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2517/3145 [16:04<04:00,  2.61it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2532/3143 [15:52<03:25,  2.97it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2498/3145 [15:54<04:33,  2.37it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2512/3145 [15:52<03:58,  2.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2518/3145 [16:05<03:55,  2.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2533/3143 [15:52<03:32,  2.87it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2499/3145 [15:55<04:22,  2.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2513/3145 [15:52<03:54,  2.69it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2519/3145 [16:05<03:44,  2.79it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2534/3143 [15:52<03:31,  2.88it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2514/3145 [15:53<03:51,  2.72it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2500/3145 [15:55<04:20,  2.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2520/3145 [16:05<03:31,  2.95it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2535/3143 [15:53<03:40,  2.76it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2515/3145 [15:53<03:50,  2.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2521/3145 [16:06<03:27,  3.01it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2501/3145 [15:55<04:17,  2.50it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2536/3143 [15:53<03:37,  2.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2516/3145 [15:53<03:53,  2.69it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2522/3145 [16:06<03:31,  2.95it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2502/3145 [15:56<04:18,  2.49it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2537/3143 [15:54<03:36,  2.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2523/3145 [16:06<03:41,  2.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2517/3145 [15:54<04:03,  2.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2503/3145 [15:56<04:16,  2.50it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2538/3143 [15:54<03:32,  2.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2518/3145 [15:54<04:03,  2.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2504/3145 [15:57<04:07,  2.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2524/3145 [16:07<03:54,  2.65it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2539/3143 [15:54<03:37,  2.78it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2505/3145 [15:57<03:43,  2.86it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2519/3145 [15:54<04:07,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2525/3145 [16:07<03:56,  2.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2540/3143 [15:55<03:44,  2.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2506/3145 [15:57<03:50,  2.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2526/3145 [16:08<03:59,  2.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2520/3145 [15:55<04:18,  2.42it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2541/3143 [15:55<03:41,  2.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2507/3145 [15:58<03:56,  2.70it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2521/3145 [15:55<04:05,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2527/3145 [16:08<04:10,  2.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2508/3145 [15:58<03:29,  3.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2542/3143 [15:55<03:44,  2.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2522/3145 [15:56<03:57,  2.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2543/3143 [15:56<03:29,  2.86it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2509/3145 [15:58<03:34,  2.96it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2528/3145 [16:09<04:17,  2.39it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2523/3145 [15:56<04:04,  2.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2510/3145 [15:59<03:39,  2.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2544/3143 [15:56<03:40,  2.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2529/3145 [16:09<04:22,  2.34it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2524/3145 [15:56<03:54,  2.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2511/3145 [15:59<03:45,  2.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2545/3143 [15:57<03:45,  2.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2530/3145 [16:09<04:06,  2.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2525/3145 [15:57<03:54,  2.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2512/3145 [15:59<03:45,  2.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2546/3143 [15:57<03:43,  2.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2531/3145 [16:10<04:02,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2526/3145 [15:57<03:48,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2513/3145 [16:00<03:51,  2.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2547/3143 [15:57<03:45,  2.64it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2532/3145 [16:10<03:54,  2.61it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2527/3145 [15:57<03:41,  2.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2548/3143 [15:58<03:40,  2.69it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2514/3145 [16:00<03:56,  2.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2533/3145 [16:10<03:49,  2.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2528/3145 [15:58<03:22,  3.05it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2549/3143 [15:58<03:40,  2.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2534/3145 [16:11<03:43,  2.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2515/3145 [16:01<04:15,  2.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2529/3145 [15:58<03:33,  2.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2550/3143 [15:58<03:33,  2.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2535/3145 [16:11<03:47,  2.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2516/3145 [16:01<04:08,  2.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2530/3145 [15:58<03:40,  2.79it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2551/3143 [15:59<03:31,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2536/3145 [16:12<03:44,  2.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2531/3145 [15:59<03:34,  2.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2517/3145 [16:01<04:03,  2.58it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2532/3145 [15:59<03:09,  3.23it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2537/3145 [16:12<03:40,  2.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2552/3143 [15:59<03:47,  2.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2518/3145 [16:02<04:18,  2.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2533/3145 [15:59<03:20,  3.06it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2538/3145 [16:12<03:44,  2.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2553/3143 [16:00<04:07,  2.39it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2519/3145 [16:02<04:03,  2.57it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2534/3145 [16:00<03:25,  2.98it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2539/3145 [16:13<03:42,  2.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2520/3145 [16:03<04:00,  2.60it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2554/3143 [16:00<04:06,  2.39it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2535/3145 [16:00<03:29,  2.91it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2540/3145 [16:13<03:41,  2.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2555/3143 [16:00<03:57,  2.48it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2521/3145 [16:03<04:21,  2.38it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2536/3145 [16:01<03:37,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2541/3145 [16:13<03:38,  2.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2556/3143 [16:01<03:53,  2.51it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2537/3145 [16:01<03:36,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2542/3145 [16:14<03:34,  2.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2522/3145 [16:03<04:27,  2.33it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2557/3143 [16:01<03:51,  2.54it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2538/3145 [16:01<03:37,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2543/3145 [16:14<03:29,  2.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2523/3145 [16:04<04:26,  2.33it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2558/3143 [16:01<03:22,  2.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2544/3145 [16:14<03:27,  2.90it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2539/3145 [16:02<03:38,  2.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2524/3145 [16:04<04:15,  2.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2559/3143 [16:02<03:24,  2.86it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2545/3145 [16:15<03:34,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2540/3145 [16:02<03:39,  2.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2560/3143 [16:02<03:26,  2.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2525/3145 [16:05<04:12,  2.46it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2541/3145 [16:02<03:35,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2546/3145 [16:15<03:40,  2.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2561/3143 [16:03<03:27,  2.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2526/3145 [16:05<04:07,  2.50it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2542/3145 [16:03<03:43,  2.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2547/3145 [16:16<03:43,  2.68it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2562/3143 [16:03<03:23,  2.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2527/3145 [16:05<03:59,  2.58it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2543/3145 [16:03<03:38,  2.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2563/3143 [16:03<03:24,  2.84it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2548/3145 [16:16<03:53,  2.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2528/3145 [16:06<03:59,  2.58it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2544/3145 [16:03<03:33,  2.82it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2564/3143 [16:04<03:27,  2.79it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2549/3145 [16:16<03:53,  2.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2529/3145 [16:06<03:51,  2.66it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2545/3145 [16:04<03:33,  2.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2550/3145 [16:17<03:45,  2.64it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2565/3143 [16:04<03:33,  2.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2530/3145 [16:07<03:53,  2.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2546/3145 [16:04<03:44,  2.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2566/3143 [16:04<03:30,  2.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2551/3145 [16:17<03:46,  2.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2531/3145 [16:07<03:49,  2.68it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2547/3145 [16:05<03:43,  2.68it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2567/3143 [16:05<03:29,  2.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2552/3145 [16:17<03:47,  2.61it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2532/3145 [16:07<03:47,  2.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2548/3145 [16:05<03:47,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2568/3143 [16:05<03:28,  2.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2553/3145 [16:18<03:47,  2.60it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2533/3145 [16:08<03:45,  2.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2549/3145 [16:05<03:39,  2.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2569/3143 [16:05<03:33,  2.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2554/3145 [16:18<03:51,  2.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2534/3145 [16:08<03:48,  2.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2570/3143 [16:06<03:08,  3.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2550/3145 [16:06<03:43,  2.66it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2555/3145 [16:19<03:46,  2.61it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2535/3145 [16:08<03:52,  2.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2551/3145 [16:06<03:18,  2.99it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2571/3143 [16:06<03:29,  2.73it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2556/3145 [16:19<03:36,  2.72it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2552/3145 [16:06<03:18,  2.98it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2536/3145 [16:09<04:10,  2.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2557/3145 [16:19<03:18,  2.96it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2572/3143 [16:06<03:28,  2.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2553/3145 [16:07<03:28,  2.85it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2573/3143 [16:07<03:04,  3.08it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2537/3145 [16:09<04:00,  2.53it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2558/3145 [16:20<03:27,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2574/3143 [16:07<03:04,  3.09it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2554/3145 [16:07<03:35,  2.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2538/3145 [16:10<03:51,  2.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2559/3145 [16:20<03:34,  2.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2575/3143 [16:07<03:10,  2.99it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2555/3145 [16:07<03:34,  2.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2539/3145 [16:10<03:48,  2.65it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2560/3145 [16:20<03:32,  2.76it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2556/3145 [16:08<03:25,  2.87it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2576/3143 [16:08<03:18,  2.86it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2540/3145 [16:10<03:43,  2.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2561/3145 [16:21<03:42,  2.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2557/3145 [16:08<03:23,  2.88it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2577/3143 [16:08<03:19,  2.84it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2541/3145 [16:11<03:36,  2.79it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2542/3145 [16:11<03:09,  3.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2562/3145 [16:21<03:44,  2.59it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2558/3145 [16:08<03:31,  2.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2578/3143 [16:09<03:24,  2.77it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2543/3145 [16:11<03:16,  3.06it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2563/3145 [16:22<03:41,  2.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2559/3145 [16:09<03:31,  2.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2579/3143 [16:09<03:27,  2.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2544/3145 [16:12<03:23,  2.96it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2564/3145 [16:22<03:35,  2.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2560/3145 [16:09<03:28,  2.81it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2580/3143 [16:09<03:21,  2.80it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2545/3145 [16:12<03:37,  2.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2565/3145 [16:22<03:38,  2.65it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2561/3145 [16:10<03:38,  2.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2581/3143 [16:10<03:20,  2.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2546/3145 [16:12<03:40,  2.71it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2566/3145 [16:23<03:39,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2582/3143 [16:10<03:23,  2.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2562/3145 [16:10<03:48,  2.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2547/3145 [16:13<03:49,  2.61it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2583/3143 [16:10<03:24,  2.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2567/3145 [16:23<03:46,  2.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2563/3145 [16:10<03:44,  2.59it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2548/3145 [16:13<03:44,  2.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2584/3143 [16:11<03:21,  2.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2564/3145 [16:11<03:35,  2.70it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2568/3145 [16:24<03:49,  2.51it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2549/3145 [16:14<03:38,  2.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2565/3145 [16:11<03:30,  2.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2569/3145 [16:24<03:41,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2585/3143 [16:11<03:34,  2.60it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2550/3145 [16:14<03:17,  3.01it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2566/3145 [16:11<03:29,  2.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2586/3143 [16:11<03:27,  2.68it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2570/3145 [16:24<04:00,  2.40it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2551/3145 [16:14<03:27,  2.86it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2567/3145 [16:12<03:25,  2.81it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2587/3143 [16:12<03:24,  2.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2571/3145 [16:25<03:49,  2.50it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2552/3145 [16:15<03:36,  2.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2588/3143 [16:12<03:19,  2.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2568/3145 [16:12<03:36,  2.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2572/3145 [16:25<03:33,  2.68it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2553/3145 [16:15<03:45,  2.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2589/3143 [16:13<03:19,  2.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2569/3145 [16:13<03:38,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2573/3145 [16:26<03:46,  2.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2590/3143 [16:13<03:19,  2.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2554/3145 [16:15<03:57,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2570/3145 [16:13<03:39,  2.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2574/3145 [16:26<03:37,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2591/3143 [16:13<03:14,  2.84it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2555/3145 [16:16<03:54,  2.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2571/3145 [16:13<03:36,  2.65it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2575/3145 [16:26<03:37,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2592/3143 [16:14<03:10,  2.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2556/3145 [16:16<03:52,  2.53it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2572/3145 [16:14<03:37,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2576/3145 [16:27<03:32,  2.68it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2593/3143 [16:14<03:09,  2.90it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2573/3145 [16:14<03:31,  2.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2557/3145 [16:17<03:56,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2577/3145 [16:27<03:28,  2.72it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2594/3143 [16:14<03:10,  2.88it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2574/3145 [16:14<03:36,  2.64it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2558/3145 [16:17<03:47,  2.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2578/3145 [16:27<03:30,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3143 [16:15<03:08,  2.91it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2575/3145 [16:15<03:34,  2.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2579/3145 [16:28<03:24,  2.77it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2596/3143 [16:15<03:05,  2.95it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2559/3145 [16:17<04:01,  2.43it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2576/3145 [16:15<03:09,  2.99it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2580/3145 [16:28<03:18,  2.85it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2597/3143 [16:15<03:08,  2.90it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2560/3145 [16:18<03:51,  2.53it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2577/3145 [16:15<02:59,  3.16it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2581/3145 [16:28<03:12,  2.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2598/3143 [16:16<03:09,  2.87it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2561/3145 [16:18<03:46,  2.57it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2578/3145 [16:16<03:04,  3.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2582/3145 [16:29<03:16,  2.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2599/3143 [16:16<03:06,  2.91it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2562/3145 [16:19<03:41,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2579/3145 [16:16<03:15,  2.89it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2583/3145 [16:29<03:22,  2.77it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2600/3143 [16:16<03:08,  2.88it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2563/3145 [16:19<03:33,  2.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2580/3145 [16:16<03:23,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2601/3143 [16:17<03:14,  2.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2584/3145 [16:30<03:35,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2564/3145 [16:19<03:33,  2.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2581/3145 [16:17<03:22,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2602/3143 [16:17<03:17,  2.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2585/3145 [16:30<03:38,  2.56it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2565/3145 [16:20<03:37,  2.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2582/3145 [16:17<03:28,  2.70it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2603/3143 [16:17<03:13,  2.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2586/3145 [16:30<03:35,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2566/3145 [16:20<03:41,  2.61it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2583/3145 [16:18<03:26,  2.72it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2604/3143 [16:18<03:16,  2.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2587/3145 [16:31<03:30,  2.65it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2584/3145 [16:18<03:25,  2.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2567/3145 [16:20<03:50,  2.51it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2605/3143 [16:18<03:15,  2.75it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2588/3145 [16:31<03:35,  2.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2585/3145 [16:18<03:24,  2.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2568/3145 [16:21<03:56,  2.44it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2606/3143 [16:18<03:09,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2589/3145 [16:31<03:30,  2.64it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2586/3145 [16:19<03:20,  2.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2569/3145 [16:21<03:52,  2.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2607/3143 [16:19<03:06,  2.87it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2587/3145 [16:19<03:26,  2.70it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2590/3145 [16:32<03:50,  2.41it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2570/3145 [16:22<03:44,  2.56it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2608/3143 [16:19<03:02,  2.94it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2588/3145 [16:19<03:24,  2.73it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2609/3143 [16:20<03:06,  2.86it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2571/3145 [16:22<03:58,  2.41it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2591/3145 [16:32<04:05,  2.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2589/3145 [16:20<03:16,  2.82it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2610/3143 [16:20<03:16,  2.71it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2572/3145 [16:22<03:44,  2.56it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2592/3145 [16:33<03:55,  2.35it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2590/3145 [16:20<03:13,  2.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2611/3143 [16:20<03:15,  2.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2573/3145 [16:23<03:44,  2.55it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2591/3145 [16:20<03:15,  2.84it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2593/3145 [16:33<04:02,  2.27it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2612/3143 [16:21<03:17,  2.69it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2592/3145 [16:21<03:13,  2.86it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2574/3145 [16:23<03:53,  2.45it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2594/3145 [16:34<03:45,  2.45it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2613/3143 [16:21<03:15,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3145 [16:34<03:14,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2593/3145 [16:21<03:17,  2.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2575/3145 [16:24<04:00,  2.37it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2596/3145 [16:34<03:19,  2.75it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2614/3143 [16:21<03:24,  2.59it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2594/3145 [16:21<03:18,  2.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2576/3145 [16:24<03:55,  2.42it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2597/3145 [16:35<03:22,  2.70it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2615/3143 [16:22<03:24,  2.59it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3145 [16:22<03:18,  2.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2577/3145 [16:24<03:41,  2.56it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2598/3145 [16:35<03:24,  2.68it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2596/3145 [16:22<03:11,  2.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2616/3143 [16:22<03:26,  2.56it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2578/3145 [16:25<03:31,  2.68it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2599/3145 [16:35<03:22,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2597/3145 [16:23<03:19,  2.75it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2617/3143 [16:23<03:23,  2.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2579/3145 [16:25<03:28,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2600/3145 [16:36<03:29,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2598/3145 [16:23<03:23,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2618/3143 [16:23<03:27,  2.53it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2580/3145 [16:26<03:36,  2.61it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2599/3145 [16:23<03:29,  2.61it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2619/3143 [16:23<03:21,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2601/3145 [16:36<03:47,  2.39it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2581/3145 [16:26<03:41,  2.55it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2600/3145 [16:24<03:15,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2620/3143 [16:24<03:14,  2.68it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2582/3145 [16:26<03:30,  2.68it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2602/3145 [16:37<03:42,  2.44it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2601/3145 [16:24<03:24,  2.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2621/3143 [16:24<03:21,  2.59it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2583/3145 [16:27<03:38,  2.57it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2603/3145 [16:37<03:38,  2.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2622/3143 [16:25<03:12,  2.70it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2602/3145 [16:25<03:32,  2.56it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2584/3145 [16:27<03:34,  2.61it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2604/3145 [16:37<03:41,  2.44it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2623/3143 [16:25<03:17,  2.63it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2603/3145 [16:25<03:26,  2.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2585/3145 [16:27<03:29,  2.67it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2605/3145 [16:38<03:37,  2.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2604/3145 [16:25<03:21,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2624/3143 [16:25<03:17,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2586/3145 [16:28<03:26,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2606/3145 [16:38<03:28,  2.58it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2605/3145 [16:26<03:03,  2.95it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2625/3143 [16:26<03:09,  2.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2587/3145 [16:28<03:23,  2.74it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2607/3145 [16:39<03:24,  2.64it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2606/3145 [16:26<02:57,  3.03it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2626/3143 [16:26<03:16,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2588/3145 [16:29<03:20,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2608/3145 [16:39<03:22,  2.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2607/3145 [16:26<02:58,  3.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2589/3145 [16:29<03:11,  2.90it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2627/3143 [16:26<03:10,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2608/3145 [16:27<03:03,  2.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2609/3145 [16:39<03:24,  2.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2590/3145 [16:29<02:56,  3.14it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2628/3143 [16:27<03:04,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2610/3145 [16:40<03:18,  2.70it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2609/3145 [16:27<03:16,  2.73it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2591/3145 [16:30<03:27,  2.67it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2629/3143 [16:27<03:18,  2.59it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2611/3145 [16:40<03:20,  2.67it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2610/3145 [16:27<03:11,  2.80it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2592/3145 [16:30<03:21,  2.75it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2630/3143 [16:28<03:21,  2.54it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2611/3145 [16:28<03:11,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2612/3145 [16:40<03:23,  2.62it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2593/3145 [16:30<03:10,  2.90it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2631/3143 [16:28<03:15,  2.62it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2612/3145 [16:28<03:07,  2.84it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2613/3145 [16:41<03:24,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2594/3145 [16:31<03:16,  2.81it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2632/3143 [16:28<02:51,  2.97it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2613/3145 [16:28<03:12,  2.76it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2614/3145 [16:41<03:29,  2.53it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2595/3145 [16:31<03:21,  2.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2633/3143 [16:29<03:08,  2.70it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2614/3145 [16:29<03:11,  2.77it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2596/3145 [16:31<03:23,  2.69it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2634/3143 [16:29<03:06,  2.73it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2615/3145 [16:42<03:44,  2.36it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2615/3145 [16:29<03:17,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2597/3145 [16:32<03:22,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2616/3145 [16:42<03:40,  2.39it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2635/3143 [16:29<03:15,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2616/3145 [16:29<03:16,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2598/3145 [16:32<03:17,  2.77it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2636/3143 [16:30<03:15,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2617/3145 [16:43<03:39,  2.41it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2617/3145 [16:30<03:12,  2.75it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2599/3145 [16:32<03:11,  2.85it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2618/3145 [16:43<03:26,  2.56it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2637/3143 [16:30<03:12,  2.63it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2618/3145 [16:30<03:09,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2600/3145 [16:33<03:13,  2.82it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2619/3145 [16:43<03:05,  2.83it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2638/3143 [16:30<02:52,  2.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2619/3145 [16:31<03:09,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2601/3145 [16:33<03:11,  2.84it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2639/3143 [16:31<02:55,  2.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2620/3145 [16:44<03:12,  2.73it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2620/3145 [16:31<03:15,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2602/3145 [16:34<03:13,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2640/3143 [16:31<02:57,  2.83it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2621/3145 [16:44<03:15,  2.68it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2621/3145 [16:31<03:17,  2.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2603/3145 [16:34<03:09,  2.86it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2641/3143 [16:32<03:02,  2.74it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2622/3145 [16:44<03:17,  2.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2604/3145 [16:34<02:53,  3.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2622/3145 [16:32<03:21,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2623/3145 [16:45<03:04,  2.82it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2605/3145 [16:34<02:46,  3.24it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2642/3143 [16:32<03:06,  2.69it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2606/3145 [16:35<02:48,  3.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2623/3145 [16:32<03:32,  2.46it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2624/3145 [16:45<03:07,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2643/3143 [16:32<03:01,  2.75it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2644/3143 [16:32<02:39,  3.13it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2607/3145 [16:35<02:53,  3.11it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2625/3145 [16:45<03:06,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2624/3145 [16:33<03:38,  2.39it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2645/3143 [16:33<02:43,  3.05it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2608/3145 [16:35<02:56,  3.04it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2626/3145 [16:46<03:11,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2625/3145 [16:33<03:40,  2.36it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2609/3145 [16:36<02:40,  3.34it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2646/3143 [16:33<02:47,  2.96it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2627/3145 [16:46<03:14,  2.66it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2626/3145 [16:33<03:36,  2.39it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2610/3145 [16:36<02:54,  3.06it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2647/3143 [16:34<03:00,  2.74it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2628/3145 [16:47<03:14,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2648/3143 [16:34<02:45,  2.99it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2611/3145 [16:36<03:07,  2.85it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2627/3145 [16:34<03:54,  2.20it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2629/3145 [16:47<03:08,  2.74it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2649/3143 [16:34<02:52,  2.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2612/3145 [16:37<03:09,  2.81it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2628/3145 [16:34<03:41,  2.33it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2630/3145 [16:47<03:10,  2.70it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2650/3143 [16:35<03:13,  2.54it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2613/3145 [16:37<03:32,  2.50it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2631/3145 [16:48<03:11,  2.69it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2629/3145 [16:35<03:57,  2.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2651/3143 [16:35<03:04,  2.66it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2614/3145 [16:38<03:30,  2.52it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2632/3145 [16:48<03:14,  2.64it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2630/3145 [16:35<03:50,  2.23it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2652/3143 [16:35<03:05,  2.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2615/3145 [16:38<03:20,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2633/3145 [16:48<03:03,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2631/3145 [16:36<03:37,  2.36it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2653/3143 [16:36<03:05,  2.63it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2616/3145 [16:38<03:18,  2.67it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2634/3145 [16:49<03:02,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2632/3145 [16:36<03:26,  2.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2617/3145 [16:39<03:13,  2.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2654/3143 [16:36<03:06,  2.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2635/3145 [16:49<03:06,  2.74it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2633/3145 [16:36<03:22,  2.53it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2618/3145 [16:39<03:06,  2.83it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2655/3143 [16:37<03:04,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2636/3145 [16:49<03:02,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2634/3145 [16:37<03:16,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2619/3145 [16:39<03:12,  2.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2656/3143 [16:37<03:01,  2.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2637/3145 [16:50<03:10,  2.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2635/3145 [16:37<03:16,  2.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2620/3145 [16:40<03:04,  2.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2657/3143 [16:37<02:57,  2.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2638/3145 [16:50<03:17,  2.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2636/3145 [16:37<03:04,  2.76it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2621/3145 [16:40<03:03,  2.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2658/3143 [16:38<03:00,  2.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2639/3145 [16:51<03:08,  2.69it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2637/3145 [16:38<03:07,  2.71it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2622/3145 [16:40<02:58,  2.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2659/3143 [16:38<03:02,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2640/3145 [16:51<03:01,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2638/3145 [16:38<03:00,  2.81it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2623/3145 [16:41<03:21,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2660/3143 [16:38<02:55,  2.75it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2641/3145 [16:51<03:00,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2639/3145 [16:39<02:56,  2.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2661/3143 [16:39<02:58,  2.70it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2642/3145 [16:52<03:00,  2.79it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2640/3145 [16:39<03:02,  2.77it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2624/3145 [16:41<03:43,  2.33it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2643/3145 [16:52<02:55,  2.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2662/3143 [16:39<02:58,  2.70it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2641/3145 [16:39<03:04,  2.73it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2625/3145 [16:42<03:38,  2.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2644/3145 [16:52<02:42,  3.08it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2663/3143 [16:40<03:01,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2642/3145 [16:40<02:59,  2.80it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2626/3145 [16:42<03:37,  2.39it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2645/3145 [16:53<02:53,  2.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2664/3143 [16:40<02:58,  2.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2643/3145 [16:40<03:10,  2.64it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2627/3145 [16:43<03:28,  2.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2665/3143 [16:40<02:37,  3.03it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2646/3145 [16:53<03:00,  2.76it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2628/3145 [16:43<03:25,  2.51it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2666/3143 [16:41<02:41,  2.95it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2644/3145 [16:41<03:22,  2.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2667/3143 [16:41<02:25,  3.26it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2647/3145 [16:54<03:23,  2.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2629/3145 [16:43<03:25,  2.51it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2645/3145 [16:41<03:20,  2.50it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2648/3145 [16:54<03:11,  2.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2668/3143 [16:41<02:30,  3.15it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2646/3145 [16:41<02:56,  2.83it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2630/3145 [16:44<03:33,  2.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2649/3145 [16:54<03:04,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2669/3143 [16:41<02:37,  3.01it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2647/3145 [16:42<02:57,  2.81it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2650/3145 [16:55<02:58,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2631/3145 [16:44<03:29,  2.45it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2670/3143 [16:42<02:46,  2.85it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2648/3145 [16:42<02:57,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2632/3145 [16:45<03:22,  2.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2651/3145 [16:55<02:59,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2671/3143 [16:42<02:57,  2.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2649/3145 [16:42<03:02,  2.71it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2652/3145 [16:55<02:54,  2.83it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2633/3145 [16:45<03:18,  2.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2672/3143 [16:43<02:53,  2.71it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2650/3145 [16:43<03:06,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2653/3145 [16:56<02:51,  2.87it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2634/3145 [16:45<03:14,  2.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2651/3145 [16:43<02:44,  2.99it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2673/3143 [16:43<02:55,  2.67it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2654/3145 [16:56<02:57,  2.76it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2635/3145 [16:46<03:11,  2.67it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2652/3145 [16:43<02:55,  2.81it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2674/3143 [16:43<02:56,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2636/3145 [16:46<03:06,  2.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2655/3145 [16:56<03:06,  2.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2653/3145 [16:44<02:59,  2.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2675/3143 [16:44<02:58,  2.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2656/3145 [16:57<02:53,  2.81it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2637/3145 [16:46<03:06,  2.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2654/3145 [16:44<02:58,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2676/3143 [16:44<02:52,  2.71it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2657/3145 [16:57<02:37,  3.09it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2638/3145 [16:47<03:04,  2.75it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2655/3145 [16:44<02:58,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2658/3145 [16:57<02:42,  2.99it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2677/3143 [16:45<02:54,  2.67it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2639/3145 [16:47<03:07,  2.70it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2656/3145 [16:45<02:52,  2.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2678/3143 [16:45<02:49,  2.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2659/3145 [16:58<02:51,  2.84it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2640/3145 [16:48<03:01,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2657/3145 [16:45<02:57,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2679/3143 [16:45<02:47,  2.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2660/3145 [16:58<02:55,  2.76it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2641/3145 [16:48<03:10,  2.65it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2658/3145 [16:46<03:00,  2.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2680/3143 [16:46<02:49,  2.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2661/3145 [16:58<02:50,  2.84it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2642/3145 [16:48<03:01,  2.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2659/3145 [16:46<02:41,  3.01it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2662/3145 [16:59<02:49,  2.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2681/3143 [16:46<02:57,  2.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2660/3145 [16:46<02:48,  2.87it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2643/3145 [16:49<03:26,  2.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2663/3145 [16:59<02:47,  2.88it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2682/3143 [16:46<02:57,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2664/3145 [16:59<02:27,  3.25it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2661/3145 [16:47<02:50,  2.84it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2644/3145 [16:49<03:32,  2.36it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2683/3143 [16:47<02:50,  2.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2665/3145 [17:00<02:34,  3.11it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2662/3145 [16:47<02:51,  2.82it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2645/3145 [16:50<03:18,  2.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2684/3143 [16:47<02:47,  2.73it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2666/3145 [17:00<02:39,  3.00it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2663/3145 [16:47<02:53,  2.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2685/3143 [16:47<02:47,  2.74it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2646/3145 [16:50<03:21,  2.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2667/3145 [17:00<02:39,  3.00it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2664/3145 [16:48<02:53,  2.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2686/3143 [16:48<02:41,  2.84it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2647/3145 [16:50<03:24,  2.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2668/3145 [17:01<02:56,  2.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2665/3145 [16:48<02:59,  2.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2687/3143 [16:48<02:39,  2.85it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2648/3145 [16:51<03:15,  2.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2666/3145 [16:48<03:02,  2.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2669/3145 [17:01<03:06,  2.56it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2688/3143 [16:49<02:42,  2.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2649/3145 [16:51<03:14,  2.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2670/3145 [17:02<02:57,  2.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2667/3145 [16:49<03:00,  2.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2689/3143 [16:49<02:43,  2.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2650/3145 [16:52<03:22,  2.45it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2668/3145 [16:49<02:58,  2.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2671/3145 [17:02<02:57,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2690/3143 [16:49<02:44,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2672/3145 [17:02<02:51,  2.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2669/3145 [16:50<02:59,  2.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2651/3145 [16:52<03:31,  2.34it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2691/3143 [16:50<02:42,  2.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2673/3145 [17:03<02:50,  2.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2670/3145 [16:50<02:58,  2.65it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2652/3145 [16:52<03:26,  2.38it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2692/3143 [16:50<02:48,  2.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2674/3145 [17:03<02:58,  2.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2671/3145 [16:50<02:56,  2.68it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2653/3145 [16:53<03:23,  2.41it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2693/3143 [16:50<02:50,  2.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2672/3145 [16:51<02:56,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2675/3145 [17:03<02:59,  2.62it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2694/3143 [16:51<02:51,  2.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2654/3145 [16:53<03:26,  2.37it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2676/3145 [17:04<02:38,  2.96it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2673/3145 [16:51<02:55,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2695/3143 [16:51<02:51,  2.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2677/3145 [17:04<02:40,  2.92it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2655/3145 [16:54<03:35,  2.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2674/3145 [16:51<02:58,  2.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2696/3143 [16:52<02:45,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2678/3145 [17:04<02:38,  2.94it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2656/3145 [16:54<03:36,  2.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2675/3145 [16:52<03:00,  2.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2697/3143 [16:52<02:44,  2.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2679/3145 [17:05<02:47,  2.79it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2698/3143 [16:52<02:25,  3.06it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2676/3145 [16:52<02:56,  2.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2657/3145 [16:55<03:43,  2.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2680/3145 [17:05<02:50,  2.73it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2699/3143 [16:53<02:33,  2.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2677/3145 [16:53<02:57,  2.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2658/3145 [16:55<03:48,  2.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2681/3145 [17:06<02:50,  2.72it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2700/3143 [16:53<02:46,  2.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2678/3145 [16:53<03:02,  2.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2682/3145 [17:06<02:43,  2.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2659/3145 [16:56<03:50,  2.11it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2701/3143 [16:53<02:47,  2.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2679/3145 [16:53<02:55,  2.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2683/3145 [17:06<02:44,  2.81it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2660/3145 [16:56<03:32,  2.28it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2702/3143 [16:54<02:46,  2.65it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2680/3145 [16:54<03:02,  2.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2684/3145 [17:07<02:52,  2.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2661/3145 [16:56<03:21,  2.40it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2703/3143 [16:54<02:47,  2.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2681/3145 [16:54<02:58,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2662/3145 [16:57<03:08,  2.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2685/3145 [17:07<03:14,  2.36it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2704/3143 [16:55<02:57,  2.47it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2663/3145 [16:57<02:59,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2682/3145 [16:55<03:11,  2.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2686/3145 [17:08<03:13,  2.38it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2705/3143 [16:55<02:48,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2683/3145 [16:55<03:08,  2.45it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2664/3145 [16:58<03:04,  2.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2706/3143 [16:55<02:44,  2.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2687/3145 [17:08<03:23,  2.25it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2665/3145 [16:58<02:59,  2.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2684/3145 [16:55<03:11,  2.40it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2707/3143 [16:56<02:41,  2.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2688/3145 [17:08<03:07,  2.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2666/3145 [16:58<02:56,  2.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2685/3145 [16:56<03:00,  2.54it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2708/3143 [16:56<02:38,  2.75it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2689/3145 [17:09<03:00,  2.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2667/3145 [16:59<03:03,  2.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2686/3145 [16:56<03:00,  2.55it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2690/3145 [17:09<02:39,  2.85it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2709/3143 [16:56<02:41,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2668/3145 [16:59<03:02,  2.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2687/3145 [16:57<02:59,  2.55it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2691/3145 [17:09<02:39,  2.84it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2710/3143 [16:57<02:42,  2.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2669/3145 [16:59<03:04,  2.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2688/3145 [16:57<02:58,  2.56it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2692/3145 [17:10<02:42,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2711/3143 [16:57<02:27,  2.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2670/3145 [17:00<03:00,  2.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2689/3145 [16:57<02:54,  2.61it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2693/3145 [17:10<02:45,  2.73it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2712/3143 [16:57<02:33,  2.81it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2671/3145 [17:00<02:37,  3.01it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2690/3145 [16:58<02:47,  2.72it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2713/3143 [16:58<02:33,  2.81it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2694/3145 [17:11<02:47,  2.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2672/3145 [17:00<02:40,  2.95it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2691/3145 [16:58<02:49,  2.67it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2714/3143 [16:58<02:28,  2.89it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2695/3145 [17:11<02:45,  2.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2673/3145 [17:01<02:44,  2.87it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2692/3145 [16:58<02:47,  2.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2715/3143 [16:58<02:30,  2.85it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2696/3145 [17:11<02:42,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2674/3145 [17:01<02:53,  2.71it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2693/3145 [16:59<02:42,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2697/3145 [17:12<02:41,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2716/3143 [16:59<02:35,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2675/3145 [17:02<02:52,  2.73it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2694/3145 [16:59<02:37,  2.86it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2698/3145 [17:12<02:41,  2.77it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2717/3143 [16:59<02:39,  2.66it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2695/3145 [16:59<02:21,  3.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2676/3145 [17:02<02:55,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2718/3143 [16:59<02:21,  3.01it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2699/3145 [17:12<02:43,  2.72it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2696/3145 [17:00<02:27,  3.05it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2719/3143 [17:00<02:06,  3.34it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2677/3145 [17:02<02:53,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2720/3143 [17:00<01:57,  3.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2700/3145 [17:13<02:50,  2.61it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2697/3145 [17:00<02:37,  2.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2678/3145 [17:03<02:51,  2.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2721/3143 [17:00<01:51,  3.79it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2701/3145 [17:13<02:46,  2.66it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2698/3145 [17:00<02:39,  2.81it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2679/3145 [17:03<02:46,  2.80it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2722/3143 [17:00<02:01,  3.47it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2702/3145 [17:14<02:48,  2.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2680/3145 [17:03<02:37,  2.95it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2699/3145 [17:01<02:37,  2.84it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2723/3143 [17:01<02:10,  3.21it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2681/3145 [17:04<02:40,  2.90it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2703/3145 [17:14<02:48,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2700/3145 [17:01<02:45,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2724/3143 [17:01<02:21,  2.97it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2704/3145 [17:14<02:42,  2.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2682/3145 [17:04<02:41,  2.87it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2725/3143 [17:02<02:19,  3.00it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2701/3145 [17:02<02:48,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2705/3145 [17:15<02:39,  2.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2683/3145 [17:04<02:39,  2.89it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2702/3145 [17:02<02:49,  2.61it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2726/3143 [17:02<02:29,  2.80it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2706/3145 [17:15<02:39,  2.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2684/3145 [17:05<02:46,  2.76it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2727/3143 [17:02<02:34,  2.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2703/3145 [17:02<02:57,  2.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2707/3145 [17:15<02:48,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2685/3145 [17:05<02:50,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2728/3143 [17:03<02:29,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2704/3145 [17:03<02:55,  2.51it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2686/3145 [17:05<02:46,  2.75it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2708/3145 [17:16<02:45,  2.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2729/3143 [17:03<02:28,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2705/3145 [17:03<03:02,  2.41it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2687/3145 [17:06<02:50,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2709/3145 [17:16<02:47,  2.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2730/3143 [17:03<02:25,  2.84it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2710/3145 [17:16<02:41,  2.69it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2706/3145 [17:04<03:07,  2.34it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2688/3145 [17:06<02:54,  2.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2731/3143 [17:04<02:47,  2.46it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2711/3145 [17:17<02:43,  2.65it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2707/3145 [17:04<03:01,  2.42it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2689/3145 [17:07<02:59,  2.55it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2732/3143 [17:04<02:48,  2.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2712/3145 [17:17<02:43,  2.65it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2690/3145 [17:07<02:49,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2708/3145 [17:04<02:58,  2.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2733/3143 [17:05<02:45,  2.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2713/3145 [17:18<02:41,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2709/3145 [17:05<02:52,  2.53it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2691/3145 [17:07<02:52,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2692/3145 [17:08<02:33,  2.96it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2734/3143 [17:05<02:42,  2.52it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2714/3145 [17:18<02:36,  2.76it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2710/3145 [17:05<02:47,  2.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2715/3145 [17:18<02:34,  2.78it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2735/3143 [17:06<02:49,  2.41it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2711/3145 [17:06<02:47,  2.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2693/3145 [17:08<02:59,  2.51it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2716/3145 [17:19<02:34,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2712/3145 [17:06<02:46,  2.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2694/3145 [17:09<02:58,  2.53it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2736/3143 [17:06<02:57,  2.30it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2717/3145 [17:19<02:29,  2.86it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2713/3145 [17:06<02:42,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2737/3143 [17:06<02:43,  2.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2695/3145 [17:09<03:10,  2.37it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2718/3145 [17:19<02:26,  2.92it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2738/3143 [17:07<02:37,  2.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2714/3145 [17:07<02:48,  2.56it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2696/3145 [17:09<03:05,  2.42it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2719/3145 [17:20<02:33,  2.78it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2739/3143 [17:07<02:33,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2715/3145 [17:07<02:43,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2697/3145 [17:10<02:58,  2.51it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2720/3145 [17:20<02:32,  2.78it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2740/3143 [17:07<02:30,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2716/3145 [17:07<02:42,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2721/3145 [17:20<02:29,  2.83it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2698/3145 [17:10<03:16,  2.27it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2717/3145 [17:08<02:42,  2.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2741/3143 [17:08<02:35,  2.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2722/3145 [17:21<02:30,  2.81it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2699/3145 [17:11<03:09,  2.36it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2718/3145 [17:08<02:40,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2742/3143 [17:08<02:41,  2.48it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2723/3145 [17:21<02:33,  2.75it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2700/3145 [17:11<03:08,  2.36it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2719/3145 [17:09<02:41,  2.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2743/3143 [17:09<02:33,  2.61it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2724/3145 [17:22<02:36,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2744/3143 [17:09<02:13,  2.99it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2720/3145 [17:09<02:38,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2701/3145 [17:12<03:00,  2.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2725/3145 [17:22<02:35,  2.70it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2745/3143 [17:09<02:12,  2.99it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2702/3145 [17:12<02:55,  2.52it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2721/3145 [17:09<02:38,  2.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2746/3143 [17:10<02:13,  2.97it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2726/3145 [17:22<02:45,  2.53it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2722/3145 [17:10<02:36,  2.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2703/3145 [17:12<02:55,  2.52it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2747/3143 [17:10<01:59,  3.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2727/3145 [17:23<02:38,  2.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2723/3145 [17:10<02:35,  2.71it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2704/3145 [17:13<02:49,  2.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2748/3143 [17:10<02:10,  3.04it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2728/3145 [17:23<02:35,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2705/3145 [17:13<02:46,  2.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2724/3145 [17:10<02:38,  2.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2749/3143 [17:11<02:13,  2.96it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2729/3145 [17:23<02:36,  2.66it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2706/3145 [17:13<02:45,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2750/3143 [17:11<02:18,  2.83it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2725/3145 [17:11<02:48,  2.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2730/3145 [17:24<02:34,  2.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2707/3145 [17:14<02:39,  2.74it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2726/3145 [17:11<02:37,  2.66it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2751/3143 [17:11<02:19,  2.81it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2731/3145 [17:24<02:35,  2.65it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2708/3145 [17:14<02:38,  2.76it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2727/3145 [17:12<02:29,  2.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2752/3143 [17:12<02:20,  2.79it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2732/3145 [17:25<02:37,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2709/3145 [17:14<02:41,  2.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2753/3143 [17:12<02:22,  2.73it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2728/3145 [17:12<02:41,  2.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2733/3145 [17:25<02:42,  2.54it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2754/3143 [17:12<02:21,  2.75it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2729/3145 [17:12<02:37,  2.65it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2710/3145 [17:15<02:57,  2.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2734/3145 [17:25<02:40,  2.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2755/3143 [17:13<02:21,  2.74it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2730/3145 [17:13<02:46,  2.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2735/3145 [17:26<02:26,  2.80it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2711/3145 [17:15<03:06,  2.32it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2756/3143 [17:13<02:18,  2.79it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2731/3145 [17:13<02:38,  2.62it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2712/3145 [17:16<02:56,  2.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2736/3145 [17:26<02:30,  2.72it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2757/3143 [17:13<02:11,  2.93it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2732/3145 [17:14<02:34,  2.67it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2737/3145 [17:26<02:26,  2.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2713/3145 [17:16<02:53,  2.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2758/3143 [17:14<02:10,  2.96it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2733/3145 [17:14<02:37,  2.61it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2738/3145 [17:27<02:30,  2.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2714/3145 [17:17<02:48,  2.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2759/3143 [17:14<02:09,  2.96it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2734/3145 [17:14<02:44,  2.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2715/3145 [17:17<02:45,  2.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2739/3145 [17:27<02:34,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2760/3143 [17:15<02:18,  2.77it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2735/3145 [17:15<02:40,  2.56it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2740/3145 [17:28<02:31,  2.67it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2716/3145 [17:17<02:51,  2.51it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2761/3143 [17:15<02:28,  2.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2741/3145 [17:28<02:28,  2.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2736/3145 [17:15<02:39,  2.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2717/3145 [17:18<02:54,  2.46it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2762/3143 [17:15<02:21,  2.70it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2742/3145 [17:28<02:16,  2.95it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2737/3145 [17:15<02:35,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2718/3145 [17:18<02:48,  2.53it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2763/3143 [17:16<02:23,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2743/3145 [17:28<02:08,  3.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2738/3145 [17:16<02:34,  2.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2719/3145 [17:19<02:44,  2.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2744/3145 [17:29<02:12,  3.03it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2764/3143 [17:16<02:24,  2.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2739/3145 [17:16<02:33,  2.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2720/3145 [17:19<02:40,  2.65it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2745/3145 [17:29<02:15,  2.94it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2765/3143 [17:16<02:21,  2.67it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2740/3145 [17:17<02:31,  2.67it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2721/3145 [17:19<02:42,  2.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2746/3145 [17:30<02:22,  2.80it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2766/3143 [17:17<02:24,  2.61it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2741/3145 [17:17<02:30,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2722/3145 [17:20<02:46,  2.54it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2747/3145 [17:30<02:23,  2.78it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2767/3143 [17:17<02:25,  2.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2742/3145 [17:17<02:26,  2.76it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2723/3145 [17:20<02:42,  2.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2748/3145 [17:30<02:23,  2.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2768/3143 [17:18<02:24,  2.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2743/3145 [17:18<02:29,  2.70it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2749/3145 [17:31<02:25,  2.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2724/3145 [17:20<02:43,  2.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2769/3143 [17:18<02:25,  2.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2744/3145 [17:18<02:20,  2.85it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2750/3145 [17:31<02:19,  2.84it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2725/3145 [17:21<02:38,  2.65it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2770/3143 [17:18<02:20,  2.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2745/3145 [17:18<02:26,  2.74it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2751/3145 [17:31<02:23,  2.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2771/3143 [17:19<02:15,  2.75it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2726/3145 [17:21<02:39,  2.62it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2746/3145 [17:19<02:23,  2.78it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2752/3145 [17:32<02:22,  2.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2772/3143 [17:19<02:14,  2.75it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2727/3145 [17:22<02:41,  2.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2747/3145 [17:19<02:23,  2.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2773/3143 [17:19<02:15,  2.74it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2753/3145 [17:32<02:28,  2.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2748/3145 [17:19<02:21,  2.81it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2728/3145 [17:22<02:53,  2.40it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2774/3143 [17:20<02:06,  2.92it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2754/3145 [17:33<02:29,  2.62it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2749/3145 [17:20<02:25,  2.73it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2729/3145 [17:22<02:49,  2.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2775/3143 [17:20<02:00,  3.06it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2755/3145 [17:33<02:26,  2.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2750/3145 [17:20<02:24,  2.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2730/3145 [17:23<02:46,  2.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2776/3143 [17:20<02:02,  2.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2756/3145 [17:33<02:24,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2751/3145 [17:21<02:20,  2.81it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2777/3143 [17:21<01:52,  3.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2731/3145 [17:23<02:41,  2.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2757/3145 [17:34<02:22,  2.72it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2778/3143 [17:21<01:55,  3.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2752/3145 [17:21<02:24,  2.73it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2732/3145 [17:24<02:38,  2.60it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2758/3145 [17:34<02:20,  2.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2779/3143 [17:21<01:57,  3.10it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2753/3145 [17:21<02:26,  2.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2733/3145 [17:24<02:38,  2.60it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2759/3145 [17:34<02:19,  2.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2780/3143 [17:22<02:01,  3.00it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2754/3145 [17:22<02:23,  2.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2734/3145 [17:24<02:38,  2.60it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2760/3145 [17:35<02:23,  2.68it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2781/3143 [17:22<02:09,  2.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2755/3145 [17:22<02:22,  2.74it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2735/3145 [17:25<02:38,  2.59it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2761/3145 [17:35<02:21,  2.71it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2782/3143 [17:22<02:09,  2.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2756/3145 [17:22<02:20,  2.78it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2736/3145 [17:25<02:33,  2.67it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2762/3145 [17:35<02:19,  2.75it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2783/3143 [17:23<02:11,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2757/3145 [17:23<02:18,  2.79it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2737/3145 [17:25<02:31,  2.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2763/3145 [17:36<02:22,  2.69it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2784/3143 [17:23<02:14,  2.67it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2758/3145 [17:23<02:30,  2.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2738/3145 [17:26<02:29,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2764/3145 [17:36<02:17,  2.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2785/3143 [17:24<02:15,  2.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2759/3145 [17:24<02:29,  2.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2739/3145 [17:26<02:24,  2.80it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2765/3145 [17:37<02:15,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2786/3143 [17:24<02:12,  2.70it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2740/3145 [17:27<02:33,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2760/3145 [17:24<02:36,  2.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2766/3145 [17:37<02:13,  2.83it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2787/3143 [17:24<02:13,  2.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2741/3145 [17:27<02:39,  2.54it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2761/3145 [17:24<02:41,  2.38it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2767/3145 [17:37<02:28,  2.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2788/3143 [17:25<02:11,  2.70it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2742/3145 [17:27<02:34,  2.60it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2768/3145 [17:38<02:20,  2.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2789/3143 [17:25<02:09,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2762/3145 [17:25<02:56,  2.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2743/3145 [17:28<02:32,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2769/3145 [17:38<02:14,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2790/3143 [17:25<02:10,  2.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2763/3145 [17:25<02:45,  2.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2744/3145 [17:28<02:28,  2.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2770/3145 [17:38<02:16,  2.74it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2791/3143 [17:26<02:10,  2.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2764/3145 [17:26<02:47,  2.28it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2745/3145 [17:28<02:30,  2.66it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2771/3145 [17:39<02:14,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2792/3143 [17:26<02:11,  2.66it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2772/3145 [17:39<02:01,  3.08it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2746/3145 [17:29<02:27,  2.71it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2765/3145 [17:26<02:48,  2.26it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2793/3143 [17:27<02:12,  2.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2773/3145 [17:39<02:04,  2.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2766/3145 [17:27<02:37,  2.41it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2747/3145 [17:29<02:30,  2.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2794/3143 [17:27<02:15,  2.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2748/3145 [17:30<02:29,  2.66it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2774/3145 [17:40<02:22,  2.60it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2767/3145 [17:27<02:34,  2.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2795/3143 [17:27<02:09,  2.69it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2749/3145 [17:30<02:27,  2.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2768/3145 [17:27<02:30,  2.51it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2775/3145 [17:40<02:37,  2.35it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2796/3143 [17:28<02:10,  2.66it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2750/3145 [17:30<02:27,  2.68it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2769/3145 [17:28<02:29,  2.52it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2776/3145 [17:41<02:32,  2.42it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2797/3143 [17:28<02:14,  2.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2751/3145 [17:31<02:21,  2.78it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2770/3145 [17:28<02:27,  2.54it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2777/3145 [17:41<02:29,  2.46it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2798/3143 [17:28<02:13,  2.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2752/3145 [17:31<02:19,  2.82it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2771/3145 [17:29<02:22,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2778/3145 [17:42<02:22,  2.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2799/3143 [17:29<02:09,  2.65it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2753/3145 [17:31<02:19,  2.81it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2772/3145 [17:29<02:30,  2.48it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2779/3145 [17:42<02:18,  2.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2754/3145 [17:32<02:19,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2800/3143 [17:29<02:10,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2773/3145 [17:29<02:24,  2.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2780/3145 [17:42<02:18,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2755/3145 [17:32<02:20,  2.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2774/3145 [17:30<02:12,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2801/3143 [17:30<02:23,  2.39it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2781/3145 [17:43<02:18,  2.62it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2756/3145 [17:33<02:31,  2.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2775/3145 [17:30<02:12,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2802/3143 [17:30<02:19,  2.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2782/3145 [17:43<02:15,  2.68it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2776/3145 [17:30<01:58,  3.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2757/3145 [17:33<02:29,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2803/3143 [17:31<02:19,  2.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2783/3145 [17:43<02:09,  2.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2777/3145 [17:31<01:59,  3.07it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2758/3145 [17:33<02:32,  2.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2784/3145 [17:44<02:07,  2.84it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2804/3143 [17:31<02:19,  2.42it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2778/3145 [17:31<02:03,  2.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2759/3145 [17:34<02:31,  2.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2785/3145 [17:44<02:07,  2.83it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2805/3143 [17:31<02:14,  2.52it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2779/3145 [17:31<02:15,  2.70it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2786/3145 [17:44<02:04,  2.88it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2760/3145 [17:34<02:30,  2.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2806/3143 [17:32<02:10,  2.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2780/3145 [17:32<02:13,  2.74it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2787/3145 [17:45<02:05,  2.85it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2761/3145 [17:34<02:29,  2.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2807/3143 [17:32<02:07,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2781/3145 [17:32<02:12,  2.75it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2788/3145 [17:45<02:06,  2.82it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2762/3145 [17:35<02:22,  2.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2808/3143 [17:32<02:10,  2.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2782/3145 [17:32<02:10,  2.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2789/3145 [17:45<02:03,  2.88it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2763/3145 [17:35<02:29,  2.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2809/3143 [17:33<02:07,  2.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2783/3145 [17:33<02:07,  2.83it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2790/3145 [17:46<02:06,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2810/3143 [17:33<02:02,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2764/3145 [17:36<02:28,  2.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2791/3145 [17:46<01:51,  3.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2784/3145 [17:33<02:14,  2.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2765/3145 [17:36<02:26,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2811/3143 [17:34<02:06,  2.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2792/3145 [17:46<01:58,  2.98it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2785/3145 [17:34<02:15,  2.65it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2793/3145 [17:47<01:56,  3.03it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2766/3145 [17:36<02:26,  2.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2812/3143 [17:34<02:08,  2.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2786/3145 [17:34<02:14,  2.67it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2794/3145 [17:47<01:59,  2.94it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2767/3145 [17:37<02:29,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2813/3143 [17:34<02:06,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2787/3145 [17:34<02:15,  2.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2814/3143 [17:35<02:02,  2.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2768/3145 [17:37<02:28,  2.53it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2795/3145 [17:48<02:10,  2.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2788/3145 [17:35<02:17,  2.60it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2815/3143 [17:35<02:00,  2.71it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2769/3145 [17:38<02:28,  2.53it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2796/3145 [17:48<02:12,  2.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2789/3145 [17:35<02:15,  2.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2816/3143 [17:35<01:59,  2.73it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2790/3145 [17:35<02:06,  2.80it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2770/3145 [17:38<02:27,  2.55it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2797/3145 [17:48<02:15,  2.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2817/3143 [17:36<02:04,  2.62it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2771/3145 [17:38<02:22,  2.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2791/3145 [17:36<02:10,  2.71it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2798/3145 [17:49<02:15,  2.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2799/3145 [17:49<01:56,  2.97it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2818/3143 [17:36<01:59,  2.73it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2792/3145 [17:36<02:08,  2.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2772/3145 [17:39<02:22,  2.61it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2800/3145 [17:49<01:47,  3.20it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2819/3143 [17:36<01:55,  2.81it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2793/3145 [17:37<02:08,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2773/3145 [17:39<02:19,  2.66it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2801/3145 [17:49<01:42,  3.35it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2820/3143 [17:37<01:53,  2.86it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2794/3145 [17:37<01:57,  3.00it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2774/3145 [17:39<02:11,  2.82it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2802/3145 [17:50<01:37,  3.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2821/3143 [17:37<01:50,  2.91it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2795/3145 [17:37<01:58,  2.97it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2803/3145 [17:50<01:42,  3.35it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2775/3145 [17:40<02:12,  2.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2822/3143 [17:37<01:47,  2.98it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2804/3145 [17:50<01:43,  3.29it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2796/3145 [17:38<02:04,  2.81it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2776/3145 [17:40<02:19,  2.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2823/3143 [17:38<01:55,  2.78it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2805/3145 [17:51<01:49,  3.10it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2797/3145 [17:38<02:04,  2.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2777/3145 [17:41<02:16,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2824/3143 [17:38<01:54,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2806/3145 [17:51<01:52,  3.02it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2798/3145 [17:38<02:07,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2778/3145 [17:41<02:12,  2.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2825/3143 [17:39<01:53,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2807/3145 [17:51<01:57,  2.87it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2799/3145 [17:39<02:06,  2.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2779/3145 [17:41<02:15,  2.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2826/3143 [17:39<02:01,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2800/3145 [17:39<02:02,  2.81it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2808/3145 [17:52<02:01,  2.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2780/3145 [17:42<02:24,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2827/3143 [17:39<02:00,  2.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2801/3145 [17:39<02:04,  2.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2809/3145 [17:52<02:02,  2.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2781/3145 [17:42<02:23,  2.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2810/3145 [17:53<02:00,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2802/3145 [17:40<02:05,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2828/3143 [17:40<02:04,  2.54it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2782/3145 [17:43<02:23,  2.53it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2811/3145 [17:53<02:00,  2.78it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2803/3145 [17:40<02:04,  2.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2829/3143 [17:40<02:01,  2.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2783/3145 [17:43<02:22,  2.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2812/3145 [17:53<01:56,  2.85it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2804/3145 [17:40<02:02,  2.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2830/3143 [17:41<01:58,  2.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2784/3145 [17:43<02:21,  2.55it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2813/3145 [17:54<01:57,  2.82it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2805/3145 [17:41<02:01,  2.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2831/3143 [17:41<02:02,  2.55it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2785/3145 [17:44<02:18,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2814/3145 [17:54<02:00,  2.74it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2806/3145 [17:41<02:00,  2.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2832/3143 [17:41<01:58,  2.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2786/3145 [17:44<02:19,  2.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2815/3145 [17:54<02:03,  2.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2807/3145 [17:42<02:05,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2833/3143 [17:42<02:04,  2.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2816/3145 [17:55<01:48,  3.03it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2787/3145 [17:44<02:16,  2.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2808/3145 [17:42<02:05,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2834/3143 [17:42<02:00,  2.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2817/3145 [17:55<01:54,  2.88it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2809/3145 [17:42<02:06,  2.66it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2788/3145 [17:45<02:27,  2.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2835/3143 [17:42<01:54,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2818/3145 [17:55<01:54,  2.86it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2836/3143 [17:43<01:40,  3.06it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2810/3145 [17:43<02:04,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2819/3145 [17:56<01:41,  3.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2789/3145 [17:45<02:25,  2.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2837/3143 [17:43<01:44,  2.94it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2820/3145 [17:56<01:42,  3.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2811/3145 [17:43<02:06,  2.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2790/3145 [17:46<02:21,  2.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2838/3143 [17:43<01:43,  2.96it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2812/3145 [17:44<02:07,  2.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2821/3145 [17:56<01:52,  2.87it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2791/3145 [17:46<02:20,  2.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2839/3143 [17:44<01:44,  2.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2822/3145 [17:57<01:52,  2.87it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2813/3145 [17:44<02:08,  2.59it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2792/3145 [17:47<02:26,  2.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2840/3143 [17:44<01:43,  2.94it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2823/3145 [17:57<01:53,  2.84it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2814/3145 [17:44<02:09,  2.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2793/3145 [17:47<02:18,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2841/3143 [17:44<01:41,  2.97it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2824/3145 [17:57<01:54,  2.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2815/3145 [17:45<02:03,  2.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2842/3143 [17:45<01:43,  2.91it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2794/3145 [17:47<02:18,  2.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2825/3145 [17:58<01:54,  2.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2816/3145 [17:45<02:04,  2.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2795/3145 [17:48<02:15,  2.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2843/3143 [17:45<01:51,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2826/3145 [17:58<01:59,  2.68it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2817/3145 [17:45<02:01,  2.69it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2796/3145 [17:48<02:11,  2.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2844/3143 [17:46<01:49,  2.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2827/3145 [17:59<01:58,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2818/3145 [17:46<02:04,  2.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2797/3145 [17:48<02:12,  2.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2845/3143 [17:46<01:49,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2828/3145 [17:59<01:56,  2.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2819/3145 [17:46<02:05,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2798/3145 [17:49<02:10,  2.66it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2846/3143 [17:46<01:46,  2.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2829/3145 [17:59<01:57,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2820/3145 [17:47<02:01,  2.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2799/3145 [17:49<02:03,  2.80it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2847/3143 [17:47<01:48,  2.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2830/3145 [18:00<01:55,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2821/3145 [17:47<01:59,  2.71it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2800/3145 [17:49<02:01,  2.85it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2848/3143 [17:47<01:45,  2.79it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2831/3145 [18:00<01:57,  2.67it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2801/3145 [17:50<02:02,  2.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2822/3145 [17:47<01:59,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2849/3143 [17:47<01:46,  2.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2802/3145 [17:50<01:54,  2.99it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2832/3145 [18:00<01:56,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2823/3145 [17:48<02:00,  2.68it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2850/3143 [17:48<01:47,  2.72it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2803/3145 [17:50<01:59,  2.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2824/3145 [17:48<01:56,  2.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2833/3145 [18:01<01:57,  2.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2851/3143 [17:48<01:46,  2.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2825/3145 [17:48<01:53,  2.82it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2804/3145 [17:51<02:05,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2834/3145 [18:01<01:57,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2852/3143 [17:48<01:47,  2.71it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2826/3145 [17:49<01:53,  2.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2835/3145 [18:01<01:50,  2.79it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2805/3145 [17:51<02:04,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2853/3143 [17:49<01:49,  2.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2836/3145 [18:02<01:49,  2.83it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2806/3145 [17:52<02:09,  2.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2827/3145 [17:49<02:04,  2.55it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2854/3143 [17:49<01:46,  2.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2837/3145 [18:02<01:49,  2.81it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2807/3145 [17:52<02:06,  2.66it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2828/3145 [17:50<02:04,  2.55it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2855/3143 [17:50<01:43,  2.79it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2838/3145 [18:03<01:57,  2.61it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2808/3145 [17:52<02:08,  2.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2829/3145 [17:50<02:02,  2.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2856/3143 [17:50<01:46,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2839/3145 [18:03<01:57,  2.60it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2809/3145 [17:53<02:07,  2.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2830/3145 [17:50<01:58,  2.67it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2857/3143 [17:50<01:44,  2.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2840/3145 [18:03<01:52,  2.72it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2858/3143 [17:51<01:43,  2.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2831/3145 [17:51<02:01,  2.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2810/3145 [17:53<02:14,  2.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2841/3145 [18:04<01:53,  2.67it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2859/3143 [17:51<01:43,  2.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2832/3145 [17:51<01:58,  2.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2811/3145 [17:54<02:11,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2842/3145 [18:04<01:51,  2.72it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2812/3145 [17:54<02:04,  2.67it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2860/3143 [17:51<01:44,  2.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2833/3145 [17:51<02:02,  2.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2843/3145 [18:04<01:50,  2.72it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2813/3145 [17:54<01:59,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2861/3143 [17:52<01:46,  2.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2834/3145 [17:52<02:02,  2.55it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2814/3145 [17:55<01:57,  2.83it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2844/3145 [18:05<01:58,  2.54it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2862/3143 [17:52<01:44,  2.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2835/3145 [17:52<02:08,  2.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2815/3145 [17:55<01:57,  2.81it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2863/3143 [17:52<01:35,  2.94it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2845/3145 [18:05<01:58,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2836/3145 [17:53<02:02,  2.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2816/3145 [17:55<01:59,  2.76it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2864/3143 [17:53<01:39,  2.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2846/3145 [18:06<01:53,  2.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2837/3145 [17:53<02:02,  2.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2817/3145 [17:56<01:55,  2.85it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2865/3143 [17:53<01:40,  2.77it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2847/3145 [18:06<01:54,  2.60it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2838/3145 [17:53<01:58,  2.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2818/3145 [17:56<01:58,  2.76it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2866/3143 [17:54<01:39,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2848/3145 [18:06<01:59,  2.48it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2839/3145 [17:54<02:01,  2.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2819/3145 [17:56<01:56,  2.79it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2867/3143 [17:54<01:41,  2.73it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2849/3145 [18:07<02:03,  2.40it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2820/3145 [17:57<01:57,  2.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2840/3145 [17:54<02:01,  2.51it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2868/3143 [17:54<01:39,  2.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2821/3145 [17:57<01:57,  2.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2841/3145 [17:55<01:57,  2.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2850/3145 [18:07<02:10,  2.26it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2869/3143 [17:55<01:41,  2.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2842/3145 [17:55<01:41,  2.99it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2851/3145 [18:08<02:01,  2.42it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2870/3143 [17:55<01:41,  2.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2822/3145 [17:58<02:10,  2.48it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2843/3145 [17:55<01:45,  2.85it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2852/3145 [18:08<01:52,  2.61it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2871/3143 [17:55<01:39,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2823/3145 [17:58<02:05,  2.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2844/3145 [17:56<01:49,  2.75it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2853/3145 [18:08<01:51,  2.61it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2872/3143 [17:56<01:39,  2.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2824/3145 [17:58<02:02,  2.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2845/3145 [17:56<01:47,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2854/3145 [18:09<01:54,  2.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2825/3145 [17:59<01:57,  2.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2846/3145 [17:56<01:48,  2.75it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2873/3143 [17:56<01:55,  2.33it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2855/3145 [18:09<01:51,  2.61it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2826/3145 [17:59<01:55,  2.75it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2847/3145 [17:57<01:50,  2.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2874/3143 [17:57<01:55,  2.34it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2827/3145 [17:59<01:53,  2.81it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2856/3145 [18:10<01:56,  2.48it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2848/3145 [17:57<01:50,  2.68it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2875/3143 [17:57<01:51,  2.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2828/3145 [18:00<01:52,  2.81it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2857/3145 [18:10<02:00,  2.39it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2849/3145 [17:57<01:44,  2.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2876/3143 [17:58<01:48,  2.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2829/3145 [18:00<01:55,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2858/3145 [18:10<01:51,  2.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2850/3145 [17:58<01:46,  2.76it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2877/3143 [17:58<01:36,  2.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2830/3145 [18:00<01:56,  2.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2859/3145 [18:11<01:53,  2.53it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2851/3145 [17:58<01:45,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2878/3143 [17:58<01:41,  2.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2831/3145 [18:01<01:55,  2.72it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2860/3145 [18:11<01:49,  2.60it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2852/3145 [17:59<01:45,  2.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2832/3145 [18:01<01:50,  2.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2879/3143 [17:59<01:57,  2.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2861/3145 [18:12<01:49,  2.60it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2853/3145 [17:59<01:49,  2.67it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2862/3145 [18:12<01:36,  2.93it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2833/3145 [18:02<02:00,  2.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2880/3143 [17:59<01:52,  2.34it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2854/3145 [17:59<01:47,  2.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2863/3145 [18:12<01:40,  2.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2834/3145 [18:02<02:00,  2.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2855/3145 [18:00<01:44,  2.77it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2881/3143 [18:00<01:54,  2.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2864/3145 [18:13<01:42,  2.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2835/3145 [18:02<01:57,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2856/3145 [18:00<01:43,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2882/3143 [18:00<01:50,  2.36it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2865/3145 [18:13<01:34,  2.96it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2836/3145 [18:03<01:56,  2.66it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2857/3145 [18:00<01:43,  2.79it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2866/3145 [18:13<01:23,  3.33it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2883/3143 [18:00<01:45,  2.47it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2858/3145 [18:01<01:30,  3.16it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2837/3145 [18:03<01:53,  2.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2867/3145 [18:13<01:27,  3.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2884/3143 [18:01<01:39,  2.60it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2859/3145 [18:01<01:34,  3.04it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2838/3145 [18:04<01:58,  2.59it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2868/3145 [18:14<01:31,  3.02it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2885/3143 [18:01<01:37,  2.66it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2860/3145 [18:01<01:38,  2.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2839/3145 [18:04<01:53,  2.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2869/3145 [18:14<01:34,  2.94it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2886/3143 [18:01<01:33,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2861/3145 [18:02<01:39,  2.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2840/3145 [18:04<01:52,  2.72it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2870/3145 [18:15<01:36,  2.86it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2887/3143 [18:02<01:32,  2.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2841/3145 [18:04<01:41,  2.98it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2862/3145 [18:02<01:40,  2.82it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2871/3145 [18:15<01:36,  2.84it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2888/3143 [18:02<01:34,  2.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2863/3145 [18:02<01:38,  2.86it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2842/3145 [18:05<01:46,  2.84it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2872/3145 [18:15<01:35,  2.86it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2889/3143 [18:03<01:34,  2.68it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2864/3145 [18:03<01:37,  2.89it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2843/3145 [18:05<01:48,  2.79it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2873/3145 [18:16<01:37,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2890/3143 [18:03<01:37,  2.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2844/3145 [18:06<01:45,  2.84it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2865/3145 [18:03<01:44,  2.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2874/3145 [18:16<01:42,  2.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2845/3145 [18:06<01:34,  3.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2866/3145 [18:03<01:32,  3.01it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2891/3143 [18:03<01:38,  2.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2875/3145 [18:16<01:43,  2.61it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2892/3143 [18:04<01:34,  2.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2846/3145 [18:06<01:48,  2.75it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2867/3145 [18:04<01:42,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2876/3145 [18:17<01:42,  2.61it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2893/3143 [18:04<01:32,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2847/3145 [18:07<01:48,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2868/3145 [18:04<01:41,  2.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2894/3143 [18:04<01:30,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2877/3145 [18:17<01:42,  2.61it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2869/3145 [18:05<01:38,  2.81it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2848/3145 [18:07<01:51,  2.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2878/3145 [18:18<01:40,  2.65it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2895/3143 [18:05<01:34,  2.62it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2870/3145 [18:05<01:38,  2.80it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2849/3145 [18:07<01:52,  2.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2879/3145 [18:18<01:36,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2871/3145 [18:05<01:34,  2.89it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2896/3143 [18:05<01:34,  2.61it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2850/3145 [18:08<01:57,  2.51it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2880/3145 [18:18<01:37,  2.71it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2897/3143 [18:06<01:36,  2.54it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2872/3145 [18:06<01:45,  2.59it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2851/3145 [18:08<01:57,  2.51it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2881/3145 [18:19<01:38,  2.67it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2898/3143 [18:06<01:32,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2873/3145 [18:06<01:43,  2.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2882/3145 [18:19<01:34,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2852/3145 [18:09<02:06,  2.31it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2874/3145 [18:06<01:41,  2.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2899/3143 [18:07<01:40,  2.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2853/3145 [18:09<02:00,  2.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2883/3145 [18:19<01:39,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2875/3145 [18:07<01:37,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3143 [18:07<01:35,  2.54it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2854/3145 [18:10<01:56,  2.50it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2876/3145 [18:07<01:34,  2.85it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2884/3145 [18:20<01:42,  2.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2901/3143 [18:07<01:37,  2.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2855/3145 [18:10<01:54,  2.53it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2877/3145 [18:07<01:34,  2.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2885/3145 [18:20<01:42,  2.54it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2902/3143 [18:08<01:35,  2.53it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2856/3145 [18:10<01:51,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2878/3145 [18:08<01:36,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2886/3145 [18:21<01:41,  2.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2903/3143 [18:08<01:32,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2879/3145 [18:08<01:35,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2857/3145 [18:11<01:52,  2.56it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2887/3145 [18:21<01:39,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2904/3143 [18:08<01:30,  2.65it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2880/3145 [18:08<01:27,  3.01it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2858/3145 [18:11<01:49,  2.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2888/3145 [18:21<01:37,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2905/3143 [18:09<01:28,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2859/3145 [18:11<01:36,  2.98it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2881/3145 [18:09<01:34,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2889/3145 [18:22<01:37,  2.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2906/3143 [18:09<01:28,  2.67it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2860/3145 [18:12<01:38,  2.91it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2882/3145 [18:09<01:37,  2.70it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2890/3145 [18:22<01:36,  2.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2907/3143 [18:09<01:27,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2861/3145 [18:12<01:42,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2883/3145 [18:10<01:43,  2.54it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2891/3145 [18:23<01:33,  2.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2908/3143 [18:10<01:25,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2862/3145 [18:12<01:49,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2884/3145 [18:10<01:39,  2.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2892/3145 [18:23<01:32,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2909/3143 [18:10<01:24,  2.75it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2863/3145 [18:13<01:49,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2885/3145 [18:10<01:39,  2.61it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2893/3145 [18:23<01:33,  2.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2910/3143 [18:11<01:25,  2.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2864/3145 [18:13<01:47,  2.60it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2886/3145 [18:11<01:37,  2.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2911/3143 [18:11<01:20,  2.88it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2894/3145 [18:24<01:42,  2.46it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2865/3145 [18:14<01:48,  2.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2887/3145 [18:11<01:38,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2912/3143 [18:11<01:22,  2.79it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2895/3145 [18:24<01:41,  2.46it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2888/3145 [18:12<01:35,  2.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2866/3145 [18:14<01:54,  2.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2913/3143 [18:12<01:24,  2.71it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2896/3145 [18:25<01:37,  2.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2889/3145 [18:12<01:39,  2.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2867/3145 [18:14<01:49,  2.53it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2914/3143 [18:12<01:22,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2897/3145 [18:25<01:37,  2.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2890/3145 [18:12<01:27,  2.92it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2868/3145 [18:15<01:46,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2915/3143 [18:12<01:23,  2.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2898/3145 [18:25<01:33,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2891/3145 [18:13<01:35,  2.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2869/3145 [18:15<01:44,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2899/3145 [18:26<01:22,  2.98it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2916/3143 [18:13<01:25,  2.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3145 [18:26<01:16,  3.22it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2892/3145 [18:13<01:32,  2.74it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2870/3145 [18:16<01:42,  2.67it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2917/3143 [18:13<01:26,  2.60it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2893/3145 [18:13<01:31,  2.75it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2901/3145 [18:26<01:22,  2.96it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2871/3145 [18:16<01:43,  2.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2918/3143 [18:14<01:22,  2.71it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2902/3145 [18:27<01:23,  2.90it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2894/3145 [18:14<01:32,  2.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2872/3145 [18:16<01:41,  2.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2919/3143 [18:14<01:20,  2.79it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2903/3145 [18:27<01:24,  2.87it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2895/3145 [18:14<01:34,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2920/3143 [18:14<01:20,  2.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2873/3145 [18:17<01:45,  2.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2896/3145 [18:14<01:32,  2.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2904/3145 [18:27<01:29,  2.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2874/3145 [18:17<01:43,  2.61it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2921/3143 [18:15<01:23,  2.67it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2905/3145 [18:28<01:26,  2.77it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2897/3145 [18:15<01:33,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2875/3145 [18:17<01:43,  2.62it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2922/3143 [18:15<01:22,  2.69it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2906/3145 [18:28<01:25,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2898/3145 [18:15<01:33,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2923/3143 [18:15<01:22,  2.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2876/3145 [18:18<01:46,  2.54it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2907/3145 [18:28<01:25,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2899/3145 [18:16<01:32,  2.67it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2924/3143 [18:16<01:22,  2.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2877/3145 [18:18<01:43,  2.60it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2908/3145 [18:29<01:24,  2.80it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3145 [18:16<01:30,  2.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2925/3143 [18:16<01:21,  2.69it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2878/3145 [18:19<01:43,  2.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2901/3145 [18:16<01:29,  2.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2909/3145 [18:29<01:34,  2.51it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2926/3143 [18:16<01:19,  2.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2879/3145 [18:19<01:41,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2910/3145 [18:30<01:28,  2.65it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2902/3145 [18:17<01:31,  2.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2880/3145 [18:19<01:33,  2.83it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2927/3143 [18:17<01:18,  2.77it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2911/3145 [18:30<01:29,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2903/3145 [18:17<01:32,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2881/3145 [18:20<01:34,  2.78it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2928/3143 [18:17<01:22,  2.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2904/3145 [18:17<01:28,  2.73it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2912/3145 [18:30<01:29,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2929/3143 [18:18<01:20,  2.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2882/3145 [18:20<01:39,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2905/3145 [18:18<01:27,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2913/3145 [18:31<01:26,  2.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2930/3143 [18:18<01:19,  2.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2883/3145 [18:21<01:43,  2.53it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2906/3145 [18:18<01:27,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2914/3145 [18:31<01:25,  2.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2931/3143 [18:18<01:18,  2.71it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2884/3145 [18:21<01:42,  2.53it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2907/3145 [18:18<01:18,  3.05it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2915/3145 [18:31<01:24,  2.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2932/3143 [18:19<01:18,  2.67it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2885/3145 [18:21<01:38,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2908/3145 [18:19<01:20,  2.94it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2916/3145 [18:32<01:25,  2.68it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2933/3143 [18:19<01:16,  2.76it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2886/3145 [18:22<01:41,  2.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2909/3145 [18:19<01:25,  2.75it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2917/3145 [18:32<01:26,  2.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2934/3143 [18:19<01:13,  2.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2887/3145 [18:22<01:38,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2910/3145 [18:20<01:25,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2935/3143 [18:20<01:12,  2.85it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2918/3145 [18:33<01:26,  2.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2888/3145 [18:22<01:38,  2.62it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2911/3145 [18:20<01:30,  2.59it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2936/3143 [18:20<01:13,  2.83it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2919/3145 [18:33<01:25,  2.65it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2889/3145 [18:23<01:36,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2937/3143 [18:20<01:08,  3.00it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2912/3145 [18:20<01:27,  2.67it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2920/3145 [18:33<01:24,  2.67it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2938/3143 [18:21<01:09,  2.96it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2890/3145 [18:23<01:39,  2.56it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2913/3145 [18:21<01:29,  2.58it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2921/3145 [18:34<01:22,  2.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2939/3143 [18:21<01:10,  2.88it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2891/3145 [18:24<01:37,  2.59it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2914/3145 [18:21<01:28,  2.61it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2922/3145 [18:34<01:24,  2.65it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2940/3143 [18:21<01:11,  2.84it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2892/3145 [18:24<01:38,  2.56it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2915/3145 [18:22<01:30,  2.53it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2923/3145 [18:34<01:27,  2.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2941/3143 [18:22<01:11,  2.81it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2893/3145 [18:24<01:36,  2.61it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2924/3145 [18:35<01:24,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2916/3145 [18:22<01:34,  2.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2942/3143 [18:22<01:06,  3.04it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2894/3145 [18:25<01:36,  2.61it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2925/3145 [18:35<01:20,  2.72it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2917/3145 [18:22<01:28,  2.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2943/3143 [18:22<01:09,  2.88it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2895/3145 [18:25<01:36,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2918/3145 [18:23<01:26,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2926/3145 [18:36<01:21,  2.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2944/3143 [18:23<01:09,  2.87it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2896/3145 [18:26<01:38,  2.54it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2927/3145 [18:36<01:20,  2.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2919/3145 [18:23<01:25,  2.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2945/3143 [18:23<01:19,  2.48it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2897/3145 [18:26<01:34,  2.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2920/3145 [18:23<01:24,  2.67it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2928/3145 [18:36<01:24,  2.56it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2946/3143 [18:24<01:18,  2.52it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2898/3145 [18:26<01:37,  2.54it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2921/3145 [18:24<01:25,  2.62it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2929/3145 [18:37<01:25,  2.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2947/3143 [18:24<01:15,  2.58it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2922/3145 [18:24<01:21,  2.75it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2899/3145 [18:27<01:38,  2.50it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2930/3145 [18:37<01:21,  2.65it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2948/3143 [18:25<01:17,  2.51it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2923/3145 [18:25<01:21,  2.72it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2931/3145 [18:37<01:19,  2.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3145 [18:27<01:41,  2.41it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2949/3143 [18:25<01:13,  2.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2924/3145 [18:25<01:21,  2.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2932/3145 [18:38<01:18,  2.70it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2901/3145 [18:28<01:41,  2.40it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2950/3143 [18:25<01:13,  2.62it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2925/3145 [18:25<01:20,  2.73it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2933/3145 [18:38<01:17,  2.74it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2902/3145 [18:28<01:41,  2.39it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2951/3143 [18:26<01:12,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2926/3145 [18:26<01:17,  2.84it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2934/3145 [18:39<01:16,  2.77it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2903/3145 [18:28<01:35,  2.54it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2952/3143 [18:26<01:10,  2.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2927/3145 [18:26<01:19,  2.75it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2935/3145 [18:39<01:14,  2.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2904/3145 [18:29<01:36,  2.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2953/3143 [18:26<01:11,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2928/3145 [18:26<01:20,  2.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2936/3145 [18:39<01:24,  2.47it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2905/3145 [18:29<01:37,  2.46it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2954/3143 [18:27<01:11,  2.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2929/3145 [18:27<01:27,  2.47it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2937/3145 [18:40<01:21,  2.55it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2955/3143 [18:27<01:09,  2.69it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2906/3145 [18:30<01:36,  2.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2938/3145 [18:40<01:19,  2.62it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2930/3145 [18:27<01:27,  2.45it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2956/3143 [18:27<01:09,  2.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2907/3145 [18:30<01:40,  2.37it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2939/3145 [18:40<01:17,  2.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2931/3145 [18:28<01:26,  2.48it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2957/3143 [18:28<01:08,  2.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2932/3145 [18:28<01:16,  2.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2908/3145 [18:30<01:38,  2.41it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2940/3145 [18:41<01:16,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2958/3143 [18:28<01:06,  2.80it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2933/3145 [18:28<01:17,  2.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2909/3145 [18:31<01:37,  2.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2941/3145 [18:41<01:15,  2.71it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2959/3143 [18:29<01:03,  2.89it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2934/3145 [18:29<01:18,  2.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2910/3145 [18:31<01:33,  2.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2942/3145 [18:42<01:14,  2.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2960/3143 [18:29<01:01,  2.96it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2935/3145 [18:29<01:17,  2.72it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2911/3145 [18:32<01:30,  2.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2943/3145 [18:42<01:15,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2961/3143 [18:29<01:02,  2.91it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2936/3145 [18:29<01:16,  2.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2944/3145 [18:42<01:13,  2.75it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2912/3145 [18:32<01:30,  2.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2962/3143 [18:30<01:04,  2.80it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2937/3145 [18:30<01:17,  2.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2945/3145 [18:43<01:16,  2.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2963/3143 [18:30<01:04,  2.80it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2913/3145 [18:32<01:34,  2.46it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2938/3145 [18:30<01:19,  2.60it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2946/3145 [18:43<01:14,  2.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2914/3145 [18:33<01:28,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2964/3143 [18:30<01:04,  2.76it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2947/3145 [18:43<01:08,  2.90it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2939/3145 [18:31<01:17,  2.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2915/3145 [18:33<01:27,  2.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2965/3143 [18:31<01:10,  2.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2948/3145 [18:44<01:06,  2.95it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2940/3145 [18:31<01:14,  2.76it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2916/3145 [18:33<01:23,  2.75it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2966/3143 [18:31<01:07,  2.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2949/3145 [18:44<01:09,  2.83it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2941/3145 [18:31<01:15,  2.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2917/3145 [18:34<01:23,  2.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2967/3143 [18:31<01:06,  2.66it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2950/3145 [18:44<01:07,  2.89it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2918/3145 [18:34<01:21,  2.80it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2942/3145 [18:32<01:18,  2.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2951/3145 [18:45<01:06,  2.91it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2968/3143 [18:32<01:11,  2.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2919/3145 [18:35<01:22,  2.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2943/3145 [18:32<01:20,  2.51it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2952/3145 [18:45<01:06,  2.88it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2969/3143 [18:32<01:07,  2.56it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2920/3145 [18:35<01:23,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2970/3143 [18:33<00:58,  2.95it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2944/3145 [18:33<01:22,  2.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2953/3145 [18:45<01:09,  2.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2971/3143 [18:33<00:57,  3.00it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2921/3145 [18:35<01:25,  2.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2954/3145 [18:46<01:09,  2.76it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2945/3145 [18:33<01:22,  2.42it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2922/3145 [18:36<01:23,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2972/3143 [18:33<00:59,  2.87it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2946/3145 [18:33<01:19,  2.51it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2955/3145 [18:46<01:11,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2973/3143 [18:34<00:54,  3.10it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2923/3145 [18:36<01:23,  2.67it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2947/3145 [18:34<01:17,  2.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2974/3143 [18:34<00:54,  3.11it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2956/3145 [18:47<01:12,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2924/3145 [18:36<01:22,  2.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2948/3145 [18:34<01:14,  2.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2975/3143 [18:34<00:55,  3.00it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2957/3145 [18:47<01:10,  2.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2925/3145 [18:37<01:21,  2.70it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2949/3145 [18:34<01:15,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2976/3143 [18:35<00:56,  2.95it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2958/3145 [18:47<01:11,  2.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2926/3145 [18:37<01:22,  2.66it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2950/3145 [18:35<01:14,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2977/3143 [18:35<00:58,  2.82it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2959/3145 [18:48<01:12,  2.58it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2927/3145 [18:38<01:22,  2.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2978/3143 [18:35<01:00,  2.75it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2951/3145 [18:35<01:15,  2.56it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2960/3145 [18:48<01:09,  2.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2928/3145 [18:38<01:24,  2.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2979/3143 [18:36<00:58,  2.78it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2952/3145 [18:36<01:17,  2.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2961/3145 [18:49<01:11,  2.56it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2929/3145 [18:38<01:26,  2.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2980/3143 [18:36<00:58,  2.79it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2953/3145 [18:36<01:13,  2.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2962/3145 [18:49<01:13,  2.50it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2930/3145 [18:39<01:25,  2.52it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2954/3145 [18:36<01:11,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2981/3143 [18:36<01:02,  2.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2963/3145 [18:49<01:12,  2.51it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2931/3145 [18:39<01:21,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2955/3145 [18:37<01:10,  2.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2982/3143 [18:37<01:01,  2.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2964/3145 [18:50<01:10,  2.58it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2932/3145 [18:40<01:23,  2.54it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2956/3145 [18:37<01:09,  2.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2983/3143 [18:37<00:59,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2965/3145 [18:50<01:08,  2.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2957/3145 [18:37<01:02,  3.01it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2933/3145 [18:40<01:19,  2.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2984/3143 [18:37<00:54,  2.89it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2966/3145 [18:50<01:06,  2.68it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2934/3145 [18:40<01:16,  2.75it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2958/3145 [18:38<01:05,  2.87it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2985/3143 [18:38<00:56,  2.81it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2967/3145 [18:51<01:05,  2.70it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2959/3145 [18:38<01:05,  2.84it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2935/3145 [18:41<01:17,  2.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2986/3143 [18:38<00:57,  2.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2968/3145 [18:51<01:11,  2.46it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2960/3145 [18:39<01:09,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2936/3145 [18:41<01:22,  2.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2987/3143 [18:39<00:58,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2969/3145 [18:52<01:08,  2.55it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2961/3145 [18:39<01:09,  2.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2937/3145 [18:42<01:22,  2.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2988/3143 [18:39<01:00,  2.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2970/3145 [18:52<01:05,  2.67it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2962/3145 [18:39<01:06,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2938/3145 [18:42<01:19,  2.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2989/3143 [18:39<00:58,  2.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2963/3145 [18:40<01:06,  2.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2939/3145 [18:42<01:16,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2971/3145 [18:53<01:11,  2.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2990/3143 [18:40<00:58,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2964/3145 [18:40<01:05,  2.75it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2940/3145 [18:43<01:17,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2991/3143 [18:40<00:56,  2.67it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2972/3145 [18:53<01:13,  2.35it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2965/3145 [18:40<01:07,  2.67it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2941/3145 [18:43<01:18,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2992/3143 [18:41<00:56,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2973/3145 [18:53<01:10,  2.46it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2966/3145 [18:41<01:06,  2.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2942/3145 [18:43<01:17,  2.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2974/3145 [18:54<01:08,  2.48it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2993/3143 [18:41<00:59,  2.52it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2967/3145 [18:41<01:05,  2.70it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2943/3145 [18:44<01:19,  2.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2994/3143 [18:41<00:56,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2975/3145 [18:54<01:10,  2.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2968/3145 [18:42<01:06,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2995/3143 [18:42<00:55,  2.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2944/3145 [18:44<01:19,  2.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2976/3145 [18:55<01:13,  2.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2996/3143 [18:42<00:47,  3.08it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2969/3145 [18:42<01:07,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2945/3145 [18:45<01:25,  2.35it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2997/3143 [18:42<00:48,  2.99it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2977/3145 [18:55<01:12,  2.31it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2970/3145 [18:42<01:06,  2.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2946/3145 [18:45<01:20,  2.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2998/3143 [18:43<00:49,  2.93it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2978/3145 [18:55<01:11,  2.34it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2971/3145 [18:43<01:06,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2947/3145 [18:45<01:15,  2.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2979/3145 [18:56<01:07,  2.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2972/3145 [18:43<01:07,  2.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2999/3143 [18:43<00:56,  2.55it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2948/3145 [18:46<01:13,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3000/3143 [18:43<00:52,  2.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2980/3145 [18:56<01:06,  2.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2973/3145 [18:43<01:08,  2.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2949/3145 [18:46<01:14,  2.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2981/3145 [18:57<01:02,  2.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3001/3143 [18:44<00:52,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2974/3145 [18:44<01:06,  2.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2950/3145 [18:46<01:12,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3002/3143 [18:44<00:52,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2982/3145 [18:57<01:02,  2.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2975/3145 [18:44<01:04,  2.65it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2951/3145 [18:47<01:13,  2.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2983/3145 [18:57<01:00,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3003/3143 [18:45<00:53,  2.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2976/3145 [18:45<01:03,  2.67it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2952/3145 [18:47<01:15,  2.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2984/3145 [18:58<00:58,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3004/3143 [18:45<00:51,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2977/3145 [18:45<01:02,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2985/3145 [18:58<00:56,  2.83it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2953/3145 [18:48<01:15,  2.53it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3005/3143 [18:45<00:52,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2978/3145 [18:45<01:01,  2.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2954/3145 [18:48<01:13,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2986/3145 [18:58<00:59,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3006/3143 [18:46<00:49,  2.79it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2979/3145 [18:46<01:04,  2.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2955/3145 [18:48<01:10,  2.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2987/3145 [18:59<00:58,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3007/3143 [18:46<00:50,  2.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2980/3145 [18:46<01:02,  2.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2956/3145 [18:49<01:08,  2.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2988/3145 [18:59<00:57,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3008/3143 [18:46<00:51,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2981/3145 [18:47<01:03,  2.60it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2957/3145 [18:49<01:08,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3009/3143 [18:47<00:46,  2.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2989/3145 [18:59<00:57,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2982/3145 [18:47<01:02,  2.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2990/3145 [19:00<00:52,  2.95it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2958/3145 [18:50<01:10,  2.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3010/3143 [18:47<00:47,  2.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2983/3145 [18:47<00:59,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2991/3145 [19:00<00:52,  2.95it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3011/3143 [18:47<00:47,  2.76it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2959/3145 [18:50<01:13,  2.52it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2984/3145 [18:48<00:56,  2.87it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2992/3145 [19:01<00:55,  2.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2960/3145 [18:50<01:13,  2.53it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3012/3143 [18:48<00:49,  2.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2985/3145 [18:48<00:58,  2.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2993/3145 [19:01<00:55,  2.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2961/3145 [18:51<01:10,  2.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3013/3143 [18:48<00:48,  2.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2986/3145 [18:48<00:58,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2994/3145 [19:01<00:57,  2.61it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2962/3145 [18:51<01:08,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3014/3143 [18:49<00:48,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2987/3145 [18:49<00:57,  2.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2995/3145 [19:02<00:56,  2.67it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3015/3143 [18:49<00:47,  2.72it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2963/3145 [18:51<01:09,  2.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2988/3145 [18:49<00:58,  2.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2996/3145 [19:02<00:54,  2.76it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2964/3145 [18:52<01:05,  2.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3016/3143 [18:49<00:47,  2.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2989/3145 [18:49<00:58,  2.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2997/3145 [19:02<00:47,  3.08it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2965/3145 [18:52<01:06,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3017/3143 [18:50<00:46,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2990/3145 [18:50<00:57,  2.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2998/3145 [19:03<00:49,  2.98it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2966/3145 [18:53<01:07,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3018/3143 [18:50<00:47,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2991/3145 [18:50<00:57,  2.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2999/3145 [19:03<00:51,  2.85it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2967/3145 [18:53<01:04,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3019/3143 [18:50<00:46,  2.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2992/3145 [18:51<00:56,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3000/3145 [19:03<00:52,  2.77it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2968/3145 [18:53<01:02,  2.83it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3020/3143 [18:51<00:45,  2.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2993/3145 [18:51<00:58,  2.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3001/3145 [19:04<00:51,  2.78it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2969/3145 [18:54<01:02,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3021/3143 [18:51<00:49,  2.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2994/3145 [18:51<00:56,  2.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3002/3145 [19:04<00:51,  2.75it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2970/3145 [18:54<01:02,  2.81it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3022/3143 [18:52<00:46,  2.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2995/3145 [18:52<00:55,  2.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2971/3145 [18:54<01:02,  2.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3003/3145 [19:05<00:57,  2.48it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3023/3143 [18:52<00:45,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2996/3145 [18:52<00:53,  2.78it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2972/3145 [18:55<01:02,  2.77it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3004/3145 [19:05<00:55,  2.55it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2997/3145 [18:52<00:54,  2.71it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3024/3143 [18:52<00:45,  2.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2973/3145 [18:55<01:02,  2.77it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3005/3145 [19:05<00:54,  2.59it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3025/3143 [18:53<00:45,  2.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2998/3145 [18:53<00:56,  2.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2974/3145 [18:55<01:01,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3006/3145 [19:06<00:52,  2.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2999/3145 [18:53<00:54,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3026/3143 [18:53<00:46,  2.52it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2975/3145 [18:56<01:02,  2.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3007/3145 [19:06<00:52,  2.62it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3027/3143 [18:54<00:43,  2.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3000/3145 [18:54<00:55,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2976/3145 [18:56<01:00,  2.79it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3008/3145 [19:06<00:52,  2.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3028/3143 [18:54<00:42,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3001/3145 [18:54<00:53,  2.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2977/3145 [18:56<00:59,  2.82it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3009/3145 [19:07<00:49,  2.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3002/3145 [18:54<00:51,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3029/3143 [18:54<00:41,  2.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2978/3145 [18:57<01:03,  2.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3010/3145 [19:07<00:50,  2.67it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3030/3143 [18:55<00:37,  3.04it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3003/3145 [18:55<00:52,  2.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2979/3145 [18:57<01:03,  2.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3011/3145 [19:08<00:50,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3031/3143 [18:55<00:40,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3004/3145 [18:55<00:50,  2.79it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2980/3145 [18:58<01:02,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3012/3145 [19:08<00:49,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3032/3143 [18:55<00:40,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3005/3145 [18:55<00:51,  2.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2981/3145 [18:58<01:04,  2.56it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3013/3145 [19:08<00:53,  2.47it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3033/3143 [18:56<00:41,  2.67it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3006/3145 [18:56<00:52,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3014/3145 [19:09<00:50,  2.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2982/3145 [18:59<01:05,  2.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3034/3143 [18:56<00:39,  2.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3007/3145 [18:56<00:52,  2.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3015/3145 [19:09<00:45,  2.84it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2983/3145 [18:59<01:04,  2.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3035/3143 [18:56<00:39,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3008/3145 [18:56<00:50,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3016/3145 [19:09<00:44,  2.90it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2984/3145 [18:59<01:02,  2.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3036/3143 [18:57<00:38,  2.77it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3009/3145 [18:57<00:49,  2.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3017/3145 [19:10<00:46,  2.74it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3037/3143 [18:57<00:37,  2.82it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2985/3145 [19:00<01:00,  2.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3010/3145 [18:57<00:48,  2.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2986/3145 [19:00<00:59,  2.67it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3018/3145 [19:10<00:51,  2.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3038/3143 [18:58<00:38,  2.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3011/3145 [18:58<00:49,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3039/3143 [18:58<00:37,  2.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3019/3145 [19:11<00:52,  2.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2987/3145 [19:00<01:03,  2.47it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3012/3145 [18:58<00:48,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3040/3143 [18:58<00:38,  2.70it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3020/3145 [19:11<00:49,  2.52it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3013/3145 [18:58<00:49,  2.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2988/3145 [19:01<01:07,  2.31it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3041/3143 [18:59<00:37,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3021/3145 [19:11<00:49,  2.51it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3014/3145 [18:59<00:48,  2.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2989/3145 [19:01<01:03,  2.45it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3042/3143 [18:59<00:35,  2.83it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3015/3145 [18:59<00:49,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3022/3145 [19:12<00:51,  2.40it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2990/3145 [19:02<01:02,  2.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3043/3143 [18:59<00:35,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3016/3145 [18:59<00:47,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3023/3145 [19:12<00:48,  2.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2991/3145 [19:02<01:02,  2.46it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3044/3143 [19:00<00:35,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3017/3145 [19:00<00:46,  2.74it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3024/3145 [19:13<00:46,  2.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2992/3145 [19:03<01:01,  2.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3045/3143 [19:00<00:35,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3018/3145 [19:00<00:45,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3025/3145 [19:13<00:47,  2.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2993/3145 [19:03<01:01,  2.47it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3019/3145 [19:01<00:45,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3046/3143 [19:01<00:39,  2.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3026/3145 [19:13<00:47,  2.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2994/3145 [19:03<01:00,  2.51it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3020/3145 [19:01<00:43,  2.85it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3047/3143 [19:01<00:38,  2.48it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3027/3145 [19:14<00:46,  2.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2995/3145 [19:04<00:57,  2.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3021/3145 [19:01<00:44,  2.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3048/3143 [19:01<00:38,  2.45it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3028/3145 [19:14<00:45,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2996/3145 [19:04<00:56,  2.62it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3022/3145 [19:02<00:44,  2.78it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3049/3143 [19:02<00:36,  2.54it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3029/3145 [19:15<00:42,  2.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2997/3145 [19:04<00:55,  2.67it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3023/3145 [19:02<00:41,  2.91it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3030/3145 [19:15<00:41,  2.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3050/3143 [19:02<00:36,  2.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2998/3145 [19:05<00:57,  2.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3024/3145 [19:02<00:43,  2.77it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3031/3145 [19:15<00:40,  2.82it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3051/3143 [19:02<00:34,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3025/3145 [19:03<00:43,  2.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2999/3145 [19:05<00:57,  2.54it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3052/3143 [19:03<00:34,  2.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3032/3145 [19:16<00:45,  2.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3000/3145 [19:06<00:54,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3026/3145 [19:03<00:44,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3053/3143 [19:03<00:32,  2.79it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3033/3145 [19:16<00:42,  2.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3027/3145 [19:03<00:45,  2.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3001/3145 [19:06<00:57,  2.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3054/3143 [19:04<00:34,  2.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3034/3145 [19:17<00:44,  2.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3055/3143 [19:04<00:30,  2.89it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3002/3145 [19:06<00:57,  2.50it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3028/3145 [19:04<00:46,  2.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3035/3145 [19:17<00:42,  2.59it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3056/3143 [19:04<00:31,  2.80it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3029/3145 [19:04<00:48,  2.40it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3003/3145 [19:07<01:01,  2.32it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3036/3145 [19:17<00:40,  2.71it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3057/3143 [19:05<00:30,  2.86it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3030/3145 [19:05<00:45,  2.51it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3037/3145 [19:18<00:39,  2.74it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3004/3145 [19:07<00:59,  2.37it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3058/3143 [19:05<00:30,  2.81it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3031/3145 [19:05<00:44,  2.55it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3038/3145 [19:18<00:39,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3005/3145 [19:08<01:00,  2.31it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3059/3143 [19:05<00:30,  2.76it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3032/3145 [19:05<00:42,  2.66it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3039/3145 [19:18<00:38,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3006/3145 [19:08<00:59,  2.33it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3060/3143 [19:06<00:30,  2.71it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3033/3145 [19:06<00:40,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3040/3145 [19:19<00:38,  2.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3061/3143 [19:06<00:30,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3034/3145 [19:06<00:39,  2.78it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3007/3145 [19:09<01:01,  2.26it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3041/3145 [19:19<00:39,  2.66it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3062/3143 [19:06<00:29,  2.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3008/3145 [19:09<00:53,  2.55it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3035/3145 [19:07<00:43,  2.55it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3042/3145 [19:19<00:38,  2.69it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3063/3143 [19:07<00:29,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3009/3145 [19:09<00:54,  2.52it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3043/3145 [19:20<00:36,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3036/3145 [19:07<00:42,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3064/3143 [19:07<00:30,  2.59it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3044/3145 [19:20<00:36,  2.78it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3037/3145 [19:07<00:41,  2.62it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3010/3145 [19:10<00:59,  2.28it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3065/3143 [19:08<00:29,  2.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3045/3145 [19:20<00:36,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3038/3145 [19:08<00:40,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3011/3145 [19:10<00:56,  2.36it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3066/3143 [19:08<00:28,  2.68it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3012/3145 [19:11<00:53,  2.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3039/3145 [19:08<00:41,  2.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3046/3145 [19:21<00:39,  2.52it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3067/3143 [19:08<00:28,  2.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3040/3145 [19:08<00:39,  2.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3047/3145 [19:21<00:37,  2.60it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3013/3145 [19:11<00:53,  2.45it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3068/3143 [19:09<00:28,  2.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3041/3145 [19:09<00:38,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3048/3145 [19:22<00:36,  2.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3014/3145 [19:12<00:55,  2.35it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3069/3143 [19:09<00:27,  2.72it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3042/3145 [19:09<00:36,  2.82it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3049/3145 [19:22<00:35,  2.68it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3070/3143 [19:09<00:26,  2.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3015/3145 [19:12<00:55,  2.35it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3043/3145 [19:10<00:37,  2.72it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3071/3143 [19:10<00:23,  3.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3050/3145 [19:22<00:36,  2.59it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3016/3145 [19:12<00:53,  2.41it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3072/3143 [19:10<00:23,  2.99it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3051/3145 [19:23<00:35,  2.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3044/3145 [19:10<00:42,  2.40it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3017/3145 [19:13<00:52,  2.45it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3073/3143 [19:10<00:23,  2.99it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3052/3145 [19:23<00:34,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3045/3145 [19:10<00:39,  2.53it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3018/3145 [19:13<00:49,  2.55it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3074/3143 [19:11<00:23,  2.98it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3053/3145 [19:24<00:34,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3019/3145 [19:13<00:47,  2.67it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3046/3145 [19:11<00:42,  2.36it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3054/3145 [19:24<00:30,  3.03it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3075/3143 [19:11<00:23,  2.84it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3020/3145 [19:14<00:45,  2.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3055/3145 [19:24<00:31,  2.89it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3076/3143 [19:11<00:23,  2.88it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3047/3145 [19:11<00:43,  2.26it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3021/3145 [19:14<00:44,  2.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3056/3145 [19:24<00:30,  2.90it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3077/3143 [19:12<00:23,  2.80it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3048/3145 [19:12<00:41,  2.33it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3022/3145 [19:14<00:44,  2.78it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3057/3145 [19:25<00:30,  2.86it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3049/3145 [19:12<00:38,  2.50it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3078/3143 [19:12<00:24,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3023/3145 [19:15<00:43,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3058/3145 [19:25<00:30,  2.86it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3079/3143 [19:13<00:23,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3050/3145 [19:12<00:37,  2.52it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3024/3145 [19:15<00:46,  2.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3059/3145 [19:26<00:30,  2.85it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3051/3145 [19:13<00:36,  2.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3080/3143 [19:13<00:25,  2.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3025/3145 [19:16<00:45,  2.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3060/3145 [19:26<00:31,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3052/3145 [19:13<00:36,  2.53it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3081/3143 [19:13<00:24,  2.53it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3026/3145 [19:16<00:46,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3061/3145 [19:26<00:30,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3053/3145 [19:14<00:36,  2.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3082/3143 [19:14<00:23,  2.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3027/3145 [19:16<00:44,  2.62it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3062/3145 [19:27<00:30,  2.69it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3054/3145 [19:14<00:35,  2.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3083/3143 [19:14<00:23,  2.54it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3028/3145 [19:17<00:43,  2.68it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3063/3145 [19:27<00:30,  2.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3055/3145 [19:14<00:33,  2.67it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3084/3143 [19:15<00:23,  2.51it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3029/3145 [19:17<00:43,  2.69it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3064/3145 [19:27<00:30,  2.69it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3056/3145 [19:15<00:32,  2.72it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3085/3143 [19:15<00:22,  2.54it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3030/3145 [19:17<00:42,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3057/3145 [19:15<00:32,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3065/3145 [19:28<00:31,  2.51it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3031/3145 [19:18<00:41,  2.72it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3086/3143 [19:15<00:22,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3058/3145 [19:15<00:31,  2.74it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3066/3145 [19:28<00:30,  2.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3087/3143 [19:16<00:20,  2.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3032/3145 [19:18<00:41,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3059/3145 [19:16<00:31,  2.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3067/3145 [19:29<00:29,  2.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3088/3143 [19:16<00:20,  2.66it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3033/3145 [19:19<00:43,  2.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3060/3145 [19:16<00:31,  2.68it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3068/3145 [19:29<00:28,  2.66it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3089/3143 [19:16<00:20,  2.70it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3034/3145 [19:19<00:41,  2.66it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3061/3145 [19:17<00:30,  2.75it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3069/3145 [19:29<00:28,  2.66it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3090/3143 [19:17<00:20,  2.61it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3035/3145 [19:19<00:42,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3062/3145 [19:17<00:29,  2.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3070/3145 [19:30<00:27,  2.70it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3091/3143 [19:17<00:19,  2.66it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3036/3145 [19:20<00:41,  2.60it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3063/3145 [19:17<00:31,  2.60it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3071/3145 [19:30<00:29,  2.53it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3092/3143 [19:18<00:18,  2.75it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3037/3145 [19:20<00:42,  2.55it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3064/3145 [19:18<00:30,  2.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3072/3145 [19:31<00:28,  2.59it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3093/3143 [19:18<00:18,  2.69it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3038/3145 [19:21<00:41,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3065/3145 [19:18<00:29,  2.71it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3073/3145 [19:31<00:28,  2.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3094/3143 [19:18<00:18,  2.71it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3039/3145 [19:21<00:37,  2.81it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3066/3145 [19:18<00:29,  2.69it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3074/3145 [19:31<00:27,  2.61it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3040/3145 [19:21<00:34,  3.06it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3095/3143 [19:19<00:17,  2.76it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3067/3145 [19:19<00:28,  2.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3041/3145 [19:22<00:36,  2.87it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3075/3145 [19:32<00:28,  2.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3096/3143 [19:19<00:18,  2.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3068/3145 [19:19<00:28,  2.66it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3076/3145 [19:32<00:26,  2.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3042/3145 [19:22<00:37,  2.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3097/3143 [19:19<00:17,  2.62it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3069/3145 [19:20<00:27,  2.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3077/3145 [19:32<00:25,  2.68it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3043/3145 [19:22<00:36,  2.83it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3098/3143 [19:20<00:16,  2.68it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3070/3145 [19:20<00:26,  2.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3044/3145 [19:23<00:34,  2.97it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3071/3145 [19:20<00:23,  3.10it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3078/3145 [19:33<00:26,  2.52it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3099/3143 [19:20<00:16,  2.67it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3045/3145 [19:23<00:34,  2.89it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3072/3145 [19:20<00:24,  3.03it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3079/3145 [19:33<00:24,  2.65it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3100/3143 [19:21<00:15,  2.71it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3046/3145 [19:23<00:31,  3.10it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3073/3145 [19:21<00:23,  3.01it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3080/3145 [19:34<00:24,  2.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3101/3143 [19:21<00:16,  2.55it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3047/3145 [19:24<00:32,  3.06it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3074/3145 [19:21<00:24,  2.95it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3081/3145 [19:34<00:23,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3102/3143 [19:21<00:15,  2.59it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3048/3145 [19:24<00:33,  2.90it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3075/3145 [19:22<00:24,  2.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3082/3145 [19:34<00:23,  2.73it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3103/3143 [19:22<00:14,  2.68it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3049/3145 [19:24<00:32,  2.92it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3076/3145 [19:22<00:24,  2.86it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3083/3145 [19:35<00:23,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3104/3143 [19:22<00:14,  2.72it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3050/3145 [19:25<00:32,  2.93it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3077/3145 [19:22<00:21,  3.20it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3051/3145 [19:25<00:29,  3.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3105/3143 [19:22<00:13,  2.79it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3084/3145 [19:35<00:25,  2.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3078/3145 [19:22<00:21,  3.07it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3106/3143 [19:23<00:11,  3.13it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3052/3145 [19:25<00:31,  2.98it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3079/3145 [19:23<00:21,  3.04it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3085/3145 [19:36<00:23,  2.53it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3107/3143 [19:23<00:11,  3.01it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3053/3145 [19:26<00:30,  3.01it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3086/3145 [19:36<00:23,  2.46it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3080/3145 [19:23<00:24,  2.65it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3108/3143 [19:23<00:11,  2.96it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3054/3145 [19:26<00:30,  3.01it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3087/3145 [19:36<00:22,  2.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3081/3145 [19:24<00:23,  2.69it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3109/3143 [19:24<00:11,  2.97it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3055/3145 [19:26<00:30,  2.98it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3088/3145 [19:37<00:22,  2.52it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3082/3145 [19:24<00:23,  2.70it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3110/3143 [19:24<00:11,  2.80it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3056/3145 [19:27<00:30,  2.92it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3089/3145 [19:37<00:21,  2.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3083/3145 [19:24<00:22,  2.71it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3111/3143 [19:24<00:11,  2.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3057/3145 [19:27<00:31,  2.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3090/3145 [19:38<00:20,  2.67it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3084/3145 [19:25<00:22,  2.77it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3112/3143 [19:25<00:10,  2.83it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3058/3145 [19:27<00:31,  2.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3059/3145 [19:28<00:27,  3.13it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3091/3145 [19:38<00:20,  2.65it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3085/3145 [19:25<00:22,  2.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3113/3143 [19:25<00:11,  2.71it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3060/3145 [19:28<00:27,  3.12it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3086/3145 [19:25<00:21,  2.77it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3092/3145 [19:38<00:20,  2.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3114/3143 [19:26<00:10,  2.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3061/3145 [19:28<00:29,  2.85it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3087/3145 [19:26<00:20,  2.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3093/3145 [19:39<00:19,  2.65it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3115/3143 [19:26<00:10,  2.55it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3088/3145 [19:26<00:20,  2.81it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3094/3145 [19:39<00:19,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3116/3143 [19:26<00:10,  2.68it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3062/3145 [19:29<00:33,  2.47it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3089/3145 [19:27<00:20,  2.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3095/3145 [19:39<00:17,  2.78it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3117/3143 [19:27<00:09,  2.79it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3063/3145 [19:29<00:35,  2.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3090/3145 [19:27<00:19,  2.76it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3096/3145 [19:40<00:17,  2.78it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3118/3143 [19:27<00:09,  2.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3064/3145 [19:30<00:33,  2.41it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3091/3145 [19:27<00:20,  2.70it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3097/3145 [19:40<00:17,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3119/3143 [19:27<00:08,  2.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3065/3145 [19:30<00:32,  2.46it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3092/3145 [19:28<00:19,  2.73it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3120/3143 [19:28<00:08,  2.84it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3098/3145 [19:41<00:17,  2.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3066/3145 [19:30<00:30,  2.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3093/3145 [19:28<00:18,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3099/3145 [19:41<00:17,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3121/3143 [19:28<00:08,  2.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3067/3145 [19:31<00:28,  2.73it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3094/3145 [19:28<00:18,  2.70it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3100/3145 [19:41<00:16,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3122/3143 [19:29<00:07,  2.67it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3068/3145 [19:31<00:28,  2.69it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3095/3145 [19:29<00:18,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3101/3145 [19:42<00:16,  2.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3123/3143 [19:29<00:07,  2.72it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3069/3145 [19:31<00:27,  2.72it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3096/3145 [19:29<00:18,  2.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3124/3143 [19:29<00:07,  2.71it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3102/3145 [19:42<00:17,  2.47it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3070/3145 [19:32<00:28,  2.65it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3097/3145 [19:30<00:18,  2.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3125/3143 [19:30<00:06,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3103/3145 [19:42<00:16,  2.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3071/3145 [19:32<00:27,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3126/3143 [19:30<00:06,  2.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3072/3145 [19:32<00:23,  3.07it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3098/3145 [19:30<00:18,  2.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3104/3145 [19:43<00:16,  2.52it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3073/3145 [19:33<00:24,  2.95it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3127/3143 [19:30<00:06,  2.57it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3099/3145 [19:30<00:18,  2.50it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3105/3145 [19:43<00:15,  2.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3074/3145 [19:33<00:25,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3128/3143 [19:31<00:05,  2.62it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3100/3145 [19:31<00:18,  2.48it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3106/3145 [19:44<00:15,  2.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3101/3145 [19:31<00:15,  2.86it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3075/3145 [19:34<00:25,  2.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3129/3143 [19:31<00:05,  2.61it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3107/3145 [19:44<00:15,  2.51it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3102/3145 [19:31<00:15,  2.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3076/3145 [19:34<00:24,  2.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3130/3143 [19:32<00:05,  2.54it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3108/3145 [19:44<00:14,  2.47it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3103/3145 [19:32<00:15,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3131/3143 [19:32<00:04,  2.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3077/3145 [19:34<00:27,  2.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3109/3145 [19:45<00:14,  2.51it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3104/3145 [19:32<00:14,  2.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3078/3145 [19:35<00:26,  2.52it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3110/3145 [19:45<00:13,  2.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3132/3143 [19:32<00:04,  2.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3105/3145 [19:32<00:14,  2.77it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3079/3145 [19:35<00:25,  2.56it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3111/3145 [19:46<00:12,  2.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3133/3143 [19:33<00:04,  2.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3106/3145 [19:33<00:16,  2.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3134/3143 [19:33<00:03,  2.81it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3080/3145 [19:36<00:24,  2.61it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3112/3145 [19:46<00:12,  2.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3107/3145 [19:33<00:14,  2.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3135/3143 [19:33<00:02,  2.77it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3081/3145 [19:36<00:24,  2.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3113/3145 [19:46<00:12,  2.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3108/3145 [19:34<00:13,  2.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3136/3143 [19:34<00:02,  2.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3082/3145 [19:36<00:23,  2.65it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3114/3145 [19:47<00:11,  2.70it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3109/3145 [19:34<00:11,  3.02it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3137/3143 [19:34<00:02,  2.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3083/3145 [19:37<00:22,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3110/3145 [19:34<00:11,  2.95it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3115/3145 [19:47<00:11,  2.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3138/3143 [19:35<00:01,  2.81it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3084/3145 [19:37<00:21,  2.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3116/3145 [19:47<00:10,  2.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3111/3145 [19:35<00:11,  2.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3139/3143 [19:35<00:01,  2.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3085/3145 [19:37<00:22,  2.62it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3112/3145 [19:35<00:12,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3117/3145 [19:48<00:11,  2.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3140/3143 [19:35<00:01,  2.75it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3086/3145 [19:38<00:22,  2.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3113/3145 [19:35<00:11,  2.76it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3118/3145 [19:48<00:10,  2.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3141/3143 [19:36<00:00,  2.81it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3087/3145 [19:38<00:22,  2.61it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3114/3145 [19:36<00:11,  2.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3142/3143 [19:36<00:00,  2.90it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3119/3145 [19:49<00:10,  2.46it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3088/3145 [19:39<00:21,  2.61it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3115/3145 [19:36<00:11,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3120/3145 [19:49<00:09,  2.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3143/3143 [19:36<00:00,  2.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3143/3143 [19:36<00:00,  2.67it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3089/3145 [19:39<00:21,  2.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3116/3145 [19:37<00:10,  2.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3121/3145 [19:49<00:08,  2.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3122/3145 [19:50<00:07,  3.01it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3090/3145 [19:39<00:20,  2.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3117/3145 [19:37<00:10,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3123/3145 [19:50<00:07,  2.96it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3091/3145 [19:40<00:20,  2.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3118/3145 [19:37<00:09,  2.76it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3124/3145 [19:50<00:06,  3.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3119/3145 [19:38<00:09,  2.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3125/3145 [19:50<00:05,  3.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3092/3145 [19:40<00:21,  2.52it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3093/3145 [19:40<00:19,  2.73it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3126/3145 [19:51<00:05,  3.30it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3120/3145 [19:38<00:09,  2.66it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3094/3145 [19:41<00:18,  2.76it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3127/3145 [19:51<00:05,  3.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3121/3145 [19:38<00:08,  2.75it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3095/3145 [19:41<00:18,  2.71it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3122/3145 [19:39<00:08,  2.77it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3128/3145 [19:52<00:05,  2.97it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3123/3145 [19:39<00:07,  2.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3096/3145 [19:42<00:17,  2.73it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3129/3145 [19:52<00:05,  2.92it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3124/3145 [19:39<00:07,  2.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3097/3145 [19:42<00:18,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3130/3145 [19:52<00:05,  2.80it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3125/3145 [19:40<00:07,  2.83it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3098/3145 [19:42<00:17,  2.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3131/3145 [19:53<00:05,  2.76it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3126/3145 [19:40<00:06,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3132/3145 [19:53<00:04,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3099/3145 [19:43<00:18,  2.53it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3127/3145 [19:41<00:06,  2.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3133/3145 [19:53<00:04,  2.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3100/3145 [19:43<00:18,  2.48it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3128/3145 [19:41<00:06,  2.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3134/3145 [19:54<00:04,  2.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3101/3145 [19:44<00:19,  2.26it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3129/3145 [19:41<00:05,  2.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3135/3145 [19:54<00:03,  2.57it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3102/3145 [19:44<00:17,  2.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3130/3145 [19:42<00:05,  2.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3136/3145 [19:55<00:03,  2.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3103/3145 [19:44<00:16,  2.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3131/3145 [19:42<00:05,  2.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3137/3145 [19:55<00:02,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3132/3145 [19:42<00:04,  3.06it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3104/3145 [19:45<00:16,  2.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3138/3145 [19:55<00:02,  2.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3133/3145 [19:42<00:03,  3.39it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3105/3145 [19:45<00:15,  2.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3134/3145 [19:43<00:03,  3.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3139/3145 [19:56<00:02,  2.78it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3106/3145 [19:46<00:15,  2.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3135/3145 [19:43<00:03,  3.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3140/3145 [19:56<00:01,  2.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3136/3145 [19:43<00:02,  3.19it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3107/3145 [19:46<00:14,  2.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3141/3145 [19:56<00:01,  2.84it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3108/3145 [19:46<00:14,  2.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3142/3145 [19:57<00:01,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3137/3145 [19:44<00:02,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3143/3145 [19:57<00:00,  2.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3138/3145 [19:44<00:02,  2.65it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3109/3145 [19:47<00:15,  2.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3144/3145 [19:57<00:00,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3139/3145 [19:45<00:02,  2.69it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3110/3145 [19:47<00:14,  2.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3140/3145 [19:45<00:01,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [19:58<00:00,  2.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [19:58<00:00,  2.62it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3111/3145 [19:48<00:14,  2.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3141/3145 [19:45<00:01,  2.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3112/3145 [19:48<00:13,  2.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3142/3145 [19:46<00:01,  2.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3113/3145 [19:48<00:12,  2.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3143/3145 [19:46<00:00,  2.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3114/3145 [19:49<00:12,  2.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3144/3145 [19:47<00:00,  2.67it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3115/3145 [19:49<00:11,  2.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [19:47<00:00,  2.72it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3116/3145 [19:49<00:10,  2.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [19:47<00:00,  2.65it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3117/3145 [19:50<00:09,  2.80it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3118/3145 [19:50<00:09,  2.80it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3119/3145 [19:51<00:09,  2.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3120/3145 [19:51<00:09,  2.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3121/3145 [19:51<00:08,  2.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3122/3145 [19:52<00:08,  2.62it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3123/3145 [19:52<00:07,  2.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3124/3145 [19:52<00:07,  2.82it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3125/3145 [19:53<00:07,  2.82it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3126/3145 [19:53<00:06,  2.82it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3127/3145 [19:53<00:06,  2.73it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3128/3145 [19:54<00:06,  2.81it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3129/3145 [19:54<00:06,  2.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3130/3145 [19:55<00:05,  2.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3131/3145 [19:55<00:05,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3132/3145 [19:55<00:04,  2.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3133/3145 [19:56<00:04,  2.73it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3134/3145 [19:56<00:04,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3135/3145 [19:56<00:03,  2.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3136/3145 [19:57<00:03,  2.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3137/3145 [19:57<00:02,  2.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3138/3145 [19:58<00:02,  2.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3139/3145 [19:58<00:02,  2.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3140/3145 [19:58<00:01,  2.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3141/3145 [19:59<00:01,  2.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3142/3145 [19:59<00:01,  2.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3143/3145 [19:59<00:00,  2.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3144/3145 [20:00<00:00,  2.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [20:00<00:00,  2.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3145/3145 [20:00<00:00,  2.62it/s]
Please make sure to use our provided visual features as gqadataset.org for better comparability. We provide both spatial and object-based features trained on GQA train set.
In particular please avoid using features from https://github.com/peteanderson80/bottom-up-attention since they were trained on images contained in the GQA validation set and thus may give false scores improvement.

Please consider using --consistency to compute consistency scores for entailed questions.
If you do so, please provide answers to all questions in val_all_questions.json.

Please consider using --grounding to compute attention scores.
If you do so, please provide attention maps through --attentions.

Loading questions...
Loading predictions...
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/eval/eval_gqa.py", line 132, in <module>
    predictions = loadFile(args.predictions.format(tier = args.tier))
  File "/home/akane38/LLaVA/llava/eval/eval_gqa.py", line 115, in loadFile
    raise Exception("Can't find {}".format(name))
Exception: Can't find testdev_balanced_predictions.json
Please make sure to use our provided visual features as gqadataset.org for better comparability. We provide both spatial and object-based features trained on GQA train set.
In particular please avoid using features from https://github.com/peteanderson80/bottom-up-attention since they were trained on images contained in the GQA validation set and thus may give false scores improvement.

Please consider using --consistency to compute consistency scores for entailed questions.
If you do so, please provide answers to all questions in val_all_questions.json.

Please consider using --grounding to compute attention scores.
If you do so, please provide attention maps through --attentions.

Loading questions...
Loading predictions...
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/eval/eval_gqa.py", line 132, in <module>
    predictions = loadFile(args.predictions.format(tier = args.tier))
  File "/home/akane38/LLaVA/llava/eval/eval_gqa.py", line 115, in loadFile
    raise Exception("Can't find {}".format(name))
Exception: Can't find //testdev_balanced_predictions.json
Please make sure to use our provided visual features as gqadataset.org for better comparability. We provide both spatial and object-based features trained on GQA train set.
In particular please avoid using features from https://github.com/peteanderson80/bottom-up-attention since they were trained on images contained in the GQA validation set and thus may give false scores improvement.

Please consider using --consistency to compute consistency scores for entailed questions.
If you do so, please provide answers to all questions in val_all_questions.json.

Please consider using --grounding to compute attention scores.
If you do so, please provide attention maps through --attentions.

Loading questions...
Loading predictions...
  0%|          | 0/12578 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12578/12578 [00:00<00:00, 158834.11it/s]

Binary: 80.39%
Open: 48.23%
Accuracy: 62.99%
Validity: 0.00%
Plausibility: 0.00%
Distribution: 1.68 (lower is better)

Accuracy / structural type:
  choose: 85.39% (1129 questions)
  compare: 64.86% (589 questions)
  logical: 78.76% (1803 questions)
  query: 48.23% (6805 questions)
  verify: 83.26% (2252 questions)

Accuracy / semantic type:
  attr: 70.50% (5186 questions)
  cat: 50.91% (1149 questions)
  global: 59.87% (157 questions)
  obj: 87.92% (778 questions)
  rel: 54.71% (5308 questions)

Accuracy / steps number:
  1: 79.32% (237 questions)
  2: 57.06% (6395 questions)
  3: 66.39% (4266 questions)
  4: 69.74% (793 questions)
  5: 77.98% (822 questions)
  6: 90.24% (41 questions)
  7: 95.00% (20 questions)
  8: 100.00% (3 questions)
  9: 100.00% (1 questions)

Accuracy / words number:
  3: 41.06% (151 questions)
  4: 56.98% (630 questions)
  5: 49.07% (1290 questions)
  6: 58.53% (2074 questions)
  7: 61.94% (1642 questions)
  8: 65.23% (1185 questions)
  9: 69.09% (1281 questions)
  10: 69.98% (1249 questions)
  11: 64.29% (994 questions)
  12: 69.59% (638 questions)
  13: 66.45% (462 questions)
  14: 71.88% (345 questions)
  15: 73.00% (237 questions)
  16: 75.21% (117 questions)
  17: 67.02% (94 questions)
  18: 77.63% (76 questions)
  19: 79.07% (43 questions)
  20: 75.00% (32 questions)
  21: 73.68% (19 questions)
  22: 75.00% (12 questions)
  23: 25.00% (4 questions)
  24: 100.00% (2 questions)
  25: 100.00% (1 questions)
