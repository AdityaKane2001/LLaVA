=================================================-Fri Mar 29 04:22:21 PM UTC 2024=========================================================
[2024-03-29 12:22:23,298] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:24,855] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:22:24,856] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:22:26,796] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:29,943] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:22:29,943] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:22:29,943] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:22:29,943] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:22:29,943] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:22:32,935] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:22:33,955] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:22:33,955] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
ModelArguments(model_name_or_path='lmsys/vicuna-13b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 819, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:22:34,949] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2755495
[2024-03-29 12:22:34,950] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1
=================================================-Fri Mar 29 04:23:29 PM UTC 2024=========================================================
[2024-03-29 12:23:31,411] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:32,888] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:23:32,889] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:23:34,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:37,993] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:23:37,993] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:23:37,993] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:23:37,993] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:23:37,993] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:23:41,033] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:23:42,074] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:23:42,074] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards:  33%|███▎      | 1/3 [03:18<06:36, 198.29s/it][2024-03-29 12:27:02,109] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:03,459] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:27:03,459] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-03-29 12:27:05,383] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:06,922] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:27:06,922] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:27:06,922] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:27:06,922] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:27:06,922] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:27:09,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:27:10,837] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:27:10,837] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 793, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[2024-03-29 12:27:11,928] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2767012
[2024-03-29 12:27:11,928] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1

=================================================-Fri Mar 29 04:28:14 PM UTC 2024=========================================================

[2024-03-29 12:28:16,554] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:18,119] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:28:18,120] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:28:19,970] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:21,668] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:28:21,668] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:28:21,668] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:28:21,668] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:28:21,668] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:28:24,642] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:28:25,733] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:28:25,733] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 794, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[2024-03-29 12:28:26,674] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2770161
[2024-03-29 12:28:26,675] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

Downloading shards:  67%|██████▋   | 2/3 [06:38<03:19, 199.34s/it][2024-03-29 12:30:30,407] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2758305
[2024-03-29 12:30:30,407] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

=================================================-Fri Mar 29 04:30:42 PM UTC 2024=========================================================

[2024-03-29 12:30:44,410] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:45,914] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:30:45,914] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:30:47,838] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:51,146] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:30:51,146] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:30:51,146] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:30:51,146] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:30:51,146] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:30:54,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:30:54,992] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:30:54,992] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:  50%|█████     | 1/2 [00:26<00:26, 26.96s/it]
Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 16.13s/it]
Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 17.76s/it]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:31:32,301] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 912, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:31:40,195] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2776101
[2024-03-29 12:31:40,196] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

=================================================-Fri Mar 29 04:32:32 PM UTC 2024=========================================================

[2024-03-29 12:32:34,633] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:36,097] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:32:36,098] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:32:38,038] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:41,262] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:32:41,262] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:32:41,262] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:32:41,262] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:32:41,262] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:32:44,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:32:45,249] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:32:45,252] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:32:46,816] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 914, in train
    if model_args.vision_tower is not None:
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:32:53,276] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2780731
[2024-03-29 12:32:53,276] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:39:22 PM UTC 2024 =========================================================

[2024-03-29 12:39:24,429] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:39:26,026] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:39:26,026] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:39:27,958] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:39:31,063] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:39:31,063] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:39:31,064] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:39:31,064] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:39:31,064] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:39:33,990] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 1, in <module>
    from llava.train.multi_ve_train import train
  File "/home/akane38/LLaVA/llava/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/akane38/LLaVA/llava/model/__init__.py)
[2024-03-29 12:39:35,069] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2796221
[2024-03-29 12:39:35,069] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:40:50 PM UTC 2024 =========================================================

[2024-03-29 12:40:52,446] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:40:53,981] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:40:53,981] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:40:55,875] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:40:58,955] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:40:58,955] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:40:58,955] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:40:58,956] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:40:58,956] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:41:01,833] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:41:02,836] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:41:02,836] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:41:04,318] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 51, in initialize_vision_modules
    vision_tower = model_args.vision_tower
AttributeError: 'ModelArguments' object has no attribute 'vision_tower'. Did you mean: 'vision_towers'?
[2024-03-29 12:41:10,969] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2799679
[2024-03-29 12:41:10,969] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 04:47:42 PM UTC 2024 =========================================================

[2024-03-29 12:47:44,476] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:45,960] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 12:47:45,961] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 12:47:47,902] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:51,130] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 12:47:51,130] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 12:47:51,130] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 12:47:51,130] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 12:47:51,130] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 12:47:53,992] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 12:47:55,031] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 12:47:55,031] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 12:47:58,225] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, multiple_vision_towers=['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large'], mm_vision_select_layer=-2, pretrain_mm_mlp_adapter='./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
['openai/clip-vit-large-patch14-336', 'facebook/dinov2-large']
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 12:48:05,145] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2815319
[2024-03-29 12:48:05,146] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:07:59 PM UTC 2024 =========================================================

[2024-03-29 13:08:01,882] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:03,415] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:08:03,415] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:08:05,278] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:08,420] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:08:08,420] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:08:08,420] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:08:08,420] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:08:08,420] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:08:11,331] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:08:12,339] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:08:12,339] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:08:13,870] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:08:19,937] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 13:08:21,434] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2859305
[2024-03-29 13:08:21,434] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:09:09 PM UTC 2024 =========================================================

[2024-03-29 13:09:11,743] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:13,255] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:09:13,255] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:09:16,065] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:19,138] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:09:19,138] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:09:19,138] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:09:19,138] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:09:19,138] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:09:22,052] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:09:23,068] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:09:23,068] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:09:24,803] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:09:30,897] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:09:35,418] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
[CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
), DINOVisionTower(
  (vision_tower): Dinov2Model(
    (embeddings): Dinov2Embeddings(
      (patch_embeddings): Dinov2PatchEmbeddings(
        (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): Dinov2Encoder(
      (layer): ModuleList(
        (0-23): 24 x Dinov2Layer(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attention): Dinov2Attention(
            (attention): Dinov2SelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): Dinov2SelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (layer_scale1): Dinov2LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Dinov2MLP(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (activation): GELUActivation()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (layer_scale2): Dinov2LayerScale()
          (drop_path2): Identity()
        )
      )
    )
    (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
)]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 69, in initialize_vision_modules
    self.vision_tower = vision_tower
UnboundLocalError: local variable 'vision_tower' referenced before assignment
[2024-03-29 13:09:37,157] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2862337
[2024-03-29 13:09:37,158] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:23:32 PM UTC 2024 =========================================================

[2024-03-29 13:23:34,288] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:35,773] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:23:35,773] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:23:37,690] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:40,921] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:23:40,921] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:23:40,922] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:23:40,922] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:23:40,922] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:23:43,873] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:23:44,877] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:23:44,877] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:23:47,319] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:23:52,976] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:23:53,780] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 103, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:23:55,937] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2894192
[2024-03-29 13:23:55,938] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:24:48 PM UTC 2024 =========================================================

[2024-03-29 13:24:50,741] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:24:52,236] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:24:52,236] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:24:54,168] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:24:57,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:24:57,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:24:57,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:24:57,452] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:24:57,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:25:00,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:25:01,728] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:25:01,728] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:25:03,300] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:25:09,064] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:25:09,949] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 103, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:25:12,469] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2897495
[2024-03-29 13:25:12,469] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:26:42 PM UTC 2024 =========================================================

[2024-03-29 13:26:44,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:45,893] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:26:45,893] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:26:47,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:50,919] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:26:50,919] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:26:50,919] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:26:50,919] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:26:50,919] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:26:53,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:26:54,826] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:26:54,826] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:26:56,368] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:27:02,225] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:27:03,122] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:27:04,935] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2902250
[2024-03-29 13:27:04,935] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:27:48 PM UTC 2024 =========================================================

[2024-03-29 13:27:50,145] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:27:51,695] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:27:51,695] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:27:53,628] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:27:56,657] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:27:56,658] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:27:56,658] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:27:56,658] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:27:56,658] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:27:59,542] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:28:00,551] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:28:00,551] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:28:02,043] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.75s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[2024-03-29 13:28:07,997] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:28:09,082] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 916, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:28:10,673] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2905220
[2024-03-29 13:28:10,673] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:29:45 PM UTC 2024 =========================================================

[2024-03-29 13:29:46,963] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:48,539] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:29:48,539] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:29:50,463] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:53,646] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:29:53,646] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:29:53,646] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:29:53,646] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:29:53,646] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:29:56,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:29:57,570] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:29:57,570] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:29:59,275] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:30:05,014] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:30:06,033] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
ModuleList(
  (0): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (1): DINOVisionTower(
    (vision_tower): Dinov2Model(
      (embeddings): Dinov2Embeddings(
        (patch_embeddings): Dinov2PatchEmbeddings(
          (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): Dinov2Encoder(
        (layer): ModuleList(
          (0-23): 24 x Dinov2Layer(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attention): Dinov2Attention(
              (attention): Dinov2SelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): Dinov2SelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (layer_scale1): Dinov2LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Dinov2MLP(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (activation): GELUActivation()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_scale2): Dinov2LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
  )
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 914, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 104, in initialize_vision_modules
    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin'
[2024-03-29 13:30:07,661] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2910111
[2024-03-29 13:30:07,662] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:31:16 PM UTC 2024 =========================================================

[2024-03-29 13:31:18,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:20,181] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:31:20,182] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:31:22,126] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:25,424] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:31:25,424] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:31:25,424] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:31:25,424] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:31:25,424] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:31:28,555] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:31:29,691] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:31:29,692] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:31:31,289] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:31:36,926] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:31:38,160] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 921, in train
    print(model.get_multiple_vision_towers())
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MultiVELlavaLlamaForCausalLM' object has no attribute 'get_multiple_vision_towers'
[2024-03-29 13:31:40,440] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2914015
[2024-03-29 13:31:40,440] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Fri Mar 29 05:38:14 PM UTC 2024 =========================================================

[2024-03-29 13:38:16,046] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:17,642] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-29 13:38:17,643] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-29 13:38:19,558] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:22,835] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-29 13:38:22,835] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-29 13:38:22,835] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-29 13:38:22,835] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-29 13:38:22,835] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-29 13:38:25,715] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 13:38:26,696] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-29 13:38:26,696] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-29 13:38:28,340] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-29 13:38:33,929] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-29 13:38:34,989] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 968, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 779, in make_supervised_data_module
    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 665, in __init__
    list_data_dict = json.load(open(data_path, "r"))
FileNotFoundError: [Errno 2] No such file or directory: './playground/data/llava_v1_5_mix665k.json'
[2024-03-29 13:38:36,851] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2929555
[2024-03-29 13:38:36,852] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 03:20:57 AM UTC 2024 =========================================================

scripts/v1_5/multi_ve_finetune.sh: 5: deepspeed: not found

================================================= Sun Mar 31 03:27:45 AM UTC 2024 =========================================================

[2024-03-30 23:27:47,005] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:48,577] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-30 23:27:48,577] [INFO] [runner.py:571:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data0/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 2 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-30 23:27:50,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:53,861] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-30 23:27:53,863] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-30 23:27:53,863] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-30 23:27:53,863] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-30 23:27:53,863] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-03-30 23:27:56,680] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-30 23:27:57,666] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-30 23:27:57,666] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-30 23:28:00,064] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-30 23:28:05,502] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-30 23:28:06,646] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 968, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 779, in make_supervised_data_module
    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 665, in __init__
    list_data_dict = json.load(open(data_path, "r"))
FileNotFoundError: [Errno 2] No such file or directory: './playground/data/llava_v1_5_mix665k.json'
[2024-03-30 23:28:08,878] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3338325
[2024-03-30 23:28:08,879] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data0/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 08:50:46 PM UTC 2024 =========================================================

scripts/v1_5/multi_ve_finetune.sh: 5: deepspeed: not found

================================================= Sun Mar 31 08:51:04 PM UTC 2024 =========================================================

[2024-03-31 16:51:06,115] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 16:51:08,022] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 16:51:08,022] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 16:51:09,911] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 16:51:11,783] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 16:51:11,784] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 16:51:11,784] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 16:51:11,784] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 16:51:11,784] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 16:51:14,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 1, in <module>
    from llava.train.multi_ve_train import train
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 947
    model.get_model().mm_projector.dtype=compute_dtype, device=training_args.device)
                                                                                   ^
SyntaxError: unmatched ')'
[2024-03-31 16:51:16,790] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2095691
[2024-03-31 16:51:16,791] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 08:51:59 PM UTC 2024 =========================================================

[2024-03-31 16:52:01,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 16:52:03,489] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 16:52:03,490] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 16:52:05,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 16:52:07,383] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 16:52:07,383] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 16:52:07,383] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 16:52:07,383] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 16:52:07,383] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 16:52:10,665] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 16:52:11,970] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 16:52:11,971] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 26.84it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 16:52:13,881] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]
[2024-03-31 16:52:24,835] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B

preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]
preprocessor_config.json: 100%|██████████| 436/436 [00:00<00:00, 2.47MB/s]

config.json:   0%|          | 0.00/549 [00:00<?, ?B/s]
config.json: 100%|██████████| 549/549 [00:00<00:00, 3.70MB/s]

model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]
model.safetensors:   3%|▎         | 41.9M/1.22G [00:00<00:03, 360MB/s]
model.safetensors:   7%|▋         | 83.9M/1.22G [00:00<00:03, 349MB/s]
model.safetensors:  10%|█         | 126M/1.22G [00:00<00:02, 375MB/s] 
model.safetensors:  14%|█▍        | 168M/1.22G [00:00<00:02, 390MB/s]
model.safetensors:  18%|█▊        | 220M/1.22G [00:00<00:02, 408MB/s]
model.safetensors:  22%|██▏       | 262M/1.22G [00:00<00:02, 411MB/s]
model.safetensors:  25%|██▍       | 304M/1.22G [00:00<00:02, 412MB/s]
model.safetensors:  29%|██▉       | 357M/1.22G [00:00<00:02, 417MB/s]
model.safetensors:  33%|███▎      | 398M/1.22G [00:00<00:01, 411MB/s]
model.safetensors:  36%|███▌      | 440M/1.22G [00:01<00:01, 412MB/s]
model.safetensors:  40%|███▉      | 482M/1.22G [00:01<00:01, 408MB/s]
model.safetensors:  43%|████▎     | 524M/1.22G [00:01<00:01, 407MB/s]
model.safetensors:  47%|████▋     | 566M/1.22G [00:01<00:01, 408MB/s]
model.safetensors:  50%|████▉     | 608M/1.22G [00:01<00:01, 403MB/s]
model.safetensors:  54%|█████▍    | 661M/1.22G [00:01<00:01, 431MB/s]
model.safetensors:  59%|█████▊    | 713M/1.22G [00:01<00:01, 450MB/s]
model.safetensors:  63%|██████▎   | 765M/1.22G [00:01<00:00, 457MB/s]
model.safetensors:  67%|██████▋   | 818M/1.22G [00:01<00:00, 467MB/s]
model.safetensors:  71%|███████▏  | 870M/1.22G [00:02<00:00, 475MB/s]
model.safetensors:  76%|███████▌  | 923M/1.22G [00:02<00:00, 473MB/s]
model.safetensors:  80%|████████  | 975M/1.22G [00:02<00:00, 472MB/s]
model.safetensors:  84%|████████▍ | 1.03G/1.22G [00:02<00:00, 478MB/s]
model.safetensors:  89%|████████▊ | 1.08G/1.22G [00:02<00:00, 468MB/s]
model.safetensors:  93%|█████████▎| 1.13G/1.22G [00:02<00:00, 446MB/s]
model.safetensors:  97%|█████████▋| 1.18G/1.22G [00:02<00:00, 436MB/s]
model.safetensors: 100%|██████████| 1.22G/1.22G [00:02<00:00, 429MB/s]
[2024-03-31 16:52:34,030] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 972, in train
    raise ValueError()
ValueError
[2024-03-31 16:52:51,431] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2103223
[2024-03-31 16:52:51,431] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:34:58 PM UTC 2024 =========================================================

[2024-03-31 17:35:00,987] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:35:02,581] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:35:02,581] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:35:04,745] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:35:08,964] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:35:08,964] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:35:08,964] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:35:08,964] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:35:08,964] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:35:13,100] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:35:14,312] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:35:14,312] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:35:19,413] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.20s/it]
[2024-03-31 17:35:26,244] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:35:28,583] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1120, in train
    data_module = multi_ve_make_supervised_data_module(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 931, in multi_ve_make_supervised_data_module
    train_dataset = MultiVELazySupervisedDataset(tokenizer=tokenizer,
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 792, in __init__
    super(LazySupervisedDataset, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type
[2024-03-31 17:35:33,991] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2274518
[2024-03-31 17:35:33,991] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:38:45 PM UTC 2024 =========================================================

[2024-03-31 17:38:47,878] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:38:52,264] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:38:52,264] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:38:55,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:38:59,580] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:38:59,580] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:38:59,580] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:38:59,580] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:38:59,580] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:39:03,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:39:04,731] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:39:04,731] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:39:10,003] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[2024-03-31 17:39:16,604] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:39:18,954] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1124, in train
    raise ValueError()
ValueError
[2024-03-31 17:39:35,618] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2291058
[2024-03-31 17:39:35,618] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:40:56 PM UTC 2024 =========================================================

[2024-03-31 17:41:00,834] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:41:05,147] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:41:05,147] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:41:08,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:41:12,469] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:41:12,469] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:41:12,469] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:41:12,469] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:41:12,469] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:41:16,634] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:41:17,861] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:41:17,861] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:41:22,887] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]
[2024-03-31 17:41:29,420] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:41:31,856] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1134, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:42:12,535] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2299706
[2024-03-31 17:42:12,535] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:43:11 PM UTC 2024 =========================================================

[2024-03-31 17:43:16,157] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:43:20,418] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:43:20,419] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:43:23,399] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:43:27,904] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:43:27,904] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:43:27,904] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:43:27,904] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:43:27,908] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:43:32,086] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:43:33,110] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:43:33,110] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:43:38,377] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
[2024-03-31 17:43:44,680] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:43:47,023] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1134, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:44:27,972] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2311026
[2024-03-31 17:44:27,973] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:44:58 PM UTC 2024 =========================================================

[2024-03-31 17:45:00,416] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:45:04,373] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:45:04,374] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:45:07,331] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:45:11,780] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:45:11,780] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:45:11,780] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:45:11,780] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:45:11,780] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:45:15,846] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:45:16,857] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:45:16,857] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:45:22,264] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
[2024-03-31 17:45:28,384] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:45:30,708] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1134, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:46:10,850] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2316368
[2024-03-31 17:46:10,850] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:48:33 PM UTC 2024 =========================================================

[2024-03-31 17:48:35,216] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:48:39,168] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:48:39,168] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:48:42,061] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:48:46,482] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:48:46,482] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:48:46,482] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:48:46,482] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:48:46,482] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:48:50,646] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:48:51,738] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:48:51,739] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:48:56,876] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.96s/it]
[2024-03-31 17:49:03,863] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:49:06,200] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1134, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:49:49,549] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2338981
[2024-03-31 17:49:49,549] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:50:21 PM UTC 2024 =========================================================

[2024-03-31 17:50:25,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:50:29,940] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:50:29,943] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:50:32,832] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:50:37,379] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:50:37,379] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:50:37,379] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:50:37,379] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:50:37,379] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:50:41,064] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:50:42,140] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:50:42,140] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:50:47,099] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]
[2024-03-31 17:50:53,870] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:50:56,388] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1134, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:51:48,454] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2348844
[2024-03-31 17:51:48,454] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:55:04 PM UTC 2024 =========================================================

[2024-03-31 17:55:08,877] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:55:13,075] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:55:13,075] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:55:15,734] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:55:20,173] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:55:20,173] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:55:20,173] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:55:20,173] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:55:20,173] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:55:24,288] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:55:25,343] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:55:25,343] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:55:30,503] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
[2024-03-31 17:55:36,834] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:55:39,183] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1144, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:56:17,235] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2371681
[2024-03-31 17:56:17,235] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 09:57:02 PM UTC 2024 =========================================================

[2024-03-31 17:57:05,949] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:57:07,773] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 17:57:07,773] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 17:57:09,979] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:57:14,396] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 17:57:14,397] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 17:57:14,397] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 17:57:14,397] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 17:57:14,397] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 17:57:18,685] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 17:57:19,788] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 17:57:19,788] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 17:57:24,919] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]
[2024-03-31 17:57:31,401] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 17:57:33,755] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 599, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 72.86 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 17:58:11,457] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2380023
[2024-03-31 17:58:11,457] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:05:07 PM UTC 2024 =========================================================

[2024-03-31 18:05:09,346] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:05:13,473] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2024-03-31 18:05:13,474] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:05:16,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:05:20,703] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2024-03-31 18:05:20,703] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 18:05:20,703] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 18:05:20,703] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 18:05:20,703] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2024-03-31 18:05:24,835] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:05:25,858] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:05:25,858] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:05:31,182] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]
[2024-03-31 18:05:37,869] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 599040 in 312 params
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1701, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1198, in prepare
    result = self._prepare_deepspeed(*args)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py", line 1537, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 304, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1234, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1563, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 362, in __init__
    self._setup_for_real_optimizer()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 474, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 1008, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 934, in _optimizer_step
    self.optimizer.step()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 173, in step
    self._init_group(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py", line 125, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 0 has a total capacty of 79.15 GiB of which 672.19 MiB is free. Including non-PyTorch memory, this process has 78.48 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-31 18:06:20,767] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2411269
[2024-03-31 18:06:20,770] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:09:01 PM UTC 2024 =========================================================

[2024-03-31 18:09:05,240] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:09:09,601] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:09:09,601] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:09:12,516] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:09:14,988] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:09:14,988] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:09:14,988] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:09:14,988] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:09:14,988] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:09:18,066] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:09:18,231] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:09:19,082] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:09:19,284] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:09:19,284] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:09:37,892] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.84s/it]
[2024-03-31 18:09:50,958] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 599040 in 312 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_181036-3ncgafk4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-cherry-1
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/3ncgafk4

  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
UnboundLocalError: Caught UnboundLocalError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 871, in __getitem__
    for processor in multiple_image_processors:
UnboundLocalError: local variable 'multiple_image_processors' referenced before assignment

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
UnboundLocalError: Caught UnboundLocalError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 871, in __getitem__
    for processor in multiple_image_processors:
UnboundLocalError: local variable 'multiple_image_processors' referenced before assignment

[2024-03-31 18:10:52,091] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2427763
[2024-03-31 18:10:52,709] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2427764
[2024-03-31 18:10:52,709] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1
[2024-03-31 18:11:17,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:11:21,374] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:11:21,374] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --max_steps 5 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-03-31 18:11:24,400] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:11:26,844] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:11:26,844] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:11:26,844] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:11:26,844] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:11:26,844] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:11:29,920] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:11:29,958] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:11:31,001] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:11:31,133] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:11:31,133] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl

config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]
config.json: 100%|██████████| 638/638 [00:00<00:00, 2.84MB/s]
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.

pytorch_model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]
pytorch_model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 10.2MB/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]

pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s][A

pytorch_model-00001-of-00003.bin:   0%|          | 31.5M/9.95G [00:00<00:35, 280MB/s][A

pytorch_model-00001-of-00003.bin:   1%|          | 83.9M/9.95G [00:00<00:26, 374MB/s][A

pytorch_model-00001-of-00003.bin:   1%|▏         | 136M/9.95G [00:00<00:24, 403MB/s] [A

pytorch_model-00001-of-00003.bin:   2%|▏         | 178M/9.95G [00:00<00:24, 404MB/s][A

pytorch_model-00001-of-00003.bin:   2%|▏         | 231M/9.95G [00:00<00:22, 427MB/s][A

pytorch_model-00001-of-00003.bin:   3%|▎         | 283M/9.95G [00:00<00:22, 431MB/s][A

pytorch_model-00001-of-00003.bin:   3%|▎         | 336M/9.95G [00:00<00:21, 448MB/s][A

pytorch_model-00001-of-00003.bin:   4%|▍         | 388M/9.95G [00:00<00:20, 458MB/s][A

pytorch_model-00001-of-00003.bin:   4%|▍         | 440M/9.95G [00:01<00:20, 465MB/s][A

pytorch_model-00001-of-00003.bin:   5%|▍         | 493M/9.95G [00:01<00:20, 470MB/s][A

pytorch_model-00001-of-00003.bin:   5%|▌         | 545M/9.95G [00:01<00:20, 465MB/s][A

pytorch_model-00001-of-00003.bin:   6%|▌         | 598M/9.95G [00:01<00:20, 455MB/s][A

pytorch_model-00001-of-00003.bin:   7%|▋         | 650M/9.95G [00:01<00:20, 457MB/s][A

pytorch_model-00001-of-00003.bin:   7%|▋         | 703M/9.95G [00:01<00:20, 462MB/s][A

pytorch_model-00001-of-00003.bin:   8%|▊         | 755M/9.95G [00:01<00:19, 468MB/s][A

pytorch_model-00001-of-00003.bin:   8%|▊         | 807M/9.95G [00:01<00:19, 476MB/s][A

pytorch_model-00001-of-00003.bin:   9%|▊         | 860M/9.95G [00:01<00:18, 479MB/s][A

pytorch_model-00001-of-00003.bin:   9%|▉         | 912M/9.95G [00:02<00:19, 461MB/s][A

pytorch_model-00001-of-00003.bin:  10%|▉         | 965M/9.95G [00:02<00:20, 445MB/s][A

pytorch_model-00001-of-00003.bin:  10%|█         | 1.02G/9.95G [00:02<00:19, 453MB/s][A

pytorch_model-00001-of-00003.bin:  11%|█         | 1.07G/9.95G [00:02<00:19, 455MB/s][A

pytorch_model-00001-of-00003.bin:  11%|█▏        | 1.12G/9.95G [00:02<00:19, 449MB/s][A

pytorch_model-00001-of-00003.bin:  12%|█▏        | 1.17G/9.95G [00:02<00:19, 453MB/s][A

pytorch_model-00001-of-00003.bin:  12%|█▏        | 1.23G/9.95G [00:02<00:20, 429MB/s][A

pytorch_model-00001-of-00003.bin:  13%|█▎        | 1.28G/9.95G [00:02<00:19, 444MB/s][A

pytorch_model-00001-of-00003.bin:  13%|█▎        | 1.33G/9.95G [00:02<00:19, 436MB/s][A

pytorch_model-00001-of-00003.bin:  14%|█▍        | 1.38G/9.95G [00:03<00:19, 447MB/s][A

pytorch_model-00001-of-00003.bin:  14%|█▍        | 1.44G/9.95G [00:03<00:18, 450MB/s][A

pytorch_model-00001-of-00003.bin:  15%|█▍        | 1.49G/9.95G [00:03<00:19, 444MB/s][A

pytorch_model-00001-of-00003.bin:  15%|█▌        | 1.54G/9.95G [00:03<00:18, 459MB/s][A

pytorch_model-00001-of-00003.bin:  16%|█▌        | 1.59G/9.95G [00:03<00:18, 455MB/s][A

pytorch_model-00001-of-00003.bin:  17%|█▋        | 1.65G/9.95G [00:03<00:17, 468MB/s][A

pytorch_model-00001-of-00003.bin:  17%|█▋        | 1.70G/9.95G [00:03<00:18, 448MB/s][A

pytorch_model-00001-of-00003.bin:  18%|█▊        | 1.75G/9.95G [00:03<00:18, 451MB/s][A

pytorch_model-00001-of-00003.bin:  18%|█▊        | 1.80G/9.95G [00:04<00:18, 433MB/s][A

pytorch_model-00001-of-00003.bin:  19%|█▊        | 1.86G/9.95G [00:04<00:18, 437MB/s][A

pytorch_model-00001-of-00003.bin:  19%|█▉        | 1.91G/9.95G [00:04<00:18, 440MB/s][A

pytorch_model-00001-of-00003.bin:  20%|█▉        | 1.96G/9.95G [00:04<00:18, 438MB/s][A

pytorch_model-00001-of-00003.bin:  20%|██        | 2.01G/9.95G [00:04<00:18, 434MB/s][A

pytorch_model-00001-of-00003.bin:  21%|██        | 2.07G/9.95G [00:04<00:18, 417MB/s][A

pytorch_model-00001-of-00003.bin:  21%|██▏       | 2.12G/9.95G [00:04<00:18, 431MB/s][A

pytorch_model-00001-of-00003.bin:  22%|██▏       | 2.17G/9.95G [00:04<00:18, 417MB/s][A

pytorch_model-00001-of-00003.bin:  22%|██▏       | 2.22G/9.95G [00:05<00:17, 431MB/s][A

pytorch_model-00001-of-00003.bin:  23%|██▎       | 2.28G/9.95G [00:05<00:17, 438MB/s][A

pytorch_model-00001-of-00003.bin:  23%|██▎       | 2.33G/9.95G [00:05<00:17, 433MB/s][A

pytorch_model-00001-of-00003.bin:  24%|██▍       | 2.38G/9.95G [00:05<00:17, 439MB/s][A

pytorch_model-00001-of-00003.bin:  24%|██▍       | 2.43G/9.95G [00:05<00:16, 447MB/s][A

pytorch_model-00001-of-00003.bin:  25%|██▍       | 2.49G/9.95G [00:05<00:17, 438MB/s][A

pytorch_model-00001-of-00003.bin:  26%|██▌       | 2.54G/9.95G [00:05<00:17, 431MB/s][A

pytorch_model-00001-of-00003.bin:  26%|██▌       | 2.59G/9.95G [00:05<00:16, 439MB/s][A

pytorch_model-00001-of-00003.bin:  27%|██▋       | 2.64G/9.95G [00:05<00:16, 441MB/s][A

pytorch_model-00001-of-00003.bin:  27%|██▋       | 2.69G/9.95G [00:06<00:17, 407MB/s][A

pytorch_model-00001-of-00003.bin:  28%|██▊       | 2.75G/9.95G [00:06<00:17, 420MB/s][A

pytorch_model-00001-of-00003.bin:  28%|██▊       | 2.80G/9.95G [00:06<00:16, 433MB/s][A

pytorch_model-00001-of-00003.bin:  29%|██▊       | 2.85G/9.95G [00:06<00:15, 444MB/s][A

pytorch_model-00001-of-00003.bin:  29%|██▉       | 2.90G/9.95G [00:06<00:15, 442MB/s][A

pytorch_model-00001-of-00003.bin:  30%|██▉       | 2.96G/9.95G [00:06<00:15, 449MB/s][A

pytorch_model-00001-of-00003.bin:  30%|███       | 3.01G/9.95G [00:06<00:15, 451MB/s][A

pytorch_model-00001-of-00003.bin:  31%|███       | 3.06G/9.95G [00:06<00:15, 450MB/s][A

pytorch_model-00001-of-00003.bin:  31%|███▏      | 3.11G/9.95G [00:07<00:15, 441MB/s][A

pytorch_model-00001-of-00003.bin:  32%|███▏      | 3.17G/9.95G [00:07<00:15, 450MB/s][A

pytorch_model-00001-of-00003.bin:  32%|███▏      | 3.22G/9.95G [00:07<00:14, 462MB/s][A

pytorch_model-00001-of-00003.bin:  33%|███▎      | 3.27G/9.95G [00:07<00:14, 454MB/s][A

pytorch_model-00001-of-00003.bin:  33%|███▎      | 3.32G/9.95G [00:07<00:14, 442MB/s][A

pytorch_model-00001-of-00003.bin:  34%|███▍      | 3.38G/9.95G [00:07<00:15, 436MB/s][A

pytorch_model-00001-of-00003.bin:  34%|███▍      | 3.43G/9.95G [00:07<00:14, 442MB/s][A

pytorch_model-00001-of-00003.bin:  35%|███▍      | 3.48G/9.95G [00:07<00:14, 439MB/s][A

pytorch_model-00001-of-00003.bin:  36%|███▌      | 3.53G/9.95G [00:07<00:14, 437MB/s][A

pytorch_model-00001-of-00003.bin:  36%|███▌      | 3.59G/9.95G [00:08<00:14, 425MB/s][A

pytorch_model-00001-of-00003.bin:  37%|███▋      | 3.64G/9.95G [00:08<00:14, 428MB/s][A

pytorch_model-00001-of-00003.bin:  37%|███▋      | 3.69G/9.95G [00:08<00:14, 427MB/s][A

pytorch_model-00001-of-00003.bin:  38%|███▊      | 3.74G/9.95G [00:08<00:15, 410MB/s][A

pytorch_model-00001-of-00003.bin:  38%|███▊      | 3.79G/9.95G [00:08<00:15, 409MB/s][A

pytorch_model-00001-of-00003.bin:  38%|███▊      | 3.83G/9.95G [00:08<00:15, 400MB/s][A

pytorch_model-00001-of-00003.bin:  39%|███▉      | 3.87G/9.95G [00:08<00:15, 402MB/s][A

pytorch_model-00001-of-00003.bin:  39%|███▉      | 3.91G/9.95G [00:08<00:15, 400MB/s][A

pytorch_model-00001-of-00003.bin:  40%|███▉      | 3.95G/9.95G [00:09<00:14, 401MB/s][A

pytorch_model-00001-of-00003.bin:  40%|████      | 4.01G/9.95G [00:09<00:14, 423MB/s][A

pytorch_model-00001-of-00003.bin:  41%|████      | 4.06G/9.95G [00:09<00:13, 434MB/s][A

pytorch_model-00001-of-00003.bin:  41%|████▏     | 4.11G/9.95G [00:09<00:13, 431MB/s][A

pytorch_model-00001-of-00003.bin:  42%|████▏     | 4.16G/9.95G [00:09<00:13, 440MB/s][A

pytorch_model-00001-of-00003.bin:  42%|████▏     | 4.22G/9.95G [00:09<00:13, 436MB/s][A

pytorch_model-00001-of-00003.bin:  43%|████▎     | 4.27G/9.95G [00:09<00:12, 439MB/s][A

pytorch_model-00001-of-00003.bin:  43%|████▎     | 4.32G/9.95G [00:09<00:12, 442MB/s][A

pytorch_model-00001-of-00003.bin:  44%|████▍     | 4.37G/9.95G [00:09<00:12, 443MB/s][A

pytorch_model-00001-of-00003.bin:  44%|████▍     | 4.42G/9.95G [00:10<00:12, 454MB/s][A

pytorch_model-00001-of-00003.bin:  45%|████▌     | 4.48G/9.95G [00:10<00:12, 443MB/s][A

pytorch_model-00001-of-00003.bin:  46%|████▌     | 4.53G/9.95G [00:10<00:12, 436MB/s][A

pytorch_model-00001-of-00003.bin:  46%|████▌     | 4.58G/9.95G [00:10<00:12, 427MB/s][A

pytorch_model-00001-of-00003.bin:  47%|████▋     | 4.63G/9.95G [00:10<00:12, 434MB/s][A

pytorch_model-00001-of-00003.bin:  47%|████▋     | 4.69G/9.95G [00:10<00:11, 444MB/s][A

pytorch_model-00001-of-00003.bin:  48%|████▊     | 4.74G/9.95G [00:10<00:11, 457MB/s][A

pytorch_model-00001-of-00003.bin:  48%|████▊     | 4.79G/9.95G [00:10<00:11, 457MB/s][A

pytorch_model-00001-of-00003.bin:  49%|████▊     | 4.84G/9.95G [00:11<00:11, 440MB/s][A

pytorch_model-00001-of-00003.bin:  49%|████▉     | 4.90G/9.95G [00:11<00:11, 433MB/s][A

pytorch_model-00001-of-00003.bin:  50%|████▉     | 4.95G/9.95G [00:11<00:11, 420MB/s][A

pytorch_model-00001-of-00003.bin:  50%|█████     | 5.00G/9.95G [00:11<00:11, 428MB/s][A

pytorch_model-00001-of-00003.bin:  51%|█████     | 5.05G/9.95G [00:11<00:11, 432MB/s][A

pytorch_model-00001-of-00003.bin:  51%|█████▏    | 5.11G/9.95G [00:11<00:11, 440MB/s][A

pytorch_model-00001-of-00003.bin:  52%|█████▏    | 5.16G/9.95G [00:11<00:10, 441MB/s][A

pytorch_model-00001-of-00003.bin:  52%|█████▏    | 5.21G/9.95G [00:12<00:16, 287MB/s][A

pytorch_model-00001-of-00003.bin:  53%|█████▎    | 5.25G/9.95G [00:12<00:15, 301MB/s][A

pytorch_model-00001-of-00003.bin:  53%|█████▎    | 5.31G/9.95G [00:12<00:13, 333MB/s][A

pytorch_model-00001-of-00003.bin:  54%|█████▍    | 5.36G/9.95G [00:12<00:12, 358MB/s][A

pytorch_model-00001-of-00003.bin:  54%|█████▍    | 5.40G/9.95G [00:12<00:12, 367MB/s][A

pytorch_model-00001-of-00003.bin:  55%|█████▍    | 5.45G/9.95G [00:12<00:11, 387MB/s][A

pytorch_model-00001-of-00003.bin:  55%|█████▌    | 5.51G/9.95G [00:12<00:11, 396MB/s][A

pytorch_model-00001-of-00003.bin:  56%|█████▌    | 5.55G/9.95G [00:12<00:12, 354MB/s][A

pytorch_model-00001-of-00003.bin:  56%|█████▋    | 5.60G/9.95G [00:13<00:11, 379MB/s][A

pytorch_model-00001-of-00003.bin:  57%|█████▋    | 5.64G/9.95G [00:13<00:11, 385MB/s][A

pytorch_model-00001-of-00003.bin:  57%|█████▋    | 5.69G/9.95G [00:13<00:10, 409MB/s][A

pytorch_model-00001-of-00003.bin:  58%|█████▊    | 5.75G/9.95G [00:13<00:09, 434MB/s][A

pytorch_model-00001-of-00003.bin:  58%|█████▊    | 5.80G/9.95G [00:13<00:09, 448MB/s][A

pytorch_model-00001-of-00003.bin:  59%|█████▉    | 5.85G/9.95G [00:13<00:09, 442MB/s][A

pytorch_model-00001-of-00003.bin:  59%|█████▉    | 5.90G/9.95G [00:13<00:09, 416MB/s][A

pytorch_model-00001-of-00003.bin:  60%|█████▉    | 5.96G/9.95G [00:13<00:09, 423MB/s][A

pytorch_model-00001-of-00003.bin:  60%|██████    | 6.01G/9.95G [00:14<00:09, 406MB/s][A

pytorch_model-00001-of-00003.bin:  61%|██████    | 6.05G/9.95G [00:14<00:09, 399MB/s][A

pytorch_model-00001-of-00003.bin:  61%|██████    | 6.09G/9.95G [00:14<00:09, 400MB/s][A

pytorch_model-00001-of-00003.bin:  62%|██████▏   | 6.14G/9.95G [00:14<00:09, 416MB/s][A

pytorch_model-00001-of-00003.bin:  62%|██████▏   | 6.20G/9.95G [00:14<00:08, 428MB/s][A

pytorch_model-00001-of-00003.bin:  63%|██████▎   | 6.25G/9.95G [00:14<00:08, 430MB/s][A

pytorch_model-00001-of-00003.bin:  63%|██████▎   | 6.30G/9.95G [00:14<00:08, 436MB/s][A

pytorch_model-00001-of-00003.bin:  64%|██████▍   | 6.35G/9.95G [00:14<00:08, 442MB/s][A

pytorch_model-00001-of-00003.bin:  64%|██████▍   | 6.41G/9.95G [00:14<00:07, 452MB/s][A

pytorch_model-00001-of-00003.bin:  65%|██████▍   | 6.46G/9.95G [00:15<00:08, 419MB/s][A

pytorch_model-00001-of-00003.bin:  65%|██████▌   | 6.51G/9.95G [00:15<00:08, 428MB/s][A

pytorch_model-00001-of-00003.bin:  66%|██████▌   | 6.56G/9.95G [00:15<00:07, 442MB/s][A

pytorch_model-00001-of-00003.bin:  67%|██████▋   | 6.62G/9.95G [00:15<00:08, 414MB/s][A

pytorch_model-00001-of-00003.bin:  67%|██████▋   | 6.67G/9.95G [00:15<00:07, 414MB/s][A

pytorch_model-00001-of-00003.bin:  67%|██████▋   | 6.71G/9.95G [00:15<00:11, 279MB/s][A

pytorch_model-00001-of-00003.bin:  68%|██████▊   | 6.75G/9.95G [00:15<00:10, 303MB/s][A

pytorch_model-00001-of-00003.bin:  68%|██████▊   | 6.81G/9.95G [00:16<00:09, 340MB/s][A

pytorch_model-00001-of-00003.bin:  69%|██████▉   | 6.86G/9.95G [00:16<00:08, 369MB/s][A

pytorch_model-00001-of-00003.bin:  69%|██████▉   | 6.91G/9.95G [00:16<00:07, 393MB/s][A

pytorch_model-00001-of-00003.bin:  70%|██████▉   | 6.96G/9.95G [00:16<00:07, 416MB/s][A

pytorch_model-00001-of-00003.bin:  71%|███████   | 7.01G/9.95G [00:16<00:06, 425MB/s][A

pytorch_model-00001-of-00003.bin:  71%|███████   | 7.07G/9.95G [00:16<00:06, 429MB/s][A

pytorch_model-00001-of-00003.bin:  72%|███████▏  | 7.12G/9.95G [00:16<00:06, 421MB/s][A

pytorch_model-00001-of-00003.bin:  72%|███████▏  | 7.17G/9.95G [00:16<00:06, 421MB/s][A

pytorch_model-00001-of-00003.bin:  73%|███████▎  | 7.22G/9.95G [00:17<00:06, 421MB/s][A

pytorch_model-00001-of-00003.bin:  73%|███████▎  | 7.28G/9.95G [00:17<00:06, 430MB/s][A

pytorch_model-00001-of-00003.bin:  74%|███████▎  | 7.33G/9.95G [00:17<00:05, 437MB/s][A

pytorch_model-00001-of-00003.bin:  74%|███████▍  | 7.38G/9.95G [00:17<00:05, 434MB/s][A

pytorch_model-00001-of-00003.bin:  75%|███████▍  | 7.43G/9.95G [00:17<00:05, 447MB/s][A

pytorch_model-00001-of-00003.bin:  75%|███████▌  | 7.49G/9.95G [00:17<00:05, 410MB/s][A

pytorch_model-00001-of-00003.bin:  76%|███████▌  | 7.53G/9.95G [00:17<00:06, 390MB/s][A

pytorch_model-00001-of-00003.bin:  76%|███████▌  | 7.58G/9.95G [00:17<00:05, 396MB/s][A

pytorch_model-00001-of-00003.bin:  77%|███████▋  | 7.62G/9.95G [00:18<00:05, 388MB/s][A

pytorch_model-00001-of-00003.bin:  77%|███████▋  | 7.68G/9.95G [00:18<00:05, 403MB/s][A

pytorch_model-00001-of-00003.bin:  78%|███████▊  | 7.72G/9.95G [00:18<00:05, 392MB/s][A

pytorch_model-00001-of-00003.bin:  78%|███████▊  | 7.77G/9.95G [00:18<00:05, 413MB/s][A

pytorch_model-00001-of-00003.bin:  79%|███████▊  | 7.82G/9.95G [00:18<00:05, 420MB/s][A

pytorch_model-00001-of-00003.bin:  79%|███████▉  | 7.87G/9.95G [00:18<00:04, 435MB/s][A

pytorch_model-00001-of-00003.bin:  80%|███████▉  | 7.93G/9.95G [00:18<00:04, 447MB/s][A

pytorch_model-00001-of-00003.bin:  80%|████████  | 7.98G/9.95G [00:18<00:04, 429MB/s][A

pytorch_model-00001-of-00003.bin:  81%|████████  | 8.03G/9.95G [00:18<00:04, 429MB/s][A

pytorch_model-00001-of-00003.bin:  81%|████████▏ | 8.08G/9.95G [00:19<00:04, 427MB/s][A

pytorch_model-00001-of-00003.bin:  82%|████████▏ | 8.14G/9.95G [00:19<00:04, 426MB/s][A

pytorch_model-00001-of-00003.bin:  82%|████████▏ | 8.19G/9.95G [00:19<00:04, 426MB/s][A

pytorch_model-00001-of-00003.bin:  83%|████████▎ | 8.24G/9.95G [00:19<00:04, 425MB/s][A

pytorch_model-00001-of-00003.bin:  83%|████████▎ | 8.29G/9.95G [00:19<00:03, 426MB/s][A

pytorch_model-00001-of-00003.bin:  84%|████████▍ | 8.35G/9.95G [00:19<00:03, 406MB/s][A

pytorch_model-00001-of-00003.bin:  84%|████████▍ | 8.40G/9.95G [00:19<00:03, 421MB/s][A

pytorch_model-00001-of-00003.bin:  85%|████████▍ | 8.45G/9.95G [00:19<00:03, 432MB/s][A

pytorch_model-00001-of-00003.bin:  85%|████████▌ | 8.50G/9.95G [00:20<00:03, 442MB/s][A

pytorch_model-00001-of-00003.bin:  86%|████████▌ | 8.56G/9.95G [00:20<00:03, 446MB/s][A

pytorch_model-00001-of-00003.bin:  87%|████████▋ | 8.61G/9.95G [00:20<00:02, 454MB/s][A

pytorch_model-00001-of-00003.bin:  87%|████████▋ | 8.66G/9.95G [00:20<00:02, 460MB/s][A

pytorch_model-00001-of-00003.bin:  88%|████████▊ | 8.71G/9.95G [00:20<00:02, 453MB/s][A

pytorch_model-00001-of-00003.bin:  88%|████████▊ | 8.77G/9.95G [00:20<00:02, 452MB/s][A

pytorch_model-00001-of-00003.bin:  89%|████████▊ | 8.82G/9.95G [00:20<00:02, 443MB/s][A

pytorch_model-00001-of-00003.bin:  89%|████████▉ | 8.87G/9.95G [00:20<00:02, 444MB/s][A

pytorch_model-00001-of-00003.bin:  90%|████████▉ | 8.92G/9.95G [00:21<00:02, 453MB/s][A

pytorch_model-00001-of-00003.bin:  90%|█████████ | 8.98G/9.95G [00:21<00:02, 469MB/s][A

pytorch_model-00001-of-00003.bin:  91%|█████████ | 9.03G/9.95G [00:21<00:01, 463MB/s][A

pytorch_model-00001-of-00003.bin:  91%|█████████▏| 9.08G/9.95G [00:21<00:01, 462MB/s][A

pytorch_model-00001-of-00003.bin:  92%|█████████▏| 9.13G/9.95G [00:21<00:01, 459MB/s][A

pytorch_model-00001-of-00003.bin:  92%|█████████▏| 9.19G/9.95G [00:21<00:01, 429MB/s][A

pytorch_model-00001-of-00003.bin:  93%|█████████▎| 9.24G/9.95G [00:21<00:01, 434MB/s][A

pytorch_model-00001-of-00003.bin:  93%|█████████▎| 9.29G/9.95G [00:21<00:01, 435MB/s][A

pytorch_model-00001-of-00003.bin:  94%|█████████▍| 9.34G/9.95G [00:21<00:01, 435MB/s][A

pytorch_model-00001-of-00003.bin:  94%|█████████▍| 9.40G/9.95G [00:22<00:02, 199MB/s][A

pytorch_model-00001-of-00003.bin:  95%|█████████▍| 9.44G/9.95G [00:22<00:02, 225MB/s][A

pytorch_model-00001-of-00003.bin:  95%|█████████▌| 9.49G/9.95G [00:22<00:01, 261MB/s][A

pytorch_model-00001-of-00003.bin:  96%|█████████▌| 9.54G/9.95G [00:22<00:01, 296MB/s][A

pytorch_model-00001-of-00003.bin:  96%|█████████▋| 9.58G/9.95G [00:23<00:01, 312MB/s][A

pytorch_model-00001-of-00003.bin:  97%|█████████▋| 9.63G/9.95G [00:23<00:01, 321MB/s][A

pytorch_model-00001-of-00003.bin:  97%|█████████▋| 9.67G/9.95G [00:23<00:00, 334MB/s][A

pytorch_model-00001-of-00003.bin:  98%|█████████▊| 9.71G/9.95G [00:23<00:00, 354MB/s][A

pytorch_model-00001-of-00003.bin:  98%|█████████▊| 9.76G/9.95G [00:23<00:00, 384MB/s][A

pytorch_model-00001-of-00003.bin:  99%|█████████▊| 9.81G/9.95G [00:23<00:00, 405MB/s][A

pytorch_model-00001-of-00003.bin:  99%|█████████▉| 9.87G/9.95G [00:23<00:00, 419MB/s][A

pytorch_model-00001-of-00003.bin: 100%|█████████▉| 9.92G/9.95G [00:23<00:00, 436MB/s][A
pytorch_model-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [00:23<00:00, 417MB/s]

Downloading shards:  33%|███▎      | 1/3 [00:24<00:48, 24.15s/it]
Downloading shards:  33%|███▎      | 1/3 [00:24<00:48, 24.19s/it]

pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 10.5M/9.90G [00:00<02:30, 65.7MB/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 21.0M/9.90G [00:00<02:43, 60.5MB/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 41.9M/9.90G [00:00<02:25, 67.8MB/s][A

pytorch_model-00002-of-00003.bin:   1%|          | 52.4M/9.90G [00:00<02:41, 61.1MB/s][A

pytorch_model-00002-of-00003.bin:   1%|          | 73.4M/9.90G [00:01<02:20, 69.9MB/s][A

pytorch_model-00002-of-00003.bin:   1%|          | 94.4M/9.90G [00:01<01:49, 89.6MB/s][A

pytorch_model-00002-of-00003.bin:   1%|          | 105M/9.90G [00:01<01:51, 87.7MB/s] [A

pytorch_model-00002-of-00003.bin:   1%|          | 115M/9.90G [00:01<02:02, 79.7MB/s][A

pytorch_model-00002-of-00003.bin:   1%|▏         | 136M/9.90G [00:01<02:10, 75.1MB/s][A

pytorch_model-00002-of-00003.bin:   1%|▏         | 147M/9.90G [00:01<02:03, 78.8MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 168M/9.90G [00:02<02:21, 68.8MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 178M/9.90G [00:02<02:41, 60.1MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 199M/9.90G [00:02<02:22, 68.0MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 210M/9.90G [00:03<03:14, 49.9MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 231M/9.90G [00:03<02:22, 67.7MB/s][A

pytorch_model-00002-of-00003.bin:   2%|▏         | 241M/9.90G [00:03<02:51, 56.5MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 262M/9.90G [00:03<02:37, 61.1MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 273M/9.90G [00:04<03:41, 43.4MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 294M/9.90G [00:04<02:45, 58.0MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 304M/9.90G [00:04<02:36, 61.5MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 325M/9.90G [00:05<02:30, 63.6MB/s][A

pytorch_model-00002-of-00003.bin:   3%|▎         | 346M/9.90G [00:05<02:06, 75.4MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▎         | 357M/9.90G [00:05<02:24, 66.1MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▎         | 367M/9.90G [00:05<02:15, 70.4MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▍         | 377M/9.90G [00:05<02:16, 69.6MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▍         | 388M/9.90G [00:05<02:16, 69.7MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▍         | 409M/9.90G [00:06<01:52, 84.2MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▍         | 419M/9.90G [00:06<02:00, 79.0MB/s][A

pytorch_model-00002-of-00003.bin:   4%|▍         | 440M/9.90G [00:06<01:50, 85.5MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▍         | 451M/9.90G [00:06<02:05, 75.1MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▍         | 472M/9.90G [00:06<01:49, 86.4MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▍         | 482M/9.90G [00:06<01:55, 81.8MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▍         | 493M/9.90G [00:07<02:07, 73.6MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▌         | 503M/9.90G [00:07<02:45, 56.7MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▌         | 514M/9.90G [00:07<02:49, 55.5MB/s][A

pytorch_model-00002-of-00003.bin:   5%|▌         | 535M/9.90G [00:08<02:41, 58.1MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▌         | 545M/9.90G [00:08<02:42, 57.7MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▌         | 566M/9.90G [00:08<02:06, 73.8MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▌         | 577M/9.90G [00:08<02:27, 63.2MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▌         | 598M/9.90G [00:08<02:02, 76.0MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▌         | 608M/9.90G [00:09<02:34, 60.2MB/s][A

pytorch_model-00002-of-00003.bin:   6%|▋         | 629M/9.90G [00:09<01:58, 78.4MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 650M/9.90G [00:09<01:59, 77.2MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 661M/9.90G [00:09<02:13, 69.2MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 682M/9.90G [00:09<01:51, 82.4MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 692M/9.90G [00:10<02:00, 76.6MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 713M/9.90G [00:10<02:03, 74.3MB/s][A

pytorch_model-00002-of-00003.bin:   7%|▋         | 724M/9.90G [00:10<02:26, 62.5MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 744M/9.90G [00:10<02:14, 68.2MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 755M/9.90G [00:11<02:35, 59.0MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 776M/9.90G [00:11<02:22, 64.2MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 786M/9.90G [00:11<02:21, 64.6MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 807M/9.90G [00:11<02:08, 71.0MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 818M/9.90G [00:12<02:28, 61.4MB/s][A

pytorch_model-00002-of-00003.bin:   8%|▊         | 839M/9.90G [00:12<02:10, 69.7MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▊         | 849M/9.90G [00:12<02:17, 66.0MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▉         | 870M/9.90G [00:12<02:03, 72.9MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▉         | 881M/9.90G [00:13<02:23, 62.8MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▉         | 902M/9.90G [00:13<02:25, 61.9MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▉         | 912M/9.90G [00:13<02:39, 56.5MB/s][A

pytorch_model-00002-of-00003.bin:   9%|▉         | 933M/9.90G [00:13<02:27, 61.0MB/s][A

pytorch_model-00002-of-00003.bin:  10%|▉         | 954M/9.90G [00:14<01:57, 76.4MB/s][A

pytorch_model-00002-of-00003.bin:  10%|▉         | 965M/9.90G [00:14<02:03, 72.4MB/s][A

pytorch_model-00002-of-00003.bin:  10%|▉         | 986M/9.90G [00:14<01:48, 82.1MB/s][A

pytorch_model-00002-of-00003.bin:  10%|█         | 996M/9.90G [00:14<02:05, 70.7MB/s][A

pytorch_model-00002-of-00003.bin:  10%|█         | 1.01G/9.90G [00:14<01:57, 76.0MB/s][A

pytorch_model-00002-of-00003.bin:  10%|█         | 1.02G/9.90G [00:14<02:04, 71.5MB/s][A

pytorch_model-00002-of-00003.bin:  10%|█         | 1.03G/9.90G [00:15<02:10, 68.2MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█         | 1.05G/9.90G [00:15<02:20, 62.8MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█         | 1.06G/9.90G [00:15<02:13, 66.5MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█         | 1.08G/9.90G [00:15<02:12, 66.6MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█         | 1.09G/9.90G [00:16<02:02, 72.2MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█         | 1.11G/9.90G [00:16<01:43, 84.9MB/s][A

pytorch_model-00002-of-00003.bin:  11%|█▏        | 1.12G/9.90G [00:16<01:45, 83.2MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.14G/9.90G [00:16<01:28, 99.0MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.15G/9.90G [00:16<01:43, 84.6MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.17G/9.90G [00:16<01:44, 83.7MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.18G/9.90G [00:17<02:16, 64.0MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.21G/9.90G [00:17<01:57, 74.0MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.22G/9.90G [00:17<02:14, 64.4MB/s][A

pytorch_model-00002-of-00003.bin:  12%|█▏        | 1.24G/9.90G [00:17<02:01, 71.2MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.25G/9.90G [00:18<01:53, 76.2MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.26G/9.90G [00:18<02:10, 66.1MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.27G/9.90G [00:18<02:11, 65.8MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.29G/9.90G [00:18<02:01, 71.2MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.30G/9.90G [00:18<02:17, 62.6MB/s][A

pytorch_model-00002-of-00003.bin:  13%|█▎        | 1.32G/9.90G [00:19<02:14, 63.7MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▎        | 1.34G/9.90G [00:19<01:48, 78.9MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▎        | 1.35G/9.90G [00:19<01:47, 79.6MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▍        | 1.36G/9.90G [00:19<01:55, 73.8MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▍        | 1.38G/9.90G [00:19<01:38, 86.5MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▍        | 1.39G/9.90G [00:20<01:50, 77.3MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▍        | 1.42G/9.90G [00:20<01:54, 74.0MB/s][A

pytorch_model-00002-of-00003.bin:  14%|█▍        | 1.43G/9.90G [00:20<02:08, 66.0MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▍        | 1.45G/9.90G [00:20<01:54, 73.6MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▍        | 1.46G/9.90G [00:21<02:04, 68.0MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▍        | 1.48G/9.90G [00:21<02:06, 66.8MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▌        | 1.49G/9.90G [00:21<02:13, 63.2MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▌        | 1.51G/9.90G [00:21<01:55, 72.5MB/s][A

pytorch_model-00002-of-00003.bin:  15%|█▌        | 1.52G/9.90G [00:22<02:16, 61.4MB/s][A

pytorch_model-00002-of-00003.bin:  16%|█▌        | 1.54G/9.90G [00:22<02:02, 68.5MB/s][A

pytorch_model-00002-of-00003.bin:  16%|█▌        | 1.56G/9.90G [00:22<01:54, 73.0MB/s][A

pytorch_model-00002-of-00003.bin:  16%|█▌        | 1.57G/9.90G [00:22<01:51, 74.5MB/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 10.5M/9.90G [00:00<02:25, 68.0MB/s][A

pytorch_model-00002-of-00003.bin:   0%|          | 31.5M/9.90G [00:00<01:13, 135MB/s] [A

pytorch_model-00002-of-00003.bin:   1%|          | 52.4M/9.90G [00:00<01:32, 107MB/s][A

pytorch_model-00002-of-00003.bin:   1%|          | 73.4M/9.90G [00:00<01:31, 107MB/s][A[2024-03-31 18:12:20,903] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2437303
[2024-03-31 18:12:21,995] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2437304
[2024-03-31 18:12:21,996] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--max_steps', '5', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -15
[2024-03-31 18:14:14,494] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:14:18,865] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:14:18,865] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:14:21,786] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:14:24,224] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:14:24,224] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:14:24,224] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:14:24,224] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:14:24,224] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:14:27,455] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:14:28,473] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:14:30,111] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:14:31,139] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:14:31,140] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:14:49,740] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.45s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.00s/it]
[2024-03-31 18:15:02,995] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 599040 in 312 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_181541-0nnlpf6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-glitter-1
wandb: ⭐️ View project at https://wandb.ai/compyle/huggingface
wandb: 🚀 View run at https://wandb.ai/compyle/huggingface/runs/0nnlpf6q

  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/train.py", line 980, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 220, in forward
    return super().forward(
  File "/home/akane38/LLaVA/transformers/src/transformers/models/llama/modeling_llama.py", line 1215, in forward
    loss = loss_fct(shift_logits, shift_labels)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/functional.py", line 3053, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.39 GiB. GPU 0 has a total capacty of 79.15 GiB of which 3.36 GiB is free. Including non-PyTorch memory, this process has 75.78 GiB memory in use. Of the allocated memory 68.08 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.216 MB of 0.216 MB uploaded
wandb: \ 0.216 MB of 0.216 MB uploaded
wandb: | 0.216 MB of 0.216 MB uploaded
wandb: / 0.216 MB of 0.216 MB uploaded
wandb: - 0.428 MB of 0.449 MB uploaded
wandb: \ 0.449 MB of 0.449 MB uploaded
wandb: 🚀 View run hearty-glitter-1 at: https://wandb.ai/compyle/huggingface/runs/0nnlpf6q
wandb: ️⚡ View job at https://wandb.ai/compyle/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NDU4NzkzNA==/version_details/v0
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240331_181541-0nnlpf6q/logs
[2024-03-31 18:16:45,378] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2449472
[2024-03-31 18:16:45,378] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2449473
[2024-03-31 18:16:45,995] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:17:38 PM UTC 2024 =========================================================

[2024-03-31 18:17:42,540] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:17:46,790] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:17:46,790] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:17:49,707] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:17:52,167] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:17:52,167] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:17:52,167] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:17:52,167] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:17:52,167] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:17:58,194] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:17:58,198] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:17:59,219] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:17:59,219] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:17:59,351] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:18:17,833] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.04s/it]
[2024-03-31 18:18:31,132] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:18:34,561] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_181913-ygk9t06q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-salad-2
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/ygk9t06q

  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
UnboundLocalError: Caught UnboundLocalError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 871, in __getitem__
    for processor in multiple_image_processors:
UnboundLocalError: local variable 'multiple_image_processors' referenced before assignment

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
UnboundLocalError: Caught UnboundLocalError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 871, in __getitem__
    for processor in multiple_image_processors:
UnboundLocalError: local variable 'multiple_image_processors' referenced before assignment

[2024-03-31 18:19:26,268] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2466321
[2024-03-31 18:19:29,111] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2466322
[2024-03-31 18:19:29,111] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:21:22 PM UTC 2024 =========================================================

[2024-03-31 18:21:24,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:21:28,628] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:21:28,629] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:21:31,494] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:21:33,927] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:21:33,927] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:21:33,927] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:21:33,927] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:21:33,927] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:21:40,114] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:21:40,135] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:21:41,220] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:21:41,226] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:21:41,226] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:22:00,272] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.83s/it]
[2024-03-31 18:22:12,919] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:22:16,122] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_182255-azhqzl20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-cloud-3
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/azhqzl20

  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 912, in __call__
    num_encoders = len(instances["image"][0])
TypeError: list indices must be integers or slices, not str

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 384, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 912, in __call__
    num_encoders = len(instances["image"][0])
TypeError: list indices must be integers or slices, not str

[2024-03-31 18:23:09,032] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2483235
[2024-03-31 18:23:11,206] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2483236
[2024-03-31 18:23:11,206] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:24:08 PM UTC 2024 =========================================================

[2024-03-31 18:24:11,931] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:24:16,251] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:24:16,251] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:24:19,171] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:24:21,605] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:24:21,605] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:24:21,605] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:24:21,605] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:24:21,605] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:24:27,817] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:24:28,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:24:28,918] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:24:29,133] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:24:29,133] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:24:47,805] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.47s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.35s/it]
[2024-03-31 18:25:01,335] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:25:04,840] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_182546-0odpcpbt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-dream-4
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/0odpcpbt

  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 135, in prepare_inputs_labels_for_multimodal
    vision_tower = self.get_vision_tower()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MultiVELlavaLlamaForCausalLM' object has no attribute 'get_vision_tower'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 135, in prepare_inputs_labels_for_multimodal
    vision_tower = self.get_vision_tower()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MultiVELlavaLlamaForCausalLM' object has no attribute 'get_vision_tower'. Did you mean: '_get_vision_tower'?
[2024-03-31 18:26:02,712] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2496256
[2024-03-31 18:26:03,298] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2496257
[2024-03-31 18:26:03,298] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:35:14 PM UTC 2024 =========================================================

[2024-03-31 18:35:17,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:35:21,410] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:35:21,411] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:35:24,407] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:35:26,548] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:35:26,548] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:35:26,548] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:35:26,548] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:35:26,548] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:35:29,630] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:35:29,681] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:35:30,662] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:35:30,662] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:35:30,795] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:35:49,149] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]
[2024-03-31 18:36:02,166] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:36:05,654] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_183641-bqyv408r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-aardvark-5
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/bqyv408r

  0%|          | 0/5 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 199, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1145, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 199, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
[2024-03-31 18:36:57,645] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2540757
[2024-03-31 18:37:00,232] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2540758
[2024-03-31 18:37:00,233] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:39:16 PM UTC 2024 =========================================================

[2024-03-31 18:39:20,492] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:39:24,868] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:39:24,869] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:39:27,750] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:39:30,194] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:39:30,194] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:39:30,194] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:39:30,194] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:39:30,194] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:39:36,084] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:39:36,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:39:37,121] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:39:37,123] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:39:37,201] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:39:56,005] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.78s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.10s/it]
[2024-03-31 18:40:09,508] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:40:12,763] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_184051-b6wwdjcd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-totem-6
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/b6wwdjcd
  0%|          | 0/5 [00:00<?, ?it/s][tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-9.8845e-02, -2.0103e-01, -2.1563e-01,  ..., -1.8644e-01,
          -1.8644e-01, -9.8845e-02],
         [-1.7184e-01,  5.1429e-01,  6.0188e-01,  ...,  4.7049e-01,
           4.4130e-01, -1.5724e-01],
         [-1.2804e-01,  1.5508e+00,  1.6968e+00,  ...,  1.4340e+00,
           1.3172e+00, -1.4264e-01],
         ...,
         [-8.4247e-02,  8.7925e-01,  1.0252e+00,  ..., -4.6381e-01,
          -3.7622e-01, -1.7184e-01],
         [-1.4264e-01, -1.5724e-01, -1.2804e-01,  ..., -1.7184e-01,
          -1.4264e-01, -1.5724e-01],
         [-1.1255e-02, -2.5853e-02, -2.5853e-02,  ..., -1.1255e-02,
          -2.5853e-02, -1.1255e-02]],

        [[-5.6219e-02, -1.4627e-01, -1.7628e-01,  ..., -1.3126e-01,
          -1.3126e-01, -5.6219e-02],
         [-5.6219e-02,  6.3414e-01,  7.2418e-01,  ...,  6.1913e-01,
           5.7411e-01, -7.1227e-02],
         [-4.1212e-02,  1.6847e+00,  1.8348e+00,  ...,  1.6547e+00,
           1.4746e+00, -2.6204e-02],
         ...,
         [-1.0124e-01,  6.1913e-01,  6.3414e-01,  ..., -5.2146e-01,
          -3.8639e-01, -1.0124e-01],
         [-1.0124e-01, -5.6219e-02, -1.0124e-01,  ..., -1.0124e-01,
          -8.6235e-02, -8.6235e-02],
         [-1.1196e-02, -2.6204e-02, -2.6204e-02,  ..., -1.1196e-02,
          -1.1196e-02, -1.1196e-02]],

        [[ 5.5547e-02, -5.8213e-02, -8.6653e-02,  ..., -5.8213e-02,
          -4.3993e-02, -1.3329e-03],
         [ 1.4087e-01,  8.2343e-01,  9.0875e-01,  ...,  8.5187e-01,
           8.3765e-01,  9.8208e-02],
         [ 1.2665e-01,  1.8615e+00,  2.0606e+00,  ...,  1.9610e+00,
           1.7904e+00,  1.4087e-01],
         ...,
         [ 6.9767e-02,  3.2573e-01,  2.8307e-01,  ..., -3.8527e-01,
          -2.8573e-01,  6.9767e-02],
         [ 1.5509e-01,  5.5547e-02,  1.2665e-01,  ...,  6.9767e-02,
           6.9767e-02,  5.5547e-02],
         [-1.3329e-03, -1.3329e-03, -1.3329e-03,  ...,  1.2887e-02,
           1.2887e-02, -1.3329e-03]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-1.1075, -0.8335, -0.6452,  ..., -1.8097, -1.8610, -1.8953],
         [-0.6109, -0.1314, -0.1999,  ..., -1.7925, -1.9809, -1.3644],
         [ 0.0227, -0.4397, -0.5253,  ..., -1.8439, -1.9295, -1.6727],
         ...,
         [ 1.4269,  1.9920,  2.0777,  ..., -0.2684, -0.4054, -0.3198],
         [-1.3473, -1.0390, -0.3712,  ...,  1.8722,  1.5125,  0.8961],
         [-1.0219, -0.9705,  0.5364,  ...,  2.1804,  2.1975,  2.1462]],

        [[-0.5476,  0.2227,  0.5903,  ..., -1.5455, -1.5980, -1.6331],
         [-0.5826,  0.3277,  0.0476,  ..., -1.5630, -1.7206, -1.0378],
         [-0.5651, -0.6702, -0.6176,  ..., -1.6681, -1.7031, -1.3880],
         ...,
         [ 0.5553,  1.1506,  1.2906,  ..., -0.8803, -1.2304, -1.3004],
         [-1.6681, -1.5455, -1.1954,  ...,  1.1155,  0.6254, -0.0924],
         [-1.2654, -1.2829, -0.2325,  ...,  1.6408,  1.6057,  1.5182]],

        [[-0.8110, -0.4973, -0.2184,  ..., -1.5604, -1.5953, -1.5604],
         [-0.9504, -0.1661, -0.0790,  ..., -1.4559, -1.6302, -1.1421],
         [-0.8284, -0.8284, -0.7064,  ..., -1.4907, -1.6302, -1.3861],
         ...,
         [ 0.1825,  0.7054,  0.8797,  ..., -1.0027, -1.3687, -1.4384],
         [-1.5430, -1.5779, -1.4210,  ...,  0.8274,  0.3393, -0.3753],
         [-1.2816, -1.1596, -0.1835,  ...,  1.3851,  1.3154,  1.2282]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])][tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]

[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-1.7923, -1.7923, -1.7923,  ..., -0.6974, -0.6098, -0.7850],
         [-1.7923, -1.7923, -1.7923,  ..., -0.6682, -0.6828, -0.8726],
         [-1.7923, -1.7923, -1.7923,  ..., -0.5806, -0.5952, -0.7850],
         ...,
         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],
         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],
         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],

        [[-1.7521, -1.7521, -1.7521,  ..., -0.0712, -0.1463, -0.3264],
         [-1.7521, -1.7521, -1.7521,  ..., -0.1163, -0.1463, -0.3114],
         [-1.7521, -1.7521, -1.7521,  ..., -0.1463, -0.1613, -0.3414],
         ...,
         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],
         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],
         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],

        [[-1.4802, -1.4802, -1.4802,  ...,  0.3257,  0.3257,  0.1409],
         [-1.4802, -1.4802, -1.4802,  ...,  0.3257,  0.2831,  0.1409],
         [-1.4802, -1.4802, -1.4802,  ...,  0.2262,  0.2404,  0.0698],
         ...,
         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],
         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],
         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.7120, -0.7996, -0.6974,  ..., -1.0623, -1.0623, -1.0623],
         [-0.4054,  0.2515,  1.0398,  ..., -1.0477, -1.1061, -1.0769],
         [-0.9456, -1.1061,  0.8938,  ..., -1.0769, -1.0477, -1.0477],
         ...,
         [ 0.2369,  0.3391,  0.1055,  ...,  0.2223,  0.3537,  0.1201],
         [ 0.2515,  0.0325,  0.0179,  ..., -0.0113,  0.0909,  0.0763],
         [ 0.5289,  0.7187,  0.4705,  ..., -0.1280,  0.0471,  0.4851]],

        [[-0.5065, -0.6265, -0.5515,  ..., -1.3769, -1.4219, -1.4219],
         [-0.2663,  0.3940,  1.1894,  ..., -1.3769, -1.4219, -1.3919],
         [-0.9417, -1.0617,  1.0393,  ..., -1.4519, -1.4369, -1.4219],
         ...,
         [ 0.0789,  0.1689, -0.0712,  ...,  0.0338,  0.1839, -0.0562],
         [ 0.1239, -0.1463, -0.1613,  ..., -0.1463, -0.0712, -0.1163],
         [ 0.4090,  0.5741,  0.3190,  ..., -0.2513, -0.1163,  0.3040]],

        [[-0.6412, -0.6697, -0.4848,  ...,  0.4821,  0.4679,  0.4537],
         [-0.2715,  0.3968,  1.2500,  ...,  0.4537,  0.4395,  0.4679],
         [-0.7408, -0.8403,  1.2074,  ...,  0.5248,  0.5106,  0.5106],
         ...,
         [ 0.2404,  0.3115,  0.0698,  ...,  0.1977,  0.3399,  0.0982],
         [ 0.2546, -0.0156, -0.0582,  ...,  0.0129,  0.0413, -0.0156],
         [ 0.5248,  0.6670,  0.3826,  ..., -0.1151, -0.0440,  0.3257]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-2.1179, -2.1179, -2.1179,  ...,  0.1597,  0.0741,  0.1768],
         [-2.1179, -2.1179, -2.1179,  ...,  0.0056,  0.0398,  0.1768],
         [-2.1179, -2.1179, -2.1179,  ...,  0.0398,  0.0569,  0.2111],
         ...,
         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],
         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],
         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],

        [[-2.0357, -2.0357, -2.0357,  ...,  0.8004,  0.8179,  0.8880],
         [-2.0357, -2.0357, -2.0357,  ...,  0.8004,  0.8179,  0.9405],
         [-2.0357, -2.0357, -2.0357,  ...,  0.7479,  0.8004,  0.9930],
         ...,
         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],
         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],
         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],

        [[-1.8044, -1.8044, -1.8044,  ...,  1.5768,  1.5942,  1.6291],
         [-1.8044, -1.8044, -1.8044,  ...,  1.5420,  1.5768,  1.6640],
         [-1.8044, -1.8044, -1.8044,  ...,  1.5594,  1.5768,  1.7860],
         ...,
         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[ 1.5125,  1.4098,  0.5193,  ..., -0.7137, -0.8164, -0.7479],
         [ 1.3927,  0.9817,  1.0844,  ..., -0.6794, -0.6452, -0.2856],
         [ 1.4783,  1.0844,  1.2899,  ..., -0.9363, -0.7822, -0.7822],
         ...,
         [ 0.5193,  0.0227, -0.0458,  ...,  0.0056,  0.6563,  0.4166],
         [-0.1314, -0.1486,  0.2624,  ...,  0.0056,  0.0227,  0.2624],
         [ 0.0398,  0.6734,  0.3481,  ...,  0.7591,  0.4508,  0.0056]],

        [[ 1.6232,  1.6232,  0.7829,  ..., -0.7577, -0.8803, -0.8277],
         [ 1.6408,  1.3256,  1.4657,  ..., -0.7052, -0.6877, -0.3550],
         [ 1.7633,  1.4307,  1.6583,  ..., -1.0028, -0.8452, -0.8803],
         ...,
         [ 0.3803, -0.1099, -0.1450,  ..., -0.1800,  0.4853,  0.2402],
         [-0.2850, -0.2850,  0.1176,  ..., -0.1975, -0.1800,  0.1001],
         [-0.1450,  0.5203,  0.1527,  ...,  0.5553,  0.2577, -0.1275]],

        [[ 2.0648,  2.0997,  1.2457,  ..., -0.6018, -0.7587, -0.6715],
         [ 2.0823,  1.7860,  1.9080,  ..., -0.5670, -0.5495, -0.1487],
         [ 2.2217,  1.9080,  2.1171,  ..., -0.8633, -0.7064, -0.6541],
         ...,
         [ 0.4788, -0.0092, -0.0615,  ..., -0.1138,  0.5834,  0.3219],
         [-0.1661, -0.1835,  0.2173,  ..., -0.0790, -0.0615,  0.1651],
         [ 0.0082,  0.6356,  0.2696,  ...,  0.7228,  0.4091, -0.0441]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],
         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],
         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],
         ...,
         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],
         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],
         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303]],

        [[2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],
         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],
         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],
         ...,
         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],
         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],
         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749]],

        [[2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],
         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],
         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],
         ...,
         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],
         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],
         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],
         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],
         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],
         ...,
         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],
         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],
         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],

        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],
         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],
         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],
         ...,
         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],
         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],
         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],

        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],
         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],
         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],
         ...,
         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],
         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],
         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
[tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]), tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         ...,
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],
         [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],

        [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         ...,
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],
         [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],

        [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         ...,
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],
         [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])]
[tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]), tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         ...,
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],
         [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],

        [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         ...,
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],
         [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],

        [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         ...,
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],
         [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]])]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1146, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1146, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 18:41:08,301] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2557743
[2024-03-31 18:41:10,951] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2557744
[2024-03-31 18:41:10,951] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:42:20 PM UTC 2024 =========================================================

[2024-03-31 18:42:22,697] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:42:27,168] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:42:27,168] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:42:29,956] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:42:34,464] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:42:34,464] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:42:34,464] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:42:34,464] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:42:34,464] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:42:38,695] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:42:38,710] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:42:39,793] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:42:39,982] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:42:39,982] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:42:58,944] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
[2024-03-31 18:43:12,062] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:43:15,513] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_184354-u9cdl768
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-field-7
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/u9cdl768
  0%|          | 0/5 [00:00<?, ?it/s]enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0enc_idx=0

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 18:44:10,569] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2573387
[2024-03-31 18:44:13,345] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2573388
[2024-03-31 18:44:13,346] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:46:39 PM UTC 2024 =========================================================

[2024-03-31 18:46:44,286] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:46:48,681] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:46:48,681] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 1 --dataloader_prefetch_factor 0 --lazy_preprocess True
[2024-03-31 18:46:51,634] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:46:54,077] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:46:54,077] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:46:54,078] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:46:54,078] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:46:54,078] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:47:00,361] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:47:00,364] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:47:01,659] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:47:01,659] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:47:01,688] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:47:20,379] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.94s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.79s/it]
[2024-03-31 18:47:34,432] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:47:37,148] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_184819-0doxrcm8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-dream-8
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/0doxrcm8
  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 381, in __iter__
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1850, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py", line 381, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
    dataloader_iter = super().__iter__()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 996, in __init__
    assert self._prefetch_factor > 0
AssertionError
    return self._get_iterator()
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 996, in __init__
    assert self._prefetch_factor > 0
AssertionError
[2024-03-31 18:48:29,174] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2590332
[2024-03-31 18:48:31,061] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2590333
[2024-03-31 18:48:31,062] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '1', '--dataloader_prefetch_factor', '0', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:48:32 PM UTC 2024 =========================================================

[2024-03-31 18:48:34,993] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:48:39,577] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:48:39,577] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 1 --dataloader_prefetch_factor 1 --lazy_preprocess True
[2024-03-31 18:48:42,002] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:48:45,300] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:48:45,300] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:48:45,301] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:48:45,301] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:48:45,301] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:48:50,683] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:48:50,821] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:48:51,719] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:48:51,719] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:48:52,092] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:49:10,517] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.78s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.89s/it]
[2024-03-31 18:49:23,296] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:49:26,466] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_185005-r1nw4yrp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-shape-9
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/r1nw4yrp
  0%|          | 0/5 [00:00<?, ?it/s]enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 18:50:21,403] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2598866
[2024-03-31 18:50:22,016] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2598867
[2024-03-31 18:50:22,016] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '1', '--dataloader_prefetch_factor', '1', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:55:59 PM UTC 2024 =========================================================

[2024-03-31 18:56:02,548] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:56:06,982] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:56:06,983] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:56:09,800] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:56:12,226] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:56:12,226] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:56:12,226] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:56:12,226] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:56:12,226] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:56:18,411] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:56:18,469] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:56:19,660] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:56:19,661] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:56:19,661] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:56:38,734] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.96s/it]
[2024-03-31 18:56:52,015] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:56:55,024] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_185734-r15zerxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-pyramid-10
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/r15zerxo
  0%|          | 0/5 [00:00<?, ?it/s]enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1enc_idx=1

enc_idx=1image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])


image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1',
       dtype=torch.bfloat16)]
[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)]
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 156, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 156, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 18:57:50,331] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2630342
[2024-03-31 18:57:53,275] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2630343
[2024-03-31 18:57:53,276] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 10:58:26 PM UTC 2024 =========================================================

[2024-03-31 18:58:31,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:58:35,917] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 18:58:35,918] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 18:58:38,851] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:58:41,294] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 18:58:41,294] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 18:58:41,294] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 18:58:41,294] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 18:58:41,294] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 18:58:44,606] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:58:45,888] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 18:58:45,888] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 18:58:47,344] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 18:58:48,383] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 18:59:07,445] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.95s/it]
[2024-03-31 18:59:20,793] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 18:59:24,277] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_190003-hwdja4bv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-armadillo-11
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/hwdja4bv
  0%|          | 0/5 [00:00<?, ?it/s]enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
enc_idx=0image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])enc_idx=1

image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=0
enc_idx=0image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1enc_idx=1enc_idx=1enc_idx=1



image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])


enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0enc_idx=0

image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])image.shape=torch.Size([3, 336, 336])

image.shape=torch.Size([3, 336, 336])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=1enc_idx=1

image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])image.shape=torch.Size([3, 224, 224])

image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
enc_idx=0
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
image.shape=torch.Size([3, 336, 336])
enc_idx=1
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
image.shape=torch.Size([3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 158, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 158, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 19:00:19,397] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2642535
[2024-03-31 19:00:22,125] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2642536
[2024-03-31 19:00:22,125] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 11:01:03 PM UTC 2024 =========================================================

[2024-03-31 19:01:05,343] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:01:09,578] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 19:01:09,578] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 19:01:12,336] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:01:14,810] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 19:01:14,810] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 19:01:14,810] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 19:01:14,810] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 19:01:14,810] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 19:01:21,201] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:01:21,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:01:22,325] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:01:22,378] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:01:22,379] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 19:01:40,454] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.96s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
[2024-03-31 19:01:53,819] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 19:01:57,282] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_190237-ppfv3qcj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-moon-12
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/ppfv3qcj
  0%|          | 0/5 [00:00<?, ?it/s]torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 158, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 158, in prepare_inputs_labels_for_multimodal
    raise ValueError()
ValueError
[2024-03-31 19:02:52,912] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2652378
[2024-03-31 19:02:53,473] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2652379
[2024-03-31 19:02:53,473] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1
[2024-03-31 19:15:00,352] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:15:04,609] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 19:15:04,610] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 19:15:06,889] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:15:09,314] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 19:15:09,314] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 19:15:09,315] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 19:15:09,315] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 19:15:09,315] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 19:15:15,617] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:15:15,665] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:15:16,794] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:15:16,794] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 19:15:16,818] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 19:15:35,747] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
[2024-03-31 19:15:49,027] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 599040 in 312 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_191628-6l963dn6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-paper-2
wandb: ⭐️ View project at https://wandb.ai/compyle/huggingface
wandb: 🚀 View run at https://wandb.ai/compyle/huggingface/runs/6l963dn6
  0%|          | 0/5 [00:00<?, ?it/s]In else branch
In else branch
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
In else branch
In else branch
[2024-03-31 19:17:04,439] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2710997
[2024-03-31 19:17:04,439] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2710998
[2024-03-31 19:17:05,050] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

================================================= Sun Mar 31 11:19:31 PM UTC 2024 =========================================================

[2024-03-31 19:19:35,362] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:19:39,690] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 19:19:39,690] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 19:19:42,659] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:19:44,790] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 19:19:44,790] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 19:19:44,790] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 19:19:44,790] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 19:19:44,790] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 19:19:48,135] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:19:48,305] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:19:49,149] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:19:49,339] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:19:49,340] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 19:20:07,320] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.96s/it]
[2024-03-31 19:20:20,598] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 19:20:24,055] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_192102-jhu09f9a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-universe-13
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/jhu09f9a
  0%|          | 0/5 [00:00<?, ?it/s]torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 214, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 214, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
[2024-03-31 19:21:18,890] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2728885
[2024-03-31 19:21:19,485] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2728886
[2024-03-31 19:21:19,485] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Sun Mar 31 11:24:22 PM UTC 2024 =========================================================

[2024-03-31 19:24:26,717] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:24:31,050] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 19:24:31,051] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 19:24:34,023] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:24:36,470] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 19:24:36,470] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 19:24:36,470] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 19:24:36,470] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 19:24:36,470] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 19:24:42,574] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:24:42,624] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 19:24:43,683] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 19:24:43,683] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 19:24:43,687] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 19:25:02,590] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]
[2024-03-31 19:25:15,981] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 19:25:19,271] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_192557-yqtxhlcm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-resonance-14
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/yqtxhlcm
  0%|          | 0/5 [00:00<?, ?it/s]torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
in else branch
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
in else branch
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 216, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 216, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 131, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
[2024-03-31 19:26:13,574] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2749058
[2024-03-31 19:26:14,180] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2749059
[2024-03-31 19:26:14,181] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 02:36:57 AM UTC 2024 =========================================================

[2024-03-31 22:37:00,137] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:37:04,144] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 22:37:04,144] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 22:37:07,033] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:37:09,450] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 22:37:09,451] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 22:37:09,451] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 22:37:09,451] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 22:37:09,451] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 22:37:15,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:37:15,641] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:37:16,810] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 22:37:16,810] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-31 22:37:16,831] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 22:37:35,544] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]
[2024-03-31 22:37:49,106] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 22:37:52,552] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_223834-qi9bmulu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-water-15
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/qi9bmulu
  0%|          | 0/5 [00:00<?, ?it/s]torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
torch.Size([8, 3, 336, 336])
torch.Size([8, 3, 224, 224])
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 218, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 133, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 218, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 133, in encode_images
    image_features = ve(images)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_encoder/clip_encoder.py", line 50, in forward
    image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 917, in forward
    return self.vision_model(
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 841, in forward
    hidden_states = self.embeddings(pixel_values)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/transformers/src/transformers/models/clip/modeling_clip.py", line 182, in forward
    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 8, 3, 336, 336]
[2024-03-31 22:38:50,561] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3500566
[2024-03-31 22:38:51,009] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3500567
[2024-03-31 22:38:51,009] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 02:55:32 AM UTC 2024 =========================================================

[2024-03-31 22:55:35,202] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:55:39,657] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 22:55:39,657] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 22:55:41,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:55:43,905] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 22:55:43,905] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 22:55:43,905] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 22:55:43,905] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 22:55:43,905] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 22:55:47,120] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:55:47,143] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:55:48,139] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 22:55:48,200] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 22:55:48,200] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 22:56:06,413] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.84s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.01s/it]
[2024-03-31 22:56:19,833] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 22:56:23,269] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_225701-1rt7pmlb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-microwave-16
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/1rt7pmlb
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 216, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 134, in encode_images
    image_features = torch.cat(image_features, ve(images[idx]), dim=-2)
TypeError: cat() received an invalid combination of arguments - got (Tensor, Tensor, dim=int), but expected one of:
 * (tuple of Tensors tensors, int dim, *, Tensor out)
 * (tuple of Tensors tensors, name dim, *, Tensor out)

Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 216, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 134, in encode_images
    image_features = torch.cat(image_features, ve(images[idx]), dim=-2)
TypeError: cat() received an invalid combination of arguments - got (Tensor, Tensor, dim=int), but expected one of:
 * (tuple of Tensors tensors, int dim, *, Tensor out)
 * (tuple of Tensors tensors, name dim, *, Tensor out)

[2024-03-31 22:57:18,005] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3570066
[2024-03-31 22:57:18,639] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3570067
[2024-03-31 22:57:18,639] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 02:57:37 AM UTC 2024 =========================================================

[2024-03-31 22:57:40,166] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:57:44,718] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-03-31 22:57:44,718] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-03-31 22:57:47,199] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:57:51,229] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-03-31 22:57:51,229] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-03-31 22:57:51,229] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-03-31 22:57:51,229] [INFO] [launch.py:163:main] dist_world_size=2
[2024-03-31 22:57:51,230] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-03-31 22:57:55,553] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:57:55,589] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 22:57:56,658] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 22:57:56,954] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 22:57:56,954] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-03-31 22:58:14,761] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.87s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]
[2024-03-31 22:58:28,012] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-03-31 22:58:31,362] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240331_225910-re3p6u59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-wind-17
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/re3p6u59
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
[2024-03-31 22:59:43,347] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3579432
[2024-03-31 22:59:43,347] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3579433
[2024-03-31 22:59:46,376] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

================================================= Mon Apr  1 05:54:00 AM UTC 2024 =========================================================

[2024-04-01 01:54:02,574] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:54:07,273] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 01:54:07,274] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 01:54:09,509] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:54:11,842] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 01:54:11,842] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 01:54:11,842] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 01:54:11,842] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 01:54:11,842] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 01:54:17,740] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:54:17,781] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:54:19,002] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 01:54:19,003] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 01:54:19,012] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 01:54:37,551] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.17s/it]
[2024-04-01 01:54:50,847] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 01:54:54,286] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1069, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 96, in initialize_vision_modules
    self.mm_projector = build_vision_projector(self.config)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/builder.py", line 44, in build_vision_projector
    modules = Resampler()
TypeError: Resampler.__init__() missing 2 required positional arguments: 'grid_size' and 'embed_dim'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1069, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 96, in initialize_vision_modules
    self.mm_projector = build_vision_projector(self.config)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/builder.py", line 44, in build_vision_projector
    modules = Resampler()
TypeError: Resampler.__init__() missing 2 required positional arguments: 'grid_size' and 'embed_dim'
[2024-04-01 01:54:58,892] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 49589
[2024-04-01 01:54:58,892] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 49590
[2024-04-01 01:54:58,944] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 05:56:23 AM UTC 2024 =========================================================

[2024-04-01 01:56:27,480] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:56:31,832] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 01:56:31,832] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 01:56:34,753] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:56:37,203] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 01:56:37,203] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 01:56:37,203] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 01:56:37,203] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 01:56:37,203] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 01:56:40,307] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:56:40,504] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 01:56:41,423] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 01:56:41,824] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 01:56:41,824] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 01:56:59,860] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.93s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]
[2024-04-01 01:57:12,842] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 01:57:16,007] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1069, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 96, in initialize_vision_modules
    self.mm_projector = build_vision_projector(self.config)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/builder.py", line 44, in build_vision_projector
    modules = Resampler()
TypeError: Resampler.__init__() missing 2 required positional arguments: 'grid_size' and 'embed_dim'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1069, in train
    model.get_model().initialize_vision_modules(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 96, in initialize_vision_modules
    self.mm_projector = build_vision_projector(self.config)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/builder.py", line 44, in build_vision_projector
    modules = Resampler()
TypeError: Resampler.__init__() missing 2 required positional arguments: 'grid_size' and 'embed_dim'
[2024-04-01 01:57:20,250] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 61107
[2024-04-01 01:57:20,250] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 61108
[2024-04-01 01:57:20,303] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:04:41 AM UTC 2024 =========================================================

[2024-04-01 02:04:44,002] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:04:48,247] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:04:48,248] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:04:50,784] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:04:54,707] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:04:54,707] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:04:54,707] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:04:54,707] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:04:54,707] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:04:59,126] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:04:59,152] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:05:00,223] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:05:00,467] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:05:00,467] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:05:19,275] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
[2024-04-01 02:05:32,484] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:05:35,920] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_020621-6s9feggq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-vortex-18
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/6s9feggq
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 832, 1024])torch.Size([8, 832, 1024])

torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
 20%|██        | 1/5 [00:45<03:03, 45.87s/it]                                             {'loss': 0.3125, 'learning_rate': 2e-05, 'epoch': 0.0}
 20%|██        | 1/5 [00:45<03:03, 45.87s/it]torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
 40%|████      | 2/5 [01:00<01:22, 27.40s/it]                                             {'loss': 1.4362, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.0}
 40%|████      | 2/5 [01:00<01:22, 27.40s/it]torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
 60%|██████    | 3/5 [01:14<00:43, 21.56s/it]                                             {'loss': 1.2231, 'learning_rate': 1e-05, 'epoch': 0.0}
 60%|██████    | 3/5 [01:14<00:43, 21.56s/it]torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
 80%|████████  | 4/5 [01:32<00:20, 20.01s/it]                                             {'loss': 1.1481, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.0}
 80%|████████  | 4/5 [01:32<00:20, 20.01s/it]torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
torch.Size([8, 832, 1024])
100%|██████████| 5/5 [01:47<00:00, 18.12s/it]                                             {'loss': 1.172, 'learning_rate': 0.0, 'epoch': 0.0}
100%|██████████| 5/5 [01:47<00:00, 18.12s/it]                                             {'train_runtime': 112.9211, 'train_samples_per_second': 2.834, 'train_steps_per_second': 0.044, 'train_loss': 1.058371365070343, 'epoch': 0.0}
100%|██████████| 5/5 [01:47<00:00, 18.12s/it]100%|██████████| 5/5 [01:47<00:00, 21.53s/it]
[2024-04-01 02:08:27,937] [INFO] [launch.py:347:main] Process 95294 exits successfully.
wandb: - 1.054 MB of 1.054 MB uploadedwandb: \ 1.054 MB of 1.054 MB uploadedwandb: | 1.054 MB of 1.054 MB uploadedwandb: / 1.054 MB of 1.054 MB uploadedwandb: - 2.105 MB of 2.124 MB uploadedwandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▁▁
wandb:              train/global_step ▁▃▅▆██
wandb:            train/learning_rate █▇▅▂▁
wandb:                     train/loss ▁█▇▆▆
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.0
wandb:              train/global_step 5
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.172
wandb:               train/total_flos 6132268335104.0
wandb:               train/train_loss 1.05837
wandb:            train/train_runtime 112.9211
wandb: train/train_samples_per_second 2.834
wandb:   train/train_steps_per_second 0.044
wandb: 
wandb: 🚀 View run generous-vortex-18 at: https://wandb.ai/compyle/multi-ve-llava/runs/6s9feggq
wandb: ️⚡ View job at https://wandb.ai/compyle/multi-ve-llava/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NDcyNDExNg==/version_details/v0
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240401_020621-6s9feggq/logs
[2024-04-01 02:09:09,979] [INFO] [launch.py:347:main] Process 95293 exits successfully.

================================================= Mon Apr  1 06:11:50 AM UTC 2024 =========================================================

[2024-04-01 02:11:54,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:11:59,075] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:11:59,075] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:12:02,003] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:12:04,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:12:04,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:12:04,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:12:04,452] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:12:04,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:12:10,657] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:12:10,670] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:12:11,789] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:12:11,789] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 02:12:11,792] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:12:29,579] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.90s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.10s/it]
[2024-04-01 02:12:42,871] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:12:46,364] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_021329-6gv88jb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-rain-19
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/6gv88jb3
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 229, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 148, in encode_images
    print(image_features.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 229, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 148, in encode_images
    print(image_features.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
[2024-04-01 02:13:45,560] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 120477
[2024-04-01 02:13:47,735] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 120478
[2024-04-01 02:13:47,735] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:16:46 AM UTC 2024 =========================================================

[2024-04-01 02:16:51,527] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:16:55,918] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:16:55,918] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:16:58,755] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:17:01,217] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:17:01,217] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:17:01,217] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:17:01,217] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:17:01,217] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:17:07,599] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:17:07,750] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:17:08,741] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:17:09,084] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:17:09,084] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:17:30,649] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.01s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.42s/it]
[2024-04-01 02:17:44,013] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:17:47,254] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=20979712
frozen=0
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 972800 in 605 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_021826-eu8g2cyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-water-20
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/eu8g2cyr
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 233, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 152, in encode_images
    print(image_features.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 233, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 152, in encode_images
    print(image_features.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
[2024-04-01 02:18:42,323] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 143860
[2024-04-01 02:18:44,938] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 143861
[2024-04-01 02:18:44,938] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:25:17 AM UTC 2024 =========================================================

[2024-04-01 02:25:19,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:25:26,428] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:25:26,428] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:25:29,259] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:25:31,711] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:25:31,711] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:25:31,711] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:25:31,711] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:25:31,711] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:25:34,947] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:25:36,111] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:25:37,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:25:38,778] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:25:38,778] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:25:56,348] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.94s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]
[2024-04-01 02:26:12,972] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:26:20,029] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_022659-l3mzokc4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-spaceship-21
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/l3mzokc4
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 228, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 140, in encode_images
    image_features = resampler(_image_features)
UnboundLocalError: local variable 'resampler' referenced before assignment
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 228, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 140, in encode_images
    image_features = resampler(_image_features)
UnboundLocalError: local variable 'resampler' referenced before assignment
[2024-04-01 02:27:18,826] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 186203
[2024-04-01 02:27:21,187] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 186204
[2024-04-01 02:27:21,187] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:27:39 AM UTC 2024 =========================================================

[2024-04-01 02:27:42,575] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:27:47,023] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:27:47,023] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:27:49,816] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:27:52,289] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:27:52,289] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:27:52,289] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:27:52,289] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:27:52,289] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:27:58,487] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:27:58,612] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:27:59,717] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:27:59,890] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:27:59,891] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:28:18,799] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]
[2024-04-01 02:28:31,753] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:28:34,894] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_022917-ozdp8v1y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-totem-22
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/ozdp8v1y
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
 20%|██        | 1/5 [00:47<03:08, 47.18s/it]                                             {'loss': 0.3125, 'learning_rate': 2e-05, 'epoch': 0.0}
 20%|██        | 1/5 [00:47<03:08, 47.18s/it]torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])

================================================= Mon Apr  1 06:30:12 AM UTC 2024 =========================================================

torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
[2024-04-01 02:30:14,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:15,681] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:30:15,681] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
[2024-04-01 02:30:17,697] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:19,958] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:30:19,958] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:30:19,958] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:30:19,958] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:30:19,958] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
[2024-04-01 02:30:22,445] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 200180
[2024-04-01 02:30:22,445] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 200181
[2024-04-01 02:30:24,892] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15
[2024-04-01 02:30:26,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:26,421] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:27,603] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:30:27,608] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:30:27,608] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 949, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 949, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/akane38/LLaVA/transformers/src/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 137, in __init__
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/home/akane38/LLaVA/transformers/src/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/akane38/LLaVA/transformers/src/transformers/training_args.py", line 1871, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: Connection reset by peer
[2024-04-01 02:30:28,969] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 212347
[2024-04-01 02:30:28,970] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 212348
[2024-04-01 02:30:29,015] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:30:37 AM UTC 2024 =========================================================

[2024-04-01 02:30:39,777] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:46,744] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:30:46,744] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:30:49,672] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:52,115] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:30:52,115] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:30:52,115] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:30:52,115] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:30:52,115] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:30:58,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:58,100] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:30:59,190] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:30:59,202] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:30:59,202] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:31:17,808] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.92s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
[2024-04-01 02:31:31,176] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:31:34,618] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_023213-6cyba0mx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-oath-23
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/6cyba0mx
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 228, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 140, in encode_images
    image_features = resampler(_image_features)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/resampler.py", line 170, in forward
    x + pos_embed.unsqueeze(1),
RuntimeError: The size of tensor a (576) must match the size of tensor b (256) at non-singleton dimension 0
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 228, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 140, in encode_images
    image_features = resampler(_image_features)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/resampler.py", line 170, in forward
    x + pos_embed.unsqueeze(1),
RuntimeError: The size of tensor a (576) must match the size of tensor b (256) at non-singleton dimension 0
wandb: - 1.177 MB of 1.177 MB uploaded[2024-04-01 02:32:33,223] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 213336
[2024-04-01 02:32:35,988] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 213337
[2024-04-01 02:32:35,988] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = 1

================================================= Mon Apr  1 06:33:37 AM UTC 2024 =========================================================

[2024-04-01 02:33:40,541] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:33:44,943] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:33:44,944] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:33:47,789] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:33:50,227] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:33:50,227] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:33:50,227] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:33:50,227] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:33:50,227] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:33:53,424] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:33:53,498] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:33:54,525] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:33:54,526] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 02:33:54,821] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:34:12,223] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.09s/it]
[2024-04-01 02:34:28,988] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:34:32,468] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_023515-wwllqslk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sun-24
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/wwllqslk
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
 20%|██        | 1/5 [00:46<03:06, 46.72s/it]                                             {'loss': 0.3125, 'learning_rate': 2e-05, 'epoch': 0.0}
 20%|██        | 1/5 [00:46<03:06, 46.72s/it]torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
[2024-04-01 02:36:14,382] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 226202
[2024-04-01 02:36:14,382] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 226203
[2024-04-01 02:36:15,708] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

================================================= Mon Apr  1 06:36:16 AM UTC 2024 =========================================================

[2024-04-01 02:36:18,640] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:36:20,330] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:36:20,331] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:36:22,974] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:36:27,826] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:36:27,826] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:36:27,826] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:36:27,826] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:36:27,826] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:36:31,123] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:36:31,160] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:36:32,286] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:36:32,287] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 02:36:32,310] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:36:50,169] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.13s/it]
[2024-04-01 02:37:03,538] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:37:06,999] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26822656
frozen=589824
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_023752-x18651zf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-glade-25
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/x18651zf
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
torch.Size([8, 1152, 1024])
[2024-04-01 02:38:32,962] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 238820
[2024-04-01 02:38:32,963] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 238821
[2024-04-01 02:38:36,119] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

================================================= Mon Apr  1 06:38:37 AM UTC 2024 =========================================================

[2024-04-01 02:38:39,343] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:38:43,520] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:38:43,520] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --max_steps 5 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2024-04-01 02:38:46,484] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:38:48,949] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:38:48,949] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:38:48,949] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:38:48,949] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:38:48,949] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:38:55,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:38:55,369] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:38:56,428] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:38:56,428] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 02:38:56,652] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:39:15,344] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  4.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]
[2024-04-01 02:39:27,284] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:39:30,592] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_024010-y00677sv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-elevator-26
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/y00677sv
  0%|          | 0/5 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.Size([8, 128, 1024])torch.Size([8, 128, 1024])

[2024-04-01 02:40:33,060] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 248513
[2024-04-01 02:40:33,060] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 248514
[2024-04-01 02:40:36,277] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--max_steps', '5', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -15

================================================= Mon Apr  1 06:48:39 AM UTC 2024 =========================================================

[2024-04-01 02:48:41,066] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:48:43,279] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:48:43,280] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-shared-resampler-clip-dino-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it
[2024-04-01 02:48:45,263] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:48:46,854] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:48:46,854] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:48:46,854] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:48:46,854] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:48:46,854] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:48:50,112] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:48:50,235] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:48:51,394] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:48:51,395] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-01 02:48:51,408] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:48:58,116] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  4.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]
[2024-04-01 02:49:09,799] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:49:12,201] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_024945-x1gh22rp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/x1gh22rp
  0%|          | 0/10395 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 225, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in encode_images
    image_features = self.get_model().resampler(image_features)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/resampler.py", line 170, in forward
    x + pos_embed.unsqueeze(1),
RuntimeError: The size of tensor a (832) must match the size of tensor b (784) at non-singleton dimension 0
Traceback (most recent call last):
  File "/home/akane38/LLaVA/llava/train/multi_ve_train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/akane38/LLaVA/llava/train/multi_ve_train.py", line 1148, in train
    trainer.train()
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 1883, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2786, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/akane38/LLaVA/transformers/src/transformers/trainer.py", line 2809, in compute_loss
    outputs = model(**inputs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1833, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/language_model/llava_llama.py", line 82, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 225, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images)
  File "/home/akane38/LLaVA/llava/model/llava_arch.py", line 144, in encode_images
    image_features = self.get_model().resampler(image_features)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/akane38/LLaVA/llava/model/multimodal_projector/resampler.py", line 170, in forward
    x + pos_embed.unsqueeze(1),
RuntimeError: The size of tensor a (832) must match the size of tensor b (784) at non-singleton dimension 0
wandb: - 1.262 MB of 1.262 MB uploadedwandb: \ 1.262 MB of 1.262 MB uploadedwandb: | 1.262 MB of 1.262 MB uploadedwandb: / 1.262 MB of 1.262 MB uploaded[2024-04-01 02:49:58,932] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 298671
wandb: - 2.519 MB of 2.539 MB uploaded (0.002 MB deduped)[2024-04-01 02:49:59,523] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 298672
[2024-04-01 02:49:59,523] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-shared-resampler-clip-dino-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it'] exits with return code = 1

================================================= Mon Apr  1 06:50:52 AM UTC 2024 =========================================================

[2024-04-01 02:50:54,671] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:50:56,785] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=6,7: setting --include=localhost:6,7
[2024-04-01 02:50:56,786] [INFO] [runner.py:573:main] cmd = /home/akane38/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/multi_ve_train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json --image_folder /data/data1/akane/LLaVA/data --multiple_vision_towers openai/clip-vit-large-patch14-336 facebook/dinov2-large --pretrain_mm_mlp_adapter /data/data1/akane/pretrained/mm_projector_7b.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data/data0/akane/multi-ve-shared-resampler-clip-dino-llava-pretrained-v1.5-7b/checkpoints --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it
[2024-04-01 02:50:58,767] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:51:00,354] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2024-04-01 02:51:00,354] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-04-01 02:51:00,354] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-04-01 02:51:00,354] [INFO] [launch.py:163:main] dist_world_size=2
[2024-04-01 02:51:00,354] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2024-04-01 02:51:03,657] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:51:03,668] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-01 02:51:04,798] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:51:04,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-01 02:51:04,923] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-04-01 02:51:14,086] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]
[2024-04-01 02:51:25,595] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 682, num_elems = 7.04B
[2024-04-01 02:51:28,339] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1121, num_elems = 7.35B
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
MultiVELlavaLlamaForCausalLM(
  (model): MultiVELlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (multiple_vision_towers): ModuleList(
      (0): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DINOVisionTower(
        (vision_tower): Dinov2Model(
          (embeddings): Dinov2Embeddings(
            (patch_embeddings): Dinov2PatchEmbeddings(
              (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (encoder): Dinov2Encoder(
            (layer): ModuleList(
              (0-23): 24 x Dinov2Layer(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attention): Dinov2Attention(
                  (attention): Dinov2SelfAttention(
                    (query): Linear(in_features=1024, out_features=1024, bias=True)
                    (key): Linear(in_features=1024, out_features=1024, bias=True)
                    (value): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): Dinov2SelfOutput(
                    (dense): Linear(in_features=1024, out_features=1024, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (layer_scale1): Dinov2LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Dinov2MLP(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (activation): GELUActivation()
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_scale2): Dinov2LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (resampler): Resampler(
      (kv_proj): Identity()
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
      )
      (ln_q): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_kv): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (ln_post): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
trainable=26298368
frozen=65536
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/akane38/LLaVA/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Parameter Offload: Total persistent parameters: 983040 in 613 params
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/akane38/LLaVA/wandb/run-20240401_025204-q1lxfaec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it
wandb: ⭐️ View project at https://wandb.ai/compyle/multi-ve-llava
wandb: 🚀 View run at https://wandb.ai/compyle/multi-ve-llava/runs/q1lxfaec
  0%|          | 0/10395 [00:00<?, ?it/s]/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/akane38/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/10395 [00:28<82:18:35, 28.51s/it]                                                    {'loss': 0.3125, 'learning_rate': 6.41025641025641e-08, 'epoch': 0.0}
  0%|          | 1/10395 [00:28<82:18:35, 28.51s/it][2024-04-01 02:52:40,464] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 310481
[2024-04-01 02:52:40,464] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 310482
[2024-04-01 02:52:41,188] [ERROR] [launch.py:321:sigkill_handler] ['/home/akane38/miniconda3/envs/llava/bin/python', '-u', 'llava/train/multi_ve_train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/data1/akane/LLaVA/data/llava_v1_5_mix665k.json', '--image_folder', '/data/data1/akane/LLaVA/data', '--multiple_vision_towers', 'openai/clip-vit-large-patch14-336', 'facebook/dinov2-large', '--pretrain_mm_mlp_adapter', '/data/data1/akane/pretrained/mm_projector_7b.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/data/data0/akane/multi-ve-shared-resampler-clip-dino-llava-pretrained-v1.5-7b/checkpoints', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'multi-ve-shared-resampler-clip-dino-llava-pretrained-7b-it'] exits with return code = -15
